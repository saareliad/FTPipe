"""AutoGenerated with:
python partition_gpt2_models.py --analysis_batch_size 1 --async_pipeline --auto_file_name --block_size -1 --bwd_to_fwd_ratio 3 --lmhead --model_name_or_path gpt2-xl --model_type gpt2 --n_iter 50 --n_partitions 8 --output_file results/gpt2xl_tied_p8/ --overwrite_cache --partitioning_batch_size 1 --seed 42 --stateless_tied --train_data_file wikitext-2-raw/wiki.train.raw
"""
import torch
import torch.functional
import torch.nn.functional
import math
from torch import Tensor
import torch.nn as nn
from itertools import chain
from typing import Optional, Tuple, Iterator, Iterable, OrderedDict, Dict
import collections
import os
from models.normal.NLP_models.modeling_gpt2_tied_weights import Gelu
from models.normal.NLP_models.stateless import StatelessLinear
from transformers.modeling_utils import Conv1D
from torch.nn.modules.sparse import Embedding
from models.normal.NLP_models.stateless import StatelessEmbedding
from models.normal.NLP_models.modeling_gpt2_tied_weights import LMOutput
from torch.nn.modules.normalization import LayerNorm
from torch.nn.modules.dropout import Dropout
# this is an auto generated file do not edit unless you know what you are doing


# partition adjacency
# model inputs {0, 8}
# partition 0 {'inputs': {'input0'}, 'outputs': {8, 1}}
# partition 1 {'inputs': {0}, 'outputs': {2}}
# partition 2 {'inputs': {1}, 'outputs': {3}}
# partition 3 {'inputs': {2}, 'outputs': {4}}
# partition 4 {'inputs': {3}, 'outputs': {5}}
# partition 5 {'inputs': {4}, 'outputs': {6}}
# partition 6 {'inputs': {5}, 'outputs': {7}}
# partition 7 {'inputs': {6}, 'outputs': {8}}
# partition 8 {'inputs': {'input1', 0, 7}, 'outputs': {'output'}}
# model outputs {8}


def create_pipeline_configuration(DEBUG=False):
    depth = 10000
    basic_blocks = (Gelu,StatelessLinear,Conv1D,Embedding,StatelessEmbedding,LMOutput,LayerNorm,Dropout)
    blocks_path = [ 'models.normal.NLP_models.modeling_gpt2_tied_weights.Gelu',
            'models.normal.NLP_models.stateless.StatelessLinear',
            'transformers.modeling_utils.Conv1D',
            'torch.nn.modules.sparse.Embedding',
            'models.normal.NLP_models.stateless.StatelessEmbedding',
            'models.normal.NLP_models.modeling_gpt2_tied_weights.LMOutput',
            'torch.nn.modules.normalization.LayerNorm',
            'torch.nn.modules.dropout.Dropout']
    module_path = os.path.relpath(__file__).replace("/",".")[:-3]
    

    # creating configuration
    stages = {0: {"inputs": {'input0': {'shape': torch.Size([1, 1024]), 'dtype': 'torch.int64', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/Parameter[w_wte]': {'shape': torch.Size([50257, 1600]), 'dtype': 'torch.float32', 'is_batched': False}, 'GPT2LMHeadModel/GPT2Model[transformer]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}}},
            1: {"inputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2]': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}}},
            2: {"inputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2]': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj]': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}}},
            3: {"inputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj]': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}}},
            4: {"inputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2]': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}}},
            5: {"inputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2]': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}}},
            6: {"inputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}}},
            7: {"inputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Tensor::permute': {'shape': torch.Size([1, 1024, 25, 64]), 'dtype': 'torch.float32', 'is_batched': True}}},
            8: {"inputs": {'input1': {'shape': torch.Size([1, 1024]), 'dtype': 'torch.int64', 'is_batched': True}, 'GPT2LMHeadModel/Parameter[w_wte]': {'shape': torch.Size([50257, 1600]), 'dtype': 'torch.float32', 'is_batched': False}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Tensor::__add__': {'shape': torch.Size([1, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Tensor::permute': {'shape': torch.Size([1, 1024, 25, 64]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/LMOutput[compute_output]': {'shape': torch.Size([1]), 'dtype': 'torch.float32', 'is_batched': False}}}
            }
    

    stages[0]['stage_cls'] = module_path + '.Partition0'
    device = 'cpu' if DEBUG else 'cuda:0'
    stages[0]['devices'] = [device]
    

    stages[1]['stage_cls'] = module_path + '.Partition1'
    device = 'cpu' if DEBUG else 'cuda:1'
    stages[1]['devices'] = [device]
    

    stages[2]['stage_cls'] = module_path + '.Partition2'
    device = 'cpu' if DEBUG else 'cuda:2'
    stages[2]['devices'] = [device]
    

    stages[3]['stage_cls'] = module_path + '.Partition3'
    device = 'cpu' if DEBUG else 'cuda:3'
    stages[3]['devices'] = [device]
    

    stages[4]['stage_cls'] = module_path + '.Partition4'
    device = 'cpu' if DEBUG else 'cuda:4'
    stages[4]['devices'] = [device]
    

    stages[5]['stage_cls'] = module_path + '.Partition5'
    device = 'cpu' if DEBUG else 'cuda:5'
    stages[5]['devices'] = [device]
    

    stages[6]['stage_cls'] = module_path + '.Partition6'
    device = 'cpu' if DEBUG else 'cuda:6'
    stages[6]['devices'] = [device]
    

    stages[7]['stage_cls'] = module_path + '.Partition7'
    device = 'cpu' if DEBUG else 'cuda:7'
    stages[7]['devices'] = [device]
    

    stages[8]['stage_cls'] = module_path + '.Partition8'
    device = 'cpu' if DEBUG else 'cuda:0'
    stages[8]['devices'] = [device]
    

    config = dict()
    config['batch_dim'] = 0
    config['depth'] = depth
    config['basic_blocks'] = blocks_path
    config['model_inputs'] = {'input0': {"shape": torch.Size([1, 1024]),
        "dtype": 'torch.int64',
        "is_batched": True},
            'input1': {"shape": torch.Size([1, 1024]),
        "dtype": 'torch.int64',
        "is_batched": True}}
    config['model_outputs'] = {'GPT2LMHeadModel/LMOutput[compute_output]': {"shape": torch.Size([1]),
        "dtype": 'torch.float32',
        "is_batched": False}}
    config['stages'] = stages
    
    return config

class Partition0(nn.Module):
    BASIC_BLOCKS=(
            Embedding,
            StatelessEmbedding,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/StatelessEmbedding[stateless_wte]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe]',
        ]
    TENSORS=[
            'GPT2LMHeadModel/Parameter[w_wte]',
        ]
    def __init__(self, layers, tensors):
        super(Partition0, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:0')
        self.lookup = { 'l_0': 'transformer.stateless_wte',
                        'l_1': 'transformer.wpe',
                        'p_0': 'w_wte'}

    def forward(self, x0):
        # GPT2LMHeadModel/GPT2Model[transformer]/StatelessEmbedding[stateless_wte] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe] <=> self.l_1
        # GPT2LMHeadModel/Parameter[w_wte] <=> self.p_0
        # input0 <=> x0

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)

        t_0 = x0.size()
        t_0 = t_0[-1]
        t_0 = x0.view(-1, t_0)
        del x0
        t_1 = t_0.size(-1)
        t_2 = t_0.device
        t_2 = torch.arange(t_1, dtype=torch.int64, device=t_2)
        del t_1
        t_2 = t_2.unsqueeze(0)
        t_2 = t_2.expand_as(t_0)
        t_0 = self.l_0(self.p_0, t_0)
        t_2 = self.l_1(t_2)
        t_2 = t_0 + t_2
        del t_0
        # returning:
        # GPT2LMHeadModel/Parameter[w_wte]
        # GPT2LMHeadModel/GPT2Model[transformer]/Tensor::__add__
        return (self.p_0, t_2)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition1(nn.Module):
    BASIC_BLOCKS=(
            Conv1D,
            Dropout,
            Gelu,
            LayerNorm,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2]',
        ]
    TENSORS=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Tensor[bias]',
        ]
    def __init__(self, layers, tensors):
        super(Partition1, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:1')
        self.lookup = { 'l_0': 'transformer.drop',
                        'l_1': 'transformer.blocks.0.ln_1',
                        'l_2': 'transformer.blocks.0.attn.c_attn',
                        'l_3': 'transformer.blocks.0.attn.attn_dropout',
                        'l_4': 'transformer.blocks.0.attn.c_proj',
                        'l_5': 'transformer.blocks.0.attn.resid_dropout',
                        'l_6': 'transformer.blocks.0.ln_2',
                        'l_7': 'transformer.blocks.0.mlp.c_fc',
                        'l_8': 'transformer.blocks.0.mlp.act',
                        'l_9': 'transformer.blocks.0.mlp.c_proj',
                        'l_10': 'transformer.blocks.0.mlp.dropout',
                        'l_11': 'transformer.blocks.1.ln_1',
                        'l_12': 'transformer.blocks.1.attn.c_attn',
                        'l_13': 'transformer.blocks.1.attn.attn_dropout',
                        'l_14': 'transformer.blocks.1.attn.c_proj',
                        'l_15': 'transformer.blocks.1.attn.resid_dropout',
                        'l_16': 'transformer.blocks.1.ln_2',
                        'l_17': 'transformer.blocks.1.mlp.c_fc',
                        'l_18': 'transformer.blocks.1.mlp.act',
                        'l_19': 'transformer.blocks.1.mlp.c_proj',
                        'l_20': 'transformer.blocks.1.mlp.dropout',
                        'l_21': 'transformer.blocks.2.ln_1',
                        'l_22': 'transformer.blocks.2.attn.c_attn',
                        'l_23': 'transformer.blocks.2.attn.attn_dropout',
                        'l_24': 'transformer.blocks.2.attn.c_proj',
                        'l_25': 'transformer.blocks.2.attn.resid_dropout',
                        'l_26': 'transformer.blocks.2.ln_2',
                        'l_27': 'transformer.blocks.2.mlp.c_fc',
                        'l_28': 'transformer.blocks.2.mlp.act',
                        'l_29': 'transformer.blocks.2.mlp.c_proj',
                        'l_30': 'transformer.blocks.2.mlp.dropout',
                        'l_31': 'transformer.blocks.3.ln_1',
                        'l_32': 'transformer.blocks.3.attn.c_attn',
                        'l_33': 'transformer.blocks.3.attn.attn_dropout',
                        'l_34': 'transformer.blocks.3.attn.c_proj',
                        'l_35': 'transformer.blocks.3.attn.resid_dropout',
                        'l_36': 'transformer.blocks.3.ln_2',
                        'l_37': 'transformer.blocks.3.mlp.c_fc',
                        'l_38': 'transformer.blocks.3.mlp.act',
                        'l_39': 'transformer.blocks.3.mlp.c_proj',
                        'l_40': 'transformer.blocks.3.mlp.dropout',
                        'l_41': 'transformer.blocks.4.ln_1',
                        'l_42': 'transformer.blocks.4.attn.c_attn',
                        'l_43': 'transformer.blocks.4.attn.attn_dropout',
                        'l_44': 'transformer.blocks.4.attn.c_proj',
                        'l_45': 'transformer.blocks.4.attn.resid_dropout',
                        'l_46': 'transformer.blocks.4.ln_2',
                        'l_47': 'transformer.blocks.4.mlp.c_fc',
                        'l_48': 'transformer.blocks.4.mlp.act',
                        'l_49': 'transformer.blocks.4.mlp.c_proj',
                        'l_50': 'transformer.blocks.4.mlp.dropout',
                        'l_51': 'transformer.blocks.5.ln_1',
                        'l_52': 'transformer.blocks.5.attn.c_attn',
                        'l_53': 'transformer.blocks.5.attn.attn_dropout',
                        'l_54': 'transformer.blocks.5.attn.c_proj',
                        'l_55': 'transformer.blocks.5.attn.resid_dropout',
                        'l_56': 'transformer.blocks.5.ln_2',
                        'l_57': 'transformer.blocks.5.mlp.c_fc',
                        'l_58': 'transformer.blocks.5.mlp.act',
                        'l_59': 'transformer.blocks.5.mlp.c_proj',
                        'l_60': 'transformer.blocks.5.mlp.dropout',
                        'l_61': 'transformer.blocks.6.ln_1',
                        'l_62': 'transformer.blocks.6.attn.c_attn',
                        'l_63': 'transformer.blocks.6.attn.attn_dropout',
                        'l_64': 'transformer.blocks.6.attn.c_proj',
                        'l_65': 'transformer.blocks.6.attn.resid_dropout',
                        'l_66': 'transformer.blocks.6.ln_2',
                        'b_0': 'transformer.blocks.0.attn.bias',
                        'b_1': 'transformer.blocks.1.attn.bias',
                        'b_2': 'transformer.blocks.2.attn.bias',
                        'b_3': 'transformer.blocks.3.attn.bias',
                        'b_4': 'transformer.blocks.4.attn.bias',
                        'b_5': 'transformer.blocks.5.attn.bias',
                        'b_6': 'transformer.blocks.6.attn.bias'}

    def forward(self, x0):
        # GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_attn] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[attn_dropout] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_proj] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[resid_dropout] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Gelu[act] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_attn] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[attn_dropout] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_proj] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[resid_dropout] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Gelu[act] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_attn] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[attn_dropout] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_proj] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[resid_dropout] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Gelu[act] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_attn] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[attn_dropout] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_proj] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[resid_dropout] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Gelu[act] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_attn] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[attn_dropout] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_proj] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[resid_dropout] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Gelu[act] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_attn] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[attn_dropout] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_proj] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[resid_dropout] <=> self.l_55
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2] <=> self.l_56
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc] <=> self.l_57
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Gelu[act] <=> self.l_58
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj] <=> self.l_59
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout] <=> self.l_60
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1] <=> self.l_61
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_attn] <=> self.l_62
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[attn_dropout] <=> self.l_63
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_proj] <=> self.l_64
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[resid_dropout] <=> self.l_65
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2] <=> self.l_66
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Tensor[bias] <=> self.b_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Tensor::__add__ <=> x0

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)

        t_0 = self.l_0(x0)
        del x0
        t_1 = self.l_1(t_0)
        t_1 = self.l_2(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_2 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_7 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_7, t_4)
        del t_7
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_3.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_2 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_3.view(t_6, t_5, t_2, t_7)
        del t_2
        del t_5
        del t_6
        del t_3
        t_7 = t_7.permute(0, 2, 3, 1)
        t_2 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_1.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_1.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_1
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_4, t_7)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_7 / t_4
        del t_7
        t_7 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_7 = t_3 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_7, t_3)
        del t_7
        del t_5
        del t_6
        t_3 = self.b_0[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_3(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_7 = t_2.size(-1)
        t_7 = t_3 * t_7
        del t_3
        t_7 = (t_7,)
        t_7 = t_4 + t_7
        del t_4
        t_4 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_2.view(t_4, t_3, t_7)
        del t_3
        del t_4
        del t_2
        t_7 = self.l_4(t_7)
        t_7 = self.l_5(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_6(t_7)
        t_0 = self.l_7(t_0)
        t_0 = self.l_8(t_0)
        t_0 = self.l_9(t_0)
        t_0 = self.l_10(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_11(t_0)
        t_7 = self.l_12(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_3 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_2 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_3.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_1 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_5, t_6, t_1, t_2)
        del t_1
        del t_6
        del t_5
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_1 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_4.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_3 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_4.view(t_6, t_5, t_3, t_1)
        del t_3
        del t_5
        del t_6
        del t_4
        t_1 = t_1.permute(0, 2, 3, 1)
        t_3 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_7.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_7.view(t_5, t_6, t_4, t_3)
        del t_4
        del t_6
        del t_5
        del t_7
        t_3 = t_3.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_2, t_1)
        del t_2
        t_2 = t_3.size(-1)
        t_2 = math.sqrt(t_2)
        t_2 = t_1 / t_2
        del t_1
        t_1 = t_2.size(-2)
        t_4 = t_2.size(-1)
        t_1 = t_4 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_4, None)
        t_4 = slice(None, t_4, None)
        t_4 = (t_6, t_5, t_1, t_4)
        del t_1
        del t_5
        del t_6
        t_4 = self.b_1[t_4]
        t_2 = t_2 * t_4
        t_4 = 1 - t_4
        t_4 = 10000.0 * t_4
        t_4 = t_2 - t_4
        del t_2
        t_4 = torch.softmax(t_4, dim=-1)
        t_4 = self.l_13(t_4)
        t_3 = torch.matmul(t_4, t_3)
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_4 = t_3.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        del t_4
        t_4 = t_3.size(-2)
        t_1 = t_3.size(-1)
        t_1 = t_4 * t_1
        del t_4
        t_1 = (t_1,)
        t_1 = t_2 + t_1
        del t_2
        t_2 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_3.view(t_2, t_4, t_1)
        del t_4
        del t_2
        del t_3
        t_1 = self.l_14(t_1)
        t_1 = self.l_15(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_16(t_1)
        t_0 = self.l_17(t_0)
        t_0 = self.l_18(t_0)
        t_0 = self.l_19(t_0)
        t_0 = self.l_20(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_21(t_0)
        t_1 = self.l_22(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_4 = t_1[0]
        t_2 = t_1[1]
        t_1 = t_1[2]
        t_3 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_4.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_7 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_5, t_6, t_7, t_3)
        del t_7
        del t_6
        del t_5
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_7 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_2.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_4 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_2.view(t_6, t_5, t_4, t_7)
        del t_4
        del t_5
        del t_6
        del t_2
        t_7 = t_7.permute(0, 2, 3, 1)
        t_4 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_1.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_1.view(t_5, t_6, t_2, t_4)
        del t_2
        del t_6
        del t_5
        del t_1
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_3, t_7)
        del t_3
        t_3 = t_4.size(-1)
        t_3 = math.sqrt(t_3)
        t_3 = t_7 / t_3
        del t_7
        t_7 = t_3.size(-2)
        t_2 = t_3.size(-1)
        t_7 = t_2 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_5, t_7, t_2)
        del t_7
        del t_5
        del t_6
        t_2 = self.b_2[t_2]
        t_3 = t_3 * t_2
        t_2 = 1 - t_2
        t_2 = 10000.0 * t_2
        t_2 = t_3 - t_2
        del t_3
        t_2 = torch.softmax(t_2, dim=-1)
        t_2 = self.l_23(t_2)
        t_4 = torch.matmul(t_2, t_4)
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_3 = slice(None, -2, None)
        t_3 = t_2[t_3]
        del t_2
        t_2 = t_4.size(-2)
        t_7 = t_4.size(-1)
        t_7 = t_2 * t_7
        del t_2
        t_7 = (t_7,)
        t_7 = t_3 + t_7
        del t_3
        t_3 = t_7[0]
        t_2 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_4.view(t_3, t_2, t_7)
        del t_2
        del t_3
        del t_4
        t_7 = self.l_24(t_7)
        t_7 = self.l_25(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_26(t_7)
        t_0 = self.l_27(t_0)
        t_0 = self.l_28(t_0)
        t_0 = self.l_29(t_0)
        t_0 = self.l_30(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_31(t_0)
        t_7 = self.l_32(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_2 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_1 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_1, t_4)
        del t_1
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_1 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_3.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_2 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_3.view(t_6, t_5, t_2, t_1)
        del t_2
        del t_5
        del t_6
        del t_3
        t_1 = t_1.permute(0, 2, 3, 1)
        t_2 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_7.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_7.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_7
        t_2 = t_2.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_4, t_1)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_1 / t_4
        del t_1
        t_1 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_1 = t_3 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_1, t_3)
        del t_1
        del t_5
        del t_6
        t_3 = self.b_3[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_33(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_1 = t_2.size(-1)
        t_1 = t_3 * t_1
        del t_3
        t_1 = (t_1,)
        t_1 = t_4 + t_1
        del t_4
        t_4 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_2.view(t_4, t_3, t_1)
        del t_3
        del t_4
        del t_2
        t_1 = self.l_34(t_1)
        t_1 = self.l_35(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_36(t_1)
        t_0 = self.l_37(t_0)
        t_0 = self.l_38(t_0)
        t_0 = self.l_39(t_0)
        t_0 = self.l_40(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_41(t_0)
        t_1 = self.l_42(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_3 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        t_2 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_3.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_7 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_5, t_6, t_7, t_2)
        del t_7
        del t_6
        del t_5
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_4.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_3 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_4.view(t_6, t_5, t_3, t_7)
        del t_3
        del t_5
        del t_6
        del t_4
        t_7 = t_7.permute(0, 2, 3, 1)
        t_3 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_1.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_1.view(t_5, t_6, t_4, t_3)
        del t_4
        del t_6
        del t_5
        del t_1
        t_3 = t_3.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_2, t_7)
        del t_2
        t_2 = t_3.size(-1)
        t_2 = math.sqrt(t_2)
        t_2 = t_7 / t_2
        del t_7
        t_7 = t_2.size(-2)
        t_4 = t_2.size(-1)
        t_7 = t_4 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_4, None)
        t_4 = slice(None, t_4, None)
        t_4 = (t_6, t_5, t_7, t_4)
        del t_7
        del t_5
        del t_6
        t_4 = self.b_4[t_4]
        t_2 = t_2 * t_4
        t_4 = 1 - t_4
        t_4 = 10000.0 * t_4
        t_4 = t_2 - t_4
        del t_2
        t_4 = torch.softmax(t_4, dim=-1)
        t_4 = self.l_43(t_4)
        t_3 = torch.matmul(t_4, t_3)
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_4 = t_3.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        del t_4
        t_4 = t_3.size(-2)
        t_7 = t_3.size(-1)
        t_7 = t_4 * t_7
        del t_4
        t_7 = (t_7,)
        t_7 = t_2 + t_7
        del t_2
        t_2 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_3.view(t_2, t_4, t_7)
        del t_4
        del t_2
        del t_3
        t_7 = self.l_44(t_7)
        t_7 = self.l_45(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_46(t_7)
        t_0 = self.l_47(t_0)
        t_0 = self.l_48(t_0)
        t_0 = self.l_49(t_0)
        t_0 = self.l_50(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_51(t_0)
        t_7 = self.l_52(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_4 = t_7[0]
        t_2 = t_7[1]
        t_7 = t_7[2]
        t_3 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_4.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_1 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_5, t_6, t_1, t_3)
        del t_1
        del t_6
        del t_5
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_1 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_2.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_4 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_2.view(t_6, t_5, t_4, t_1)
        del t_4
        del t_5
        del t_6
        del t_2
        t_1 = t_1.permute(0, 2, 3, 1)
        t_4 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_7.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_7.view(t_5, t_6, t_2, t_4)
        del t_2
        del t_6
        del t_5
        del t_7
        t_4 = t_4.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_3, t_1)
        del t_3
        t_3 = t_4.size(-1)
        t_3 = math.sqrt(t_3)
        t_3 = t_1 / t_3
        del t_1
        t_1 = t_3.size(-2)
        t_2 = t_3.size(-1)
        t_1 = t_2 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_5, t_1, t_2)
        del t_1
        del t_5
        del t_6
        t_2 = self.b_5[t_2]
        t_3 = t_3 * t_2
        t_2 = 1 - t_2
        t_2 = 10000.0 * t_2
        t_2 = t_3 - t_2
        del t_3
        t_2 = torch.softmax(t_2, dim=-1)
        t_2 = self.l_53(t_2)
        t_4 = torch.matmul(t_2, t_4)
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_3 = slice(None, -2, None)
        t_3 = t_2[t_3]
        del t_2
        t_2 = t_4.size(-2)
        t_1 = t_4.size(-1)
        t_1 = t_2 * t_1
        del t_2
        t_1 = (t_1,)
        t_1 = t_3 + t_1
        del t_3
        t_3 = t_1[0]
        t_2 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_4.view(t_3, t_2, t_1)
        del t_2
        del t_3
        del t_4
        t_1 = self.l_54(t_1)
        t_1 = self.l_55(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_56(t_1)
        t_0 = self.l_57(t_0)
        t_0 = self.l_58(t_0)
        t_0 = self.l_59(t_0)
        t_0 = self.l_60(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_61(t_0)
        t_1 = self.l_62(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_2 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_7 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_7, t_4)
        del t_7
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_3.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_2 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_3.view(t_6, t_5, t_2, t_7)
        del t_2
        del t_5
        del t_6
        del t_3
        t_7 = t_7.permute(0, 2, 3, 1)
        t_2 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_1.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_1.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_1
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_4, t_7)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_7 / t_4
        del t_7
        t_7 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_7 = t_3 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_7, t_3)
        del t_7
        del t_5
        del t_6
        t_3 = self.b_6[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_63(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_7 = t_2.size(-1)
        t_7 = t_3 * t_7
        del t_3
        t_7 = (t_7,)
        t_7 = t_4 + t_7
        del t_4
        t_4 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_2.view(t_4, t_3, t_7)
        del t_3
        del t_4
        del t_2
        t_7 = self.l_64(t_7)
        t_7 = self.l_65(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_66(t_7)
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Tensor::__add__
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2]
        return (t_7, t_0)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition2(nn.Module):
    BASIC_BLOCKS=(
            Conv1D,
            Dropout,
            Gelu,
            LayerNorm,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj]',
        ]
    TENSORS=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Tensor[bias]',
        ]
    def __init__(self, layers, tensors):
        super(Partition2, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:2')
        self.lookup = { 'l_0': 'transformer.blocks.6.mlp.c_fc',
                        'l_1': 'transformer.blocks.6.mlp.act',
                        'l_2': 'transformer.blocks.6.mlp.c_proj',
                        'l_3': 'transformer.blocks.6.mlp.dropout',
                        'l_4': 'transformer.blocks.7.ln_1',
                        'l_5': 'transformer.blocks.7.attn.c_attn',
                        'l_6': 'transformer.blocks.7.attn.attn_dropout',
                        'l_7': 'transformer.blocks.7.attn.c_proj',
                        'l_8': 'transformer.blocks.7.attn.resid_dropout',
                        'l_9': 'transformer.blocks.7.ln_2',
                        'l_10': 'transformer.blocks.7.mlp.c_fc',
                        'l_11': 'transformer.blocks.7.mlp.act',
                        'l_12': 'transformer.blocks.7.mlp.c_proj',
                        'l_13': 'transformer.blocks.7.mlp.dropout',
                        'l_14': 'transformer.blocks.8.ln_1',
                        'l_15': 'transformer.blocks.8.attn.c_attn',
                        'l_16': 'transformer.blocks.8.attn.attn_dropout',
                        'l_17': 'transformer.blocks.8.attn.c_proj',
                        'l_18': 'transformer.blocks.8.attn.resid_dropout',
                        'l_19': 'transformer.blocks.8.ln_2',
                        'l_20': 'transformer.blocks.8.mlp.c_fc',
                        'l_21': 'transformer.blocks.8.mlp.act',
                        'l_22': 'transformer.blocks.8.mlp.c_proj',
                        'l_23': 'transformer.blocks.8.mlp.dropout',
                        'l_24': 'transformer.blocks.9.ln_1',
                        'l_25': 'transformer.blocks.9.attn.c_attn',
                        'l_26': 'transformer.blocks.9.attn.attn_dropout',
                        'l_27': 'transformer.blocks.9.attn.c_proj',
                        'l_28': 'transformer.blocks.9.attn.resid_dropout',
                        'l_29': 'transformer.blocks.9.ln_2',
                        'l_30': 'transformer.blocks.9.mlp.c_fc',
                        'l_31': 'transformer.blocks.9.mlp.act',
                        'l_32': 'transformer.blocks.9.mlp.c_proj',
                        'l_33': 'transformer.blocks.9.mlp.dropout',
                        'l_34': 'transformer.blocks.10.ln_1',
                        'l_35': 'transformer.blocks.10.attn.c_attn',
                        'l_36': 'transformer.blocks.10.attn.attn_dropout',
                        'l_37': 'transformer.blocks.10.attn.c_proj',
                        'l_38': 'transformer.blocks.10.attn.resid_dropout',
                        'l_39': 'transformer.blocks.10.ln_2',
                        'l_40': 'transformer.blocks.10.mlp.c_fc',
                        'l_41': 'transformer.blocks.10.mlp.act',
                        'l_42': 'transformer.blocks.10.mlp.c_proj',
                        'l_43': 'transformer.blocks.10.mlp.dropout',
                        'l_44': 'transformer.blocks.11.ln_1',
                        'l_45': 'transformer.blocks.11.attn.c_attn',
                        'l_46': 'transformer.blocks.11.attn.attn_dropout',
                        'l_47': 'transformer.blocks.11.attn.c_proj',
                        'l_48': 'transformer.blocks.11.attn.resid_dropout',
                        'l_49': 'transformer.blocks.11.ln_2',
                        'l_50': 'transformer.blocks.11.mlp.c_fc',
                        'l_51': 'transformer.blocks.11.mlp.act',
                        'l_52': 'transformer.blocks.11.mlp.c_proj',
                        'l_53': 'transformer.blocks.11.mlp.dropout',
                        'l_54': 'transformer.blocks.12.ln_1',
                        'l_55': 'transformer.blocks.12.attn.c_attn',
                        'l_56': 'transformer.blocks.12.attn.attn_dropout',
                        'l_57': 'transformer.blocks.12.attn.c_proj',
                        'l_58': 'transformer.blocks.12.attn.resid_dropout',
                        'l_59': 'transformer.blocks.12.ln_2',
                        'l_60': 'transformer.blocks.12.mlp.c_fc',
                        'l_61': 'transformer.blocks.12.mlp.act',
                        'l_62': 'transformer.blocks.12.mlp.c_proj',
                        'b_0': 'transformer.blocks.7.attn.bias',
                        'b_1': 'transformer.blocks.8.attn.bias',
                        'b_2': 'transformer.blocks.9.attn.bias',
                        'b_3': 'transformer.blocks.10.attn.bias',
                        'b_4': 'transformer.blocks.11.attn.bias',
                        'b_5': 'transformer.blocks.12.attn.bias'}

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Gelu[act] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_attn] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[attn_dropout] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_proj] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[resid_dropout] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Gelu[act] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_attn] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[attn_dropout] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_proj] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[resid_dropout] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Gelu[act] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_attn] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[attn_dropout] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_proj] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[resid_dropout] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Gelu[act] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_attn] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[attn_dropout] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_proj] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[resid_dropout] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Gelu[act] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_attn] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[attn_dropout] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_proj] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[resid_dropout] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Gelu[act] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_attn] <=> self.l_55
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[attn_dropout] <=> self.l_56
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_proj] <=> self.l_57
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[resid_dropout] <=> self.l_58
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2] <=> self.l_59
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc] <=> self.l_60
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Gelu[act] <=> self.l_61
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj] <=> self.l_62
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Tensor::__add__ <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2] <=> x1

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)

        t_0 = self.l_0(x1)
        del x1
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = x0 + t_0
        del x0
        t_1 = self.l_4(t_0)
        t_1 = self.l_5(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_2 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_7 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_7, t_4)
        del t_7
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_3.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_2 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_3.view(t_6, t_5, t_2, t_7)
        del t_2
        del t_5
        del t_6
        del t_3
        t_7 = t_7.permute(0, 2, 3, 1)
        t_2 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_1.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_1.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_1
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_4, t_7)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_7 / t_4
        del t_7
        t_7 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_7 = t_3 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_7, t_3)
        del t_7
        del t_5
        del t_6
        t_3 = self.b_0[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_6(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_7 = t_2.size(-1)
        t_7 = t_3 * t_7
        del t_3
        t_7 = (t_7,)
        t_7 = t_4 + t_7
        del t_4
        t_4 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_2.view(t_4, t_3, t_7)
        del t_3
        del t_4
        del t_2
        t_7 = self.l_7(t_7)
        t_7 = self.l_8(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_9(t_7)
        t_0 = self.l_10(t_0)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_14(t_0)
        t_7 = self.l_15(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_3 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_2 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_3.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_1 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_5, t_6, t_1, t_2)
        del t_1
        del t_6
        del t_5
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_1 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_4.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_3 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_4.view(t_6, t_5, t_3, t_1)
        del t_3
        del t_5
        del t_6
        del t_4
        t_1 = t_1.permute(0, 2, 3, 1)
        t_3 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_7.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_7.view(t_5, t_6, t_4, t_3)
        del t_4
        del t_6
        del t_5
        del t_7
        t_3 = t_3.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_2, t_1)
        del t_2
        t_2 = t_3.size(-1)
        t_2 = math.sqrt(t_2)
        t_2 = t_1 / t_2
        del t_1
        t_1 = t_2.size(-2)
        t_4 = t_2.size(-1)
        t_1 = t_4 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_4, None)
        t_4 = slice(None, t_4, None)
        t_4 = (t_6, t_5, t_1, t_4)
        del t_1
        del t_5
        del t_6
        t_4 = self.b_1[t_4]
        t_2 = t_2 * t_4
        t_4 = 1 - t_4
        t_4 = 10000.0 * t_4
        t_4 = t_2 - t_4
        del t_2
        t_4 = torch.softmax(t_4, dim=-1)
        t_4 = self.l_16(t_4)
        t_3 = torch.matmul(t_4, t_3)
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_4 = t_3.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        del t_4
        t_4 = t_3.size(-2)
        t_1 = t_3.size(-1)
        t_1 = t_4 * t_1
        del t_4
        t_1 = (t_1,)
        t_1 = t_2 + t_1
        del t_2
        t_2 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_3.view(t_2, t_4, t_1)
        del t_4
        del t_2
        del t_3
        t_1 = self.l_17(t_1)
        t_1 = self.l_18(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_19(t_1)
        t_0 = self.l_20(t_0)
        t_0 = self.l_21(t_0)
        t_0 = self.l_22(t_0)
        t_0 = self.l_23(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_24(t_0)
        t_1 = self.l_25(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_4 = t_1[0]
        t_2 = t_1[1]
        t_1 = t_1[2]
        t_3 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_4.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_7 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_5, t_6, t_7, t_3)
        del t_7
        del t_6
        del t_5
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_7 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_2.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_4 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_2.view(t_6, t_5, t_4, t_7)
        del t_4
        del t_5
        del t_6
        del t_2
        t_7 = t_7.permute(0, 2, 3, 1)
        t_4 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_1.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_1.view(t_5, t_6, t_2, t_4)
        del t_2
        del t_6
        del t_5
        del t_1
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_3, t_7)
        del t_3
        t_3 = t_4.size(-1)
        t_3 = math.sqrt(t_3)
        t_3 = t_7 / t_3
        del t_7
        t_7 = t_3.size(-2)
        t_2 = t_3.size(-1)
        t_7 = t_2 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_5, t_7, t_2)
        del t_7
        del t_5
        del t_6
        t_2 = self.b_2[t_2]
        t_3 = t_3 * t_2
        t_2 = 1 - t_2
        t_2 = 10000.0 * t_2
        t_2 = t_3 - t_2
        del t_3
        t_2 = torch.softmax(t_2, dim=-1)
        t_2 = self.l_26(t_2)
        t_4 = torch.matmul(t_2, t_4)
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_3 = slice(None, -2, None)
        t_3 = t_2[t_3]
        del t_2
        t_2 = t_4.size(-2)
        t_7 = t_4.size(-1)
        t_7 = t_2 * t_7
        del t_2
        t_7 = (t_7,)
        t_7 = t_3 + t_7
        del t_3
        t_3 = t_7[0]
        t_2 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_4.view(t_3, t_2, t_7)
        del t_2
        del t_3
        del t_4
        t_7 = self.l_27(t_7)
        t_7 = self.l_28(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_29(t_7)
        t_0 = self.l_30(t_0)
        t_0 = self.l_31(t_0)
        t_0 = self.l_32(t_0)
        t_0 = self.l_33(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_34(t_0)
        t_7 = self.l_35(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_2 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_1 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_1, t_4)
        del t_1
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_1 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_3.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_2 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_3.view(t_6, t_5, t_2, t_1)
        del t_2
        del t_5
        del t_6
        del t_3
        t_1 = t_1.permute(0, 2, 3, 1)
        t_2 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_7.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_7.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_7
        t_2 = t_2.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_4, t_1)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_1 / t_4
        del t_1
        t_1 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_1 = t_3 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_1, t_3)
        del t_1
        del t_5
        del t_6
        t_3 = self.b_3[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_36(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_1 = t_2.size(-1)
        t_1 = t_3 * t_1
        del t_3
        t_1 = (t_1,)
        t_1 = t_4 + t_1
        del t_4
        t_4 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_2.view(t_4, t_3, t_1)
        del t_3
        del t_4
        del t_2
        t_1 = self.l_37(t_1)
        t_1 = self.l_38(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_39(t_1)
        t_0 = self.l_40(t_0)
        t_0 = self.l_41(t_0)
        t_0 = self.l_42(t_0)
        t_0 = self.l_43(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_44(t_0)
        t_1 = self.l_45(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_3 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        t_2 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_3.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_7 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_5, t_6, t_7, t_2)
        del t_7
        del t_6
        del t_5
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_4.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_3 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_4.view(t_6, t_5, t_3, t_7)
        del t_3
        del t_5
        del t_6
        del t_4
        t_7 = t_7.permute(0, 2, 3, 1)
        t_3 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_1.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_1.view(t_5, t_6, t_4, t_3)
        del t_4
        del t_6
        del t_5
        del t_1
        t_3 = t_3.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_2, t_7)
        del t_2
        t_2 = t_3.size(-1)
        t_2 = math.sqrt(t_2)
        t_2 = t_7 / t_2
        del t_7
        t_7 = t_2.size(-2)
        t_4 = t_2.size(-1)
        t_7 = t_4 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_4, None)
        t_4 = slice(None, t_4, None)
        t_4 = (t_6, t_5, t_7, t_4)
        del t_7
        del t_5
        del t_6
        t_4 = self.b_4[t_4]
        t_2 = t_2 * t_4
        t_4 = 1 - t_4
        t_4 = 10000.0 * t_4
        t_4 = t_2 - t_4
        del t_2
        t_4 = torch.softmax(t_4, dim=-1)
        t_4 = self.l_46(t_4)
        t_3 = torch.matmul(t_4, t_3)
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_4 = t_3.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        del t_4
        t_4 = t_3.size(-2)
        t_7 = t_3.size(-1)
        t_7 = t_4 * t_7
        del t_4
        t_7 = (t_7,)
        t_7 = t_2 + t_7
        del t_2
        t_2 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_3.view(t_2, t_4, t_7)
        del t_4
        del t_2
        del t_3
        t_7 = self.l_47(t_7)
        t_7 = self.l_48(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_49(t_7)
        t_0 = self.l_50(t_0)
        t_0 = self.l_51(t_0)
        t_0 = self.l_52(t_0)
        t_0 = self.l_53(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_54(t_0)
        t_7 = self.l_55(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_4 = t_7[0]
        t_2 = t_7[1]
        t_7 = t_7[2]
        t_3 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_4.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_1 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_5, t_6, t_1, t_3)
        del t_1
        del t_6
        del t_5
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_1 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_2.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_4 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_2.view(t_6, t_5, t_4, t_1)
        del t_4
        del t_5
        del t_6
        del t_2
        t_1 = t_1.permute(0, 2, 3, 1)
        t_4 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_7.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_7.view(t_5, t_6, t_2, t_4)
        del t_2
        del t_6
        del t_5
        del t_7
        t_4 = t_4.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_3, t_1)
        del t_3
        t_3 = t_4.size(-1)
        t_3 = math.sqrt(t_3)
        t_3 = t_1 / t_3
        del t_1
        t_1 = t_3.size(-2)
        t_2 = t_3.size(-1)
        t_1 = t_2 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_5, t_1, t_2)
        del t_1
        del t_5
        del t_6
        t_2 = self.b_5[t_2]
        t_3 = t_3 * t_2
        t_2 = 1 - t_2
        t_2 = 10000.0 * t_2
        t_2 = t_3 - t_2
        del t_3
        t_2 = torch.softmax(t_2, dim=-1)
        t_2 = self.l_56(t_2)
        t_4 = torch.matmul(t_2, t_4)
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_3 = slice(None, -2, None)
        t_3 = t_2[t_3]
        del t_2
        t_2 = t_4.size(-2)
        t_1 = t_4.size(-1)
        t_1 = t_2 * t_1
        del t_2
        t_1 = (t_1,)
        t_1 = t_3 + t_1
        del t_3
        t_3 = t_1[0]
        t_2 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_4.view(t_3, t_2, t_1)
        del t_2
        del t_3
        del t_4
        t_1 = self.l_57(t_1)
        t_1 = self.l_58(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_59(t_1)
        t_0 = self.l_60(t_0)
        t_0 = self.l_61(t_0)
        t_0 = self.l_62(t_0)
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Tensor::__add__
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj]
        return (t_1, t_0)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition3(nn.Module):
    BASIC_BLOCKS=(
            Conv1D,
            Dropout,
            Gelu,
            LayerNorm,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[resid_dropout]',
        ]
    TENSORS=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Tensor[bias]',
        ]
    def __init__(self, layers, tensors):
        super(Partition3, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:3')
        self.lookup = { 'l_0': 'transformer.blocks.12.mlp.dropout',
                        'l_1': 'transformer.blocks.13.ln_1',
                        'l_2': 'transformer.blocks.13.attn.c_attn',
                        'l_3': 'transformer.blocks.13.attn.attn_dropout',
                        'l_4': 'transformer.blocks.13.attn.c_proj',
                        'l_5': 'transformer.blocks.13.attn.resid_dropout',
                        'l_6': 'transformer.blocks.13.ln_2',
                        'l_7': 'transformer.blocks.13.mlp.c_fc',
                        'l_8': 'transformer.blocks.13.mlp.act',
                        'l_9': 'transformer.blocks.13.mlp.c_proj',
                        'l_10': 'transformer.blocks.13.mlp.dropout',
                        'l_11': 'transformer.blocks.14.ln_1',
                        'l_12': 'transformer.blocks.14.attn.c_attn',
                        'l_13': 'transformer.blocks.14.attn.attn_dropout',
                        'l_14': 'transformer.blocks.14.attn.c_proj',
                        'l_15': 'transformer.blocks.14.attn.resid_dropout',
                        'l_16': 'transformer.blocks.14.ln_2',
                        'l_17': 'transformer.blocks.14.mlp.c_fc',
                        'l_18': 'transformer.blocks.14.mlp.act',
                        'l_19': 'transformer.blocks.14.mlp.c_proj',
                        'l_20': 'transformer.blocks.14.mlp.dropout',
                        'l_21': 'transformer.blocks.15.ln_1',
                        'l_22': 'transformer.blocks.15.attn.c_attn',
                        'l_23': 'transformer.blocks.15.attn.attn_dropout',
                        'l_24': 'transformer.blocks.15.attn.c_proj',
                        'l_25': 'transformer.blocks.15.attn.resid_dropout',
                        'l_26': 'transformer.blocks.15.ln_2',
                        'l_27': 'transformer.blocks.15.mlp.c_fc',
                        'l_28': 'transformer.blocks.15.mlp.act',
                        'l_29': 'transformer.blocks.15.mlp.c_proj',
                        'l_30': 'transformer.blocks.15.mlp.dropout',
                        'l_31': 'transformer.blocks.16.ln_1',
                        'l_32': 'transformer.blocks.16.attn.c_attn',
                        'l_33': 'transformer.blocks.16.attn.attn_dropout',
                        'l_34': 'transformer.blocks.16.attn.c_proj',
                        'l_35': 'transformer.blocks.16.attn.resid_dropout',
                        'l_36': 'transformer.blocks.16.ln_2',
                        'l_37': 'transformer.blocks.16.mlp.c_fc',
                        'l_38': 'transformer.blocks.16.mlp.act',
                        'l_39': 'transformer.blocks.16.mlp.c_proj',
                        'l_40': 'transformer.blocks.16.mlp.dropout',
                        'l_41': 'transformer.blocks.17.ln_1',
                        'l_42': 'transformer.blocks.17.attn.c_attn',
                        'l_43': 'transformer.blocks.17.attn.attn_dropout',
                        'l_44': 'transformer.blocks.17.attn.c_proj',
                        'l_45': 'transformer.blocks.17.attn.resid_dropout',
                        'l_46': 'transformer.blocks.17.ln_2',
                        'l_47': 'transformer.blocks.17.mlp.c_fc',
                        'l_48': 'transformer.blocks.17.mlp.act',
                        'l_49': 'transformer.blocks.17.mlp.c_proj',
                        'l_50': 'transformer.blocks.17.mlp.dropout',
                        'l_51': 'transformer.blocks.18.ln_1',
                        'l_52': 'transformer.blocks.18.attn.c_attn',
                        'l_53': 'transformer.blocks.18.attn.attn_dropout',
                        'l_54': 'transformer.blocks.18.attn.c_proj',
                        'l_55': 'transformer.blocks.18.attn.resid_dropout',
                        'l_56': 'transformer.blocks.18.ln_2',
                        'l_57': 'transformer.blocks.18.mlp.c_fc',
                        'l_58': 'transformer.blocks.18.mlp.act',
                        'l_59': 'transformer.blocks.18.mlp.c_proj',
                        'l_60': 'transformer.blocks.18.mlp.dropout',
                        'l_61': 'transformer.blocks.19.ln_1',
                        'l_62': 'transformer.blocks.19.attn.c_attn',
                        'l_63': 'transformer.blocks.19.attn.attn_dropout',
                        'l_64': 'transformer.blocks.19.attn.c_proj',
                        'l_65': 'transformer.blocks.19.attn.resid_dropout',
                        'b_0': 'transformer.blocks.13.attn.bias',
                        'b_1': 'transformer.blocks.14.attn.bias',
                        'b_2': 'transformer.blocks.15.attn.bias',
                        'b_3': 'transformer.blocks.16.attn.bias',
                        'b_4': 'transformer.blocks.17.attn.bias',
                        'b_5': 'transformer.blocks.18.attn.bias',
                        'b_6': 'transformer.blocks.19.attn.bias'}

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_attn] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[attn_dropout] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_proj] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[resid_dropout] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Gelu[act] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_attn] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[attn_dropout] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_proj] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[resid_dropout] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Gelu[act] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_attn] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[attn_dropout] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_proj] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[resid_dropout] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Gelu[act] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_attn] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[attn_dropout] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_proj] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[resid_dropout] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Gelu[act] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_attn] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[attn_dropout] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_proj] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[resid_dropout] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Gelu[act] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_attn] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[attn_dropout] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_proj] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[resid_dropout] <=> self.l_55
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2] <=> self.l_56
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc] <=> self.l_57
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Gelu[act] <=> self.l_58
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj] <=> self.l_59
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout] <=> self.l_60
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1] <=> self.l_61
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_attn] <=> self.l_62
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[attn_dropout] <=> self.l_63
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_proj] <=> self.l_64
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[resid_dropout] <=> self.l_65
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Tensor[bias] <=> self.b_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Tensor::__add__ <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj] <=> x1

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)

        t_0 = self.l_0(x1)
        del x1
        t_0 = x0 + t_0
        del x0
        t_1 = self.l_1(t_0)
        t_1 = self.l_2(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_2 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_7 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_7, t_4)
        del t_7
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_3.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_2 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_3.view(t_6, t_5, t_2, t_7)
        del t_2
        del t_5
        del t_6
        del t_3
        t_7 = t_7.permute(0, 2, 3, 1)
        t_2 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_1.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_1.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_1
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_4, t_7)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_7 / t_4
        del t_7
        t_7 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_7 = t_3 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_7, t_3)
        del t_7
        del t_5
        del t_6
        t_3 = self.b_0[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_3(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_7 = t_2.size(-1)
        t_7 = t_3 * t_7
        del t_3
        t_7 = (t_7,)
        t_7 = t_4 + t_7
        del t_4
        t_4 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_2.view(t_4, t_3, t_7)
        del t_3
        del t_4
        del t_2
        t_7 = self.l_4(t_7)
        t_7 = self.l_5(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_6(t_7)
        t_0 = self.l_7(t_0)
        t_0 = self.l_8(t_0)
        t_0 = self.l_9(t_0)
        t_0 = self.l_10(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_11(t_0)
        t_7 = self.l_12(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_3 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_2 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_3.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_1 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_5, t_6, t_1, t_2)
        del t_1
        del t_6
        del t_5
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_1 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_4.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_3 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_4.view(t_6, t_5, t_3, t_1)
        del t_3
        del t_5
        del t_6
        del t_4
        t_1 = t_1.permute(0, 2, 3, 1)
        t_3 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_7.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_7.view(t_5, t_6, t_4, t_3)
        del t_4
        del t_6
        del t_5
        del t_7
        t_3 = t_3.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_2, t_1)
        del t_2
        t_2 = t_3.size(-1)
        t_2 = math.sqrt(t_2)
        t_2 = t_1 / t_2
        del t_1
        t_1 = t_2.size(-2)
        t_4 = t_2.size(-1)
        t_1 = t_4 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_4, None)
        t_4 = slice(None, t_4, None)
        t_4 = (t_6, t_5, t_1, t_4)
        del t_1
        del t_5
        del t_6
        t_4 = self.b_1[t_4]
        t_2 = t_2 * t_4
        t_4 = 1 - t_4
        t_4 = 10000.0 * t_4
        t_4 = t_2 - t_4
        del t_2
        t_4 = torch.softmax(t_4, dim=-1)
        t_4 = self.l_13(t_4)
        t_3 = torch.matmul(t_4, t_3)
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_4 = t_3.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        del t_4
        t_4 = t_3.size(-2)
        t_1 = t_3.size(-1)
        t_1 = t_4 * t_1
        del t_4
        t_1 = (t_1,)
        t_1 = t_2 + t_1
        del t_2
        t_2 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_3.view(t_2, t_4, t_1)
        del t_4
        del t_2
        del t_3
        t_1 = self.l_14(t_1)
        t_1 = self.l_15(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_16(t_1)
        t_0 = self.l_17(t_0)
        t_0 = self.l_18(t_0)
        t_0 = self.l_19(t_0)
        t_0 = self.l_20(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_21(t_0)
        t_1 = self.l_22(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_4 = t_1[0]
        t_2 = t_1[1]
        t_1 = t_1[2]
        t_3 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_4.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_7 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_5, t_6, t_7, t_3)
        del t_7
        del t_6
        del t_5
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_7 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_2.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_4 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_2.view(t_6, t_5, t_4, t_7)
        del t_4
        del t_5
        del t_6
        del t_2
        t_7 = t_7.permute(0, 2, 3, 1)
        t_4 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_1.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_1.view(t_5, t_6, t_2, t_4)
        del t_2
        del t_6
        del t_5
        del t_1
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_3, t_7)
        del t_3
        t_3 = t_4.size(-1)
        t_3 = math.sqrt(t_3)
        t_3 = t_7 / t_3
        del t_7
        t_7 = t_3.size(-2)
        t_2 = t_3.size(-1)
        t_7 = t_2 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_5, t_7, t_2)
        del t_7
        del t_5
        del t_6
        t_2 = self.b_2[t_2]
        t_3 = t_3 * t_2
        t_2 = 1 - t_2
        t_2 = 10000.0 * t_2
        t_2 = t_3 - t_2
        del t_3
        t_2 = torch.softmax(t_2, dim=-1)
        t_2 = self.l_23(t_2)
        t_4 = torch.matmul(t_2, t_4)
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_3 = slice(None, -2, None)
        t_3 = t_2[t_3]
        del t_2
        t_2 = t_4.size(-2)
        t_7 = t_4.size(-1)
        t_7 = t_2 * t_7
        del t_2
        t_7 = (t_7,)
        t_7 = t_3 + t_7
        del t_3
        t_3 = t_7[0]
        t_2 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_4.view(t_3, t_2, t_7)
        del t_2
        del t_3
        del t_4
        t_7 = self.l_24(t_7)
        t_7 = self.l_25(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_26(t_7)
        t_0 = self.l_27(t_0)
        t_0 = self.l_28(t_0)
        t_0 = self.l_29(t_0)
        t_0 = self.l_30(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_31(t_0)
        t_7 = self.l_32(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_2 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_1 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_1, t_4)
        del t_1
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_1 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_3.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_2 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_3.view(t_6, t_5, t_2, t_1)
        del t_2
        del t_5
        del t_6
        del t_3
        t_1 = t_1.permute(0, 2, 3, 1)
        t_2 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_7.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_7.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_7
        t_2 = t_2.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_4, t_1)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_1 / t_4
        del t_1
        t_1 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_1 = t_3 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_1, t_3)
        del t_1
        del t_5
        del t_6
        t_3 = self.b_3[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_33(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_1 = t_2.size(-1)
        t_1 = t_3 * t_1
        del t_3
        t_1 = (t_1,)
        t_1 = t_4 + t_1
        del t_4
        t_4 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_2.view(t_4, t_3, t_1)
        del t_3
        del t_4
        del t_2
        t_1 = self.l_34(t_1)
        t_1 = self.l_35(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_36(t_1)
        t_0 = self.l_37(t_0)
        t_0 = self.l_38(t_0)
        t_0 = self.l_39(t_0)
        t_0 = self.l_40(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_41(t_0)
        t_1 = self.l_42(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_3 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        t_2 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_3.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_7 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_5, t_6, t_7, t_2)
        del t_7
        del t_6
        del t_5
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_4.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_3 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_4.view(t_6, t_5, t_3, t_7)
        del t_3
        del t_5
        del t_6
        del t_4
        t_7 = t_7.permute(0, 2, 3, 1)
        t_3 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_1.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_1.view(t_5, t_6, t_4, t_3)
        del t_4
        del t_6
        del t_5
        del t_1
        t_3 = t_3.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_2, t_7)
        del t_2
        t_2 = t_3.size(-1)
        t_2 = math.sqrt(t_2)
        t_2 = t_7 / t_2
        del t_7
        t_7 = t_2.size(-2)
        t_4 = t_2.size(-1)
        t_7 = t_4 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_4, None)
        t_4 = slice(None, t_4, None)
        t_4 = (t_6, t_5, t_7, t_4)
        del t_7
        del t_5
        del t_6
        t_4 = self.b_4[t_4]
        t_2 = t_2 * t_4
        t_4 = 1 - t_4
        t_4 = 10000.0 * t_4
        t_4 = t_2 - t_4
        del t_2
        t_4 = torch.softmax(t_4, dim=-1)
        t_4 = self.l_43(t_4)
        t_3 = torch.matmul(t_4, t_3)
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_4 = t_3.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        del t_4
        t_4 = t_3.size(-2)
        t_7 = t_3.size(-1)
        t_7 = t_4 * t_7
        del t_4
        t_7 = (t_7,)
        t_7 = t_2 + t_7
        del t_2
        t_2 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_3.view(t_2, t_4, t_7)
        del t_4
        del t_2
        del t_3
        t_7 = self.l_44(t_7)
        t_7 = self.l_45(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_46(t_7)
        t_0 = self.l_47(t_0)
        t_0 = self.l_48(t_0)
        t_0 = self.l_49(t_0)
        t_0 = self.l_50(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_51(t_0)
        t_7 = self.l_52(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_4 = t_7[0]
        t_2 = t_7[1]
        t_7 = t_7[2]
        t_3 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_4.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_1 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_5, t_6, t_1, t_3)
        del t_1
        del t_6
        del t_5
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_1 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_2.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_4 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_2.view(t_6, t_5, t_4, t_1)
        del t_4
        del t_5
        del t_6
        del t_2
        t_1 = t_1.permute(0, 2, 3, 1)
        t_4 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_7.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_7.view(t_5, t_6, t_2, t_4)
        del t_2
        del t_6
        del t_5
        del t_7
        t_4 = t_4.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_3, t_1)
        del t_3
        t_3 = t_4.size(-1)
        t_3 = math.sqrt(t_3)
        t_3 = t_1 / t_3
        del t_1
        t_1 = t_3.size(-2)
        t_2 = t_3.size(-1)
        t_1 = t_2 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_5, t_1, t_2)
        del t_1
        del t_5
        del t_6
        t_2 = self.b_5[t_2]
        t_3 = t_3 * t_2
        t_2 = 1 - t_2
        t_2 = 10000.0 * t_2
        t_2 = t_3 - t_2
        del t_3
        t_2 = torch.softmax(t_2, dim=-1)
        t_2 = self.l_53(t_2)
        t_4 = torch.matmul(t_2, t_4)
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_3 = slice(None, -2, None)
        t_3 = t_2[t_3]
        del t_2
        t_2 = t_4.size(-2)
        t_1 = t_4.size(-1)
        t_1 = t_2 * t_1
        del t_2
        t_1 = (t_1,)
        t_1 = t_3 + t_1
        del t_3
        t_3 = t_1[0]
        t_2 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_4.view(t_3, t_2, t_1)
        del t_2
        del t_3
        del t_4
        t_1 = self.l_54(t_1)
        t_1 = self.l_55(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_56(t_1)
        t_0 = self.l_57(t_0)
        t_0 = self.l_58(t_0)
        t_0 = self.l_59(t_0)
        t_0 = self.l_60(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_61(t_0)
        t_1 = self.l_62(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_2 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_7 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_7, t_4)
        del t_7
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_3.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_2 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_3.view(t_6, t_5, t_2, t_7)
        del t_2
        del t_5
        del t_6
        del t_3
        t_7 = t_7.permute(0, 2, 3, 1)
        t_2 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_1.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_1.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_1
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_4, t_7)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_7 / t_4
        del t_7
        t_7 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_7 = t_3 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_7, t_3)
        del t_7
        del t_5
        del t_6
        t_3 = self.b_6[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_63(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_7 = t_2.size(-1)
        t_7 = t_3 * t_7
        del t_3
        t_7 = (t_7,)
        t_7 = t_4 + t_7
        del t_4
        t_4 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_2.view(t_4, t_3, t_7)
        del t_3
        del t_4
        del t_2
        t_7 = self.l_64(t_7)
        t_7 = self.l_65(t_7)
        t_7 = t_0 + t_7
        del t_0
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Tensor::__add__
        return (t_7,)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition4(nn.Module):
    BASIC_BLOCKS=(
            Conv1D,
            Dropout,
            Gelu,
            LayerNorm,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2]',
        ]
    TENSORS=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Tensor[bias]',
        ]
    def __init__(self, layers, tensors):
        super(Partition4, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:4')
        self.lookup = { 'l_0': 'transformer.blocks.19.ln_2',
                        'l_1': 'transformer.blocks.19.mlp.c_fc',
                        'l_2': 'transformer.blocks.19.mlp.act',
                        'l_3': 'transformer.blocks.19.mlp.c_proj',
                        'l_4': 'transformer.blocks.19.mlp.dropout',
                        'l_5': 'transformer.blocks.20.ln_1',
                        'l_6': 'transformer.blocks.20.attn.c_attn',
                        'l_7': 'transformer.blocks.20.attn.attn_dropout',
                        'l_8': 'transformer.blocks.20.attn.c_proj',
                        'l_9': 'transformer.blocks.20.attn.resid_dropout',
                        'l_10': 'transformer.blocks.20.ln_2',
                        'l_11': 'transformer.blocks.20.mlp.c_fc',
                        'l_12': 'transformer.blocks.20.mlp.act',
                        'l_13': 'transformer.blocks.20.mlp.c_proj',
                        'l_14': 'transformer.blocks.20.mlp.dropout',
                        'l_15': 'transformer.blocks.21.ln_1',
                        'l_16': 'transformer.blocks.21.attn.c_attn',
                        'l_17': 'transformer.blocks.21.attn.attn_dropout',
                        'l_18': 'transformer.blocks.21.attn.c_proj',
                        'l_19': 'transformer.blocks.21.attn.resid_dropout',
                        'l_20': 'transformer.blocks.21.ln_2',
                        'l_21': 'transformer.blocks.21.mlp.c_fc',
                        'l_22': 'transformer.blocks.21.mlp.act',
                        'l_23': 'transformer.blocks.21.mlp.c_proj',
                        'l_24': 'transformer.blocks.21.mlp.dropout',
                        'l_25': 'transformer.blocks.22.ln_1',
                        'l_26': 'transformer.blocks.22.attn.c_attn',
                        'l_27': 'transformer.blocks.22.attn.attn_dropout',
                        'l_28': 'transformer.blocks.22.attn.c_proj',
                        'l_29': 'transformer.blocks.22.attn.resid_dropout',
                        'l_30': 'transformer.blocks.22.ln_2',
                        'l_31': 'transformer.blocks.22.mlp.c_fc',
                        'l_32': 'transformer.blocks.22.mlp.act',
                        'l_33': 'transformer.blocks.22.mlp.c_proj',
                        'l_34': 'transformer.blocks.22.mlp.dropout',
                        'l_35': 'transformer.blocks.23.ln_1',
                        'l_36': 'transformer.blocks.23.attn.c_attn',
                        'l_37': 'transformer.blocks.23.attn.attn_dropout',
                        'l_38': 'transformer.blocks.23.attn.c_proj',
                        'l_39': 'transformer.blocks.23.attn.resid_dropout',
                        'l_40': 'transformer.blocks.23.ln_2',
                        'l_41': 'transformer.blocks.23.mlp.c_fc',
                        'l_42': 'transformer.blocks.23.mlp.act',
                        'l_43': 'transformer.blocks.23.mlp.c_proj',
                        'l_44': 'transformer.blocks.23.mlp.dropout',
                        'l_45': 'transformer.blocks.24.ln_1',
                        'l_46': 'transformer.blocks.24.attn.c_attn',
                        'l_47': 'transformer.blocks.24.attn.attn_dropout',
                        'l_48': 'transformer.blocks.24.attn.c_proj',
                        'l_49': 'transformer.blocks.24.attn.resid_dropout',
                        'l_50': 'transformer.blocks.24.ln_2',
                        'l_51': 'transformer.blocks.24.mlp.c_fc',
                        'l_52': 'transformer.blocks.24.mlp.act',
                        'l_53': 'transformer.blocks.24.mlp.c_proj',
                        'l_54': 'transformer.blocks.24.mlp.dropout',
                        'l_55': 'transformer.blocks.25.ln_1',
                        'l_56': 'transformer.blocks.25.attn.c_attn',
                        'l_57': 'transformer.blocks.25.attn.attn_dropout',
                        'l_58': 'transformer.blocks.25.attn.c_proj',
                        'l_59': 'transformer.blocks.25.attn.resid_dropout',
                        'l_60': 'transformer.blocks.25.ln_2',
                        'b_0': 'transformer.blocks.20.attn.bias',
                        'b_1': 'transformer.blocks.21.attn.bias',
                        'b_2': 'transformer.blocks.22.attn.bias',
                        'b_3': 'transformer.blocks.23.attn.bias',
                        'b_4': 'transformer.blocks.24.attn.bias',
                        'b_5': 'transformer.blocks.25.attn.bias'}

    def forward(self, x0):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Gelu[act] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_attn] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[attn_dropout] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_proj] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[resid_dropout] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Gelu[act] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_attn] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[attn_dropout] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_proj] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[resid_dropout] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Gelu[act] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_attn] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[attn_dropout] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_proj] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[resid_dropout] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Gelu[act] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_attn] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[attn_dropout] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_proj] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[resid_dropout] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Gelu[act] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_attn] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[attn_dropout] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_proj] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[resid_dropout] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Gelu[act] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1] <=> self.l_55
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_attn] <=> self.l_56
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[attn_dropout] <=> self.l_57
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_proj] <=> self.l_58
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[resid_dropout] <=> self.l_59
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2] <=> self.l_60
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Tensor::__add__ <=> x0

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)

        t_0 = self.l_0(x0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x0 + t_0
        del x0
        t_1 = self.l_5(t_0)
        t_1 = self.l_6(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_2 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_7 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_7, t_4)
        del t_7
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_3.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_2 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_3.view(t_6, t_5, t_2, t_7)
        del t_2
        del t_5
        del t_6
        del t_3
        t_7 = t_7.permute(0, 2, 3, 1)
        t_2 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_1.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_1.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_1
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_4, t_7)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_7 / t_4
        del t_7
        t_7 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_7 = t_3 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_7, t_3)
        del t_7
        del t_5
        del t_6
        t_3 = self.b_0[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_7(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_7 = t_2.size(-1)
        t_7 = t_3 * t_7
        del t_3
        t_7 = (t_7,)
        t_7 = t_4 + t_7
        del t_4
        t_4 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_2.view(t_4, t_3, t_7)
        del t_3
        del t_4
        del t_2
        t_7 = self.l_8(t_7)
        t_7 = self.l_9(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_10(t_7)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_0 = self.l_14(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_15(t_0)
        t_7 = self.l_16(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_3 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_2 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_3.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_1 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_5, t_6, t_1, t_2)
        del t_1
        del t_6
        del t_5
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_1 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_4.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_3 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_4.view(t_6, t_5, t_3, t_1)
        del t_3
        del t_5
        del t_6
        del t_4
        t_1 = t_1.permute(0, 2, 3, 1)
        t_3 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_7.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_7.view(t_5, t_6, t_4, t_3)
        del t_4
        del t_6
        del t_5
        del t_7
        t_3 = t_3.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_2, t_1)
        del t_2
        t_2 = t_3.size(-1)
        t_2 = math.sqrt(t_2)
        t_2 = t_1 / t_2
        del t_1
        t_1 = t_2.size(-2)
        t_4 = t_2.size(-1)
        t_1 = t_4 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_4, None)
        t_4 = slice(None, t_4, None)
        t_4 = (t_6, t_5, t_1, t_4)
        del t_1
        del t_5
        del t_6
        t_4 = self.b_1[t_4]
        t_2 = t_2 * t_4
        t_4 = 1 - t_4
        t_4 = 10000.0 * t_4
        t_4 = t_2 - t_4
        del t_2
        t_4 = torch.softmax(t_4, dim=-1)
        t_4 = self.l_17(t_4)
        t_3 = torch.matmul(t_4, t_3)
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_4 = t_3.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        del t_4
        t_4 = t_3.size(-2)
        t_1 = t_3.size(-1)
        t_1 = t_4 * t_1
        del t_4
        t_1 = (t_1,)
        t_1 = t_2 + t_1
        del t_2
        t_2 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_3.view(t_2, t_4, t_1)
        del t_4
        del t_2
        del t_3
        t_1 = self.l_18(t_1)
        t_1 = self.l_19(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_20(t_1)
        t_0 = self.l_21(t_0)
        t_0 = self.l_22(t_0)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_25(t_0)
        t_1 = self.l_26(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_4 = t_1[0]
        t_2 = t_1[1]
        t_1 = t_1[2]
        t_3 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_4.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_7 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_5, t_6, t_7, t_3)
        del t_7
        del t_6
        del t_5
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_7 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_2.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_4 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_2.view(t_6, t_5, t_4, t_7)
        del t_4
        del t_5
        del t_6
        del t_2
        t_7 = t_7.permute(0, 2, 3, 1)
        t_4 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_1.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_1.view(t_5, t_6, t_2, t_4)
        del t_2
        del t_6
        del t_5
        del t_1
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_3, t_7)
        del t_3
        t_3 = t_4.size(-1)
        t_3 = math.sqrt(t_3)
        t_3 = t_7 / t_3
        del t_7
        t_7 = t_3.size(-2)
        t_2 = t_3.size(-1)
        t_7 = t_2 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_5, t_7, t_2)
        del t_7
        del t_5
        del t_6
        t_2 = self.b_2[t_2]
        t_3 = t_3 * t_2
        t_2 = 1 - t_2
        t_2 = 10000.0 * t_2
        t_2 = t_3 - t_2
        del t_3
        t_2 = torch.softmax(t_2, dim=-1)
        t_2 = self.l_27(t_2)
        t_4 = torch.matmul(t_2, t_4)
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_3 = slice(None, -2, None)
        t_3 = t_2[t_3]
        del t_2
        t_2 = t_4.size(-2)
        t_7 = t_4.size(-1)
        t_7 = t_2 * t_7
        del t_2
        t_7 = (t_7,)
        t_7 = t_3 + t_7
        del t_3
        t_3 = t_7[0]
        t_2 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_4.view(t_3, t_2, t_7)
        del t_2
        del t_3
        del t_4
        t_7 = self.l_28(t_7)
        t_7 = self.l_29(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_30(t_7)
        t_0 = self.l_31(t_0)
        t_0 = self.l_32(t_0)
        t_0 = self.l_33(t_0)
        t_0 = self.l_34(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_35(t_0)
        t_7 = self.l_36(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_2 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_1 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_1, t_4)
        del t_1
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_1 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_3.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_2 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_3.view(t_6, t_5, t_2, t_1)
        del t_2
        del t_5
        del t_6
        del t_3
        t_1 = t_1.permute(0, 2, 3, 1)
        t_2 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_7.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_7.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_7
        t_2 = t_2.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_4, t_1)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_1 / t_4
        del t_1
        t_1 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_1 = t_3 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_1, t_3)
        del t_1
        del t_5
        del t_6
        t_3 = self.b_3[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_37(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_1 = t_2.size(-1)
        t_1 = t_3 * t_1
        del t_3
        t_1 = (t_1,)
        t_1 = t_4 + t_1
        del t_4
        t_4 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_2.view(t_4, t_3, t_1)
        del t_3
        del t_4
        del t_2
        t_1 = self.l_38(t_1)
        t_1 = self.l_39(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_40(t_1)
        t_0 = self.l_41(t_0)
        t_0 = self.l_42(t_0)
        t_0 = self.l_43(t_0)
        t_0 = self.l_44(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_45(t_0)
        t_1 = self.l_46(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_3 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        t_2 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_3.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_7 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_5, t_6, t_7, t_2)
        del t_7
        del t_6
        del t_5
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_4.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_3 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_4.view(t_6, t_5, t_3, t_7)
        del t_3
        del t_5
        del t_6
        del t_4
        t_7 = t_7.permute(0, 2, 3, 1)
        t_3 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_1.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_1.view(t_5, t_6, t_4, t_3)
        del t_4
        del t_6
        del t_5
        del t_1
        t_3 = t_3.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_2, t_7)
        del t_2
        t_2 = t_3.size(-1)
        t_2 = math.sqrt(t_2)
        t_2 = t_7 / t_2
        del t_7
        t_7 = t_2.size(-2)
        t_4 = t_2.size(-1)
        t_7 = t_4 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_4, None)
        t_4 = slice(None, t_4, None)
        t_4 = (t_6, t_5, t_7, t_4)
        del t_7
        del t_5
        del t_6
        t_4 = self.b_4[t_4]
        t_2 = t_2 * t_4
        t_4 = 1 - t_4
        t_4 = 10000.0 * t_4
        t_4 = t_2 - t_4
        del t_2
        t_4 = torch.softmax(t_4, dim=-1)
        t_4 = self.l_47(t_4)
        t_3 = torch.matmul(t_4, t_3)
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_4 = t_3.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        del t_4
        t_4 = t_3.size(-2)
        t_7 = t_3.size(-1)
        t_7 = t_4 * t_7
        del t_4
        t_7 = (t_7,)
        t_7 = t_2 + t_7
        del t_2
        t_2 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_3.view(t_2, t_4, t_7)
        del t_4
        del t_2
        del t_3
        t_7 = self.l_48(t_7)
        t_7 = self.l_49(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_50(t_7)
        t_0 = self.l_51(t_0)
        t_0 = self.l_52(t_0)
        t_0 = self.l_53(t_0)
        t_0 = self.l_54(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_55(t_0)
        t_7 = self.l_56(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_4 = t_7[0]
        t_2 = t_7[1]
        t_7 = t_7[2]
        t_3 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_4.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_1 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_5, t_6, t_1, t_3)
        del t_1
        del t_6
        del t_5
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_1 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_2.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_4 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_2.view(t_6, t_5, t_4, t_1)
        del t_4
        del t_5
        del t_6
        del t_2
        t_1 = t_1.permute(0, 2, 3, 1)
        t_4 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_7.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_7.view(t_5, t_6, t_2, t_4)
        del t_2
        del t_6
        del t_5
        del t_7
        t_4 = t_4.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_3, t_1)
        del t_3
        t_3 = t_4.size(-1)
        t_3 = math.sqrt(t_3)
        t_3 = t_1 / t_3
        del t_1
        t_1 = t_3.size(-2)
        t_2 = t_3.size(-1)
        t_1 = t_2 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_5, t_1, t_2)
        del t_1
        del t_5
        del t_6
        t_2 = self.b_5[t_2]
        t_3 = t_3 * t_2
        t_2 = 1 - t_2
        t_2 = 10000.0 * t_2
        t_2 = t_3 - t_2
        del t_3
        t_2 = torch.softmax(t_2, dim=-1)
        t_2 = self.l_57(t_2)
        t_4 = torch.matmul(t_2, t_4)
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_3 = slice(None, -2, None)
        t_3 = t_2[t_3]
        del t_2
        t_2 = t_4.size(-2)
        t_1 = t_4.size(-1)
        t_1 = t_2 * t_1
        del t_2
        t_1 = (t_1,)
        t_1 = t_3 + t_1
        del t_3
        t_3 = t_1[0]
        t_2 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_4.view(t_3, t_2, t_1)
        del t_2
        del t_3
        del t_4
        t_1 = self.l_58(t_1)
        t_1 = self.l_59(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_60(t_1)
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Tensor::__add__
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2]
        return (t_1, t_0)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition5(nn.Module):
    BASIC_BLOCKS=(
            Conv1D,
            Dropout,
            Gelu,
            LayerNorm,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[resid_dropout]',
        ]
    TENSORS=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Tensor[bias]',
        ]
    def __init__(self, layers, tensors):
        super(Partition5, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:5')
        self.lookup = { 'l_0': 'transformer.blocks.25.mlp.c_fc',
                        'l_1': 'transformer.blocks.25.mlp.act',
                        'l_2': 'transformer.blocks.25.mlp.c_proj',
                        'l_3': 'transformer.blocks.25.mlp.dropout',
                        'l_4': 'transformer.blocks.26.ln_1',
                        'l_5': 'transformer.blocks.26.attn.c_attn',
                        'l_6': 'transformer.blocks.26.attn.attn_dropout',
                        'l_7': 'transformer.blocks.26.attn.c_proj',
                        'l_8': 'transformer.blocks.26.attn.resid_dropout',
                        'l_9': 'transformer.blocks.26.ln_2',
                        'l_10': 'transformer.blocks.26.mlp.c_fc',
                        'l_11': 'transformer.blocks.26.mlp.act',
                        'l_12': 'transformer.blocks.26.mlp.c_proj',
                        'l_13': 'transformer.blocks.26.mlp.dropout',
                        'l_14': 'transformer.blocks.27.ln_1',
                        'l_15': 'transformer.blocks.27.attn.c_attn',
                        'l_16': 'transformer.blocks.27.attn.attn_dropout',
                        'l_17': 'transformer.blocks.27.attn.c_proj',
                        'l_18': 'transformer.blocks.27.attn.resid_dropout',
                        'l_19': 'transformer.blocks.27.ln_2',
                        'l_20': 'transformer.blocks.27.mlp.c_fc',
                        'l_21': 'transformer.blocks.27.mlp.act',
                        'l_22': 'transformer.blocks.27.mlp.c_proj',
                        'l_23': 'transformer.blocks.27.mlp.dropout',
                        'l_24': 'transformer.blocks.28.ln_1',
                        'l_25': 'transformer.blocks.28.attn.c_attn',
                        'l_26': 'transformer.blocks.28.attn.attn_dropout',
                        'l_27': 'transformer.blocks.28.attn.c_proj',
                        'l_28': 'transformer.blocks.28.attn.resid_dropout',
                        'l_29': 'transformer.blocks.28.ln_2',
                        'l_30': 'transformer.blocks.28.mlp.c_fc',
                        'l_31': 'transformer.blocks.28.mlp.act',
                        'l_32': 'transformer.blocks.28.mlp.c_proj',
                        'l_33': 'transformer.blocks.28.mlp.dropout',
                        'l_34': 'transformer.blocks.29.ln_1',
                        'l_35': 'transformer.blocks.29.attn.c_attn',
                        'l_36': 'transformer.blocks.29.attn.attn_dropout',
                        'l_37': 'transformer.blocks.29.attn.c_proj',
                        'l_38': 'transformer.blocks.29.attn.resid_dropout',
                        'l_39': 'transformer.blocks.29.ln_2',
                        'l_40': 'transformer.blocks.29.mlp.c_fc',
                        'l_41': 'transformer.blocks.29.mlp.act',
                        'l_42': 'transformer.blocks.29.mlp.c_proj',
                        'l_43': 'transformer.blocks.29.mlp.dropout',
                        'l_44': 'transformer.blocks.30.ln_1',
                        'l_45': 'transformer.blocks.30.attn.c_attn',
                        'l_46': 'transformer.blocks.30.attn.attn_dropout',
                        'l_47': 'transformer.blocks.30.attn.c_proj',
                        'l_48': 'transformer.blocks.30.attn.resid_dropout',
                        'l_49': 'transformer.blocks.30.ln_2',
                        'l_50': 'transformer.blocks.30.mlp.c_fc',
                        'l_51': 'transformer.blocks.30.mlp.act',
                        'l_52': 'transformer.blocks.30.mlp.c_proj',
                        'l_53': 'transformer.blocks.30.mlp.dropout',
                        'l_54': 'transformer.blocks.31.ln_1',
                        'l_55': 'transformer.blocks.31.attn.c_attn',
                        'l_56': 'transformer.blocks.31.attn.attn_dropout',
                        'l_57': 'transformer.blocks.31.attn.c_proj',
                        'l_58': 'transformer.blocks.31.attn.resid_dropout',
                        'b_0': 'transformer.blocks.26.attn.bias',
                        'b_1': 'transformer.blocks.27.attn.bias',
                        'b_2': 'transformer.blocks.28.attn.bias',
                        'b_3': 'transformer.blocks.29.attn.bias',
                        'b_4': 'transformer.blocks.30.attn.bias',
                        'b_5': 'transformer.blocks.31.attn.bias'}

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Gelu[act] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_attn] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[attn_dropout] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_proj] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[resid_dropout] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Gelu[act] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_attn] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[attn_dropout] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_proj] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[resid_dropout] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Gelu[act] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_attn] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[attn_dropout] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_proj] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[resid_dropout] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Gelu[act] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_attn] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[attn_dropout] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_proj] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[resid_dropout] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Gelu[act] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_attn] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[attn_dropout] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_proj] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[resid_dropout] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Gelu[act] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_attn] <=> self.l_55
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[attn_dropout] <=> self.l_56
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_proj] <=> self.l_57
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[resid_dropout] <=> self.l_58
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Tensor::__add__ <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2] <=> x1

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)

        t_0 = self.l_0(x1)
        del x1
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = x0 + t_0
        del x0
        t_1 = self.l_4(t_0)
        t_1 = self.l_5(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_2 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_7 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_7, t_4)
        del t_7
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_3.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_2 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_3.view(t_6, t_5, t_2, t_7)
        del t_2
        del t_5
        del t_6
        del t_3
        t_7 = t_7.permute(0, 2, 3, 1)
        t_2 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_1.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_1.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_1
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_4, t_7)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_7 / t_4
        del t_7
        t_7 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_7 = t_3 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_7, t_3)
        del t_7
        del t_5
        del t_6
        t_3 = self.b_0[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_6(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_7 = t_2.size(-1)
        t_7 = t_3 * t_7
        del t_3
        t_7 = (t_7,)
        t_7 = t_4 + t_7
        del t_4
        t_4 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_2.view(t_4, t_3, t_7)
        del t_3
        del t_4
        del t_2
        t_7 = self.l_7(t_7)
        t_7 = self.l_8(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_9(t_7)
        t_0 = self.l_10(t_0)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_14(t_0)
        t_7 = self.l_15(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_3 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_2 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_3.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_1 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_5, t_6, t_1, t_2)
        del t_1
        del t_6
        del t_5
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_1 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_4.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_3 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_4.view(t_6, t_5, t_3, t_1)
        del t_3
        del t_5
        del t_6
        del t_4
        t_1 = t_1.permute(0, 2, 3, 1)
        t_3 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_7.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_7.view(t_5, t_6, t_4, t_3)
        del t_4
        del t_6
        del t_5
        del t_7
        t_3 = t_3.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_2, t_1)
        del t_2
        t_2 = t_3.size(-1)
        t_2 = math.sqrt(t_2)
        t_2 = t_1 / t_2
        del t_1
        t_1 = t_2.size(-2)
        t_4 = t_2.size(-1)
        t_1 = t_4 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_4, None)
        t_4 = slice(None, t_4, None)
        t_4 = (t_6, t_5, t_1, t_4)
        del t_1
        del t_5
        del t_6
        t_4 = self.b_1[t_4]
        t_2 = t_2 * t_4
        t_4 = 1 - t_4
        t_4 = 10000.0 * t_4
        t_4 = t_2 - t_4
        del t_2
        t_4 = torch.softmax(t_4, dim=-1)
        t_4 = self.l_16(t_4)
        t_3 = torch.matmul(t_4, t_3)
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_4 = t_3.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        del t_4
        t_4 = t_3.size(-2)
        t_1 = t_3.size(-1)
        t_1 = t_4 * t_1
        del t_4
        t_1 = (t_1,)
        t_1 = t_2 + t_1
        del t_2
        t_2 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_3.view(t_2, t_4, t_1)
        del t_4
        del t_2
        del t_3
        t_1 = self.l_17(t_1)
        t_1 = self.l_18(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_19(t_1)
        t_0 = self.l_20(t_0)
        t_0 = self.l_21(t_0)
        t_0 = self.l_22(t_0)
        t_0 = self.l_23(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_24(t_0)
        t_1 = self.l_25(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_4 = t_1[0]
        t_2 = t_1[1]
        t_1 = t_1[2]
        t_3 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_4.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_7 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_5, t_6, t_7, t_3)
        del t_7
        del t_6
        del t_5
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_7 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_2.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_4 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_2.view(t_6, t_5, t_4, t_7)
        del t_4
        del t_5
        del t_6
        del t_2
        t_7 = t_7.permute(0, 2, 3, 1)
        t_4 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_1.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_1.view(t_5, t_6, t_2, t_4)
        del t_2
        del t_6
        del t_5
        del t_1
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_3, t_7)
        del t_3
        t_3 = t_4.size(-1)
        t_3 = math.sqrt(t_3)
        t_3 = t_7 / t_3
        del t_7
        t_7 = t_3.size(-2)
        t_2 = t_3.size(-1)
        t_7 = t_2 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_5, t_7, t_2)
        del t_7
        del t_5
        del t_6
        t_2 = self.b_2[t_2]
        t_3 = t_3 * t_2
        t_2 = 1 - t_2
        t_2 = 10000.0 * t_2
        t_2 = t_3 - t_2
        del t_3
        t_2 = torch.softmax(t_2, dim=-1)
        t_2 = self.l_26(t_2)
        t_4 = torch.matmul(t_2, t_4)
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_3 = slice(None, -2, None)
        t_3 = t_2[t_3]
        del t_2
        t_2 = t_4.size(-2)
        t_7 = t_4.size(-1)
        t_7 = t_2 * t_7
        del t_2
        t_7 = (t_7,)
        t_7 = t_3 + t_7
        del t_3
        t_3 = t_7[0]
        t_2 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_4.view(t_3, t_2, t_7)
        del t_2
        del t_3
        del t_4
        t_7 = self.l_27(t_7)
        t_7 = self.l_28(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_29(t_7)
        t_0 = self.l_30(t_0)
        t_0 = self.l_31(t_0)
        t_0 = self.l_32(t_0)
        t_0 = self.l_33(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_34(t_0)
        t_7 = self.l_35(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_2 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_1 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_1, t_4)
        del t_1
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_1 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_3.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_2 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_3.view(t_6, t_5, t_2, t_1)
        del t_2
        del t_5
        del t_6
        del t_3
        t_1 = t_1.permute(0, 2, 3, 1)
        t_2 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_7.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_7.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_7
        t_2 = t_2.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_4, t_1)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_1 / t_4
        del t_1
        t_1 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_1 = t_3 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_1, t_3)
        del t_1
        del t_5
        del t_6
        t_3 = self.b_3[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_36(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_1 = t_2.size(-1)
        t_1 = t_3 * t_1
        del t_3
        t_1 = (t_1,)
        t_1 = t_4 + t_1
        del t_4
        t_4 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_2.view(t_4, t_3, t_1)
        del t_3
        del t_4
        del t_2
        t_1 = self.l_37(t_1)
        t_1 = self.l_38(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_39(t_1)
        t_0 = self.l_40(t_0)
        t_0 = self.l_41(t_0)
        t_0 = self.l_42(t_0)
        t_0 = self.l_43(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_44(t_0)
        t_1 = self.l_45(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_3 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        t_2 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_3.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_7 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_5, t_6, t_7, t_2)
        del t_7
        del t_6
        del t_5
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_4.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_3 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_4.view(t_6, t_5, t_3, t_7)
        del t_3
        del t_5
        del t_6
        del t_4
        t_7 = t_7.permute(0, 2, 3, 1)
        t_3 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_1.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_1.view(t_5, t_6, t_4, t_3)
        del t_4
        del t_6
        del t_5
        del t_1
        t_3 = t_3.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_2, t_7)
        del t_2
        t_2 = t_3.size(-1)
        t_2 = math.sqrt(t_2)
        t_2 = t_7 / t_2
        del t_7
        t_7 = t_2.size(-2)
        t_4 = t_2.size(-1)
        t_7 = t_4 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_4, None)
        t_4 = slice(None, t_4, None)
        t_4 = (t_6, t_5, t_7, t_4)
        del t_7
        del t_5
        del t_6
        t_4 = self.b_4[t_4]
        t_2 = t_2 * t_4
        t_4 = 1 - t_4
        t_4 = 10000.0 * t_4
        t_4 = t_2 - t_4
        del t_2
        t_4 = torch.softmax(t_4, dim=-1)
        t_4 = self.l_46(t_4)
        t_3 = torch.matmul(t_4, t_3)
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_4 = t_3.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        del t_4
        t_4 = t_3.size(-2)
        t_7 = t_3.size(-1)
        t_7 = t_4 * t_7
        del t_4
        t_7 = (t_7,)
        t_7 = t_2 + t_7
        del t_2
        t_2 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_3.view(t_2, t_4, t_7)
        del t_4
        del t_2
        del t_3
        t_7 = self.l_47(t_7)
        t_7 = self.l_48(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_49(t_7)
        t_0 = self.l_50(t_0)
        t_0 = self.l_51(t_0)
        t_0 = self.l_52(t_0)
        t_0 = self.l_53(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_54(t_0)
        t_7 = self.l_55(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_4 = t_7[0]
        t_2 = t_7[1]
        t_7 = t_7[2]
        t_3 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_4.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_1 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_5, t_6, t_1, t_3)
        del t_1
        del t_6
        del t_5
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_1 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_2.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_4 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_2.view(t_6, t_5, t_4, t_1)
        del t_4
        del t_5
        del t_6
        del t_2
        t_1 = t_1.permute(0, 2, 3, 1)
        t_4 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_7.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_7.view(t_5, t_6, t_2, t_4)
        del t_2
        del t_6
        del t_5
        del t_7
        t_4 = t_4.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_3, t_1)
        del t_3
        t_3 = t_4.size(-1)
        t_3 = math.sqrt(t_3)
        t_3 = t_1 / t_3
        del t_1
        t_1 = t_3.size(-2)
        t_2 = t_3.size(-1)
        t_1 = t_2 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_5, t_1, t_2)
        del t_1
        del t_5
        del t_6
        t_2 = self.b_5[t_2]
        t_3 = t_3 * t_2
        t_2 = 1 - t_2
        t_2 = 10000.0 * t_2
        t_2 = t_3 - t_2
        del t_3
        t_2 = torch.softmax(t_2, dim=-1)
        t_2 = self.l_56(t_2)
        t_4 = torch.matmul(t_2, t_4)
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_3 = slice(None, -2, None)
        t_3 = t_2[t_3]
        del t_2
        t_2 = t_4.size(-2)
        t_1 = t_4.size(-1)
        t_1 = t_2 * t_1
        del t_2
        t_1 = (t_1,)
        t_1 = t_3 + t_1
        del t_3
        t_3 = t_1[0]
        t_2 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_4.view(t_3, t_2, t_1)
        del t_2
        del t_3
        del t_4
        t_1 = self.l_57(t_1)
        t_1 = self.l_58(t_1)
        t_1 = t_0 + t_1
        del t_0
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Tensor::__add__
        return (t_1,)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition6(nn.Module):
    BASIC_BLOCKS=(
            Conv1D,
            Dropout,
            Gelu,
            LayerNorm,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[resid_dropout]',
        ]
    TENSORS=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Tensor[bias]',
        ]
    def __init__(self, layers, tensors):
        super(Partition6, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:6')
        self.lookup = { 'l_0': 'transformer.blocks.31.ln_2',
                        'l_1': 'transformer.blocks.31.mlp.c_fc',
                        'l_2': 'transformer.blocks.31.mlp.act',
                        'l_3': 'transformer.blocks.31.mlp.c_proj',
                        'l_4': 'transformer.blocks.31.mlp.dropout',
                        'l_5': 'transformer.blocks.32.ln_1',
                        'l_6': 'transformer.blocks.32.attn.c_attn',
                        'l_7': 'transformer.blocks.32.attn.attn_dropout',
                        'l_8': 'transformer.blocks.32.attn.c_proj',
                        'l_9': 'transformer.blocks.32.attn.resid_dropout',
                        'l_10': 'transformer.blocks.32.ln_2',
                        'l_11': 'transformer.blocks.32.mlp.c_fc',
                        'l_12': 'transformer.blocks.32.mlp.act',
                        'l_13': 'transformer.blocks.32.mlp.c_proj',
                        'l_14': 'transformer.blocks.32.mlp.dropout',
                        'l_15': 'transformer.blocks.33.ln_1',
                        'l_16': 'transformer.blocks.33.attn.c_attn',
                        'l_17': 'transformer.blocks.33.attn.attn_dropout',
                        'l_18': 'transformer.blocks.33.attn.c_proj',
                        'l_19': 'transformer.blocks.33.attn.resid_dropout',
                        'l_20': 'transformer.blocks.33.ln_2',
                        'l_21': 'transformer.blocks.33.mlp.c_fc',
                        'l_22': 'transformer.blocks.33.mlp.act',
                        'l_23': 'transformer.blocks.33.mlp.c_proj',
                        'l_24': 'transformer.blocks.33.mlp.dropout',
                        'l_25': 'transformer.blocks.34.ln_1',
                        'l_26': 'transformer.blocks.34.attn.c_attn',
                        'l_27': 'transformer.blocks.34.attn.attn_dropout',
                        'l_28': 'transformer.blocks.34.attn.c_proj',
                        'l_29': 'transformer.blocks.34.attn.resid_dropout',
                        'l_30': 'transformer.blocks.34.ln_2',
                        'l_31': 'transformer.blocks.34.mlp.c_fc',
                        'l_32': 'transformer.blocks.34.mlp.act',
                        'l_33': 'transformer.blocks.34.mlp.c_proj',
                        'l_34': 'transformer.blocks.34.mlp.dropout',
                        'l_35': 'transformer.blocks.35.ln_1',
                        'l_36': 'transformer.blocks.35.attn.c_attn',
                        'l_37': 'transformer.blocks.35.attn.attn_dropout',
                        'l_38': 'transformer.blocks.35.attn.c_proj',
                        'l_39': 'transformer.blocks.35.attn.resid_dropout',
                        'l_40': 'transformer.blocks.35.ln_2',
                        'l_41': 'transformer.blocks.35.mlp.c_fc',
                        'l_42': 'transformer.blocks.35.mlp.act',
                        'l_43': 'transformer.blocks.35.mlp.c_proj',
                        'l_44': 'transformer.blocks.35.mlp.dropout',
                        'l_45': 'transformer.blocks.36.ln_1',
                        'l_46': 'transformer.blocks.36.attn.c_attn',
                        'l_47': 'transformer.blocks.36.attn.attn_dropout',
                        'l_48': 'transformer.blocks.36.attn.c_proj',
                        'l_49': 'transformer.blocks.36.attn.resid_dropout',
                        'l_50': 'transformer.blocks.36.ln_2',
                        'l_51': 'transformer.blocks.36.mlp.c_fc',
                        'l_52': 'transformer.blocks.36.mlp.act',
                        'l_53': 'transformer.blocks.36.mlp.c_proj',
                        'l_54': 'transformer.blocks.36.mlp.dropout',
                        'l_55': 'transformer.blocks.37.ln_1',
                        'l_56': 'transformer.blocks.37.attn.c_attn',
                        'l_57': 'transformer.blocks.37.attn.attn_dropout',
                        'l_58': 'transformer.blocks.37.attn.c_proj',
                        'l_59': 'transformer.blocks.37.attn.resid_dropout',
                        'b_0': 'transformer.blocks.32.attn.bias',
                        'b_1': 'transformer.blocks.33.attn.bias',
                        'b_2': 'transformer.blocks.34.attn.bias',
                        'b_3': 'transformer.blocks.35.attn.bias',
                        'b_4': 'transformer.blocks.36.attn.bias',
                        'b_5': 'transformer.blocks.37.attn.bias'}

    def forward(self, x0):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Gelu[act] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_attn] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[attn_dropout] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_proj] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[resid_dropout] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Gelu[act] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_attn] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[attn_dropout] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_proj] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[resid_dropout] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Gelu[act] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_attn] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[attn_dropout] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_proj] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[resid_dropout] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Gelu[act] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_attn] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[attn_dropout] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_proj] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[resid_dropout] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Gelu[act] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_attn] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[attn_dropout] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_proj] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[resid_dropout] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Gelu[act] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1] <=> self.l_55
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_attn] <=> self.l_56
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[attn_dropout] <=> self.l_57
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_proj] <=> self.l_58
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[resid_dropout] <=> self.l_59
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Tensor::__add__ <=> x0

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)

        t_0 = self.l_0(x0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x0 + t_0
        del x0
        t_1 = self.l_5(t_0)
        t_1 = self.l_6(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_2 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_7 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_7, t_4)
        del t_7
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_3.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_2 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_3.view(t_6, t_5, t_2, t_7)
        del t_2
        del t_5
        del t_6
        del t_3
        t_7 = t_7.permute(0, 2, 3, 1)
        t_2 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_1.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_1.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_1
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_4, t_7)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_7 / t_4
        del t_7
        t_7 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_7 = t_3 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_7, t_3)
        del t_7
        del t_5
        del t_6
        t_3 = self.b_0[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_7(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_7 = t_2.size(-1)
        t_7 = t_3 * t_7
        del t_3
        t_7 = (t_7,)
        t_7 = t_4 + t_7
        del t_4
        t_4 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_2.view(t_4, t_3, t_7)
        del t_3
        del t_4
        del t_2
        t_7 = self.l_8(t_7)
        t_7 = self.l_9(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_10(t_7)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_0 = self.l_14(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_15(t_0)
        t_7 = self.l_16(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_3 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_2 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_3.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_1 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_5, t_6, t_1, t_2)
        del t_1
        del t_6
        del t_5
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_1 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_4.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_3 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_4.view(t_6, t_5, t_3, t_1)
        del t_3
        del t_5
        del t_6
        del t_4
        t_1 = t_1.permute(0, 2, 3, 1)
        t_3 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_7.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_7.view(t_5, t_6, t_4, t_3)
        del t_4
        del t_6
        del t_5
        del t_7
        t_3 = t_3.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_2, t_1)
        del t_2
        t_2 = t_3.size(-1)
        t_2 = math.sqrt(t_2)
        t_2 = t_1 / t_2
        del t_1
        t_1 = t_2.size(-2)
        t_4 = t_2.size(-1)
        t_1 = t_4 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_4, None)
        t_4 = slice(None, t_4, None)
        t_4 = (t_6, t_5, t_1, t_4)
        del t_1
        del t_5
        del t_6
        t_4 = self.b_1[t_4]
        t_2 = t_2 * t_4
        t_4 = 1 - t_4
        t_4 = 10000.0 * t_4
        t_4 = t_2 - t_4
        del t_2
        t_4 = torch.softmax(t_4, dim=-1)
        t_4 = self.l_17(t_4)
        t_3 = torch.matmul(t_4, t_3)
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_4 = t_3.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        del t_4
        t_4 = t_3.size(-2)
        t_1 = t_3.size(-1)
        t_1 = t_4 * t_1
        del t_4
        t_1 = (t_1,)
        t_1 = t_2 + t_1
        del t_2
        t_2 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_3.view(t_2, t_4, t_1)
        del t_4
        del t_2
        del t_3
        t_1 = self.l_18(t_1)
        t_1 = self.l_19(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_20(t_1)
        t_0 = self.l_21(t_0)
        t_0 = self.l_22(t_0)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_25(t_0)
        t_1 = self.l_26(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_4 = t_1[0]
        t_2 = t_1[1]
        t_1 = t_1[2]
        t_3 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_4.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_7 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_5, t_6, t_7, t_3)
        del t_7
        del t_6
        del t_5
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_7 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_2.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_4 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_2.view(t_6, t_5, t_4, t_7)
        del t_4
        del t_5
        del t_6
        del t_2
        t_7 = t_7.permute(0, 2, 3, 1)
        t_4 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_1.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_1.view(t_5, t_6, t_2, t_4)
        del t_2
        del t_6
        del t_5
        del t_1
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_3, t_7)
        del t_3
        t_3 = t_4.size(-1)
        t_3 = math.sqrt(t_3)
        t_3 = t_7 / t_3
        del t_7
        t_7 = t_3.size(-2)
        t_2 = t_3.size(-1)
        t_7 = t_2 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_5, t_7, t_2)
        del t_7
        del t_5
        del t_6
        t_2 = self.b_2[t_2]
        t_3 = t_3 * t_2
        t_2 = 1 - t_2
        t_2 = 10000.0 * t_2
        t_2 = t_3 - t_2
        del t_3
        t_2 = torch.softmax(t_2, dim=-1)
        t_2 = self.l_27(t_2)
        t_4 = torch.matmul(t_2, t_4)
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_3 = slice(None, -2, None)
        t_3 = t_2[t_3]
        del t_2
        t_2 = t_4.size(-2)
        t_7 = t_4.size(-1)
        t_7 = t_2 * t_7
        del t_2
        t_7 = (t_7,)
        t_7 = t_3 + t_7
        del t_3
        t_3 = t_7[0]
        t_2 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_4.view(t_3, t_2, t_7)
        del t_2
        del t_3
        del t_4
        t_7 = self.l_28(t_7)
        t_7 = self.l_29(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_30(t_7)
        t_0 = self.l_31(t_0)
        t_0 = self.l_32(t_0)
        t_0 = self.l_33(t_0)
        t_0 = self.l_34(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_35(t_0)
        t_7 = self.l_36(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_2 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_1 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_1, t_4)
        del t_1
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_1 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_3.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_2 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_3.view(t_6, t_5, t_2, t_1)
        del t_2
        del t_5
        del t_6
        del t_3
        t_1 = t_1.permute(0, 2, 3, 1)
        t_2 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_7.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_7.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_7
        t_2 = t_2.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_4, t_1)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_1 / t_4
        del t_1
        t_1 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_1 = t_3 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_1, t_3)
        del t_1
        del t_5
        del t_6
        t_3 = self.b_3[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_37(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_1 = t_2.size(-1)
        t_1 = t_3 * t_1
        del t_3
        t_1 = (t_1,)
        t_1 = t_4 + t_1
        del t_4
        t_4 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_2.view(t_4, t_3, t_1)
        del t_3
        del t_4
        del t_2
        t_1 = self.l_38(t_1)
        t_1 = self.l_39(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_40(t_1)
        t_0 = self.l_41(t_0)
        t_0 = self.l_42(t_0)
        t_0 = self.l_43(t_0)
        t_0 = self.l_44(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_45(t_0)
        t_1 = self.l_46(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_3 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        t_2 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_3.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_7 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_5, t_6, t_7, t_2)
        del t_7
        del t_6
        del t_5
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_4.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_3 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_4.view(t_6, t_5, t_3, t_7)
        del t_3
        del t_5
        del t_6
        del t_4
        t_7 = t_7.permute(0, 2, 3, 1)
        t_3 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_1.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_1.view(t_5, t_6, t_4, t_3)
        del t_4
        del t_6
        del t_5
        del t_1
        t_3 = t_3.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_2, t_7)
        del t_2
        t_2 = t_3.size(-1)
        t_2 = math.sqrt(t_2)
        t_2 = t_7 / t_2
        del t_7
        t_7 = t_2.size(-2)
        t_4 = t_2.size(-1)
        t_7 = t_4 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_4, None)
        t_4 = slice(None, t_4, None)
        t_4 = (t_6, t_5, t_7, t_4)
        del t_7
        del t_5
        del t_6
        t_4 = self.b_4[t_4]
        t_2 = t_2 * t_4
        t_4 = 1 - t_4
        t_4 = 10000.0 * t_4
        t_4 = t_2 - t_4
        del t_2
        t_4 = torch.softmax(t_4, dim=-1)
        t_4 = self.l_47(t_4)
        t_3 = torch.matmul(t_4, t_3)
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_4 = t_3.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        del t_4
        t_4 = t_3.size(-2)
        t_7 = t_3.size(-1)
        t_7 = t_4 * t_7
        del t_4
        t_7 = (t_7,)
        t_7 = t_2 + t_7
        del t_2
        t_2 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_3.view(t_2, t_4, t_7)
        del t_4
        del t_2
        del t_3
        t_7 = self.l_48(t_7)
        t_7 = self.l_49(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_50(t_7)
        t_0 = self.l_51(t_0)
        t_0 = self.l_52(t_0)
        t_0 = self.l_53(t_0)
        t_0 = self.l_54(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_55(t_0)
        t_7 = self.l_56(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_4 = t_7[0]
        t_2 = t_7[1]
        t_7 = t_7[2]
        t_3 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_4.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_1 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_5, t_6, t_1, t_3)
        del t_1
        del t_6
        del t_5
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_1 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_2.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_4 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_2.view(t_6, t_5, t_4, t_1)
        del t_4
        del t_5
        del t_6
        del t_2
        t_1 = t_1.permute(0, 2, 3, 1)
        t_4 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_7.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_7.view(t_5, t_6, t_2, t_4)
        del t_2
        del t_6
        del t_5
        del t_7
        t_4 = t_4.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_3, t_1)
        del t_3
        t_3 = t_4.size(-1)
        t_3 = math.sqrt(t_3)
        t_3 = t_1 / t_3
        del t_1
        t_1 = t_3.size(-2)
        t_2 = t_3.size(-1)
        t_1 = t_2 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_5, t_1, t_2)
        del t_1
        del t_5
        del t_6
        t_2 = self.b_5[t_2]
        t_3 = t_3 * t_2
        t_2 = 1 - t_2
        t_2 = 10000.0 * t_2
        t_2 = t_3 - t_2
        del t_3
        t_2 = torch.softmax(t_2, dim=-1)
        t_2 = self.l_57(t_2)
        t_4 = torch.matmul(t_2, t_4)
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_3 = slice(None, -2, None)
        t_3 = t_2[t_3]
        del t_2
        t_2 = t_4.size(-2)
        t_1 = t_4.size(-1)
        t_1 = t_2 * t_1
        del t_2
        t_1 = (t_1,)
        t_1 = t_3 + t_1
        del t_3
        t_3 = t_1[0]
        t_2 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_4.view(t_3, t_2, t_1)
        del t_2
        del t_3
        del t_4
        t_1 = self.l_58(t_1)
        t_1 = self.l_59(t_1)
        t_1 = t_0 + t_1
        del t_0
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Tensor::__add__
        return (t_1,)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition7(nn.Module):
    BASIC_BLOCKS=(
            Conv1D,
            Dropout,
            Gelu,
            LayerNorm,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[attn_dropout]',
        ]
    TENSORS=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Tensor[bias]',
        ]
    def __init__(self, layers, tensors):
        super(Partition7, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:7')
        self.lookup = { 'l_0': 'transformer.blocks.37.ln_2',
                        'l_1': 'transformer.blocks.37.mlp.c_fc',
                        'l_2': 'transformer.blocks.37.mlp.act',
                        'l_3': 'transformer.blocks.37.mlp.c_proj',
                        'l_4': 'transformer.blocks.37.mlp.dropout',
                        'l_5': 'transformer.blocks.38.ln_1',
                        'l_6': 'transformer.blocks.38.attn.c_attn',
                        'l_7': 'transformer.blocks.38.attn.attn_dropout',
                        'l_8': 'transformer.blocks.38.attn.c_proj',
                        'l_9': 'transformer.blocks.38.attn.resid_dropout',
                        'l_10': 'transformer.blocks.38.ln_2',
                        'l_11': 'transformer.blocks.38.mlp.c_fc',
                        'l_12': 'transformer.blocks.38.mlp.act',
                        'l_13': 'transformer.blocks.38.mlp.c_proj',
                        'l_14': 'transformer.blocks.38.mlp.dropout',
                        'l_15': 'transformer.blocks.39.ln_1',
                        'l_16': 'transformer.blocks.39.attn.c_attn',
                        'l_17': 'transformer.blocks.39.attn.attn_dropout',
                        'l_18': 'transformer.blocks.39.attn.c_proj',
                        'l_19': 'transformer.blocks.39.attn.resid_dropout',
                        'l_20': 'transformer.blocks.39.ln_2',
                        'l_21': 'transformer.blocks.39.mlp.c_fc',
                        'l_22': 'transformer.blocks.39.mlp.act',
                        'l_23': 'transformer.blocks.39.mlp.c_proj',
                        'l_24': 'transformer.blocks.39.mlp.dropout',
                        'l_25': 'transformer.blocks.40.ln_1',
                        'l_26': 'transformer.blocks.40.attn.c_attn',
                        'l_27': 'transformer.blocks.40.attn.attn_dropout',
                        'l_28': 'transformer.blocks.40.attn.c_proj',
                        'l_29': 'transformer.blocks.40.attn.resid_dropout',
                        'l_30': 'transformer.blocks.40.ln_2',
                        'l_31': 'transformer.blocks.40.mlp.c_fc',
                        'l_32': 'transformer.blocks.40.mlp.act',
                        'l_33': 'transformer.blocks.40.mlp.c_proj',
                        'l_34': 'transformer.blocks.40.mlp.dropout',
                        'l_35': 'transformer.blocks.41.ln_1',
                        'l_36': 'transformer.blocks.41.attn.c_attn',
                        'l_37': 'transformer.blocks.41.attn.attn_dropout',
                        'l_38': 'transformer.blocks.41.attn.c_proj',
                        'l_39': 'transformer.blocks.41.attn.resid_dropout',
                        'l_40': 'transformer.blocks.41.ln_2',
                        'l_41': 'transformer.blocks.41.mlp.c_fc',
                        'l_42': 'transformer.blocks.41.mlp.act',
                        'l_43': 'transformer.blocks.41.mlp.c_proj',
                        'l_44': 'transformer.blocks.41.mlp.dropout',
                        'l_45': 'transformer.blocks.42.ln_1',
                        'l_46': 'transformer.blocks.42.attn.c_attn',
                        'l_47': 'transformer.blocks.42.attn.attn_dropout',
                        'l_48': 'transformer.blocks.42.attn.c_proj',
                        'l_49': 'transformer.blocks.42.attn.resid_dropout',
                        'l_50': 'transformer.blocks.42.ln_2',
                        'l_51': 'transformer.blocks.42.mlp.c_fc',
                        'l_52': 'transformer.blocks.42.mlp.act',
                        'l_53': 'transformer.blocks.42.mlp.c_proj',
                        'l_54': 'transformer.blocks.42.mlp.dropout',
                        'l_55': 'transformer.blocks.43.ln_1',
                        'l_56': 'transformer.blocks.43.attn.c_attn',
                        'l_57': 'transformer.blocks.43.attn.attn_dropout',
                        'b_0': 'transformer.blocks.38.attn.bias',
                        'b_1': 'transformer.blocks.39.attn.bias',
                        'b_2': 'transformer.blocks.40.attn.bias',
                        'b_3': 'transformer.blocks.41.attn.bias',
                        'b_4': 'transformer.blocks.42.attn.bias',
                        'b_5': 'transformer.blocks.43.attn.bias'}

    def forward(self, x0):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Gelu[act] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_attn] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[attn_dropout] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_proj] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[resid_dropout] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Gelu[act] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_attn] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[attn_dropout] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_proj] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[resid_dropout] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Gelu[act] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_attn] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[attn_dropout] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_proj] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[resid_dropout] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Gelu[act] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_attn] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[attn_dropout] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_proj] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[resid_dropout] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Gelu[act] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_attn] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[attn_dropout] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_proj] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[resid_dropout] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Gelu[act] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1] <=> self.l_55
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_attn] <=> self.l_56
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[attn_dropout] <=> self.l_57
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Tensor::__add__ <=> x0

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)

        t_0 = self.l_0(x0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x0 + t_0
        del x0
        t_1 = self.l_5(t_0)
        t_1 = self.l_6(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_2 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_7 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_7, t_4)
        del t_7
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_3.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_2 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_3.view(t_6, t_5, t_2, t_7)
        del t_2
        del t_5
        del t_6
        del t_3
        t_7 = t_7.permute(0, 2, 3, 1)
        t_2 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_1.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_1.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_1
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_4, t_7)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_7 / t_4
        del t_7
        t_7 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_7 = t_3 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_7, t_3)
        del t_7
        del t_5
        del t_6
        t_3 = self.b_0[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_7(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_7 = t_2.size(-1)
        t_7 = t_3 * t_7
        del t_3
        t_7 = (t_7,)
        t_7 = t_4 + t_7
        del t_4
        t_4 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_2.view(t_4, t_3, t_7)
        del t_3
        del t_4
        del t_2
        t_7 = self.l_8(t_7)
        t_7 = self.l_9(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_10(t_7)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_0 = self.l_14(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_15(t_0)
        t_7 = self.l_16(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_3 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_2 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_3.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_1 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_5, t_6, t_1, t_2)
        del t_1
        del t_6
        del t_5
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_1 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_4.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_3 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_4.view(t_6, t_5, t_3, t_1)
        del t_3
        del t_5
        del t_6
        del t_4
        t_1 = t_1.permute(0, 2, 3, 1)
        t_3 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_7.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_7.view(t_5, t_6, t_4, t_3)
        del t_4
        del t_6
        del t_5
        del t_7
        t_3 = t_3.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_2, t_1)
        del t_2
        t_2 = t_3.size(-1)
        t_2 = math.sqrt(t_2)
        t_2 = t_1 / t_2
        del t_1
        t_1 = t_2.size(-2)
        t_4 = t_2.size(-1)
        t_1 = t_4 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_4, None)
        t_4 = slice(None, t_4, None)
        t_4 = (t_6, t_5, t_1, t_4)
        del t_1
        del t_5
        del t_6
        t_4 = self.b_1[t_4]
        t_2 = t_2 * t_4
        t_4 = 1 - t_4
        t_4 = 10000.0 * t_4
        t_4 = t_2 - t_4
        del t_2
        t_4 = torch.softmax(t_4, dim=-1)
        t_4 = self.l_17(t_4)
        t_3 = torch.matmul(t_4, t_3)
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_4 = t_3.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        del t_4
        t_4 = t_3.size(-2)
        t_1 = t_3.size(-1)
        t_1 = t_4 * t_1
        del t_4
        t_1 = (t_1,)
        t_1 = t_2 + t_1
        del t_2
        t_2 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_3.view(t_2, t_4, t_1)
        del t_4
        del t_2
        del t_3
        t_1 = self.l_18(t_1)
        t_1 = self.l_19(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_20(t_1)
        t_0 = self.l_21(t_0)
        t_0 = self.l_22(t_0)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_25(t_0)
        t_1 = self.l_26(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_4 = t_1[0]
        t_2 = t_1[1]
        t_1 = t_1[2]
        t_3 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_4.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_7 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_5, t_6, t_7, t_3)
        del t_7
        del t_6
        del t_5
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_7 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_2.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_4 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_2.view(t_6, t_5, t_4, t_7)
        del t_4
        del t_5
        del t_6
        del t_2
        t_7 = t_7.permute(0, 2, 3, 1)
        t_4 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_1.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_1.view(t_5, t_6, t_2, t_4)
        del t_2
        del t_6
        del t_5
        del t_1
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_3, t_7)
        del t_3
        t_3 = t_4.size(-1)
        t_3 = math.sqrt(t_3)
        t_3 = t_7 / t_3
        del t_7
        t_7 = t_3.size(-2)
        t_2 = t_3.size(-1)
        t_7 = t_2 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_5, t_7, t_2)
        del t_7
        del t_5
        del t_6
        t_2 = self.b_2[t_2]
        t_3 = t_3 * t_2
        t_2 = 1 - t_2
        t_2 = 10000.0 * t_2
        t_2 = t_3 - t_2
        del t_3
        t_2 = torch.softmax(t_2, dim=-1)
        t_2 = self.l_27(t_2)
        t_4 = torch.matmul(t_2, t_4)
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_3 = slice(None, -2, None)
        t_3 = t_2[t_3]
        del t_2
        t_2 = t_4.size(-2)
        t_7 = t_4.size(-1)
        t_7 = t_2 * t_7
        del t_2
        t_7 = (t_7,)
        t_7 = t_3 + t_7
        del t_3
        t_3 = t_7[0]
        t_2 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_4.view(t_3, t_2, t_7)
        del t_2
        del t_3
        del t_4
        t_7 = self.l_28(t_7)
        t_7 = self.l_29(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_30(t_7)
        t_0 = self.l_31(t_0)
        t_0 = self.l_32(t_0)
        t_0 = self.l_33(t_0)
        t_0 = self.l_34(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_35(t_0)
        t_7 = self.l_36(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_2 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_1 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_1, t_4)
        del t_1
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_1 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_3.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_2 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_3.view(t_6, t_5, t_2, t_1)
        del t_2
        del t_5
        del t_6
        del t_3
        t_1 = t_1.permute(0, 2, 3, 1)
        t_2 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_7.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_7.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_7
        t_2 = t_2.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_4, t_1)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_1 / t_4
        del t_1
        t_1 = t_4.size(-2)
        t_3 = t_4.size(-1)
        t_1 = t_3 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_3, None)
        t_3 = slice(None, t_3, None)
        t_3 = (t_6, t_5, t_1, t_3)
        del t_1
        del t_5
        del t_6
        t_3 = self.b_3[t_3]
        t_4 = t_4 * t_3
        t_3 = 1 - t_3
        t_3 = 10000.0 * t_3
        t_3 = t_4 - t_3
        del t_4
        t_3 = torch.softmax(t_3, dim=-1)
        t_3 = self.l_37(t_3)
        t_2 = torch.matmul(t_3, t_2)
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        del t_3
        t_3 = t_2.size(-2)
        t_1 = t_2.size(-1)
        t_1 = t_3 * t_1
        del t_3
        t_1 = (t_1,)
        t_1 = t_4 + t_1
        del t_4
        t_4 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_2.view(t_4, t_3, t_1)
        del t_3
        del t_4
        del t_2
        t_1 = self.l_38(t_1)
        t_1 = self.l_39(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_40(t_1)
        t_0 = self.l_41(t_0)
        t_0 = self.l_42(t_0)
        t_0 = self.l_43(t_0)
        t_0 = self.l_44(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_45(t_0)
        t_1 = self.l_46(t_1)
        t_1 = t_1.split(1600, dim=2)
        t_3 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        t_2 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_3.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_7 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_5, t_6, t_7, t_2)
        del t_7
        del t_6
        del t_5
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_4.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_3 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_4.view(t_6, t_5, t_3, t_7)
        del t_3
        del t_5
        del t_6
        del t_4
        t_7 = t_7.permute(0, 2, 3, 1)
        t_3 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_1.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_1.view(t_5, t_6, t_4, t_3)
        del t_4
        del t_6
        del t_5
        del t_1
        t_3 = t_3.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_2, t_7)
        del t_2
        t_2 = t_3.size(-1)
        t_2 = math.sqrt(t_2)
        t_2 = t_7 / t_2
        del t_7
        t_7 = t_2.size(-2)
        t_4 = t_2.size(-1)
        t_7 = t_4 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_4, None)
        t_4 = slice(None, t_4, None)
        t_4 = (t_6, t_5, t_7, t_4)
        del t_7
        del t_5
        del t_6
        t_4 = self.b_4[t_4]
        t_2 = t_2 * t_4
        t_4 = 1 - t_4
        t_4 = 10000.0 * t_4
        t_4 = t_2 - t_4
        del t_2
        t_4 = torch.softmax(t_4, dim=-1)
        t_4 = self.l_47(t_4)
        t_3 = torch.matmul(t_4, t_3)
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_4 = t_3.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        del t_4
        t_4 = t_3.size(-2)
        t_7 = t_3.size(-1)
        t_7 = t_4 * t_7
        del t_4
        t_7 = (t_7,)
        t_7 = t_2 + t_7
        del t_2
        t_2 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_3.view(t_2, t_4, t_7)
        del t_4
        del t_2
        del t_3
        t_7 = self.l_48(t_7)
        t_7 = self.l_49(t_7)
        t_7 = t_0 + t_7
        del t_0
        t_0 = self.l_50(t_7)
        t_0 = self.l_51(t_0)
        t_0 = self.l_52(t_0)
        t_0 = self.l_53(t_0)
        t_0 = self.l_54(t_0)
        t_0 = t_7 + t_0
        del t_7
        t_7 = self.l_55(t_0)
        t_7 = self.l_56(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_4 = t_7[0]
        t_2 = t_7[1]
        t_7 = t_7[2]
        t_3 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_3[t_5]
        del t_3
        t_3 = t_4.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_5 + t_3
        del t_5
        t_5 = t_3[0]
        t_6 = t_3[1]
        t_1 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_5, t_6, t_1, t_3)
        del t_1
        del t_6
        del t_5
        del t_4
        t_3 = t_3.permute(0, 2, 1, 3)
        t_1 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        del t_1
        t_1 = t_2.size(-1)
        t_1 = t_1 // 25
        t_1 = (25, t_1)
        t_1 = t_6 + t_1
        del t_6
        t_6 = t_1[0]
        t_5 = t_1[1]
        t_4 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_2.view(t_6, t_5, t_4, t_1)
        del t_4
        del t_5
        del t_6
        del t_2
        t_1 = t_1.permute(0, 2, 3, 1)
        t_4 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_7.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_7.view(t_5, t_6, t_2, t_4)
        del t_2
        del t_6
        del t_5
        del t_7
        t_4 = t_4.permute(0, 2, 1, 3)
        t_1 = torch.matmul(t_3, t_1)
        del t_3
        t_3 = t_4.size(-1)
        t_3 = math.sqrt(t_3)
        t_3 = t_1 / t_3
        del t_1
        t_1 = t_3.size(-2)
        t_2 = t_3.size(-1)
        t_1 = t_2 - t_1
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_1 = slice(t_1, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_5, t_1, t_2)
        del t_1
        del t_5
        del t_6
        t_2 = self.b_5[t_2]
        t_3 = t_3 * t_2
        t_2 = 1 - t_2
        t_2 = 10000.0 * t_2
        t_2 = t_3 - t_2
        del t_3
        t_2 = torch.softmax(t_2, dim=-1)
        t_2 = self.l_57(t_2)
        t_4 = torch.matmul(t_2, t_4)
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Tensor::__add__
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Tensor::permute
        return (t_0, t_4)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition8(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Conv1D,
            LMOutput,
            StatelessLinear,
            Dropout,
            Gelu,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[attn_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[resid_dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]',
            'GPT2LMHeadModel/StatelessLinear[stateless_lm_head]',
            'GPT2LMHeadModel/LMOutput[compute_output]',
        ]
    TENSORS=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Tensor[bias]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Tensor[bias]',
        ]
    def __init__(self, layers, tensors):
        super(Partition8, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:0')
        self.lookup = { 'l_0': 'transformer.blocks.43.attn.c_proj',
                        'l_1': 'transformer.blocks.43.attn.resid_dropout',
                        'l_2': 'transformer.blocks.43.ln_2',
                        'l_3': 'transformer.blocks.43.mlp.c_fc',
                        'l_4': 'transformer.blocks.43.mlp.act',
                        'l_5': 'transformer.blocks.43.mlp.c_proj',
                        'l_6': 'transformer.blocks.43.mlp.dropout',
                        'l_7': 'transformer.blocks.44.ln_1',
                        'l_8': 'transformer.blocks.44.attn.c_attn',
                        'l_9': 'transformer.blocks.44.attn.attn_dropout',
                        'l_10': 'transformer.blocks.44.attn.c_proj',
                        'l_11': 'transformer.blocks.44.attn.resid_dropout',
                        'l_12': 'transformer.blocks.44.ln_2',
                        'l_13': 'transformer.blocks.44.mlp.c_fc',
                        'l_14': 'transformer.blocks.44.mlp.act',
                        'l_15': 'transformer.blocks.44.mlp.c_proj',
                        'l_16': 'transformer.blocks.44.mlp.dropout',
                        'l_17': 'transformer.blocks.45.ln_1',
                        'l_18': 'transformer.blocks.45.attn.c_attn',
                        'l_19': 'transformer.blocks.45.attn.attn_dropout',
                        'l_20': 'transformer.blocks.45.attn.c_proj',
                        'l_21': 'transformer.blocks.45.attn.resid_dropout',
                        'l_22': 'transformer.blocks.45.ln_2',
                        'l_23': 'transformer.blocks.45.mlp.c_fc',
                        'l_24': 'transformer.blocks.45.mlp.act',
                        'l_25': 'transformer.blocks.45.mlp.c_proj',
                        'l_26': 'transformer.blocks.45.mlp.dropout',
                        'l_27': 'transformer.blocks.46.ln_1',
                        'l_28': 'transformer.blocks.46.attn.c_attn',
                        'l_29': 'transformer.blocks.46.attn.attn_dropout',
                        'l_30': 'transformer.blocks.46.attn.c_proj',
                        'l_31': 'transformer.blocks.46.attn.resid_dropout',
                        'l_32': 'transformer.blocks.46.ln_2',
                        'l_33': 'transformer.blocks.46.mlp.c_fc',
                        'l_34': 'transformer.blocks.46.mlp.act',
                        'l_35': 'transformer.blocks.46.mlp.c_proj',
                        'l_36': 'transformer.blocks.46.mlp.dropout',
                        'l_37': 'transformer.blocks.47.ln_1',
                        'l_38': 'transformer.blocks.47.attn.c_attn',
                        'l_39': 'transformer.blocks.47.attn.attn_dropout',
                        'l_40': 'transformer.blocks.47.attn.c_proj',
                        'l_41': 'transformer.blocks.47.attn.resid_dropout',
                        'l_42': 'transformer.blocks.47.ln_2',
                        'l_43': 'transformer.blocks.47.mlp.c_fc',
                        'l_44': 'transformer.blocks.47.mlp.act',
                        'l_45': 'transformer.blocks.47.mlp.c_proj',
                        'l_46': 'transformer.blocks.47.mlp.dropout',
                        'l_47': 'transformer.ln_f',
                        'l_48': 'stateless_lm_head',
                        'l_49': 'compute_output',
                        'b_0': 'transformer.blocks.44.attn.bias',
                        'b_1': 'transformer.blocks.45.attn.bias',
                        'b_2': 'transformer.blocks.46.attn.bias',
                        'b_3': 'transformer.blocks.47.attn.bias'}

    def forward(self, x0, x1, x2, x3):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_proj] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[resid_dropout] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Gelu[act] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_attn] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[attn_dropout] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_proj] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[resid_dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Gelu[act] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_attn] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[attn_dropout] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_proj] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[resid_dropout] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Gelu[act] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_attn] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[attn_dropout] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_proj] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[resid_dropout] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Gelu[act] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_attn] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[attn_dropout] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_proj] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[resid_dropout] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Gelu[act] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f] <=> self.l_47
        # GPT2LMHeadModel/StatelessLinear[stateless_lm_head] <=> self.l_48
        # GPT2LMHeadModel/LMOutput[compute_output] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Tensor[bias] <=> self.b_3
        # input1 <=> x0
        # GPT2LMHeadModel/Parameter[w_wte] <=> x1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Tensor::__add__ <=> x2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Tensor::permute <=> x3

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)
        x2 = x2.to(self.device)
        x3 = x3.to(self.device)

        t_0 = x3.contiguous()
        del x3
        t_1 = t_0.size()
        t_2 = slice(None, -2, None)
        t_2 = t_1[t_2]
        del t_1
        t_1 = t_0.size(-2)
        t_3 = t_0.size(-1)
        t_3 = t_1 * t_3
        del t_1
        t_3 = (t_3,)
        t_3 = t_2 + t_3
        del t_2
        t_2 = t_3[0]
        t_1 = t_3[1]
        t_3 = t_3[2]
        t_3 = t_0.view(t_2, t_1, t_3)
        del t_1
        del t_2
        del t_0
        t_3 = self.l_0(t_3)
        t_3 = self.l_1(t_3)
        t_3 = x2 + t_3
        del x2
        t_1 = self.l_2(t_3)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = t_3 + t_1
        del t_3
        t_3 = self.l_7(t_1)
        t_3 = self.l_8(t_3)
        t_3 = t_3.split(1600, dim=2)
        t_2 = t_3[0]
        t_0 = t_3[1]
        t_3 = t_3[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_7 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_7, t_4)
        del t_7
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = t_0.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_0.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_2 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_0.view(t_6, t_5, t_2, t_7)
        del t_2
        del t_5
        del t_6
        del t_0
        t_7 = t_7.permute(0, 2, 3, 1)
        t_2 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_3.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_0 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_5, t_6, t_0, t_2)
        del t_0
        del t_6
        del t_5
        del t_3
        t_2 = t_2.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_4, t_7)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_7 / t_4
        del t_7
        t_7 = t_4.size(-2)
        t_0 = t_4.size(-1)
        t_7 = t_0 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_6, t_5, t_7, t_0)
        del t_7
        del t_5
        del t_6
        t_0 = self.b_0[t_0]
        t_4 = t_4 * t_0
        t_0 = 1 - t_0
        t_0 = 10000.0 * t_0
        t_0 = t_4 - t_0
        del t_4
        t_0 = torch.softmax(t_0, dim=-1)
        t_0 = self.l_9(t_0)
        t_2 = torch.matmul(t_0, t_2)
        del t_0
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_0 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_0[t_4]
        del t_0
        t_0 = t_2.size(-2)
        t_7 = t_2.size(-1)
        t_7 = t_0 * t_7
        del t_0
        t_7 = (t_7,)
        t_7 = t_4 + t_7
        del t_4
        t_4 = t_7[0]
        t_0 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_2.view(t_4, t_0, t_7)
        del t_0
        del t_4
        del t_2
        t_7 = self.l_10(t_7)
        t_7 = self.l_11(t_7)
        t_7 = t_1 + t_7
        del t_1
        t_1 = self.l_12(t_7)
        t_1 = self.l_13(t_1)
        t_1 = self.l_14(t_1)
        t_1 = self.l_15(t_1)
        t_1 = self.l_16(t_1)
        t_1 = t_7 + t_1
        del t_7
        t_7 = self.l_17(t_1)
        t_7 = self.l_18(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_0 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_2 = t_0.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_0.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_0.view(t_5, t_6, t_3, t_2)
        del t_3
        del t_6
        del t_5
        del t_0
        t_2 = t_2.permute(0, 2, 1, 3)
        t_3 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_3[t_6]
        del t_3
        t_3 = t_4.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_6 + t_3
        del t_6
        t_6 = t_3[0]
        t_5 = t_3[1]
        t_0 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_6, t_5, t_0, t_3)
        del t_0
        del t_5
        del t_6
        del t_4
        t_3 = t_3.permute(0, 2, 3, 1)
        t_0 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_0[t_5]
        del t_0
        t_0 = t_7.size(-1)
        t_0 = t_0 // 25
        t_0 = (25, t_0)
        t_0 = t_5 + t_0
        del t_5
        t_5 = t_0[0]
        t_6 = t_0[1]
        t_4 = t_0[2]
        t_0 = t_0[3]
        t_0 = t_7.view(t_5, t_6, t_4, t_0)
        del t_4
        del t_6
        del t_5
        del t_7
        t_0 = t_0.permute(0, 2, 1, 3)
        t_3 = torch.matmul(t_2, t_3)
        del t_2
        t_2 = t_0.size(-1)
        t_2 = math.sqrt(t_2)
        t_2 = t_3 / t_2
        del t_3
        t_3 = t_2.size(-2)
        t_4 = t_2.size(-1)
        t_3 = t_4 - t_3
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_3 = slice(t_3, t_4, None)
        t_4 = slice(None, t_4, None)
        t_4 = (t_6, t_5, t_3, t_4)
        del t_3
        del t_5
        del t_6
        t_4 = self.b_1[t_4]
        t_2 = t_2 * t_4
        t_4 = 1 - t_4
        t_4 = 10000.0 * t_4
        t_4 = t_2 - t_4
        del t_2
        t_4 = torch.softmax(t_4, dim=-1)
        t_4 = self.l_19(t_4)
        t_0 = torch.matmul(t_4, t_0)
        del t_4
        t_0 = t_0.permute(0, 2, 1, 3)
        t_0 = t_0.contiguous()
        t_4 = t_0.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        del t_4
        t_4 = t_0.size(-2)
        t_3 = t_0.size(-1)
        t_3 = t_4 * t_3
        del t_4
        t_3 = (t_3,)
        t_3 = t_2 + t_3
        del t_2
        t_2 = t_3[0]
        t_4 = t_3[1]
        t_3 = t_3[2]
        t_3 = t_0.view(t_2, t_4, t_3)
        del t_4
        del t_2
        del t_0
        t_3 = self.l_20(t_3)
        t_3 = self.l_21(t_3)
        t_3 = t_1 + t_3
        del t_1
        t_1 = self.l_22(t_3)
        t_1 = self.l_23(t_1)
        t_1 = self.l_24(t_1)
        t_1 = self.l_25(t_1)
        t_1 = self.l_26(t_1)
        t_1 = t_3 + t_1
        del t_3
        t_3 = self.l_27(t_1)
        t_3 = self.l_28(t_3)
        t_3 = t_3.split(1600, dim=2)
        t_4 = t_3[0]
        t_2 = t_3[1]
        t_3 = t_3[2]
        t_0 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_0[t_5]
        del t_0
        t_0 = t_4.size(-1)
        t_0 = t_0 // 25
        t_0 = (25, t_0)
        t_0 = t_5 + t_0
        del t_5
        t_5 = t_0[0]
        t_6 = t_0[1]
        t_7 = t_0[2]
        t_0 = t_0[3]
        t_0 = t_4.view(t_5, t_6, t_7, t_0)
        del t_7
        del t_6
        del t_5
        del t_4
        t_0 = t_0.permute(0, 2, 1, 3)
        t_7 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        del t_7
        t_7 = t_2.size(-1)
        t_7 = t_7 // 25
        t_7 = (25, t_7)
        t_7 = t_6 + t_7
        del t_6
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_4 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_2.view(t_6, t_5, t_4, t_7)
        del t_4
        del t_5
        del t_6
        del t_2
        t_7 = t_7.permute(0, 2, 3, 1)
        t_4 = t_3.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_3.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_3.view(t_5, t_6, t_2, t_4)
        del t_2
        del t_6
        del t_5
        del t_3
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = torch.matmul(t_0, t_7)
        del t_0
        t_0 = t_4.size(-1)
        t_0 = math.sqrt(t_0)
        t_0 = t_7 / t_0
        del t_7
        t_7 = t_0.size(-2)
        t_2 = t_0.size(-1)
        t_7 = t_2 - t_7
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_7 = slice(t_7, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_5, t_7, t_2)
        del t_7
        del t_5
        del t_6
        t_2 = self.b_2[t_2]
        t_0 = t_0 * t_2
        t_2 = 1 - t_2
        t_2 = 10000.0 * t_2
        t_2 = t_0 - t_2
        del t_0
        t_2 = torch.softmax(t_2, dim=-1)
        t_2 = self.l_29(t_2)
        t_4 = torch.matmul(t_2, t_4)
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_0 = slice(None, -2, None)
        t_0 = t_2[t_0]
        del t_2
        t_2 = t_4.size(-2)
        t_7 = t_4.size(-1)
        t_7 = t_2 * t_7
        del t_2
        t_7 = (t_7,)
        t_7 = t_0 + t_7
        del t_0
        t_0 = t_7[0]
        t_2 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_4.view(t_0, t_2, t_7)
        del t_2
        del t_0
        del t_4
        t_7 = self.l_30(t_7)
        t_7 = self.l_31(t_7)
        t_7 = t_1 + t_7
        del t_1
        t_1 = self.l_32(t_7)
        t_1 = self.l_33(t_1)
        t_1 = self.l_34(t_1)
        t_1 = self.l_35(t_1)
        t_1 = self.l_36(t_1)
        t_1 = t_7 + t_1
        del t_7
        t_7 = self.l_37(t_1)
        t_7 = self.l_38(t_7)
        t_7 = t_7.split(1600, dim=2)
        t_2 = t_7[0]
        t_0 = t_7[1]
        t_7 = t_7[2]
        t_4 = t_2.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        del t_4
        t_4 = t_2.size(-1)
        t_4 = t_4 // 25
        t_4 = (25, t_4)
        t_4 = t_5 + t_4
        del t_5
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_3 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_5, t_6, t_3, t_4)
        del t_3
        del t_6
        del t_5
        del t_2
        t_4 = t_4.permute(0, 2, 1, 3)
        t_3 = t_0.size()
        t_6 = slice(None, -1, None)
        t_6 = t_3[t_6]
        del t_3
        t_3 = t_0.size(-1)
        t_3 = t_3 // 25
        t_3 = (25, t_3)
        t_3 = t_6 + t_3
        del t_6
        t_6 = t_3[0]
        t_5 = t_3[1]
        t_2 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_0.view(t_6, t_5, t_2, t_3)
        del t_2
        del t_5
        del t_6
        del t_0
        t_3 = t_3.permute(0, 2, 3, 1)
        t_2 = t_7.size()
        t_5 = slice(None, -1, None)
        t_5 = t_2[t_5]
        del t_2
        t_2 = t_7.size(-1)
        t_2 = t_2 // 25
        t_2 = (25, t_2)
        t_2 = t_5 + t_2
        del t_5
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_0 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_7.view(t_5, t_6, t_0, t_2)
        del t_0
        del t_6
        del t_5
        del t_7
        t_2 = t_2.permute(0, 2, 1, 3)
        t_3 = torch.matmul(t_4, t_3)
        del t_4
        t_4 = t_2.size(-1)
        t_4 = math.sqrt(t_4)
        t_4 = t_3 / t_4
        del t_3
        t_3 = t_4.size(-2)
        t_0 = t_4.size(-1)
        t_3 = t_0 - t_3
        t_6 = slice(None, None, None)
        t_5 = slice(None, None, None)
        t_3 = slice(t_3, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_6, t_5, t_3, t_0)
        del t_3
        del t_5
        del t_6
        t_0 = self.b_3[t_0]
        t_4 = t_4 * t_0
        t_0 = 1 - t_0
        t_0 = 10000.0 * t_0
        t_0 = t_4 - t_0
        del t_4
        t_0 = torch.softmax(t_0, dim=-1)
        t_0 = self.l_39(t_0)
        t_2 = torch.matmul(t_0, t_2)
        del t_0
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_0 = t_2.size()
        t_4 = slice(None, -2, None)
        t_4 = t_0[t_4]
        del t_0
        t_0 = t_2.size(-2)
        t_3 = t_2.size(-1)
        t_3 = t_0 * t_3
        del t_0
        t_3 = (t_3,)
        t_3 = t_4 + t_3
        del t_4
        t_4 = t_3[0]
        t_0 = t_3[1]
        t_3 = t_3[2]
        t_3 = t_2.view(t_4, t_0, t_3)
        del t_0
        del t_4
        del t_2
        t_3 = self.l_40(t_3)
        t_3 = self.l_41(t_3)
        t_3 = t_1 + t_3
        del t_1
        t_1 = self.l_42(t_3)
        t_1 = self.l_43(t_1)
        t_1 = self.l_44(t_1)
        t_1 = self.l_45(t_1)
        t_1 = self.l_46(t_1)
        t_1 = t_3 + t_1
        del t_3
        t_1 = self.l_47(t_1)
        t_1 = self.l_48(x1, t_1)
        del x1
        t_1 = self.l_49(t_1, labels=x0)
        del x0
        # returning:
        # GPT2LMHeadModel/LMOutput[compute_output]
        return (t_1,)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


def traverse_model(module: nn.Module, depth: int, prefix: Optional[str] = None,
                   basic_blocks: Tuple[nn.Module] = (), full: bool = False) -> Iterator[Tuple[nn.Module, str, nn.Module]]:
    '''
    iterate over model layers yielding the layer,layer_scope,encasing_module
    Parameters:
    -----------
    model:
        the model to iterate over
    depth:
        how far down in the model tree to go
    basic_blocks:
        a list of modules that if encountered will not be broken down
    full:
        whether to yield only layers specified by the depth and basick_block options or to yield all layers
    '''
    if prefix is None:
        prefix = type(module).__name__

    for name, sub_module in module.named_children():
        scope = prefix + "/" + type(sub_module).__name__ + f"[{name}]"
        if len(list(sub_module.children())) == 0 or isinstance(sub_module, tuple(basic_blocks)) or depth == 0:
            if full:
                yield sub_module, scope, module, True
            else:
                yield sub_module, scope, module
        else:
            if full:
                yield sub_module, scope, module, False
            yield from traverse_model(sub_module, depth - 1, scope, basic_blocks, full)


def layerDict(model: nn.Module, depth=1000, basic_blocks=None) -> Dict[str, nn.Module]:
    return {s: l for l, s, _ in traverse_model(model, depth, basic_blocks=basic_blocks)}


def traverse_params_buffs(module: nn.Module, prefix: Optional[str] = None) -> Iterator[Tuple[torch.tensor, str]]:
    '''
    iterate over model's buffers and parameters yielding obj,obj_scope

    Parameters:
    -----------
    model:
        the model to iterate over
    '''
    if prefix is None:
        prefix = type(module).__name__

    # params
    for param_name, param in module.named_parameters(recurse=False):
        param_scope = f"{prefix}/{type(param).__name__}[{param_name}]"
        yield param, param_scope

    # buffs
    for buffer_name, buffer in module.named_buffers(recurse=False):
        buffer_scope = f"{prefix}/{type(buffer).__name__}[{buffer_name}]"
        yield buffer, buffer_scope

    # recurse
    for name, sub_module in module.named_children():
        yield from traverse_params_buffs(sub_module, prefix + "/" + type(sub_module).__name__ + f"[{name}]")


def tensorDict(model: nn.Module) -> OrderedDict[str, Tensor]:
    return collections.OrderedDict((s, t)for t, s in traverse_params_buffs(model))


def state_dict(partition, device=None):
    # we return the state dict of this part as it should be in the original model
    state = nn.Module.state_dict(partition)
    lookup = partition.lookup
    result = dict()
    for k, v in state.items():
        if k in lookup:
            result[lookup[k]] = v if device is None else v.to(device)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            result[new_k] = v if device is None else v.to(device)
    return result


def load_state_dict(partition, state):
    reverse_lookup = {v: k for k, v in partition.lookup.items()}
    device = partition.device
    keys = list(partition.state_dict(None).keys())
    new_state = dict()
    for k in keys:
        if k in reverse_lookup:
            new_state[reverse_lookup[k]] = state[k].to(device)
            continue
        idx = k.rfind(".")
        to_replace = k[:idx]
        if to_replace in reverse_lookup:
            key = reverse_lookup[to_replace] + k[idx:]
            new_state[key] = state[k].to(device)
    nn.Module.load_state_dict(partition, new_state, strict=True)


def named_buffers(partition, recurse=True):
    # we return the named buffers of this part as it should be in the original model
    params = nn.Module.named_buffers(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def named_parameters(partition, recurse=True):
    # we return the named parameters of this part as it should be in the original model
    params = nn.Module.named_parameters(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def cpu(partition):
    partition.device = torch.device('cpu')
    return nn.Module.cpu(partition)


def cuda(partition, device=None):
    if device is None:
        device = torch.cuda.current_device()
    partition.device = torch.device(device)
    return nn.Module.cuda(partition, partition.device)


def to(partition, *args, **kwargs):
    device = None
    if 'device' in kwargs:
        device = kwargs['device']
    elif 'tensor' in kwargs:
        device = kwargs['tensor'].device
    if args:
        if isinstance(args[0], (torch.device, int, str)):
            device = args[0]
        if torch.is_tensor(args[0]):
            device = args[0].device
    if not (device is None):
        partition.device = torch.device(device)
    return nn.Module.to(partition, *args, **kwargs)

"""analysis summary
-I- Printing Report
warnings:
Partition7 output:GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Tensor::permute is not contiguous!
Number of stages: 8
n_partitions:9, num_dummy_stages:1
unique_stages_on_same_gpu: [{0, 8}]
cutting edges are edges between partitions
number of cutting edges: 16

backward times include recomputation
Analysis for async_pipeline=True: last partition will not do recomputation.

real times are based on real measurements of execution time of generated partitions ms
forward {0: 59.96, 1: 67.08, 2: 66.3, 3: 66.45, 4: 61.97, 5: 61.17, 6: 61.4, 7: 61.37}
backward {0: 120.81, 1: 181.45, 2: 181.54, 3: 181.79, 4: 167.54, 5: 167.8, 6: 167.4, 7: 165.72}

balance is ratio of computation time between fastest and slowest parts. (between 0 and 1 higher is better)

real balance:
forward 0.894
backward 0.665

Assuming bandwidth of 12 GBps between GPUs

communication volumes size of activations of each partition
0: input size:'13.12 MB', recieve_time:'1.09 ms', out:'6.55 MB', send time:'0.55 ms'
1: input size:'6.55 MB', recieve_time:'0.55 ms', out:'13.11 MB', send time:'1.09 ms'
2: input size:'13.11 MB', recieve_time:'1.09 ms', out:'13.11 MB', send time:'1.09 ms'
3: input size:'13.11 MB', recieve_time:'1.09 ms', out:'6.55 MB', send time:'0.55 ms'
4: input size:'6.55 MB', recieve_time:'0.55 ms', out:'13.11 MB', send time:'1.09 ms'
5: input size:'13.11 MB', recieve_time:'1.09 ms', out:'6.55 MB', send time:'0.55 ms'
6: input size:'6.55 MB', recieve_time:'0.55 ms', out:'6.55 MB', send time:'0.55 ms'
7: input size:'6.55 MB', recieve_time:'0.55 ms', out:'13.11 MB', send time:'1.09 ms'

Compuatation Communication ratio (comp/(comp+comm)):
forward {0: 0.99, 1: 0.98, 2: 0.98, 3: 0.99, 4: 0.98, 5: 0.99, 6: 0.99, 7: 0.98} 
backward {0: 0.99, 1: 1.0, 2: 0.99, 3: 0.99, 4: 1.0, 5: 0.99, 6: 1.0, 7: 1.0}

Pipeline Slowdown: (compared to sequential executation with no communication, and same recompute policy)
forward 1.075
backward 1.096

Expected utilization by partition
forward {0: 0.88, 1: 0.98, 2: 0.97, 3: 0.98, 4: 0.9, 5: 0.9, 6: 0.91, 7: 0.89}
backward {0: 0.65, 1: 1.0, 2: 0.99, 3: 0.99, 4: 0.92, 5: 0.91, 6: 0.92, 7: 0.91}

worstcase: bwd: 181.793 fwd: 67.081
expected_speedup_compared_to_seq_no_recomp_no_comm: 5.589
Expected speedup for 8 partitions is: 7.339
"""