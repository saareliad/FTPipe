"""AutoGenerated with:
python partition_gpt2_models.py --analysis_batch_size 2 --async_pipeline --auto_file_name --block_size -1 --bwd_to_fwd_ratio 3 --lmhead --model_name_or_path gpt2-xl --model_type gpt2 --n_partitions 8 --output_file results/gpt2xl_tied_p8/ --overwrite_cache --partitioning_batch_size 2 --seed 42 --stateless_tied --train_data_file wikitext-2-raw/wiki.train.raw --basic_blocks Attention --generate_explicit_del --use_graph_profiler --profile_ops --n_iter 50
"""
import torch
import torch.nn.functional
import math
import torch.functional
from torch import Tensor
import torch.nn as nn
from itertools import chain
from typing import Optional, Tuple, Iterator, Iterable, OrderedDict, Dict
import collections
import os
from torch.nn.modules.sparse import Embedding
from models.normal.NLP_models.stateless import StatelessEmbedding
from models.normal.NLP_models.stateless import StatelessLinear
from torch.nn.modules.normalization import LayerNorm
from models.normal.NLP_models.modeling_gpt2_tied_weights import LMOutput
from torch.nn.modules.dropout import Dropout
from models.normal.NLP_models.modeling_gpt2_tied_weights import Attention
from models.normal.NLP_models.modeling_gpt2_tied_weights import Gelu
from transformers.modeling_utils import Conv1D
# this is an auto generated file do not edit unless you know what you are doing


# partition adjacency
# model inputs {0, 8}
# partition 0 {'inputs': {'input0'}, 'outputs': {8, 1}}
# partition 1 {'inputs': {0}, 'outputs': {2}}
# partition 2 {'inputs': {1}, 'outputs': {3}}
# partition 3 {'inputs': {2}, 'outputs': {4}}
# partition 4 {'inputs': {3}, 'outputs': {5}}
# partition 5 {'inputs': {4}, 'outputs': {6}}
# partition 6 {'inputs': {5}, 'outputs': {7}}
# partition 7 {'inputs': {6}, 'outputs': {8}}
# partition 8 {'inputs': {'input1', 0, 7}, 'outputs': {'output'}}
# model outputs {8}


def create_pipeline_configuration(DEBUG=False):
    depth = 10000
    basic_blocks = (Embedding,StatelessEmbedding,StatelessLinear,LayerNorm,LMOutput,Dropout,Attention,Gelu,Conv1D)
    blocks_path = [ 'torch.nn.modules.sparse.Embedding',
            'models.normal.NLP_models.stateless.StatelessEmbedding',
            'models.normal.NLP_models.stateless.StatelessLinear',
            'torch.nn.modules.normalization.LayerNorm',
            'models.normal.NLP_models.modeling_gpt2_tied_weights.LMOutput',
            'torch.nn.modules.dropout.Dropout',
            'models.normal.NLP_models.modeling_gpt2_tied_weights.Attention',
            'models.normal.NLP_models.modeling_gpt2_tied_weights.Gelu',
            'transformers.modeling_utils.Conv1D']
    module_path = os.path.relpath(__file__).replace("/",".")[:-3]
    

    # creating configuration
    stages = {0: {"inputs": {'input0': {'shape': torch.Size([2, 1024]), 'dtype': 'torch.int64', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/Parameter[w_wte]': {'shape': torch.Size([50257, 1600]), 'dtype': 'torch.float32', 'is_batched': False}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Tensor::__add__': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1]': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}}},
            1: {"inputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Tensor::__add__': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1]': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Tensor::__add__': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}}},
            2: {"inputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Tensor::__add__': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Tensor::__add__': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}}},
            3: {"inputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Tensor::__add__': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Tensor::__add__': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}}},
            4: {"inputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Tensor::__add__': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Tensor::__add__': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}}},
            5: {"inputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Tensor::__add__': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Tensor::__add__': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]': {'shape': torch.Size([2, 1024, 6400]), 'dtype': 'torch.float32', 'is_batched': True}}},
            6: {"inputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Tensor::__add__': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]': {'shape': torch.Size([2, 1024, 6400]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Tensor::__add__': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}}},
            7: {"inputs": {'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Tensor::__add__': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}, 'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}}},
            8: {"inputs": {'input1': {'shape': torch.Size([2, 1024]), 'dtype': 'torch.int64', 'is_batched': True}, 'GPT2LMHeadModel/Parameter[w_wte]': {'shape': torch.Size([50257, 1600]), 'dtype': 'torch.float32', 'is_batched': False}, 'GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]': {'shape': torch.Size([2, 1024, 1600]), 'dtype': 'torch.float32', 'is_batched': True}},
        "outputs": {'GPT2LMHeadModel/LMOutput[compute_output]': {'shape': torch.Size([1]), 'dtype': 'torch.float32', 'is_batched': False}}}
            }
    

    stages[0]['stage_cls'] = module_path + '.Partition0'
    device = 'cpu' if DEBUG else 'cuda:0'
    stages[0]['devices'] = [device]
    

    stages[1]['stage_cls'] = module_path + '.Partition1'
    device = 'cpu' if DEBUG else 'cuda:1'
    stages[1]['devices'] = [device]
    

    stages[2]['stage_cls'] = module_path + '.Partition2'
    device = 'cpu' if DEBUG else 'cuda:2'
    stages[2]['devices'] = [device]
    

    stages[3]['stage_cls'] = module_path + '.Partition3'
    device = 'cpu' if DEBUG else 'cuda:3'
    stages[3]['devices'] = [device]
    

    stages[4]['stage_cls'] = module_path + '.Partition4'
    device = 'cpu' if DEBUG else 'cuda:4'
    stages[4]['devices'] = [device]
    

    stages[5]['stage_cls'] = module_path + '.Partition5'
    device = 'cpu' if DEBUG else 'cuda:5'
    stages[5]['devices'] = [device]
    

    stages[6]['stage_cls'] = module_path + '.Partition6'
    device = 'cpu' if DEBUG else 'cuda:6'
    stages[6]['devices'] = [device]
    

    stages[7]['stage_cls'] = module_path + '.Partition7'
    device = 'cpu' if DEBUG else 'cuda:7'
    stages[7]['devices'] = [device]
    

    stages[8]['stage_cls'] = module_path + '.Partition8'
    device = 'cpu' if DEBUG else 'cuda:0'
    stages[8]['devices'] = [device]
    

    config = dict()
    config['batch_dim'] = 0
    config['depth'] = depth
    config['basic_blocks'] = blocks_path
    config['model_inputs'] = {'input0': {"shape": torch.Size([2, 1024]),
        "dtype": 'torch.int64',
        "is_batched": True},
            'input1': {"shape": torch.Size([2, 1024]),
        "dtype": 'torch.int64',
        "is_batched": True}}
    config['model_outputs'] = {'GPT2LMHeadModel/LMOutput[compute_output]': {"shape": torch.Size([1]),
        "dtype": 'torch.float32',
        "is_batched": False}}
    config['stages'] = stages
    
    return config

class Partition0(nn.Module):
    BASIC_BLOCKS=(
            Embedding,
            LayerNorm,
            Conv1D,
            StatelessEmbedding,
            Dropout,
            Attention,
            Gelu,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/StatelessEmbedding[stateless_wte]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1]',
        ]
    TENSORS=[
            'GPT2LMHeadModel/Parameter[w_wte]',
        ]
    def __init__(self, layers, tensors):
        super(Partition0, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:0')
        self.lookup = { 'l_0': 'transformer.stateless_wte',
                        'l_1': 'transformer.wpe',
                        'l_2': 'transformer.drop',
                        'l_3': 'transformer.blocks.0.ln_1',
                        'l_4': 'transformer.blocks.0.attn',
                        'l_5': 'transformer.blocks.0.ln_2',
                        'l_6': 'transformer.blocks.0.mlp.c_fc',
                        'l_7': 'transformer.blocks.0.mlp.act',
                        'l_8': 'transformer.blocks.0.mlp.c_proj',
                        'l_9': 'transformer.blocks.0.mlp.dropout',
                        'l_10': 'transformer.blocks.1.ln_1',
                        'l_11': 'transformer.blocks.1.attn',
                        'l_12': 'transformer.blocks.1.ln_2',
                        'l_13': 'transformer.blocks.1.mlp.c_fc',
                        'l_14': 'transformer.blocks.1.mlp.act',
                        'l_15': 'transformer.blocks.1.mlp.c_proj',
                        'l_16': 'transformer.blocks.1.mlp.dropout',
                        'l_17': 'transformer.blocks.2.ln_1',
                        'l_18': 'transformer.blocks.2.attn',
                        'l_19': 'transformer.blocks.2.ln_2',
                        'l_20': 'transformer.blocks.2.mlp.c_fc',
                        'l_21': 'transformer.blocks.2.mlp.act',
                        'l_22': 'transformer.blocks.2.mlp.c_proj',
                        'l_23': 'transformer.blocks.2.mlp.dropout',
                        'l_24': 'transformer.blocks.3.ln_1',
                        'l_25': 'transformer.blocks.3.attn',
                        'l_26': 'transformer.blocks.3.ln_2',
                        'l_27': 'transformer.blocks.3.mlp.c_fc',
                        'l_28': 'transformer.blocks.3.mlp.act',
                        'l_29': 'transformer.blocks.3.mlp.c_proj',
                        'l_30': 'transformer.blocks.3.mlp.dropout',
                        'l_31': 'transformer.blocks.4.ln_1',
                        'l_32': 'transformer.blocks.4.attn',
                        'l_33': 'transformer.blocks.4.ln_2',
                        'l_34': 'transformer.blocks.4.mlp.c_fc',
                        'l_35': 'transformer.blocks.4.mlp.act',
                        'l_36': 'transformer.blocks.4.mlp.c_proj',
                        'l_37': 'transformer.blocks.4.mlp.dropout',
                        'l_38': 'transformer.blocks.5.ln_1',
                        'p_0': 'w_wte'}

    def forward(self, x0):
        # GPT2LMHeadModel/GPT2Model[transformer]/StatelessEmbedding[stateless_wte] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Gelu[act] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Gelu[act] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Gelu[act] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Gelu[act] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Gelu[act] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1] <=> self.l_38
        # GPT2LMHeadModel/Parameter[w_wte] <=> self.p_0
        # input0 <=> x0

        # moving inputs to current device no op if already on the correct device
        x0 = move_tensors((x0), self.device)
        t_0 = x0.size()
        t_0 = t_0[-1]
        t_0 = x0.view(-1, t_0)
        del x0
        t_1 = t_0.size(-1)
        t_2 = t_0.device
        t_2 = torch.arange(t_1, dtype=torch.int64, device=t_2)
        del t_1
        t_2 = t_2.unsqueeze(0)
        t_2 = t_2.expand_as(t_0)
        t_0 = self.l_0(self.p_0, t_0)
        t_2 = self.l_1(t_2)
        t_2 = t_0 + t_2
        del t_0
        t_2 = self.l_2(t_2)
        t_0 = self.l_3(t_2)
        t_0 = self.l_4(t_0)
        t_0 = t_2 + t_0
        del t_2
        t_2 = self.l_5(t_0)
        t_2 = self.l_6(t_2)
        t_2 = self.l_7(t_2)
        t_2 = self.l_8(t_2)
        t_2 = self.l_9(t_2)
        t_2 = t_0 + t_2
        del t_0
        t_0 = self.l_10(t_2)
        t_0 = self.l_11(t_0)
        t_0 = t_2 + t_0
        del t_2
        t_2 = self.l_12(t_0)
        t_2 = self.l_13(t_2)
        t_2 = self.l_14(t_2)
        t_2 = self.l_15(t_2)
        t_2 = self.l_16(t_2)
        t_2 = t_0 + t_2
        del t_0
        t_0 = self.l_17(t_2)
        t_0 = self.l_18(t_0)
        t_0 = t_2 + t_0
        del t_2
        t_2 = self.l_19(t_0)
        t_2 = self.l_20(t_2)
        t_2 = self.l_21(t_2)
        t_2 = self.l_22(t_2)
        t_2 = self.l_23(t_2)
        t_2 = t_0 + t_2
        del t_0
        t_0 = self.l_24(t_2)
        t_0 = self.l_25(t_0)
        t_0 = t_2 + t_0
        del t_2
        t_2 = self.l_26(t_0)
        t_2 = self.l_27(t_2)
        t_2 = self.l_28(t_2)
        t_2 = self.l_29(t_2)
        t_2 = self.l_30(t_2)
        t_2 = t_0 + t_2
        del t_0
        t_0 = self.l_31(t_2)
        t_0 = self.l_32(t_0)
        t_0 = t_2 + t_0
        del t_2
        t_2 = self.l_33(t_0)
        t_2 = self.l_34(t_2)
        t_2 = self.l_35(t_2)
        t_2 = self.l_36(t_2)
        t_2 = self.l_37(t_2)
        t_2 = t_0 + t_2
        del t_0
        t_0 = self.l_38(t_2)
        # returning:
        # GPT2LMHeadModel/Parameter[w_wte]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Tensor::__add__
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1]
        return (self.p_0, t_2, t_0)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition1(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Conv1D,
            Dropout,
            Attention,
            Gelu,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition1, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:1')
        self.lookup = { 'l_0': 'transformer.blocks.5.attn',
                        'l_1': 'transformer.blocks.5.ln_2',
                        'l_2': 'transformer.blocks.5.mlp.c_fc',
                        'l_3': 'transformer.blocks.5.mlp.act',
                        'l_4': 'transformer.blocks.5.mlp.c_proj',
                        'l_5': 'transformer.blocks.5.mlp.dropout',
                        'l_6': 'transformer.blocks.6.ln_1',
                        'l_7': 'transformer.blocks.6.attn',
                        'l_8': 'transformer.blocks.6.ln_2',
                        'l_9': 'transformer.blocks.6.mlp.c_fc',
                        'l_10': 'transformer.blocks.6.mlp.act',
                        'l_11': 'transformer.blocks.6.mlp.c_proj',
                        'l_12': 'transformer.blocks.6.mlp.dropout',
                        'l_13': 'transformer.blocks.7.ln_1',
                        'l_14': 'transformer.blocks.7.attn',
                        'l_15': 'transformer.blocks.7.ln_2',
                        'l_16': 'transformer.blocks.7.mlp.c_fc',
                        'l_17': 'transformer.blocks.7.mlp.act',
                        'l_18': 'transformer.blocks.7.mlp.c_proj',
                        'l_19': 'transformer.blocks.7.mlp.dropout',
                        'l_20': 'transformer.blocks.8.ln_1',
                        'l_21': 'transformer.blocks.8.attn',
                        'l_22': 'transformer.blocks.8.ln_2',
                        'l_23': 'transformer.blocks.8.mlp.c_fc',
                        'l_24': 'transformer.blocks.8.mlp.act',
                        'l_25': 'transformer.blocks.8.mlp.c_proj',
                        'l_26': 'transformer.blocks.8.mlp.dropout',
                        'l_27': 'transformer.blocks.9.ln_1',
                        'l_28': 'transformer.blocks.9.attn',
                        'l_29': 'transformer.blocks.9.ln_2',
                        'l_30': 'transformer.blocks.9.mlp.c_fc',
                        'l_31': 'transformer.blocks.9.mlp.act',
                        'l_32': 'transformer.blocks.9.mlp.c_proj',
                        'l_33': 'transformer.blocks.9.mlp.dropout',
                        'l_34': 'transformer.blocks.10.ln_1',
                        'l_35': 'transformer.blocks.10.attn',
                        'l_36': 'transformer.blocks.10.ln_2',
                        'l_37': 'transformer.blocks.10.mlp.c_fc',
                        'l_38': 'transformer.blocks.10.mlp.act',
                        'l_39': 'transformer.blocks.10.mlp.c_proj',
                        'l_40': 'transformer.blocks.10.mlp.dropout',
                        'l_41': 'transformer.blocks.11.ln_1',
                        'l_42': 'transformer.blocks.11.attn'}

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Gelu[act] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Gelu[act] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Gelu[act] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Gelu[act] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Gelu[act] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Gelu[act] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Tensor::__add__ <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1] <=> x1

        # moving inputs to current device no op if already on the correct device
        x0, x1 = move_tensors((x0, x1), self.device)
        t_0 = self.l_0(x1)
        del x1
        t_0 = x0 + t_0
        del x0
        t_1 = self.l_1(t_0)
        t_1 = self.l_2(t_1)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_6(t_1)
        t_0 = self.l_7(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_8(t_0)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = self.l_12(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_13(t_1)
        t_0 = self.l_14(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_15(t_0)
        t_1 = self.l_16(t_1)
        t_1 = self.l_17(t_1)
        t_1 = self.l_18(t_1)
        t_1 = self.l_19(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_20(t_1)
        t_0 = self.l_21(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_22(t_0)
        t_1 = self.l_23(t_1)
        t_1 = self.l_24(t_1)
        t_1 = self.l_25(t_1)
        t_1 = self.l_26(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_27(t_1)
        t_0 = self.l_28(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_29(t_0)
        t_1 = self.l_30(t_1)
        t_1 = self.l_31(t_1)
        t_1 = self.l_32(t_1)
        t_1 = self.l_33(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_34(t_1)
        t_0 = self.l_35(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_36(t_0)
        t_1 = self.l_37(t_1)
        t_1 = self.l_38(t_1)
        t_1 = self.l_39(t_1)
        t_1 = self.l_40(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_41(t_1)
        t_0 = self.l_42(t_0)
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Tensor::__add__
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]
        return (t_1, t_0)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition2(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Conv1D,
            Dropout,
            Attention,
            Gelu,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition2, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:2')
        self.lookup = { 'l_0': 'transformer.blocks.11.ln_2',
                        'l_1': 'transformer.blocks.11.mlp.c_fc',
                        'l_2': 'transformer.blocks.11.mlp.act',
                        'l_3': 'transformer.blocks.11.mlp.c_proj',
                        'l_4': 'transformer.blocks.11.mlp.dropout',
                        'l_5': 'transformer.blocks.12.ln_1',
                        'l_6': 'transformer.blocks.12.attn',
                        'l_7': 'transformer.blocks.12.ln_2',
                        'l_8': 'transformer.blocks.12.mlp.c_fc',
                        'l_9': 'transformer.blocks.12.mlp.act',
                        'l_10': 'transformer.blocks.12.mlp.c_proj',
                        'l_11': 'transformer.blocks.12.mlp.dropout',
                        'l_12': 'transformer.blocks.13.ln_1',
                        'l_13': 'transformer.blocks.13.attn',
                        'l_14': 'transformer.blocks.13.ln_2',
                        'l_15': 'transformer.blocks.13.mlp.c_fc',
                        'l_16': 'transformer.blocks.13.mlp.act',
                        'l_17': 'transformer.blocks.13.mlp.c_proj',
                        'l_18': 'transformer.blocks.13.mlp.dropout',
                        'l_19': 'transformer.blocks.14.ln_1',
                        'l_20': 'transformer.blocks.14.attn',
                        'l_21': 'transformer.blocks.14.ln_2',
                        'l_22': 'transformer.blocks.14.mlp.c_fc',
                        'l_23': 'transformer.blocks.14.mlp.act',
                        'l_24': 'transformer.blocks.14.mlp.c_proj',
                        'l_25': 'transformer.blocks.14.mlp.dropout',
                        'l_26': 'transformer.blocks.15.ln_1',
                        'l_27': 'transformer.blocks.15.attn',
                        'l_28': 'transformer.blocks.15.ln_2',
                        'l_29': 'transformer.blocks.15.mlp.c_fc',
                        'l_30': 'transformer.blocks.15.mlp.act',
                        'l_31': 'transformer.blocks.15.mlp.c_proj',
                        'l_32': 'transformer.blocks.15.mlp.dropout',
                        'l_33': 'transformer.blocks.16.ln_1',
                        'l_34': 'transformer.blocks.16.attn',
                        'l_35': 'transformer.blocks.16.ln_2',
                        'l_36': 'transformer.blocks.16.mlp.c_fc',
                        'l_37': 'transformer.blocks.16.mlp.act',
                        'l_38': 'transformer.blocks.16.mlp.c_proj',
                        'l_39': 'transformer.blocks.16.mlp.dropout',
                        'l_40': 'transformer.blocks.17.ln_1',
                        'l_41': 'transformer.blocks.17.attn'}

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Gelu[act] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Gelu[act] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Gelu[act] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Gelu[act] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Gelu[act] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Gelu[act] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Tensor::__add__ <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn] <=> x1

        # moving inputs to current device no op if already on the correct device
        x0, x1 = move_tensors((x0, x1), self.device)
        t_0 = x0 + x1
        del x1
        del x0
        t_1 = self.l_0(t_0)
        t_1 = self.l_1(t_1)
        t_1 = self.l_2(t_1)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_5(t_1)
        t_0 = self.l_6(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_7(t_0)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_12(t_1)
        t_0 = self.l_13(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_14(t_0)
        t_1 = self.l_15(t_1)
        t_1 = self.l_16(t_1)
        t_1 = self.l_17(t_1)
        t_1 = self.l_18(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_19(t_1)
        t_0 = self.l_20(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_21(t_0)
        t_1 = self.l_22(t_1)
        t_1 = self.l_23(t_1)
        t_1 = self.l_24(t_1)
        t_1 = self.l_25(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_26(t_1)
        t_0 = self.l_27(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_28(t_0)
        t_1 = self.l_29(t_1)
        t_1 = self.l_30(t_1)
        t_1 = self.l_31(t_1)
        t_1 = self.l_32(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_33(t_1)
        t_0 = self.l_34(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_35(t_0)
        t_1 = self.l_36(t_1)
        t_1 = self.l_37(t_1)
        t_1 = self.l_38(t_1)
        t_1 = self.l_39(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_40(t_1)
        t_0 = self.l_41(t_0)
        t_0 = t_1 + t_0
        del t_1
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Tensor::__add__
        return (t_0,)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition3(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Conv1D,
            Dropout,
            Attention,
            Gelu,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition3, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:3')
        self.lookup = { 'l_0': 'transformer.blocks.17.ln_2',
                        'l_1': 'transformer.blocks.17.mlp.c_fc',
                        'l_2': 'transformer.blocks.17.mlp.act',
                        'l_3': 'transformer.blocks.17.mlp.c_proj',
                        'l_4': 'transformer.blocks.17.mlp.dropout',
                        'l_5': 'transformer.blocks.18.ln_1',
                        'l_6': 'transformer.blocks.18.attn',
                        'l_7': 'transformer.blocks.18.ln_2',
                        'l_8': 'transformer.blocks.18.mlp.c_fc',
                        'l_9': 'transformer.blocks.18.mlp.act',
                        'l_10': 'transformer.blocks.18.mlp.c_proj',
                        'l_11': 'transformer.blocks.18.mlp.dropout',
                        'l_12': 'transformer.blocks.19.ln_1',
                        'l_13': 'transformer.blocks.19.attn',
                        'l_14': 'transformer.blocks.19.ln_2',
                        'l_15': 'transformer.blocks.19.mlp.c_fc',
                        'l_16': 'transformer.blocks.19.mlp.act',
                        'l_17': 'transformer.blocks.19.mlp.c_proj',
                        'l_18': 'transformer.blocks.19.mlp.dropout',
                        'l_19': 'transformer.blocks.20.ln_1',
                        'l_20': 'transformer.blocks.20.attn',
                        'l_21': 'transformer.blocks.20.ln_2',
                        'l_22': 'transformer.blocks.20.mlp.c_fc',
                        'l_23': 'transformer.blocks.20.mlp.act',
                        'l_24': 'transformer.blocks.20.mlp.c_proj',
                        'l_25': 'transformer.blocks.20.mlp.dropout',
                        'l_26': 'transformer.blocks.21.ln_1',
                        'l_27': 'transformer.blocks.21.attn',
                        'l_28': 'transformer.blocks.21.ln_2',
                        'l_29': 'transformer.blocks.21.mlp.c_fc',
                        'l_30': 'transformer.blocks.21.mlp.act',
                        'l_31': 'transformer.blocks.21.mlp.c_proj',
                        'l_32': 'transformer.blocks.21.mlp.dropout',
                        'l_33': 'transformer.blocks.22.ln_1',
                        'l_34': 'transformer.blocks.22.attn',
                        'l_35': 'transformer.blocks.22.ln_2',
                        'l_36': 'transformer.blocks.22.mlp.c_fc',
                        'l_37': 'transformer.blocks.22.mlp.act',
                        'l_38': 'transformer.blocks.22.mlp.c_proj',
                        'l_39': 'transformer.blocks.22.mlp.dropout',
                        'l_40': 'transformer.blocks.23.ln_1',
                        'l_41': 'transformer.blocks.23.attn'}

    def forward(self, x0):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Gelu[act] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Gelu[act] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Gelu[act] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Gelu[act] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Gelu[act] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Gelu[act] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Tensor::__add__ <=> x0

        # moving inputs to current device no op if already on the correct device
        x0 = move_tensors((x0), self.device)
        t_0 = self.l_0(x0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x0 + t_0
        del x0
        t_1 = self.l_5(t_0)
        t_1 = self.l_6(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_7(t_1)
        t_0 = self.l_8(t_0)
        t_0 = self.l_9(t_0)
        t_0 = self.l_10(t_0)
        t_0 = self.l_11(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_12(t_0)
        t_1 = self.l_13(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_14(t_1)
        t_0 = self.l_15(t_0)
        t_0 = self.l_16(t_0)
        t_0 = self.l_17(t_0)
        t_0 = self.l_18(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_19(t_0)
        t_1 = self.l_20(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_21(t_1)
        t_0 = self.l_22(t_0)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = self.l_25(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_26(t_0)
        t_1 = self.l_27(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_28(t_1)
        t_0 = self.l_29(t_0)
        t_0 = self.l_30(t_0)
        t_0 = self.l_31(t_0)
        t_0 = self.l_32(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_33(t_0)
        t_1 = self.l_34(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_35(t_1)
        t_0 = self.l_36(t_0)
        t_0 = self.l_37(t_0)
        t_0 = self.l_38(t_0)
        t_0 = self.l_39(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_40(t_0)
        t_1 = self.l_41(t_1)
        t_1 = t_0 + t_1
        del t_0
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Tensor::__add__
        return (t_1,)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition4(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Conv1D,
            Dropout,
            Attention,
            Gelu,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition4, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:4')
        self.lookup = { 'l_0': 'transformer.blocks.23.ln_2',
                        'l_1': 'transformer.blocks.23.mlp.c_fc',
                        'l_2': 'transformer.blocks.23.mlp.act',
                        'l_3': 'transformer.blocks.23.mlp.c_proj',
                        'l_4': 'transformer.blocks.23.mlp.dropout',
                        'l_5': 'transformer.blocks.24.ln_1',
                        'l_6': 'transformer.blocks.24.attn',
                        'l_7': 'transformer.blocks.24.ln_2',
                        'l_8': 'transformer.blocks.24.mlp.c_fc',
                        'l_9': 'transformer.blocks.24.mlp.act',
                        'l_10': 'transformer.blocks.24.mlp.c_proj',
                        'l_11': 'transformer.blocks.24.mlp.dropout',
                        'l_12': 'transformer.blocks.25.ln_1',
                        'l_13': 'transformer.blocks.25.attn',
                        'l_14': 'transformer.blocks.25.ln_2',
                        'l_15': 'transformer.blocks.25.mlp.c_fc',
                        'l_16': 'transformer.blocks.25.mlp.act',
                        'l_17': 'transformer.blocks.25.mlp.c_proj',
                        'l_18': 'transformer.blocks.25.mlp.dropout',
                        'l_19': 'transformer.blocks.26.ln_1',
                        'l_20': 'transformer.blocks.26.attn',
                        'l_21': 'transformer.blocks.26.ln_2',
                        'l_22': 'transformer.blocks.26.mlp.c_fc',
                        'l_23': 'transformer.blocks.26.mlp.act',
                        'l_24': 'transformer.blocks.26.mlp.c_proj',
                        'l_25': 'transformer.blocks.26.mlp.dropout',
                        'l_26': 'transformer.blocks.27.ln_1',
                        'l_27': 'transformer.blocks.27.attn',
                        'l_28': 'transformer.blocks.27.ln_2',
                        'l_29': 'transformer.blocks.27.mlp.c_fc',
                        'l_30': 'transformer.blocks.27.mlp.act',
                        'l_31': 'transformer.blocks.27.mlp.c_proj',
                        'l_32': 'transformer.blocks.27.mlp.dropout',
                        'l_33': 'transformer.blocks.28.ln_1',
                        'l_34': 'transformer.blocks.28.attn',
                        'l_35': 'transformer.blocks.28.ln_2',
                        'l_36': 'transformer.blocks.28.mlp.c_fc',
                        'l_37': 'transformer.blocks.28.mlp.act',
                        'l_38': 'transformer.blocks.28.mlp.c_proj',
                        'l_39': 'transformer.blocks.28.mlp.dropout',
                        'l_40': 'transformer.blocks.29.ln_1',
                        'l_41': 'transformer.blocks.29.attn'}

    def forward(self, x0):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Gelu[act] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Gelu[act] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Gelu[act] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Gelu[act] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Gelu[act] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Gelu[act] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Tensor::__add__ <=> x0

        # moving inputs to current device no op if already on the correct device
        x0 = move_tensors((x0), self.device)
        t_0 = self.l_0(x0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x0 + t_0
        del x0
        t_1 = self.l_5(t_0)
        t_1 = self.l_6(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_7(t_1)
        t_0 = self.l_8(t_0)
        t_0 = self.l_9(t_0)
        t_0 = self.l_10(t_0)
        t_0 = self.l_11(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_12(t_0)
        t_1 = self.l_13(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_14(t_1)
        t_0 = self.l_15(t_0)
        t_0 = self.l_16(t_0)
        t_0 = self.l_17(t_0)
        t_0 = self.l_18(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_19(t_0)
        t_1 = self.l_20(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_21(t_1)
        t_0 = self.l_22(t_0)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = self.l_25(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_26(t_0)
        t_1 = self.l_27(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_28(t_1)
        t_0 = self.l_29(t_0)
        t_0 = self.l_30(t_0)
        t_0 = self.l_31(t_0)
        t_0 = self.l_32(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_33(t_0)
        t_1 = self.l_34(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_35(t_1)
        t_0 = self.l_36(t_0)
        t_0 = self.l_37(t_0)
        t_0 = self.l_38(t_0)
        t_0 = self.l_39(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_40(t_0)
        t_1 = self.l_41(t_1)
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Tensor::__add__
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]
        return (t_0, t_1)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition5(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Conv1D,
            Dropout,
            Attention,
            Gelu,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition5, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:5')
        self.lookup = { 'l_0': 'transformer.blocks.29.ln_2',
                        'l_1': 'transformer.blocks.29.mlp.c_fc',
                        'l_2': 'transformer.blocks.29.mlp.act',
                        'l_3': 'transformer.blocks.29.mlp.c_proj',
                        'l_4': 'transformer.blocks.29.mlp.dropout',
                        'l_5': 'transformer.blocks.30.ln_1',
                        'l_6': 'transformer.blocks.30.attn',
                        'l_7': 'transformer.blocks.30.ln_2',
                        'l_8': 'transformer.blocks.30.mlp.c_fc',
                        'l_9': 'transformer.blocks.30.mlp.act',
                        'l_10': 'transformer.blocks.30.mlp.c_proj',
                        'l_11': 'transformer.blocks.30.mlp.dropout',
                        'l_12': 'transformer.blocks.31.ln_1',
                        'l_13': 'transformer.blocks.31.attn',
                        'l_14': 'transformer.blocks.31.ln_2',
                        'l_15': 'transformer.blocks.31.mlp.c_fc',
                        'l_16': 'transformer.blocks.31.mlp.act',
                        'l_17': 'transformer.blocks.31.mlp.c_proj',
                        'l_18': 'transformer.blocks.31.mlp.dropout',
                        'l_19': 'transformer.blocks.32.ln_1',
                        'l_20': 'transformer.blocks.32.attn',
                        'l_21': 'transformer.blocks.32.ln_2',
                        'l_22': 'transformer.blocks.32.mlp.c_fc',
                        'l_23': 'transformer.blocks.32.mlp.act',
                        'l_24': 'transformer.blocks.32.mlp.c_proj',
                        'l_25': 'transformer.blocks.32.mlp.dropout',
                        'l_26': 'transformer.blocks.33.ln_1',
                        'l_27': 'transformer.blocks.33.attn',
                        'l_28': 'transformer.blocks.33.ln_2',
                        'l_29': 'transformer.blocks.33.mlp.c_fc',
                        'l_30': 'transformer.blocks.33.mlp.act',
                        'l_31': 'transformer.blocks.33.mlp.c_proj',
                        'l_32': 'transformer.blocks.33.mlp.dropout',
                        'l_33': 'transformer.blocks.34.ln_1',
                        'l_34': 'transformer.blocks.34.attn',
                        'l_35': 'transformer.blocks.34.ln_2',
                        'l_36': 'transformer.blocks.34.mlp.c_fc',
                        'l_37': 'transformer.blocks.34.mlp.act',
                        'l_38': 'transformer.blocks.34.mlp.c_proj',
                        'l_39': 'transformer.blocks.34.mlp.dropout',
                        'l_40': 'transformer.blocks.35.ln_1',
                        'l_41': 'transformer.blocks.35.attn',
                        'l_42': 'transformer.blocks.35.ln_2',
                        'l_43': 'transformer.blocks.35.mlp.c_fc'}

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Gelu[act] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Gelu[act] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Gelu[act] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Gelu[act] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Gelu[act] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Gelu[act] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Tensor::__add__ <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn] <=> x1

        # moving inputs to current device no op if already on the correct device
        x0, x1 = move_tensors((x0, x1), self.device)
        t_0 = x0 + x1
        del x1
        del x0
        t_1 = self.l_0(t_0)
        t_1 = self.l_1(t_1)
        t_1 = self.l_2(t_1)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_5(t_1)
        t_0 = self.l_6(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_7(t_0)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_12(t_1)
        t_0 = self.l_13(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_14(t_0)
        t_1 = self.l_15(t_1)
        t_1 = self.l_16(t_1)
        t_1 = self.l_17(t_1)
        t_1 = self.l_18(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_19(t_1)
        t_0 = self.l_20(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_21(t_0)
        t_1 = self.l_22(t_1)
        t_1 = self.l_23(t_1)
        t_1 = self.l_24(t_1)
        t_1 = self.l_25(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_26(t_1)
        t_0 = self.l_27(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_28(t_0)
        t_1 = self.l_29(t_1)
        t_1 = self.l_30(t_1)
        t_1 = self.l_31(t_1)
        t_1 = self.l_32(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_33(t_1)
        t_0 = self.l_34(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_35(t_0)
        t_1 = self.l_36(t_1)
        t_1 = self.l_37(t_1)
        t_1 = self.l_38(t_1)
        t_1 = self.l_39(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_40(t_1)
        t_0 = self.l_41(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_42(t_0)
        t_1 = self.l_43(t_1)
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Tensor::__add__
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]
        return (t_0, t_1)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition6(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Conv1D,
            Dropout,
            Attention,
            Gelu,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition6, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:6')
        self.lookup = { 'l_0': 'transformer.blocks.35.mlp.act',
                        'l_1': 'transformer.blocks.35.mlp.c_proj',
                        'l_2': 'transformer.blocks.35.mlp.dropout',
                        'l_3': 'transformer.blocks.36.ln_1',
                        'l_4': 'transformer.blocks.36.attn',
                        'l_5': 'transformer.blocks.36.ln_2',
                        'l_6': 'transformer.blocks.36.mlp.c_fc',
                        'l_7': 'transformer.blocks.36.mlp.act',
                        'l_8': 'transformer.blocks.36.mlp.c_proj',
                        'l_9': 'transformer.blocks.36.mlp.dropout',
                        'l_10': 'transformer.blocks.37.ln_1',
                        'l_11': 'transformer.blocks.37.attn',
                        'l_12': 'transformer.blocks.37.ln_2',
                        'l_13': 'transformer.blocks.37.mlp.c_fc',
                        'l_14': 'transformer.blocks.37.mlp.act',
                        'l_15': 'transformer.blocks.37.mlp.c_proj',
                        'l_16': 'transformer.blocks.37.mlp.dropout',
                        'l_17': 'transformer.blocks.38.ln_1',
                        'l_18': 'transformer.blocks.38.attn',
                        'l_19': 'transformer.blocks.38.ln_2',
                        'l_20': 'transformer.blocks.38.mlp.c_fc',
                        'l_21': 'transformer.blocks.38.mlp.act',
                        'l_22': 'transformer.blocks.38.mlp.c_proj',
                        'l_23': 'transformer.blocks.38.mlp.dropout',
                        'l_24': 'transformer.blocks.39.ln_1',
                        'l_25': 'transformer.blocks.39.attn',
                        'l_26': 'transformer.blocks.39.ln_2',
                        'l_27': 'transformer.blocks.39.mlp.c_fc',
                        'l_28': 'transformer.blocks.39.mlp.act',
                        'l_29': 'transformer.blocks.39.mlp.c_proj',
                        'l_30': 'transformer.blocks.39.mlp.dropout',
                        'l_31': 'transformer.blocks.40.ln_1',
                        'l_32': 'transformer.blocks.40.attn',
                        'l_33': 'transformer.blocks.40.ln_2',
                        'l_34': 'transformer.blocks.40.mlp.c_fc',
                        'l_35': 'transformer.blocks.40.mlp.act',
                        'l_36': 'transformer.blocks.40.mlp.c_proj',
                        'l_37': 'transformer.blocks.40.mlp.dropout',
                        'l_38': 'transformer.blocks.41.ln_1',
                        'l_39': 'transformer.blocks.41.attn',
                        'l_40': 'transformer.blocks.41.ln_2',
                        'l_41': 'transformer.blocks.41.mlp.c_fc',
                        'l_42': 'transformer.blocks.41.mlp.act',
                        'l_43': 'transformer.blocks.41.mlp.c_proj'}

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Gelu[act] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Gelu[act] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Gelu[act] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Gelu[act] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Gelu[act] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Gelu[act] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Gelu[act] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Tensor::__add__ <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc] <=> x1

        # moving inputs to current device no op if already on the correct device
        x0, x1 = move_tensors((x0, x1), self.device)
        t_0 = self.l_0(x1)
        del x1
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = x0 + t_0
        del x0
        t_1 = self.l_3(t_0)
        t_1 = self.l_4(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_5(t_1)
        t_0 = self.l_6(t_0)
        t_0 = self.l_7(t_0)
        t_0 = self.l_8(t_0)
        t_0 = self.l_9(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_10(t_0)
        t_1 = self.l_11(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_12(t_1)
        t_0 = self.l_13(t_0)
        t_0 = self.l_14(t_0)
        t_0 = self.l_15(t_0)
        t_0 = self.l_16(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_17(t_0)
        t_1 = self.l_18(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_19(t_1)
        t_0 = self.l_20(t_0)
        t_0 = self.l_21(t_0)
        t_0 = self.l_22(t_0)
        t_0 = self.l_23(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_24(t_0)
        t_1 = self.l_25(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_26(t_1)
        t_0 = self.l_27(t_0)
        t_0 = self.l_28(t_0)
        t_0 = self.l_29(t_0)
        t_0 = self.l_30(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_31(t_0)
        t_1 = self.l_32(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_33(t_1)
        t_0 = self.l_34(t_0)
        t_0 = self.l_35(t_0)
        t_0 = self.l_36(t_0)
        t_0 = self.l_37(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_38(t_0)
        t_1 = self.l_39(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_40(t_1)
        t_0 = self.l_41(t_0)
        t_0 = self.l_42(t_0)
        t_0 = self.l_43(t_0)
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Tensor::__add__
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]
        return (t_1, t_0)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition7(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Conv1D,
            Dropout,
            Attention,
            Gelu,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition7, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:7')
        self.lookup = { 'l_0': 'transformer.blocks.41.mlp.dropout',
                        'l_1': 'transformer.blocks.42.ln_1',
                        'l_2': 'transformer.blocks.42.attn',
                        'l_3': 'transformer.blocks.42.ln_2',
                        'l_4': 'transformer.blocks.42.mlp.c_fc',
                        'l_5': 'transformer.blocks.42.mlp.act',
                        'l_6': 'transformer.blocks.42.mlp.c_proj',
                        'l_7': 'transformer.blocks.42.mlp.dropout',
                        'l_8': 'transformer.blocks.43.ln_1',
                        'l_9': 'transformer.blocks.43.attn',
                        'l_10': 'transformer.blocks.43.ln_2',
                        'l_11': 'transformer.blocks.43.mlp.c_fc',
                        'l_12': 'transformer.blocks.43.mlp.act',
                        'l_13': 'transformer.blocks.43.mlp.c_proj',
                        'l_14': 'transformer.blocks.43.mlp.dropout',
                        'l_15': 'transformer.blocks.44.ln_1',
                        'l_16': 'transformer.blocks.44.attn',
                        'l_17': 'transformer.blocks.44.ln_2',
                        'l_18': 'transformer.blocks.44.mlp.c_fc',
                        'l_19': 'transformer.blocks.44.mlp.act',
                        'l_20': 'transformer.blocks.44.mlp.c_proj',
                        'l_21': 'transformer.blocks.44.mlp.dropout',
                        'l_22': 'transformer.blocks.45.ln_1',
                        'l_23': 'transformer.blocks.45.attn',
                        'l_24': 'transformer.blocks.45.ln_2',
                        'l_25': 'transformer.blocks.45.mlp.c_fc',
                        'l_26': 'transformer.blocks.45.mlp.act',
                        'l_27': 'transformer.blocks.45.mlp.c_proj',
                        'l_28': 'transformer.blocks.45.mlp.dropout',
                        'l_29': 'transformer.blocks.46.ln_1',
                        'l_30': 'transformer.blocks.46.attn',
                        'l_31': 'transformer.blocks.46.ln_2',
                        'l_32': 'transformer.blocks.46.mlp.c_fc',
                        'l_33': 'transformer.blocks.46.mlp.act',
                        'l_34': 'transformer.blocks.46.mlp.c_proj',
                        'l_35': 'transformer.blocks.46.mlp.dropout',
                        'l_36': 'transformer.blocks.47.ln_1',
                        'l_37': 'transformer.blocks.47.attn',
                        'l_38': 'transformer.blocks.47.ln_2',
                        'l_39': 'transformer.blocks.47.mlp.c_fc',
                        'l_40': 'transformer.blocks.47.mlp.act',
                        'l_41': 'transformer.blocks.47.mlp.c_proj',
                        'l_42': 'transformer.blocks.47.mlp.dropout',
                        'l_43': 'transformer.ln_f'}

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Gelu[act] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Gelu[act] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Gelu[act] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Gelu[act] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Gelu[act] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Gelu[act] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Tensor::__add__ <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj] <=> x1

        # moving inputs to current device no op if already on the correct device
        x0, x1 = move_tensors((x0, x1), self.device)
        t_0 = self.l_0(x1)
        del x1
        t_0 = x0 + t_0
        del x0
        t_1 = self.l_1(t_0)
        t_1 = self.l_2(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_3(t_1)
        t_0 = self.l_4(t_0)
        t_0 = self.l_5(t_0)
        t_0 = self.l_6(t_0)
        t_0 = self.l_7(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_8(t_0)
        t_1 = self.l_9(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_10(t_1)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_0 = self.l_14(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_15(t_0)
        t_1 = self.l_16(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_17(t_1)
        t_0 = self.l_18(t_0)
        t_0 = self.l_19(t_0)
        t_0 = self.l_20(t_0)
        t_0 = self.l_21(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_22(t_0)
        t_1 = self.l_23(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_24(t_1)
        t_0 = self.l_25(t_0)
        t_0 = self.l_26(t_0)
        t_0 = self.l_27(t_0)
        t_0 = self.l_28(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_29(t_0)
        t_1 = self.l_30(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_31(t_1)
        t_0 = self.l_32(t_0)
        t_0 = self.l_33(t_0)
        t_0 = self.l_34(t_0)
        t_0 = self.l_35(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_1 = self.l_36(t_0)
        t_1 = self.l_37(t_1)
        t_1 = t_0 + t_1
        del t_0
        t_0 = self.l_38(t_1)
        t_0 = self.l_39(t_0)
        t_0 = self.l_40(t_0)
        t_0 = self.l_41(t_0)
        t_0 = self.l_42(t_0)
        t_0 = t_1 + t_0
        del t_1
        t_0 = self.l_43(t_0)
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]
        return (t_0,)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition8(nn.Module):
    BASIC_BLOCKS=(
            StatelessLinear,
            LMOutput,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/StatelessLinear[stateless_lm_head]',
            'GPT2LMHeadModel/LMOutput[compute_output]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition8, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:0')
        self.lookup = { 'l_0': 'stateless_lm_head',
                        'l_1': 'compute_output'}

    def forward(self, x0, x1, x2):
        # GPT2LMHeadModel/StatelessLinear[stateless_lm_head] <=> self.l_0
        # GPT2LMHeadModel/LMOutput[compute_output] <=> self.l_1
        # input1 <=> x0
        # GPT2LMHeadModel/Parameter[w_wte] <=> x1
        # GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f] <=> x2

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2 = move_tensors((x0, x1, x2), self.device)
        t_0 = self.l_0(x1, x2)
        del x2
        del x1
        t_0 = self.l_1(t_0, labels=x0)
        del x0
        # returning:
        # GPT2LMHeadModel/LMOutput[compute_output]
        return (t_0,)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


def traverse_model(module: nn.Module, depth: int, prefix: Optional[str] = None,
                   basic_blocks: Tuple[nn.Module] = (), full: bool = False) -> Iterator[Tuple[nn.Module, str, nn.Module]]:
    '''
    iterate over model layers yielding the layer,layer_scope,encasing_module
    Parameters:
    -----------
    model:
        the model to iterate over
    depth:
        how far down in the model tree to go
    basic_blocks:
        a list of modules that if encountered will not be broken down
    full:
        whether to yield only layers specified by the depth and basick_block options or to yield all layers
    '''
    if prefix is None:
        prefix = type(module).__name__

    for name, sub_module in module.named_children():
        scope = prefix + "/" + type(sub_module).__name__ + f"[{name}]"
        if len(list(sub_module.children())) == 0 or isinstance(sub_module, tuple(basic_blocks)) or depth == 0:
            if full:
                yield sub_module, scope, module, True
            else:
                yield sub_module, scope, module
        else:
            if full:
                yield sub_module, scope, module, False
            yield from traverse_model(sub_module, depth - 1, scope, basic_blocks, full)


def layerDict(model: nn.Module, depth=1000, basic_blocks=()) -> Dict[str, nn.Module]:
    return {s: l for l, s, _ in traverse_model(model, depth, basic_blocks=basic_blocks)}


def traverse_params_buffs(module: nn.Module, prefix: Optional[str] = None) -> Iterator[Tuple[torch.tensor, str]]:
    '''
    iterate over model's buffers and parameters yielding obj,obj_scope

    Parameters:
    -----------
    model:
        the model to iterate over
    '''
    if prefix is None:
        prefix = type(module).__name__

    # params
    for param_name, param in module.named_parameters(recurse=False):
        param_scope = f"{prefix}/{type(param).__name__}[{param_name}]"
        yield param, param_scope

    # buffs
    for buffer_name, buffer in module.named_buffers(recurse=False):
        buffer_scope = f"{prefix}/{type(buffer).__name__}[{buffer_name}]"
        yield buffer, buffer_scope

    # recurse
    for name, sub_module in module.named_children():
        yield from traverse_params_buffs(sub_module, prefix + "/" + type(sub_module).__name__ + f"[{name}]")


def tensorDict(model: nn.Module) -> OrderedDict[str, Tensor]:
    return collections.OrderedDict((s, t)for t, s in traverse_params_buffs(model))


def move_tensors(ts, device):
    def move(t):
        if isinstance(t, (nn.Module, Tensor)):
            return t.to(device)
        return t

    return nested_map(move, ts)


def nested_map(func, ts):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        return func(ts)
    elif isinstance(ts, (list, tuple, set)):
        return type(ts)(nested_map(func, t) for t in ts)
    elif isinstance(ts, dict):
        return {k: nested_map(func, v) for k, v in ts.items()}
    elif isinstance(ts, slice):
        start = nested_map(func, ts.start)
        stop = nested_map(func, ts.stop)
        step = nested_map(func, ts.step)
        return slice(start, stop, step)
    return func(ts)


def state_dict(partition, device=None):
    # we return the state dict of this part as it should be in the original model
    state = nn.Module.state_dict(partition)
    lookup = partition.lookup
    result = dict()
    for k, v in state.items():
        if k in lookup:
            result[lookup[k]] = v if device is None else v.to(device)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            result[new_k] = v if device is None else v.to(device)
    return result


def load_state_dict(partition, state):
    reverse_lookup = {v: k for k, v in partition.lookup.items()}
    device = partition.device
    keys = list(partition.state_dict(None).keys())
    new_state = dict()
    for k in keys:
        if k in reverse_lookup:
            new_state[reverse_lookup[k]] = state[k].to(device)
            continue
        idx = k.rfind(".")
        to_replace = k[:idx]
        if to_replace in reverse_lookup:
            key = reverse_lookup[to_replace] + k[idx:]
            new_state[key] = state[k].to(device)
    nn.Module.load_state_dict(partition, new_state, strict=True)


def named_buffers(partition, recurse=True):
    # we return the named buffers of this part as it should be in the original model
    params = nn.Module.named_buffers(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def named_parameters(partition, recurse=True):
    # we return the named parameters of this part as it should be in the original model
    params = nn.Module.named_parameters(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def cpu(partition):
    partition.device = torch.device('cpu')
    return nn.Module.cpu(partition)


def cuda(partition, device=None):
    if device is None:
        device = torch.cuda.current_device()
    partition.device = torch.device(device)
    return nn.Module.cuda(partition, partition.device)


def to(partition, *args, **kwargs):
    device = None
    if 'device' in kwargs:
        device = kwargs['device']
    elif 'tensor' in kwargs:
        device = kwargs['tensor'].device
    if args:
        if isinstance(args[0], (torch.device, int, str)):
            device = args[0]
        if torch.is_tensor(args[0]):
            device = args[0].device
    if not (device is None):
        partition.device = torch.device(device)
    return nn.Module.to(partition, *args, **kwargs)

"""analysis summary
-I- Printing Report
Number of stages: 8
n_partitions:9, num_dummy_stages:1
unique_stages_on_same_gpu: [{0, 8}]
cutting edges are edges between partitions
number of cutting edges: 16

backward times include recomputation
Analysis for async_pipeline=True: last partition will not do recomputation.

real times are based on real measurements of execution time of generated partitions ms
forward {0: 165.92, 1: 162.68, 2: 148.55, 3: 148.54, 4: 149.59, 5: 157.37, 6: 155.98, 7: 148.59}
backward {0: 423.95, 1: 430.11, 2: 396.86, 3: 395.87, 4: 395.59, 5: 410.68, 6: 418.68, 7: 397.57}

balance is ratio of computation time between fastest and slowest parts. (between 0 and 1 higher is better)

real balance:
forward 0.895
backward 0.920

Assuming bandwidth of 12 GBps between GPUs

communication volumes size of activations of each partition
0: input size:'13.14 MB', recieve_time:'1.09 ms', out:'26.21 MB', send time:'2.18 ms'
1: input size:'26.21 MB', recieve_time:'2.18 ms', out:'26.21 MB', send time:'2.18 ms'
2: input size:'26.21 MB', recieve_time:'2.18 ms', out:'13.11 MB', send time:'1.09 ms'
3: input size:'13.11 MB', recieve_time:'1.09 ms', out:'13.11 MB', send time:'1.09 ms'
4: input size:'13.11 MB', recieve_time:'1.09 ms', out:'26.21 MB', send time:'2.18 ms'
5: input size:'26.21 MB', recieve_time:'2.18 ms', out:'65.54 MB', send time:'5.46 ms'
6: input size:'65.54 MB', recieve_time:'5.46 ms', out:'26.21 MB', send time:'2.18 ms'
7: input size:'26.21 MB', recieve_time:'2.18 ms', out:'13.11 MB', send time:'1.09 ms'

Compuatation Communication ratio (comp/(comp+comm)):
forward {0: 0.99, 1: 0.99, 2: 0.99, 3: 0.99, 4: 0.99, 5: 0.97, 6: 0.99, 7: 0.99} 
backward {0: 1.0, 1: 0.99, 2: 0.99, 3: 1.0, 4: 1.0, 5: 0.99, 6: 0.99, 7: 0.99}

Pipeline Slowdown: (compared to sequential executation with no communication, and same recompute policy)
forward 1.088
backward 1.058

Expected utilization by partition
forward {0: 0.99, 1: 0.97, 2: 0.89, 3: 0.89, 4: 0.89, 5: 0.92, 6: 0.93, 7: 0.89}
backward {0: 0.99, 1: 0.99, 2: 0.91, 3: 0.92, 4: 0.92, 5: 0.94, 6: 0.96, 7: 0.91}

worstcase: bwd: 430.109 fwd: 165.916
expected_speedup_compared_to_seq_no_recomp_no_comm: 5.535
Expected speedup for 8 partitions is: 7.501
"""