"""AutoGenerated with:
python -m autopipe.partition new_t5 --model_name_or_path t5-3b --t5_task squad1 --lmhead --n_iter 10 --analysis_batch_size 32 --partitioning_batch_size 32 --ct new_trace_cache_t53b_64_4_op --cp new_prof_cache_t53b_64_4_op_ftpipe --stateless_tied --lmhead --n_partitions 8 --L 16 --max_seq_length 64 --answer_max_seq_length 4 --partitioning_method mpipe --preset ftpipe --dont_use_async_meta_alg --save_memory_mode --special_blocks T5Block --output_file op_
"""
import torch
import torch.functional
#import torch.special
import torch.fft
import math
import torch.nn.functional
import torch.linalg
from torch import Tensor
import torch.nn as nn
from itertools import chain
from typing import Optional, Tuple, Iterator, Iterable, OrderedDict, Dict
import collections

from typing import Type
from models.new_t5_example.modeling_t5 import T5LayerNorm
from models.normal.NLP_models.stateless import StatelessEmbedding
from torch.nn.modules.sparse import Embedding
from torch.nn.modules.dropout import Dropout
from torch.nn.modules.linear import Linear
# this is an auto generated file do not edit unless you know what you are doing


# partition adjacency
# model inputs {0, 8, 15}
# partition 0 {'inputs': {'decoder_input_ids', 'input_ids', 'attention_mask', 'decoder_attention_mask'}, 'outputs': {8, 1}}
# partition 1 {'inputs': {0}, 'outputs': {2}}
# partition 2 {'inputs': {1}, 'outputs': {3}}
# partition 3 {'inputs': {2}, 'outputs': {4}}
# partition 4 {'inputs': {3}, 'outputs': {5}}
# partition 5 {'inputs': {4}, 'outputs': {6}}
# partition 6 {'inputs': {5}, 'outputs': {7}}
# partition 7 {'inputs': {6}, 'outputs': {8, 9, 10, 11, 12, 13, 14, 15}}
# partition 8 {'inputs': {0, 'attention_mask', 7}, 'outputs': {9, 15}}
# partition 9 {'inputs': {8, 7}, 'outputs': {10}}
# partition 10 {'inputs': {9, 7}, 'outputs': {11}}
# partition 11 {'inputs': {10, 7}, 'outputs': {12}}
# partition 12 {'inputs': {11, 7}, 'outputs': {13}}
# partition 13 {'inputs': {12, 7}, 'outputs': {14}}
# partition 14 {'inputs': {13, 7}, 'outputs': {15}}
# partition 15 {'inputs': {8, 7, 14, 'labels'}, 'outputs': {'output'}}
# model outputs {15}


def create_pipeline_configuration(DEBUG=False, batch_size=32):
    config = {
        'batch_dim': 0,
        'depth': 10000,
        'basic_blocks': (T5LayerNorm,StatelessEmbedding,Embedding,Dropout,Linear),
        'model_inputs': {
            'attention_mask': {
                'shape': torch.Size([32, 64]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0, 8]},
            'decoder_attention_mask': {
                'shape': torch.Size([32, 4]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0]},
            'decoder_input_ids': {
                'shape': torch.Size([32, 4]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0]},
            'input_ids': {
                'shape': torch.Size([32, 64]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0]},
            'labels': {
                'shape': torch.Size([32, 4]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [15]}},
        'model_outputs': {
            'T5ForConditionalGeneration/torch.nn.functional::cross_entropy_6186': {
                'shape': torch.Size([1]),
                'dtype': torch.float32,
                'is_batched': False,
                'created_by': 15}},
        'stages': {
            0: {
                'stage_cls': Partition0,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([32, 64]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'decoder_attention_mask': {
                        'shape': torch.Size([32, 4]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'decoder_input_ids': {
                        'shape': torch.Size([32, 4]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'input_ids': {
                        'shape': torch.Size([32, 64]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___333': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___335': {
                        'shape': torch.Size([32, 32, 64, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [8]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___2239': {
                        'shape': torch.Size([32, 1, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [8]}},
                'devices': ['cpu' if DEBUG else 'cuda:0'],
                'stage_depth': 15},
            1: {
                'stage_cls': Partition1,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___333': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___335': {
                        'shape': torch.Size([32, 32, 64, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___600': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [2]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___602': {
                        'shape': torch.Size([32, 32, 64, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [2]}},
                'devices': ['cpu' if DEBUG else 'cuda:1'],
                'stage_depth': 14},
            2: {
                'stage_cls': Partition2,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___600': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___602': {
                        'shape': torch.Size([32, 32, 64, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___867': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [3]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [3]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___869': {
                        'shape': torch.Size([32, 32, 64, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [3]}},
                'devices': ['cpu' if DEBUG else 'cuda:2'],
                'stage_depth': 13},
            3: {
                'stage_cls': Partition3,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___867': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 2},
                    'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 2},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___869': {
                        'shape': torch.Size([32, 32, 64, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 2}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1134': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [4]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1136': {
                        'shape': torch.Size([32, 32, 64, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [4]}},
                'devices': ['cpu' if DEBUG else 'cuda:3'],
                'stage_depth': 12},
            4: {
                'stage_cls': Partition4,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1134': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 3},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1136': {
                        'shape': torch.Size([32, 32, 64, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 3}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1401': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [5]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1403': {
                        'shape': torch.Size([32, 32, 64, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [5]}},
                'devices': ['cpu' if DEBUG else 'cuda:4'],
                'stage_depth': 11},
            5: {
                'stage_cls': Partition5,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1401': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 4},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1403': {
                        'shape': torch.Size([32, 32, 64, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 4}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1668': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [6]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1670': {
                        'shape': torch.Size([32, 32, 64, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [6]}},
                'devices': ['cpu' if DEBUG else 'cuda:5'],
                'stage_depth': 10},
            6: {
                'stage_cls': Partition6,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1668': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 5},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1670': {
                        'shape': torch.Size([32, 32, 64, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 5}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1935': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [7]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1937': {
                        'shape': torch.Size([32, 32, 64, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [7]}},
                'devices': ['cpu' if DEBUG else 'cuda:6'],
                'stage_depth': 9},
            7: {
                'stage_cls': Partition7,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1935': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 6},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1937': {
                        'shape': torch.Size([32, 32, 64, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 6}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_8': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [8]}},
                'devices': ['cpu' if DEBUG else 'cuda:7'],
                'stage_depth': 8},
            8: {
                'stage_cls': Partition8,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([32, 64]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_8': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 7},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___2239': {
                        'shape': torch.Size([32, 1, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 0}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_9': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [9]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([32, 64, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2784': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [9]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2786': {
                        'shape': torch.Size([32, 32, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [9]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2788': {
                        'shape': torch.Size([32, 32, 4, 64]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [9]}},
                'devices': ['cpu' if DEBUG else 'cuda:8'],
                'stage_depth': 7},
            9: {
                'stage_cls': Partition9,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_9': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 8},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2784': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 8},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2786': {
                        'shape': torch.Size([32, 32, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 8},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2788': {
                        'shape': torch.Size([32, 32, 4, 64]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 8}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_10': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [10]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3267': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [10]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3269': {
                        'shape': torch.Size([32, 32, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [10]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3271': {
                        'shape': torch.Size([32, 32, 4, 64]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [10]}},
                'devices': ['cpu' if DEBUG else 'cuda:9'],
                'stage_depth': 6},
            10: {
                'stage_cls': Partition10,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_10': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 9},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3267': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 9},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3269': {
                        'shape': torch.Size([32, 32, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 9},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3271': {
                        'shape': torch.Size([32, 32, 4, 64]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 9}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_11': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3750': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3752': {
                        'shape': torch.Size([32, 32, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3754': {
                        'shape': torch.Size([32, 32, 4, 64]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [11]}},
                'devices': ['cpu' if DEBUG else 'cuda:10'],
                'stage_depth': 5},
            11: {
                'stage_cls': Partition11,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_11': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3750': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3752': {
                        'shape': torch.Size([32, 32, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3754': {
                        'shape': torch.Size([32, 32, 4, 64]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 10}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_12': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4233': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4235': {
                        'shape': torch.Size([32, 32, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4237': {
                        'shape': torch.Size([32, 32, 4, 64]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [12]}},
                'devices': ['cpu' if DEBUG else 'cuda:11'],
                'stage_depth': 4},
            12: {
                'stage_cls': Partition12,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_12': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4233': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4235': {
                        'shape': torch.Size([32, 32, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4237': {
                        'shape': torch.Size([32, 32, 4, 64]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 11}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_13': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4716': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4718': {
                        'shape': torch.Size([32, 32, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4720': {
                        'shape': torch.Size([32, 32, 4, 64]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [13]}},
                'devices': ['cpu' if DEBUG else 'cuda:12'],
                'stage_depth': 3},
            13: {
                'stage_cls': Partition13,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_13': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 12},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4716': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 12},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4718': {
                        'shape': torch.Size([32, 32, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 12},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4720': {
                        'shape': torch.Size([32, 32, 4, 64]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 12}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_14': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5199': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5201': {
                        'shape': torch.Size([32, 32, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5203': {
                        'shape': torch.Size([32, 32, 4, 64]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [14]}},
                'devices': ['cpu' if DEBUG else 'cuda:13'],
                'stage_depth': 2},
            14: {
                'stage_cls': Partition14,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_14': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 13},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5199': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 13},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5201': {
                        'shape': torch.Size([32, 32, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 13},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5203': {
                        'shape': torch.Size([32, 32, 4, 64]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 13}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_15': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5682': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5684': {
                        'shape': torch.Size([32, 32, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5686': {
                        'shape': torch.Size([32, 32, 4, 64]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [15]}},
                'devices': ['cpu' if DEBUG else 'cuda:14'],
                'stage_depth': 1},
            15: {
                'stage_cls': Partition15,
                'inputs': {
                    'labels': {
                        'shape': torch.Size([32, 4]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_15': {
                        'shape': torch.Size([32, 64, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([32, 64, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 8},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5682': {
                        'shape': torch.Size([32, 4, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5684': {
                        'shape': torch.Size([32, 32, 4, 4]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5686': {
                        'shape': torch.Size([32, 32, 4, 64]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 14}},
                'outputs': {
                    'T5ForConditionalGeneration/torch.nn.functional::cross_entropy_6186': {
                        'shape': torch.Size([1]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [-1]}},
                'devices': ['cpu' if DEBUG else 'cuda:15'],
                'stage_depth': 0}}}
    
    
    # switching batch size
    batch_dim = config['batch_dim']
    for d in chain(config['model_inputs'].values(),config['model_outputs'].values()):
        if d['is_batched']:
            shape = d['shape']
            d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    for s in config['stages'].values():
        for d in chain(s['inputs'].values(),s['outputs'].values()):
            if d['is_batched']:
                shape = d['shape']
                d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    return config

class Partition0(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/StatelessEmbedding[embed_tokens]',
            'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Embedding[relative_attention_bias]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/StatelessEmbedding[embed_tokens]',
            'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]',
        ]
    TENSORS = [
            'T5ForConditionalGeneration/Parameter[shared_embed_weight]',
        ]
    def __init__(self, layers, tensors, device='cuda:0'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'encoder.embed_tokens',
                        'l_1': 'encoder.dropout',
                        'l_2': 'encoder.block.0.layer.0.layer_norm',
                        'l_3': 'encoder.block.0.layer.0.SelfAttention.q',
                        'l_4': 'encoder.block.0.layer.0.SelfAttention.k',
                        'l_5': 'encoder.block.0.layer.0.SelfAttention.v',
                        'l_6': 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias',
                        'l_7': 'encoder.block.0.layer.0.SelfAttention.o',
                        'l_8': 'encoder.block.0.layer.0.dropout',
                        'l_9': 'encoder.block.0.layer.1.layer_norm',
                        'l_10': 'encoder.block.0.layer.1.DenseReluDense.wi',
                        'l_11': 'encoder.block.0.layer.1.DenseReluDense.dropout',
                        'l_12': 'encoder.block.0.layer.1.DenseReluDense.wo',
                        'l_13': 'encoder.block.0.layer.1.dropout',
                        'l_14': 'encoder.block.1.layer.0.layer_norm',
                        'l_15': 'encoder.block.1.layer.0.SelfAttention.q',
                        'l_16': 'encoder.block.1.layer.0.SelfAttention.k',
                        'l_17': 'encoder.block.1.layer.0.SelfAttention.v',
                        'l_18': 'encoder.block.1.layer.0.SelfAttention.o',
                        'l_19': 'encoder.block.1.layer.0.dropout',
                        'l_20': 'encoder.block.1.layer.1.layer_norm',
                        'l_21': 'encoder.block.1.layer.1.DenseReluDense.wi',
                        'l_22': 'encoder.block.1.layer.1.DenseReluDense.dropout',
                        'l_23': 'encoder.block.1.layer.1.DenseReluDense.wo',
                        'l_24': 'encoder.block.1.layer.1.dropout',
                        'l_25': 'encoder.block.2.layer.0.layer_norm',
                        'l_26': 'encoder.block.2.layer.0.SelfAttention.q',
                        'l_27': 'encoder.block.2.layer.0.SelfAttention.k',
                        'l_28': 'encoder.block.2.layer.0.SelfAttention.v',
                        'l_29': 'encoder.block.2.layer.0.SelfAttention.o',
                        'l_30': 'encoder.block.2.layer.0.dropout',
                        'l_31': 'encoder.block.2.layer.1.layer_norm',
                        'l_32': 'encoder.block.2.layer.1.DenseReluDense.wi',
                        'l_33': 'encoder.block.2.layer.1.DenseReluDense.dropout',
                        'l_34': 'encoder.block.2.layer.1.DenseReluDense.wo',
                        'l_35': 'encoder.block.2.layer.1.dropout',
                        'l_36': 'decoder.embed_tokens',
                        'l_37': 'decoder.dropout',
                        'p_0': 'shared_embed_weight'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/StatelessEmbedding[embed_tokens] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Embedding[relative_attention_bias] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/StatelessEmbedding[embed_tokens] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout] <=> self.l_37
        # T5ForConditionalGeneration/Parameter[shared_embed_weight] <=> self.p_0
        # input0 <=> attention_mask
        # input1 <=> decoder_attention_mask
        # input2 <=> decoder_input_ids
        # input3 <=> input_ids
        attention_mask, decoder_attention_mask, decoder_input_ids, input_ids = unflatten(args, self.input_structure)
        t_0 = decoder_input_ids.size()
        t_1 = input_ids.size()
        t_1 = t_1[-1]
        t_1 = input_ids.view(-1, t_1)
        t_1 = self.l_0(self.p_0, t_1)
        t_1 = self.l_1(t_1)
        t_2 = attention_mask[(slice(None, None, None), None, None, slice(None, None, None))]
        t_2 = t_2.to(dtype=torch.float32)
        t_2 = 1.0 - t_2
        t_2 = t_2 * -10000.0
        t_3 = self.l_2(t_1)
        t_4 = self.l_3(t_3)
        t_5 = self.l_4(t_3)
        t_6 = self.l_5(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_7 = t_3[0]
        t_3 = t_3[1]
        t_4 = t_4.view(t_7, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_7, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_6 = t_6.view(t_7, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_5 = t_5.transpose(3, 2)
        t_5 = torch.matmul(t_4, t_5)
        t_4 = torch.arange(t_3, dtype=torch.int64, device=self.device)
        t_4 = t_4[(slice(None, None, None), None)]
        t_3 = torch.arange(t_3, dtype=torch.int64, device=self.device)
        t_3 = t_3[(None, slice(None, None, None))]
        t_4 = t_3 - t_4
        t_3 = torch.abs(t_4)
        t_4 = t_4 > 0
        t_4 = t_4.to(torch.int64)
        t_4 = t_4 * 16
        t_4 = 0 + t_4
        t_8 = t_3.float()
        t_9 = t_3 < 8
        t_8 = t_8 / 8
        t_8 = torch.log(t_8)
        t_10 = math.log(16.0)
        t_10 = t_8 / t_10
        t_10 = t_10 * 8
        t_10 = t_10.to(torch.int64)
        t_10 = 8 + t_10
        t_8 = torch.full_like(t_10, 15, device=self.device)
        t_8 = torch.min(t_10, t_8)
        t_8 = torch.where(t_9, t_3, t_8)
        t_4 += t_8
        t_8 = t_4
        t_8 = t_8.to(self.device)
        t_8 = self.l_6(t_8)
        t_8 = t_8.permute([2, 0, 1])
        t_8 = t_8.unsqueeze(0)
        t_2 = t_8 + t_2
        t_5 += t_2
        t_8 = t_5.float()
        t_8 = torch.nn.functional.softmax(t_8, dim=-1, _stacklevel=3, dtype=None)
        t_5 = t_8.type_as(t_5)
        t_5 = torch.nn.functional.dropout(t_5, p=0.1, training=True, inplace=False)
        t_6 = torch.matmul(t_5, t_6)
        t_6 = t_6.transpose(1, 2)
        t_6 = t_6.contiguous()
        t_7 = t_6.view(t_7, -1, 4096)
        t_7 = self.l_7(t_7)
        t_6 = self.l_8(t_7)
        t_6 = t_1 + t_6
        t_2 = (t_7, None, t_2)
        t_7 = t_2[0]
        t_6 = (t_6,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_6 + t_2
        t_6 = t_2[slice(None, 2, None)]
        t_1 = t_6[0]
        t_5 = self.l_9(t_1)
        t_6 = t_6[1]
        t_2 = t_2[slice(2, None, None)]
        t_5 = self.l_10(t_5)
        t_5 = torch.nn.functional.relu(t_5, inplace=False)
        t_5 = self.l_11(t_5)
        t_5 = self.l_12(t_5)
        t_5 = self.l_13(t_5)
        t_5 = t_1 + t_5
        t_6 = (t_5, t_6)
        t_2 = t_6 + t_2
        t_6 = t_2[slice(None, 2, None)]
        t_6 = t_6[0]
        t_5 = self.l_14(t_6)
        t_2 = t_2[2]
        t_1 = self.l_15(t_5)
        t_8 = self.l_16(t_5)
        t_4 = self.l_17(t_5)
        t_5 = t_5.shape
        t_5 = t_5[slice(None, 2, None)]
        t_5 = t_5[0]
        t_1 = t_1.view(t_5, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_8 = t_8.view(t_5, -1, 32, 128)
        t_8 = t_8.transpose(1, 2)
        t_4 = t_4.view(t_5, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_8 = t_8.transpose(3, 2)
        t_8 = torch.matmul(t_1, t_8)
        t_8 += t_2
        t_1 = t_8.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_8 = t_1.type_as(t_8)
        t_8 = torch.nn.functional.dropout(t_8, p=0.1, training=True, inplace=False)
        t_4 = torch.matmul(t_8, t_4)
        t_4 = t_4.transpose(1, 2)
        t_4 = t_4.contiguous()
        t_5 = t_4.view(t_5, -1, 4096)
        t_5 = self.l_18(t_5)
        t_4 = self.l_19(t_5)
        t_4 = t_6 + t_4
        t_2 = (t_5, None, t_2)
        t_5 = t_2[0]
        t_4 = (t_4,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_4 + t_2
        t_4 = t_2[slice(None, 2, None)]
        t_6 = t_4[0]
        t_8 = self.l_20(t_6)
        t_4 = t_4[1]
        t_2 = t_2[slice(2, None, None)]
        t_8 = self.l_21(t_8)
        t_8 = torch.nn.functional.relu(t_8, inplace=False)
        t_8 = self.l_22(t_8)
        t_8 = self.l_23(t_8)
        t_8 = self.l_24(t_8)
        t_8 = t_6 + t_8
        t_4 = (t_8, t_4)
        t_2 = t_4 + t_2
        t_4 = t_2[slice(None, 2, None)]
        t_4 = t_4[0]
        t_8 = self.l_25(t_4)
        t_2 = t_2[2]
        t_6 = self.l_26(t_8)
        t_1 = self.l_27(t_8)
        t_3 = self.l_28(t_8)
        t_8 = t_8.shape
        t_8 = t_8[slice(None, 2, None)]
        t_8 = t_8[0]
        t_6 = t_6.view(t_8, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_1 = t_1.view(t_8, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_3 = t_3.view(t_8, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_6, t_1)
        t_1 += t_2
        t_6 = t_1.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_6.type_as(t_1)
        t_1 = torch.nn.functional.dropout(t_1, p=0.1, training=True, inplace=False)
        t_3 = torch.matmul(t_1, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_8 = t_3.view(t_8, -1, 4096)
        t_8 = self.l_29(t_8)
        t_3 = self.l_30(t_8)
        t_3 = t_4 + t_3
        t_2 = (t_8, None, t_2)
        t_8 = t_2[0]
        t_3 = (t_3,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_3 + t_2
        t_3 = t_2[slice(None, 2, None)]
        t_4 = t_3[0]
        t_1 = self.l_31(t_4)
        t_3 = t_3[1]
        t_2 = t_2[slice(2, None, None)]
        t_1 = self.l_32(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_33(t_1)
        t_1 = self.l_34(t_1)
        t_1 = self.l_35(t_1)
        t_1 = t_4 + t_1
        t_3 = (t_1, t_3)
        t_2 = t_3 + t_2
        t_3 = t_2[slice(None, 2, None)]
        t_3 = t_3[0]
        t_2 = t_2[2]
        t_1 = t_0[-1]
        t_1 = decoder_input_ids.view(-1, t_1)
        t_1 = self.l_36(self.p_0, t_1)
        t_1 = self.l_37(t_1)
        t_4 = t_0[0]
        t_0 = t_0[1]
        t_6 = torch.arange(t_0, device=self.device)
        t_9 = t_6[(None, None, slice(None, None, None))]
        t_0 = t_9.repeat(t_4, t_0, 1)
        t_6 = t_6[(None, slice(None, None, None), None)]
        t_6 = t_0 <= t_6
        t_0 = decoder_attention_mask.dtype
        t_0 = t_6.to(t_0)
        t_0 = t_0[(slice(None, None, None), None, slice(None, None, None), slice(None, None, None))]
        t_6 = decoder_attention_mask[(slice(None, None, None), None, None, slice(None, None, None))]
        t_6 = t_0 * t_6
        t_6 = t_6.to(dtype=torch.float32)
        t_6 = 1.0 - t_6
        t_6 = t_6 * -10000.0
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___333
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___335
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___2239
        return list(flatten((t_3, t_2, t_1, t_6)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition1(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:1'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.3.layer.0.layer_norm',
                        'l_1': 'encoder.block.3.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.3.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.3.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.3.layer.0.SelfAttention.o',
                        'l_5': 'encoder.block.3.layer.0.dropout',
                        'l_6': 'encoder.block.3.layer.1.layer_norm',
                        'l_7': 'encoder.block.3.layer.1.DenseReluDense.wi',
                        'l_8': 'encoder.block.3.layer.1.DenseReluDense.dropout',
                        'l_9': 'encoder.block.3.layer.1.DenseReluDense.wo',
                        'l_10': 'encoder.block.3.layer.1.dropout',
                        'l_11': 'encoder.block.4.layer.0.layer_norm',
                        'l_12': 'encoder.block.4.layer.0.SelfAttention.q',
                        'l_13': 'encoder.block.4.layer.0.SelfAttention.k',
                        'l_14': 'encoder.block.4.layer.0.SelfAttention.v',
                        'l_15': 'encoder.block.4.layer.0.SelfAttention.o',
                        'l_16': 'encoder.block.4.layer.0.dropout',
                        'l_17': 'encoder.block.4.layer.1.layer_norm',
                        'l_18': 'encoder.block.4.layer.1.DenseReluDense.wi',
                        'l_19': 'encoder.block.4.layer.1.DenseReluDense.dropout',
                        'l_20': 'encoder.block.4.layer.1.DenseReluDense.wo',
                        'l_21': 'encoder.block.4.layer.1.dropout',
                        'l_22': 'encoder.block.5.layer.0.layer_norm',
                        'l_23': 'encoder.block.5.layer.0.SelfAttention.q',
                        'l_24': 'encoder.block.5.layer.0.SelfAttention.k',
                        'l_25': 'encoder.block.5.layer.0.SelfAttention.v',
                        'l_26': 'encoder.block.5.layer.0.SelfAttention.o',
                        'l_27': 'encoder.block.5.layer.0.dropout',
                        'l_28': 'encoder.block.5.layer.1.layer_norm',
                        'l_29': 'encoder.block.5.layer.1.DenseReluDense.wi',
                        'l_30': 'encoder.block.5.layer.1.DenseReluDense.dropout',
                        'l_31': 'encoder.block.5.layer.1.DenseReluDense.wo',
                        'l_32': 'encoder.block.5.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___333 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___335 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_4(t_0)
        t_3 = self.l_5(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_2 = t_0[0]
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_1 = t_3[0]
        t_4 = self.l_6(t_1)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_4 = self.l_7(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_8(t_4)
        t_4 = self.l_9(t_4)
        t_4 = self.l_10(t_4)
        t_4 = t_1 + t_4
        t_3 = (t_4, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_4 = self.l_11(t_3)
        t_0 = t_0[2]
        t_1 = self.l_12(t_4)
        t_5 = self.l_13(t_4)
        t_6 = self.l_14(t_4)
        t_4 = t_4.shape
        t_4 = t_4[slice(None, 2, None)]
        t_4 = t_4[0]
        t_1 = t_1.view(t_4, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_5 = t_5.view(t_4, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_6 = t_6.view(t_4, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_5 = t_5.transpose(3, 2)
        t_5 = torch.matmul(t_1, t_5)
        t_5 += t_0
        t_1 = t_5.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_5 = t_1.type_as(t_5)
        t_5 = torch.nn.functional.dropout(t_5, p=0.1, training=True, inplace=False)
        t_6 = torch.matmul(t_5, t_6)
        t_6 = t_6.transpose(1, 2)
        t_6 = t_6.contiguous()
        t_4 = t_6.view(t_4, -1, 4096)
        t_4 = self.l_15(t_4)
        t_6 = self.l_16(t_4)
        t_6 = t_3 + t_6
        t_0 = (t_4, None, t_0)
        t_4 = t_0[0]
        t_6 = (t_6,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_6 + t_0
        t_6 = t_0[slice(None, 2, None)]
        t_3 = t_6[0]
        t_5 = self.l_17(t_3)
        t_6 = t_6[1]
        t_0 = t_0[slice(2, None, None)]
        t_5 = self.l_18(t_5)
        t_5 = torch.nn.functional.relu(t_5, inplace=False)
        t_5 = self.l_19(t_5)
        t_5 = self.l_20(t_5)
        t_5 = self.l_21(t_5)
        t_5 = t_3 + t_5
        t_6 = (t_5, t_6)
        t_0 = t_6 + t_0
        t_6 = t_0[slice(None, 2, None)]
        t_6 = t_6[0]
        t_5 = self.l_22(t_6)
        t_0 = t_0[2]
        t_3 = self.l_23(t_5)
        t_1 = self.l_24(t_5)
        t_7 = self.l_25(t_5)
        t_5 = t_5.shape
        t_5 = t_5[slice(None, 2, None)]
        t_5 = t_5[0]
        t_3 = t_3.view(t_5, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_1 = t_1.view(t_5, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_7 = t_7.view(t_5, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_3, t_1)
        t_1 += t_0
        t_3 = t_1.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_3.type_as(t_1)
        t_1 = torch.nn.functional.dropout(t_1, p=0.1, training=True, inplace=False)
        t_7 = torch.matmul(t_1, t_7)
        t_7 = t_7.transpose(1, 2)
        t_7 = t_7.contiguous()
        t_5 = t_7.view(t_5, -1, 4096)
        t_5 = self.l_26(t_5)
        t_7 = self.l_27(t_5)
        t_7 = t_6 + t_7
        t_0 = (t_5, None, t_0)
        t_5 = t_0[0]
        t_7 = (t_7,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_7 + t_0
        t_7 = t_0[slice(None, 2, None)]
        t_6 = t_7[0]
        t_1 = self.l_28(t_6)
        t_7 = t_7[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_29(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_30(t_1)
        t_1 = self.l_31(t_1)
        t_1 = self.l_32(t_1)
        t_1 = t_6 + t_1
        t_7 = (t_1, t_7)
        t_0 = t_7 + t_0
        t_7 = t_0[slice(None, 2, None)]
        t_7 = t_7[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___600
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___602
        return list(flatten((t_7, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition2(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:2'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.6.layer.0.layer_norm',
                        'l_1': 'encoder.block.6.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.6.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.6.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.6.layer.0.SelfAttention.o',
                        'l_5': 'encoder.block.6.layer.0.dropout',
                        'l_6': 'encoder.block.6.layer.1.layer_norm',
                        'l_7': 'encoder.block.6.layer.1.DenseReluDense.wi',
                        'l_8': 'encoder.block.6.layer.1.DenseReluDense.dropout',
                        'l_9': 'encoder.block.6.layer.1.DenseReluDense.wo',
                        'l_10': 'encoder.block.6.layer.1.dropout',
                        'l_11': 'encoder.block.7.layer.0.layer_norm',
                        'l_12': 'encoder.block.7.layer.0.SelfAttention.q',
                        'l_13': 'encoder.block.7.layer.0.SelfAttention.k',
                        'l_14': 'encoder.block.7.layer.0.SelfAttention.v',
                        'l_15': 'encoder.block.7.layer.0.SelfAttention.o',
                        'l_16': 'encoder.block.7.layer.0.dropout',
                        'l_17': 'encoder.block.7.layer.1.layer_norm',
                        'l_18': 'encoder.block.7.layer.1.DenseReluDense.wi',
                        'l_19': 'encoder.block.7.layer.1.DenseReluDense.dropout',
                        'l_20': 'encoder.block.7.layer.1.DenseReluDense.wo',
                        'l_21': 'encoder.block.7.layer.1.dropout',
                        'l_22': 'encoder.block.8.layer.0.layer_norm',
                        'l_23': 'encoder.block.8.layer.0.SelfAttention.q',
                        'l_24': 'encoder.block.8.layer.0.SelfAttention.k',
                        'l_25': 'encoder.block.8.layer.0.SelfAttention.v',
                        'l_26': 'encoder.block.8.layer.0.SelfAttention.o',
                        'l_27': 'encoder.block.8.layer.0.dropout',
                        'l_28': 'encoder.block.8.layer.1.layer_norm',
                        'l_29': 'encoder.block.8.layer.1.DenseReluDense.wi',
                        'l_30': 'encoder.block.8.layer.1.DenseReluDense.dropout',
                        'l_31': 'encoder.block.8.layer.1.DenseReluDense.wo',
                        'l_32': 'encoder.block.8.layer.1.dropout',
                        'l_33': 'encoder.block.9.layer.0.layer_norm'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___600 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___602 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_4(t_0)
        t_3 = self.l_5(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_2 = t_0[0]
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_1 = t_3[0]
        t_4 = self.l_6(t_1)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_4 = self.l_7(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_8(t_4)
        t_4 = self.l_9(t_4)
        t_4 = self.l_10(t_4)
        t_4 = t_1 + t_4
        t_3 = (t_4, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_4 = self.l_11(t_3)
        t_0 = t_0[2]
        t_1 = self.l_12(t_4)
        t_5 = self.l_13(t_4)
        t_6 = self.l_14(t_4)
        t_4 = t_4.shape
        t_4 = t_4[slice(None, 2, None)]
        t_4 = t_4[0]
        t_1 = t_1.view(t_4, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_5 = t_5.view(t_4, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_6 = t_6.view(t_4, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_5 = t_5.transpose(3, 2)
        t_5 = torch.matmul(t_1, t_5)
        t_5 += t_0
        t_1 = t_5.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_5 = t_1.type_as(t_5)
        t_5 = torch.nn.functional.dropout(t_5, p=0.1, training=True, inplace=False)
        t_6 = torch.matmul(t_5, t_6)
        t_6 = t_6.transpose(1, 2)
        t_6 = t_6.contiguous()
        t_4 = t_6.view(t_4, -1, 4096)
        t_4 = self.l_15(t_4)
        t_6 = self.l_16(t_4)
        t_6 = t_3 + t_6
        t_0 = (t_4, None, t_0)
        t_4 = t_0[0]
        t_6 = (t_6,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_6 + t_0
        t_6 = t_0[slice(None, 2, None)]
        t_3 = t_6[0]
        t_5 = self.l_17(t_3)
        t_6 = t_6[1]
        t_0 = t_0[slice(2, None, None)]
        t_5 = self.l_18(t_5)
        t_5 = torch.nn.functional.relu(t_5, inplace=False)
        t_5 = self.l_19(t_5)
        t_5 = self.l_20(t_5)
        t_5 = self.l_21(t_5)
        t_5 = t_3 + t_5
        t_6 = (t_5, t_6)
        t_0 = t_6 + t_0
        t_6 = t_0[slice(None, 2, None)]
        t_6 = t_6[0]
        t_5 = self.l_22(t_6)
        t_0 = t_0[2]
        t_3 = self.l_23(t_5)
        t_1 = self.l_24(t_5)
        t_7 = self.l_25(t_5)
        t_5 = t_5.shape
        t_5 = t_5[slice(None, 2, None)]
        t_5 = t_5[0]
        t_3 = t_3.view(t_5, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_1 = t_1.view(t_5, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_7 = t_7.view(t_5, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_3, t_1)
        t_1 += t_0
        t_3 = t_1.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_3.type_as(t_1)
        t_1 = torch.nn.functional.dropout(t_1, p=0.1, training=True, inplace=False)
        t_7 = torch.matmul(t_1, t_7)
        t_7 = t_7.transpose(1, 2)
        t_7 = t_7.contiguous()
        t_5 = t_7.view(t_5, -1, 4096)
        t_5 = self.l_26(t_5)
        t_7 = self.l_27(t_5)
        t_7 = t_6 + t_7
        t_0 = (t_5, None, t_0)
        t_5 = t_0[0]
        t_7 = (t_7,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_7 + t_0
        t_7 = t_0[slice(None, 2, None)]
        t_6 = t_7[0]
        t_1 = self.l_28(t_6)
        t_7 = t_7[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_29(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_30(t_1)
        t_1 = self.l_31(t_1)
        t_1 = self.l_32(t_1)
        t_1 = t_6 + t_1
        t_7 = (t_1, t_7)
        t_0 = t_7 + t_0
        t_7 = t_0[slice(None, 2, None)]
        t_7 = t_7[0]
        t_1 = self.l_33(t_7)
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___867
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___869
        return list(flatten((t_7, t_1, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition3(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:3'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = {'l_0': 'encoder.block.9.layer.0.SelfAttention.q',
                        'l_1': 'encoder.block.9.layer.0.SelfAttention.k',
                        'l_2': 'encoder.block.9.layer.0.SelfAttention.v',
                        'l_3': 'encoder.block.9.layer.0.SelfAttention.o',
                        'l_4': 'encoder.block.9.layer.0.dropout',
                        'l_5': 'encoder.block.9.layer.1.layer_norm',
                        'l_6': 'encoder.block.9.layer.1.DenseReluDense.wi',
                        'l_7': 'encoder.block.9.layer.1.DenseReluDense.dropout',
                        'l_8': 'encoder.block.9.layer.1.DenseReluDense.wo',
                        'l_9': 'encoder.block.9.layer.1.dropout',
                        'l_10': 'encoder.block.10.layer.0.layer_norm',
                        'l_11': 'encoder.block.10.layer.0.SelfAttention.q',
                        'l_12': 'encoder.block.10.layer.0.SelfAttention.k',
                        'l_13': 'encoder.block.10.layer.0.SelfAttention.v',
                        'l_14': 'encoder.block.10.layer.0.SelfAttention.o',
                        'l_15': 'encoder.block.10.layer.0.dropout',
                        'l_16': 'encoder.block.10.layer.1.layer_norm',
                        'l_17': 'encoder.block.10.layer.1.DenseReluDense.wi',
                        'l_18': 'encoder.block.10.layer.1.DenseReluDense.dropout',
                        'l_19': 'encoder.block.10.layer.1.DenseReluDense.wo',
                        'l_20': 'encoder.block.10.layer.1.dropout',
                        'l_21': 'encoder.block.11.layer.0.layer_norm',
                        'l_22': 'encoder.block.11.layer.0.SelfAttention.q',
                        'l_23': 'encoder.block.11.layer.0.SelfAttention.k',
                        'l_24': 'encoder.block.11.layer.0.SelfAttention.v',
                        'l_25': 'encoder.block.11.layer.0.SelfAttention.o',
                        'l_26': 'encoder.block.11.layer.0.dropout',
                        'l_27': 'encoder.block.11.layer.1.layer_norm',
                        'l_28': 'encoder.block.11.layer.1.DenseReluDense.wi',
                        'l_29': 'encoder.block.11.layer.1.DenseReluDense.dropout',
                        'l_30': 'encoder.block.11.layer.1.DenseReluDense.wo',
                        'l_31': 'encoder.block.11.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___867 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___869 <=> x2
        x0, x1, x2 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x1)
        t_1 = self.l_1(x1)
        t_2 = self.l_2(x1)
        t_3 = x1.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_3, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_0, t_1)
        t_1 += x2
        t_0 = t_1.float()
        t_0 = torch.nn.functional.softmax(t_0, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_0.type_as(t_1)
        t_1 = torch.nn.functional.dropout(t_1, p=0.1, training=True, inplace=False)
        t_2 = torch.matmul(t_1, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_3 = t_2.view(t_3, -1, 4096)
        t_3 = self.l_3(t_3)
        t_2 = self.l_4(t_3)
        t_2 = x0 + t_2
        t_3 = (t_3, None, x2)
        t_1 = t_3[0]
        t_2 = (t_2,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_2 + t_3
        t_2 = t_3[slice(None, 2, None)]
        t_0 = t_2[0]
        t_4 = self.l_5(t_0)
        t_2 = t_2[1]
        t_3 = t_3[slice(2, None, None)]
        t_4 = self.l_6(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_7(t_4)
        t_4 = self.l_8(t_4)
        t_4 = self.l_9(t_4)
        t_4 = t_0 + t_4
        t_2 = (t_4, t_2)
        t_3 = t_2 + t_3
        t_2 = t_3[slice(None, 2, None)]
        t_2 = t_2[0]
        t_4 = self.l_10(t_2)
        t_3 = t_3[2]
        t_0 = self.l_11(t_4)
        t_5 = self.l_12(t_4)
        t_6 = self.l_13(t_4)
        t_4 = t_4.shape
        t_4 = t_4[slice(None, 2, None)]
        t_4 = t_4[0]
        t_0 = t_0.view(t_4, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_5 = t_5.view(t_4, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_6 = t_6.view(t_4, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_5 = t_5.transpose(3, 2)
        t_5 = torch.matmul(t_0, t_5)
        t_5 += t_3
        t_0 = t_5.float()
        t_0 = torch.nn.functional.softmax(t_0, dim=-1, _stacklevel=3, dtype=None)
        t_5 = t_0.type_as(t_5)
        t_5 = torch.nn.functional.dropout(t_5, p=0.1, training=True, inplace=False)
        t_6 = torch.matmul(t_5, t_6)
        t_6 = t_6.transpose(1, 2)
        t_6 = t_6.contiguous()
        t_4 = t_6.view(t_4, -1, 4096)
        t_4 = self.l_14(t_4)
        t_6 = self.l_15(t_4)
        t_6 = t_2 + t_6
        t_3 = (t_4, None, t_3)
        t_4 = t_3[0]
        t_6 = (t_6,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_6 + t_3
        t_6 = t_3[slice(None, 2, None)]
        t_2 = t_6[0]
        t_5 = self.l_16(t_2)
        t_6 = t_6[1]
        t_3 = t_3[slice(2, None, None)]
        t_5 = self.l_17(t_5)
        t_5 = torch.nn.functional.relu(t_5, inplace=False)
        t_5 = self.l_18(t_5)
        t_5 = self.l_19(t_5)
        t_5 = self.l_20(t_5)
        t_5 = t_2 + t_5
        t_6 = (t_5, t_6)
        t_3 = t_6 + t_3
        t_6 = t_3[slice(None, 2, None)]
        t_6 = t_6[0]
        t_5 = self.l_21(t_6)
        t_3 = t_3[2]
        t_2 = self.l_22(t_5)
        t_0 = self.l_23(t_5)
        t_7 = self.l_24(t_5)
        t_5 = t_5.shape
        t_5 = t_5[slice(None, 2, None)]
        t_5 = t_5[0]
        t_2 = t_2.view(t_5, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_0 = t_0.view(t_5, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_7 = t_7.view(t_5, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_2, t_0)
        t_0 += t_3
        t_2 = t_0.float()
        t_2 = torch.nn.functional.softmax(t_2, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_2.type_as(t_0)
        t_0 = torch.nn.functional.dropout(t_0, p=0.1, training=True, inplace=False)
        t_7 = torch.matmul(t_0, t_7)
        t_7 = t_7.transpose(1, 2)
        t_7 = t_7.contiguous()
        t_5 = t_7.view(t_5, -1, 4096)
        t_5 = self.l_25(t_5)
        t_7 = self.l_26(t_5)
        t_7 = t_6 + t_7
        t_3 = (t_5, None, t_3)
        t_5 = t_3[0]
        t_7 = (t_7,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_7 + t_3
        t_7 = t_3[slice(None, 2, None)]
        t_6 = t_7[0]
        t_0 = self.l_27(t_6)
        t_7 = t_7[1]
        t_3 = t_3[slice(2, None, None)]
        t_0 = self.l_28(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_29(t_0)
        t_0 = self.l_30(t_0)
        t_0 = self.l_31(t_0)
        t_0 = t_6 + t_0
        t_7 = (t_0, t_7)
        t_3 = t_7 + t_3
        t_7 = t_3[slice(None, 2, None)]
        t_7 = t_7[0]
        t_3 = t_3[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1134
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1136
        return list(flatten((t_7, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition4(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:4'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.12.layer.0.layer_norm',
                        'l_1': 'encoder.block.12.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.12.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.12.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.12.layer.0.SelfAttention.o',
                        'l_5': 'encoder.block.12.layer.0.dropout',
                        'l_6': 'encoder.block.12.layer.1.layer_norm',
                        'l_7': 'encoder.block.12.layer.1.DenseReluDense.wi',
                        'l_8': 'encoder.block.12.layer.1.DenseReluDense.dropout',
                        'l_9': 'encoder.block.12.layer.1.DenseReluDense.wo',
                        'l_10': 'encoder.block.12.layer.1.dropout',
                        'l_11': 'encoder.block.13.layer.0.layer_norm',
                        'l_12': 'encoder.block.13.layer.0.SelfAttention.q',
                        'l_13': 'encoder.block.13.layer.0.SelfAttention.k',
                        'l_14': 'encoder.block.13.layer.0.SelfAttention.v',
                        'l_15': 'encoder.block.13.layer.0.SelfAttention.o',
                        'l_16': 'encoder.block.13.layer.0.dropout',
                        'l_17': 'encoder.block.13.layer.1.layer_norm',
                        'l_18': 'encoder.block.13.layer.1.DenseReluDense.wi',
                        'l_19': 'encoder.block.13.layer.1.DenseReluDense.dropout',
                        'l_20': 'encoder.block.13.layer.1.DenseReluDense.wo',
                        'l_21': 'encoder.block.13.layer.1.dropout',
                        'l_22': 'encoder.block.14.layer.0.layer_norm',
                        'l_23': 'encoder.block.14.layer.0.SelfAttention.q',
                        'l_24': 'encoder.block.14.layer.0.SelfAttention.k',
                        'l_25': 'encoder.block.14.layer.0.SelfAttention.v',
                        'l_26': 'encoder.block.14.layer.0.SelfAttention.o',
                        'l_27': 'encoder.block.14.layer.0.dropout',
                        'l_28': 'encoder.block.14.layer.1.layer_norm',
                        'l_29': 'encoder.block.14.layer.1.DenseReluDense.wi',
                        'l_30': 'encoder.block.14.layer.1.DenseReluDense.dropout',
                        'l_31': 'encoder.block.14.layer.1.DenseReluDense.wo',
                        'l_32': 'encoder.block.14.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1134 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1136 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_4(t_0)
        t_3 = self.l_5(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_2 = t_0[0]
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_1 = t_3[0]
        t_4 = self.l_6(t_1)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_4 = self.l_7(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_8(t_4)
        t_4 = self.l_9(t_4)
        t_4 = self.l_10(t_4)
        t_4 = t_1 + t_4
        t_3 = (t_4, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_4 = self.l_11(t_3)
        t_0 = t_0[2]
        t_1 = self.l_12(t_4)
        t_5 = self.l_13(t_4)
        t_6 = self.l_14(t_4)
        t_4 = t_4.shape
        t_4 = t_4[slice(None, 2, None)]
        t_4 = t_4[0]
        t_1 = t_1.view(t_4, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_5 = t_5.view(t_4, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_6 = t_6.view(t_4, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_5 = t_5.transpose(3, 2)
        t_5 = torch.matmul(t_1, t_5)
        t_5 += t_0
        t_1 = t_5.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_5 = t_1.type_as(t_5)
        t_5 = torch.nn.functional.dropout(t_5, p=0.1, training=True, inplace=False)
        t_6 = torch.matmul(t_5, t_6)
        t_6 = t_6.transpose(1, 2)
        t_6 = t_6.contiguous()
        t_4 = t_6.view(t_4, -1, 4096)
        t_4 = self.l_15(t_4)
        t_6 = self.l_16(t_4)
        t_6 = t_3 + t_6
        t_0 = (t_4, None, t_0)
        t_4 = t_0[0]
        t_6 = (t_6,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_6 + t_0
        t_6 = t_0[slice(None, 2, None)]
        t_3 = t_6[0]
        t_5 = self.l_17(t_3)
        t_6 = t_6[1]
        t_0 = t_0[slice(2, None, None)]
        t_5 = self.l_18(t_5)
        t_5 = torch.nn.functional.relu(t_5, inplace=False)
        t_5 = self.l_19(t_5)
        t_5 = self.l_20(t_5)
        t_5 = self.l_21(t_5)
        t_5 = t_3 + t_5
        t_6 = (t_5, t_6)
        t_0 = t_6 + t_0
        t_6 = t_0[slice(None, 2, None)]
        t_6 = t_6[0]
        t_5 = self.l_22(t_6)
        t_0 = t_0[2]
        t_3 = self.l_23(t_5)
        t_1 = self.l_24(t_5)
        t_7 = self.l_25(t_5)
        t_5 = t_5.shape
        t_5 = t_5[slice(None, 2, None)]
        t_5 = t_5[0]
        t_3 = t_3.view(t_5, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_1 = t_1.view(t_5, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_7 = t_7.view(t_5, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_3, t_1)
        t_1 += t_0
        t_3 = t_1.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_3.type_as(t_1)
        t_1 = torch.nn.functional.dropout(t_1, p=0.1, training=True, inplace=False)
        t_7 = torch.matmul(t_1, t_7)
        t_7 = t_7.transpose(1, 2)
        t_7 = t_7.contiguous()
        t_5 = t_7.view(t_5, -1, 4096)
        t_5 = self.l_26(t_5)
        t_7 = self.l_27(t_5)
        t_7 = t_6 + t_7
        t_0 = (t_5, None, t_0)
        t_5 = t_0[0]
        t_7 = (t_7,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_7 + t_0
        t_7 = t_0[slice(None, 2, None)]
        t_6 = t_7[0]
        t_1 = self.l_28(t_6)
        t_7 = t_7[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_29(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_30(t_1)
        t_1 = self.l_31(t_1)
        t_1 = self.l_32(t_1)
        t_1 = t_6 + t_1
        t_7 = (t_1, t_7)
        t_0 = t_7 + t_0
        t_7 = t_0[slice(None, 2, None)]
        t_7 = t_7[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1401
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1403
        return list(flatten((t_7, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition5(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:5'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.15.layer.0.layer_norm',
                        'l_1': 'encoder.block.15.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.15.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.15.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.15.layer.0.SelfAttention.o',
                        'l_5': 'encoder.block.15.layer.0.dropout',
                        'l_6': 'encoder.block.15.layer.1.layer_norm',
                        'l_7': 'encoder.block.15.layer.1.DenseReluDense.wi',
                        'l_8': 'encoder.block.15.layer.1.DenseReluDense.dropout',
                        'l_9': 'encoder.block.15.layer.1.DenseReluDense.wo',
                        'l_10': 'encoder.block.15.layer.1.dropout',
                        'l_11': 'encoder.block.16.layer.0.layer_norm',
                        'l_12': 'encoder.block.16.layer.0.SelfAttention.q',
                        'l_13': 'encoder.block.16.layer.0.SelfAttention.k',
                        'l_14': 'encoder.block.16.layer.0.SelfAttention.v',
                        'l_15': 'encoder.block.16.layer.0.SelfAttention.o',
                        'l_16': 'encoder.block.16.layer.0.dropout',
                        'l_17': 'encoder.block.16.layer.1.layer_norm',
                        'l_18': 'encoder.block.16.layer.1.DenseReluDense.wi',
                        'l_19': 'encoder.block.16.layer.1.DenseReluDense.dropout',
                        'l_20': 'encoder.block.16.layer.1.DenseReluDense.wo',
                        'l_21': 'encoder.block.16.layer.1.dropout',
                        'l_22': 'encoder.block.17.layer.0.layer_norm',
                        'l_23': 'encoder.block.17.layer.0.SelfAttention.q',
                        'l_24': 'encoder.block.17.layer.0.SelfAttention.k',
                        'l_25': 'encoder.block.17.layer.0.SelfAttention.v',
                        'l_26': 'encoder.block.17.layer.0.SelfAttention.o',
                        'l_27': 'encoder.block.17.layer.0.dropout',
                        'l_28': 'encoder.block.17.layer.1.layer_norm',
                        'l_29': 'encoder.block.17.layer.1.DenseReluDense.wi',
                        'l_30': 'encoder.block.17.layer.1.DenseReluDense.dropout',
                        'l_31': 'encoder.block.17.layer.1.DenseReluDense.wo',
                        'l_32': 'encoder.block.17.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1401 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1403 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_4(t_0)
        t_3 = self.l_5(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_2 = t_0[0]
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_1 = t_3[0]
        t_4 = self.l_6(t_1)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_4 = self.l_7(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_8(t_4)
        t_4 = self.l_9(t_4)
        t_4 = self.l_10(t_4)
        t_4 = t_1 + t_4
        t_3 = (t_4, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_4 = self.l_11(t_3)
        t_0 = t_0[2]
        t_1 = self.l_12(t_4)
        t_5 = self.l_13(t_4)
        t_6 = self.l_14(t_4)
        t_4 = t_4.shape
        t_4 = t_4[slice(None, 2, None)]
        t_4 = t_4[0]
        t_1 = t_1.view(t_4, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_5 = t_5.view(t_4, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_6 = t_6.view(t_4, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_5 = t_5.transpose(3, 2)
        t_5 = torch.matmul(t_1, t_5)
        t_5 += t_0
        t_1 = t_5.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_5 = t_1.type_as(t_5)
        t_5 = torch.nn.functional.dropout(t_5, p=0.1, training=True, inplace=False)
        t_6 = torch.matmul(t_5, t_6)
        t_6 = t_6.transpose(1, 2)
        t_6 = t_6.contiguous()
        t_4 = t_6.view(t_4, -1, 4096)
        t_4 = self.l_15(t_4)
        t_6 = self.l_16(t_4)
        t_6 = t_3 + t_6
        t_0 = (t_4, None, t_0)
        t_4 = t_0[0]
        t_6 = (t_6,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_6 + t_0
        t_6 = t_0[slice(None, 2, None)]
        t_3 = t_6[0]
        t_5 = self.l_17(t_3)
        t_6 = t_6[1]
        t_0 = t_0[slice(2, None, None)]
        t_5 = self.l_18(t_5)
        t_5 = torch.nn.functional.relu(t_5, inplace=False)
        t_5 = self.l_19(t_5)
        t_5 = self.l_20(t_5)
        t_5 = self.l_21(t_5)
        t_5 = t_3 + t_5
        t_6 = (t_5, t_6)
        t_0 = t_6 + t_0
        t_6 = t_0[slice(None, 2, None)]
        t_6 = t_6[0]
        t_5 = self.l_22(t_6)
        t_0 = t_0[2]
        t_3 = self.l_23(t_5)
        t_1 = self.l_24(t_5)
        t_7 = self.l_25(t_5)
        t_5 = t_5.shape
        t_5 = t_5[slice(None, 2, None)]
        t_5 = t_5[0]
        t_3 = t_3.view(t_5, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_1 = t_1.view(t_5, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_7 = t_7.view(t_5, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_3, t_1)
        t_1 += t_0
        t_3 = t_1.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_3.type_as(t_1)
        t_1 = torch.nn.functional.dropout(t_1, p=0.1, training=True, inplace=False)
        t_7 = torch.matmul(t_1, t_7)
        t_7 = t_7.transpose(1, 2)
        t_7 = t_7.contiguous()
        t_5 = t_7.view(t_5, -1, 4096)
        t_5 = self.l_26(t_5)
        t_7 = self.l_27(t_5)
        t_7 = t_6 + t_7
        t_0 = (t_5, None, t_0)
        t_5 = t_0[0]
        t_7 = (t_7,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_7 + t_0
        t_7 = t_0[slice(None, 2, None)]
        t_6 = t_7[0]
        t_1 = self.l_28(t_6)
        t_7 = t_7[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_29(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_30(t_1)
        t_1 = self.l_31(t_1)
        t_1 = self.l_32(t_1)
        t_1 = t_6 + t_1
        t_7 = (t_1, t_7)
        t_0 = t_7 + t_0
        t_7 = t_0[slice(None, 2, None)]
        t_7 = t_7[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1668
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1670
        return list(flatten((t_7, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition6(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:6'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.18.layer.0.layer_norm',
                        'l_1': 'encoder.block.18.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.18.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.18.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.18.layer.0.SelfAttention.o',
                        'l_5': 'encoder.block.18.layer.0.dropout',
                        'l_6': 'encoder.block.18.layer.1.layer_norm',
                        'l_7': 'encoder.block.18.layer.1.DenseReluDense.wi',
                        'l_8': 'encoder.block.18.layer.1.DenseReluDense.dropout',
                        'l_9': 'encoder.block.18.layer.1.DenseReluDense.wo',
                        'l_10': 'encoder.block.18.layer.1.dropout',
                        'l_11': 'encoder.block.19.layer.0.layer_norm',
                        'l_12': 'encoder.block.19.layer.0.SelfAttention.q',
                        'l_13': 'encoder.block.19.layer.0.SelfAttention.k',
                        'l_14': 'encoder.block.19.layer.0.SelfAttention.v',
                        'l_15': 'encoder.block.19.layer.0.SelfAttention.o',
                        'l_16': 'encoder.block.19.layer.0.dropout',
                        'l_17': 'encoder.block.19.layer.1.layer_norm',
                        'l_18': 'encoder.block.19.layer.1.DenseReluDense.wi',
                        'l_19': 'encoder.block.19.layer.1.DenseReluDense.dropout',
                        'l_20': 'encoder.block.19.layer.1.DenseReluDense.wo',
                        'l_21': 'encoder.block.19.layer.1.dropout',
                        'l_22': 'encoder.block.20.layer.0.layer_norm',
                        'l_23': 'encoder.block.20.layer.0.SelfAttention.q',
                        'l_24': 'encoder.block.20.layer.0.SelfAttention.k',
                        'l_25': 'encoder.block.20.layer.0.SelfAttention.v',
                        'l_26': 'encoder.block.20.layer.0.SelfAttention.o',
                        'l_27': 'encoder.block.20.layer.0.dropout',
                        'l_28': 'encoder.block.20.layer.1.layer_norm',
                        'l_29': 'encoder.block.20.layer.1.DenseReluDense.wi',
                        'l_30': 'encoder.block.20.layer.1.DenseReluDense.dropout',
                        'l_31': 'encoder.block.20.layer.1.DenseReluDense.wo',
                        'l_32': 'encoder.block.20.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1668 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1670 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_4(t_0)
        t_3 = self.l_5(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_2 = t_0[0]
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_1 = t_3[0]
        t_4 = self.l_6(t_1)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_4 = self.l_7(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_8(t_4)
        t_4 = self.l_9(t_4)
        t_4 = self.l_10(t_4)
        t_4 = t_1 + t_4
        t_3 = (t_4, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_4 = self.l_11(t_3)
        t_0 = t_0[2]
        t_1 = self.l_12(t_4)
        t_5 = self.l_13(t_4)
        t_6 = self.l_14(t_4)
        t_4 = t_4.shape
        t_4 = t_4[slice(None, 2, None)]
        t_4 = t_4[0]
        t_1 = t_1.view(t_4, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_5 = t_5.view(t_4, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_6 = t_6.view(t_4, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_5 = t_5.transpose(3, 2)
        t_5 = torch.matmul(t_1, t_5)
        t_5 += t_0
        t_1 = t_5.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_5 = t_1.type_as(t_5)
        t_5 = torch.nn.functional.dropout(t_5, p=0.1, training=True, inplace=False)
        t_6 = torch.matmul(t_5, t_6)
        t_6 = t_6.transpose(1, 2)
        t_6 = t_6.contiguous()
        t_4 = t_6.view(t_4, -1, 4096)
        t_4 = self.l_15(t_4)
        t_6 = self.l_16(t_4)
        t_6 = t_3 + t_6
        t_0 = (t_4, None, t_0)
        t_4 = t_0[0]
        t_6 = (t_6,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_6 + t_0
        t_6 = t_0[slice(None, 2, None)]
        t_3 = t_6[0]
        t_5 = self.l_17(t_3)
        t_6 = t_6[1]
        t_0 = t_0[slice(2, None, None)]
        t_5 = self.l_18(t_5)
        t_5 = torch.nn.functional.relu(t_5, inplace=False)
        t_5 = self.l_19(t_5)
        t_5 = self.l_20(t_5)
        t_5 = self.l_21(t_5)
        t_5 = t_3 + t_5
        t_6 = (t_5, t_6)
        t_0 = t_6 + t_0
        t_6 = t_0[slice(None, 2, None)]
        t_6 = t_6[0]
        t_5 = self.l_22(t_6)
        t_0 = t_0[2]
        t_3 = self.l_23(t_5)
        t_1 = self.l_24(t_5)
        t_7 = self.l_25(t_5)
        t_5 = t_5.shape
        t_5 = t_5[slice(None, 2, None)]
        t_5 = t_5[0]
        t_3 = t_3.view(t_5, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_1 = t_1.view(t_5, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_7 = t_7.view(t_5, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_3, t_1)
        t_1 += t_0
        t_3 = t_1.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_3.type_as(t_1)
        t_1 = torch.nn.functional.dropout(t_1, p=0.1, training=True, inplace=False)
        t_7 = torch.matmul(t_1, t_7)
        t_7 = t_7.transpose(1, 2)
        t_7 = t_7.contiguous()
        t_5 = t_7.view(t_5, -1, 4096)
        t_5 = self.l_26(t_5)
        t_7 = self.l_27(t_5)
        t_7 = t_6 + t_7
        t_0 = (t_5, None, t_0)
        t_5 = t_0[0]
        t_7 = (t_7,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_7 + t_0
        t_7 = t_0[slice(None, 2, None)]
        t_6 = t_7[0]
        t_1 = self.l_28(t_6)
        t_7 = t_7[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_29(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_30(t_1)
        t_1 = self.l_31(t_1)
        t_1 = self.l_32(t_1)
        t_1 = t_6 + t_1
        t_7 = (t_1, t_7)
        t_0 = t_7 + t_0
        t_7 = t_0[slice(None, 2, None)]
        t_7 = t_7[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1935
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1937
        return list(flatten((t_7, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition7(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5LayerNorm[final_layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:7'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.21.layer.0.layer_norm',
                        'l_1': 'encoder.block.21.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.21.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.21.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.21.layer.0.SelfAttention.o',
                        'l_5': 'encoder.block.21.layer.0.dropout',
                        'l_6': 'encoder.block.21.layer.1.layer_norm',
                        'l_7': 'encoder.block.21.layer.1.DenseReluDense.wi',
                        'l_8': 'encoder.block.21.layer.1.DenseReluDense.dropout',
                        'l_9': 'encoder.block.21.layer.1.DenseReluDense.wo',
                        'l_10': 'encoder.block.21.layer.1.dropout',
                        'l_11': 'encoder.block.22.layer.0.layer_norm',
                        'l_12': 'encoder.block.22.layer.0.SelfAttention.q',
                        'l_13': 'encoder.block.22.layer.0.SelfAttention.k',
                        'l_14': 'encoder.block.22.layer.0.SelfAttention.v',
                        'l_15': 'encoder.block.22.layer.0.SelfAttention.o',
                        'l_16': 'encoder.block.22.layer.0.dropout',
                        'l_17': 'encoder.block.22.layer.1.layer_norm',
                        'l_18': 'encoder.block.22.layer.1.DenseReluDense.wi',
                        'l_19': 'encoder.block.22.layer.1.DenseReluDense.dropout',
                        'l_20': 'encoder.block.22.layer.1.DenseReluDense.wo',
                        'l_21': 'encoder.block.22.layer.1.dropout',
                        'l_22': 'encoder.block.23.layer.0.layer_norm',
                        'l_23': 'encoder.block.23.layer.0.SelfAttention.q',
                        'l_24': 'encoder.block.23.layer.0.SelfAttention.k',
                        'l_25': 'encoder.block.23.layer.0.SelfAttention.v',
                        'l_26': 'encoder.block.23.layer.0.SelfAttention.o',
                        'l_27': 'encoder.block.23.layer.0.dropout',
                        'l_28': 'encoder.block.23.layer.1.layer_norm',
                        'l_29': 'encoder.block.23.layer.1.DenseReluDense.wi',
                        'l_30': 'encoder.block.23.layer.1.DenseReluDense.dropout',
                        'l_31': 'encoder.block.23.layer.1.DenseReluDense.wo',
                        'l_32': 'encoder.block.23.layer.1.dropout',
                        'l_33': 'encoder.final_layer_norm',
                        'l_34': 'encoder.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5LayerNorm[final_layer_norm] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1935 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1937 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_4(t_0)
        t_3 = self.l_5(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_2 = t_0[0]
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_1 = t_3[0]
        t_4 = self.l_6(t_1)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_4 = self.l_7(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_8(t_4)
        t_4 = self.l_9(t_4)
        t_4 = self.l_10(t_4)
        t_4 = t_1 + t_4
        t_3 = (t_4, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_4 = self.l_11(t_3)
        t_0 = t_0[2]
        t_1 = self.l_12(t_4)
        t_5 = self.l_13(t_4)
        t_6 = self.l_14(t_4)
        t_4 = t_4.shape
        t_4 = t_4[slice(None, 2, None)]
        t_4 = t_4[0]
        t_1 = t_1.view(t_4, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_5 = t_5.view(t_4, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_6 = t_6.view(t_4, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_5 = t_5.transpose(3, 2)
        t_5 = torch.matmul(t_1, t_5)
        t_5 += t_0
        t_1 = t_5.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_5 = t_1.type_as(t_5)
        t_5 = torch.nn.functional.dropout(t_5, p=0.1, training=True, inplace=False)
        t_6 = torch.matmul(t_5, t_6)
        t_6 = t_6.transpose(1, 2)
        t_6 = t_6.contiguous()
        t_4 = t_6.view(t_4, -1, 4096)
        t_4 = self.l_15(t_4)
        t_6 = self.l_16(t_4)
        t_6 = t_3 + t_6
        t_0 = (t_4, None, t_0)
        t_4 = t_0[0]
        t_6 = (t_6,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_6 + t_0
        t_6 = t_0[slice(None, 2, None)]
        t_3 = t_6[0]
        t_5 = self.l_17(t_3)
        t_6 = t_6[1]
        t_0 = t_0[slice(2, None, None)]
        t_5 = self.l_18(t_5)
        t_5 = torch.nn.functional.relu(t_5, inplace=False)
        t_5 = self.l_19(t_5)
        t_5 = self.l_20(t_5)
        t_5 = self.l_21(t_5)
        t_5 = t_3 + t_5
        t_6 = (t_5, t_6)
        t_0 = t_6 + t_0
        t_6 = t_0[slice(None, 2, None)]
        t_6 = t_6[0]
        t_5 = self.l_22(t_6)
        t_0 = t_0[2]
        t_3 = self.l_23(t_5)
        t_1 = self.l_24(t_5)
        t_7 = self.l_25(t_5)
        t_5 = t_5.shape
        t_5 = t_5[slice(None, 2, None)]
        t_5 = t_5[0]
        t_3 = t_3.view(t_5, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_1 = t_1.view(t_5, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_7 = t_7.view(t_5, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_3, t_1)
        t_1 += t_0
        t_3 = t_1.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_3.type_as(t_1)
        t_1 = torch.nn.functional.dropout(t_1, p=0.1, training=True, inplace=False)
        t_7 = torch.matmul(t_1, t_7)
        t_7 = t_7.transpose(1, 2)
        t_7 = t_7.contiguous()
        t_5 = t_7.view(t_5, -1, 4096)
        t_5 = self.l_26(t_5)
        t_7 = self.l_27(t_5)
        t_7 = t_6 + t_7
        t_0 = (t_5, None, t_0)
        t_5 = t_0[0]
        t_7 = (t_7,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_7 + t_0
        t_7 = t_0[slice(None, 2, None)]
        t_6 = t_7[0]
        t_1 = self.l_28(t_6)
        t_7 = t_7[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_29(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_30(t_1)
        t_1 = self.l_31(t_1)
        t_1 = self.l_32(t_1)
        t_1 = t_6 + t_1
        t_7 = (t_1, t_7)
        t_0 = t_7 + t_0
        t_7 = t_0[slice(None, 2, None)]
        t_7 = t_7[0]
        t_7 = self.l_33(t_7)
        t_0 = t_0[2]
        t_7 = self.l_34(t_7)
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        return (t_7,)

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition8(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Embedding[relative_attention_bias]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:8'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.0.layer.0.layer_norm',
                        'l_1': 'decoder.block.0.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.0.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.0.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias',
                        'l_5': 'decoder.block.0.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.0.layer.0.dropout',
                        'l_7': 'decoder.block.0.layer.1.layer_norm',
                        'l_8': 'decoder.block.0.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.0.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.0.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.0.layer.1.EncDecAttention.o',
                        'l_12': 'decoder.block.0.layer.1.dropout',
                        'l_13': 'decoder.block.0.layer.2.layer_norm',
                        'l_14': 'decoder.block.0.layer.2.DenseReluDense.wi',
                        'l_15': 'decoder.block.0.layer.2.DenseReluDense.dropout',
                        'l_16': 'decoder.block.0.layer.2.DenseReluDense.wo',
                        'l_17': 'decoder.block.0.layer.2.dropout',
                        'l_18': 'decoder.block.1.layer.0.layer_norm',
                        'l_19': 'decoder.block.1.layer.0.SelfAttention.q',
                        'l_20': 'decoder.block.1.layer.0.SelfAttention.k',
                        'l_21': 'decoder.block.1.layer.0.SelfAttention.v',
                        'l_22': 'decoder.block.1.layer.0.SelfAttention.o',
                        'l_23': 'decoder.block.1.layer.0.dropout',
                        'l_24': 'decoder.block.1.layer.1.layer_norm',
                        'l_25': 'decoder.block.1.layer.1.EncDecAttention.q',
                        'l_26': 'decoder.block.1.layer.1.EncDecAttention.k',
                        'l_27': 'decoder.block.1.layer.1.EncDecAttention.v',
                        'l_28': 'decoder.block.1.layer.1.EncDecAttention.o',
                        'l_29': 'decoder.block.1.layer.1.dropout',
                        'l_30': 'decoder.block.1.layer.2.layer_norm',
                        'l_31': 'decoder.block.1.layer.2.DenseReluDense.wi',
                        'l_32': 'decoder.block.1.layer.2.DenseReluDense.dropout',
                        'l_33': 'decoder.block.1.layer.2.DenseReluDense.wo',
                        'l_34': 'decoder.block.1.layer.2.dropout',
                        'l_35': 'decoder.block.2.layer.0.layer_norm',
                        'l_36': 'decoder.block.2.layer.0.SelfAttention.q',
                        'l_37': 'decoder.block.2.layer.0.SelfAttention.k',
                        'l_38': 'decoder.block.2.layer.0.SelfAttention.v',
                        'l_39': 'decoder.block.2.layer.0.SelfAttention.o',
                        'l_40': 'decoder.block.2.layer.0.dropout',
                        'l_41': 'decoder.block.2.layer.1.layer_norm',
                        'l_42': 'decoder.block.2.layer.1.EncDecAttention.q',
                        'l_43': 'decoder.block.2.layer.1.EncDecAttention.k',
                        'l_44': 'decoder.block.2.layer.1.EncDecAttention.v',
                        'l_45': 'decoder.block.2.layer.1.EncDecAttention.o',
                        'l_46': 'decoder.block.2.layer.1.dropout',
                        'l_47': 'decoder.block.2.layer.2.layer_norm',
                        'l_48': 'decoder.block.2.layer.2.DenseReluDense.wi',
                        'l_49': 'decoder.block.2.layer.2.DenseReluDense.dropout',
                        'l_50': 'decoder.block.2.layer.2.DenseReluDense.wo',
                        'l_51': 'decoder.block.2.layer.2.dropout',
                        'l_52': 'decoder.block.21.layer.1.EncDecAttention.k'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Embedding[relative_attention_bias] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_50
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_51
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_52
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout] <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___2239 <=> x2
        attention_mask, x0, x1, x2 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_26(x0)
        t_3 = self.l_27(x0)
        t_4 = self.l_43(x0)
        t_5 = self.l_44(x0)
        t_6 = self.l_52(x0)
        t_7 = attention_mask[(slice(None, None, None), None, None, slice(None, None, None))]
        t_7 = t_7.to(dtype=torch.float32)
        t_7 = 1.0 - t_7
        t_7 = t_7 * -1000000000.0
        t_8 = self.l_0(x1)
        t_9 = self.l_1(t_8)
        t_10 = self.l_2(t_8)
        t_11 = self.l_3(t_8)
        t_8 = t_8.shape
        t_8 = t_8[slice(None, 2, None)]
        t_12 = t_8[0]
        t_8 = t_8[1]
        t_9 = t_9.view(t_12, -1, 32, 128)
        t_9 = t_9.transpose(1, 2)
        t_10 = t_10.view(t_12, -1, 32, 128)
        t_10 = t_10.transpose(1, 2)
        t_11 = t_11.view(t_12, -1, 32, 128)
        t_11 = t_11.transpose(1, 2)
        t_10 = t_10.transpose(3, 2)
        t_10 = torch.matmul(t_9, t_10)
        t_9 = torch.arange(t_8, dtype=torch.int64, device=self.device)
        t_9 = t_9[(slice(None, None, None), None)]
        t_8 = torch.arange(t_8, dtype=torch.int64, device=self.device)
        t_8 = t_8[(None, slice(None, None, None))]
        t_9 = t_8 - t_9
        t_8 = torch.zeros_like(t_9, device=self.device)
        t_8 = torch.min(t_9, t_8)
        t_8 = -t_8
        t_9 = t_8.float()
        t_13 = t_8 < 16
        t_9 = t_9 / 16
        t_9 = torch.log(t_9)
        t_14 = math.log(8.0)
        t_14 = t_9 / t_14
        t_14 = t_14 * 16
        t_14 = t_14.to(torch.int64)
        t_14 = 16 + t_14
        t_9 = torch.full_like(t_14, 31, device=self.device)
        t_9 = torch.min(t_14, t_9)
        t_9 = torch.where(t_13, t_8, t_9)
        t_9 = 0 + t_9
        t_9 = t_9.to(self.device)
        t_9 = self.l_4(t_9)
        t_9 = t_9.permute([2, 0, 1])
        t_9 = t_9.unsqueeze(0)
        t_9 = t_9 + x2
        t_10 += t_9
        t_8 = t_10.float()
        t_8 = torch.nn.functional.softmax(t_8, dim=-1, _stacklevel=3, dtype=None)
        t_10 = t_8.type_as(t_10)
        t_10 = torch.nn.functional.dropout(t_10, p=0.1, training=True, inplace=False)
        t_11 = torch.matmul(t_10, t_11)
        t_11 = t_11.transpose(1, 2)
        t_11 = t_11.contiguous()
        t_12 = t_11.view(t_12, -1, 4096)
        t_12 = self.l_5(t_12)
        t_11 = self.l_6(t_12)
        t_11 = x1 + t_11
        t_9 = (t_12, None, t_9)
        t_12 = t_9[0]
        t_11 = (t_11,)
        t_9 = t_9[slice(1, None, None)]
        t_9 = t_11 + t_9
        t_11 = t_9[slice(None, 2, None)]
        t_10 = t_11[0]
        t_8 = self.l_7(t_10)
        t_11 = t_11[1]
        t_9 = t_9[slice(2, None, None)]
        t_13 = self.l_8(t_8)
        t_8 = t_8.shape
        t_8 = t_8[slice(None, 2, None)]
        t_14 = t_8[0]
        t_8 = t_8[1]
        t_15 = x0.shape
        t_15 = t_15[1]
        t_13 = t_13.view(t_14, -1, 32, 128)
        t_13 = t_13.transpose(1, 2)
        t_0 = t_0.view(t_14, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_14, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_13, t_0)
        t_13 = t_0.dtype
        t_15 = (1, 32, t_8, t_15)
        t_13 = torch.zeros(t_15, device=self.device, dtype=t_13)
        t_7 = t_13 + t_7
        t_0 += t_7
        t_13 = t_0.float()
        t_13 = torch.nn.functional.softmax(t_13, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_13.type_as(t_0)
        t_0 = torch.nn.functional.dropout(t_0, p=0.1, training=True, inplace=False)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_14 = t_1.view(t_14, -1, 4096)
        t_14 = self.l_11(t_14)
        t_1 = self.l_12(t_14)
        t_1 = t_10 + t_1
        t_7 = (t_14, None, t_7)
        t_14 = t_7[0]
        t_1 = (t_1,)
        t_7 = t_7[slice(1, None, None)]
        t_7 = t_1 + t_7
        t_1 = t_7[0]
        t_10 = self.l_13(t_1)
        t_7 = t_7[slice(2, None, None)]
        t_7 = t_9 + t_7
        t_10 = self.l_14(t_10)
        t_10 = torch.nn.functional.relu(t_10, inplace=False)
        t_10 = self.l_15(t_10)
        t_10 = self.l_16(t_10)
        t_10 = self.l_17(t_10)
        t_10 = t_1 + t_10
        t_11 = (t_10, t_11)
        t_7 = t_11 + t_7
        t_11 = t_7[slice(None, 2, None)]
        t_11 = t_11[0]
        t_10 = self.l_18(t_11)
        t_1 = t_7[2]
        t_7 = t_7[3]
        t_9 = self.l_19(t_10)
        t_0 = self.l_20(t_10)
        t_13 = self.l_21(t_10)
        t_10 = t_10.shape
        t_10 = t_10[slice(None, 2, None)]
        t_10 = t_10[0]
        t_9 = t_9.view(t_10, -1, 32, 128)
        t_9 = t_9.transpose(1, 2)
        t_0 = t_0.view(t_10, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_13 = t_13.view(t_10, -1, 32, 128)
        t_13 = t_13.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_9, t_0)
        t_0 += t_1
        t_9 = t_0.float()
        t_9 = torch.nn.functional.softmax(t_9, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_9.type_as(t_0)
        t_0 = torch.nn.functional.dropout(t_0, p=0.1, training=True, inplace=False)
        t_13 = torch.matmul(t_0, t_13)
        t_13 = t_13.transpose(1, 2)
        t_13 = t_13.contiguous()
        t_10 = t_13.view(t_10, -1, 4096)
        t_10 = self.l_22(t_10)
        t_13 = self.l_23(t_10)
        t_13 = t_11 + t_13
        t_1 = (t_10, None, t_1)
        t_10 = t_1[0]
        t_13 = (t_13,)
        t_1 = t_1[slice(1, None, None)]
        t_1 = t_13 + t_1
        t_13 = t_1[slice(None, 2, None)]
        t_11 = t_13[0]
        t_0 = self.l_24(t_11)
        t_13 = t_13[1]
        t_1 = t_1[slice(2, None, None)]
        t_9 = self.l_25(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_9 = t_9.view(t_0, -1, 32, 128)
        t_9 = t_9.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_9, t_2)
        t_2 += t_7
        t_9 = t_2.float()
        t_9 = torch.nn.functional.softmax(t_9, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_9.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_28(t_0)
        t_3 = self.l_29(t_0)
        t_3 = t_11 + t_3
        t_7 = (t_0, None, t_7)
        t_0 = t_7[0]
        t_3 = (t_3,)
        t_7 = t_7[slice(1, None, None)]
        t_7 = t_3 + t_7
        t_3 = t_7[0]
        t_11 = self.l_30(t_3)
        t_7 = t_7[slice(2, None, None)]
        t_7 = t_1 + t_7
        t_11 = self.l_31(t_11)
        t_11 = torch.nn.functional.relu(t_11, inplace=False)
        t_11 = self.l_32(t_11)
        t_11 = self.l_33(t_11)
        t_11 = self.l_34(t_11)
        t_11 = t_3 + t_11
        t_13 = (t_11, t_13)
        t_7 = t_13 + t_7
        t_13 = t_7[slice(None, 2, None)]
        t_13 = t_13[0]
        t_11 = self.l_35(t_13)
        t_3 = t_7[2]
        t_7 = t_7[3]
        t_1 = self.l_36(t_11)
        t_2 = self.l_37(t_11)
        t_9 = self.l_38(t_11)
        t_11 = t_11.shape
        t_11 = t_11[slice(None, 2, None)]
        t_11 = t_11[0]
        t_1 = t_1.view(t_11, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_11, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_9 = t_9.view(t_11, -1, 32, 128)
        t_9 = t_9.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += t_3
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_9 = torch.matmul(t_2, t_9)
        t_9 = t_9.transpose(1, 2)
        t_9 = t_9.contiguous()
        t_11 = t_9.view(t_11, -1, 4096)
        t_11 = self.l_39(t_11)
        t_9 = self.l_40(t_11)
        t_9 = t_13 + t_9
        t_3 = (t_11, None, t_3)
        t_11 = t_3[0]
        t_9 = (t_9,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_9 + t_3
        t_9 = t_3[slice(None, 2, None)]
        t_13 = t_9[0]
        t_2 = self.l_41(t_13)
        t_9 = t_9[1]
        t_3 = t_3[slice(2, None, None)]
        t_1 = self.l_42(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_1, t_4)
        t_4 += t_7
        t_1 = t_4.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_1.type_as(t_4)
        t_4 = torch.nn.functional.dropout(t_4, p=0.1, training=True, inplace=False)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_45(t_2)
        t_5 = self.l_46(t_2)
        t_5 = t_13 + t_5
        t_7 = (t_2, None, t_7)
        t_2 = t_7[0]
        t_5 = (t_5,)
        t_7 = t_7[slice(1, None, None)]
        t_7 = t_5 + t_7
        t_5 = t_7[0]
        t_13 = self.l_47(t_5)
        t_7 = t_7[slice(2, None, None)]
        t_7 = t_3 + t_7
        t_13 = self.l_48(t_13)
        t_13 = torch.nn.functional.relu(t_13, inplace=False)
        t_13 = self.l_49(t_13)
        t_13 = self.l_50(t_13)
        t_13 = self.l_51(t_13)
        t_13 = t_5 + t_13
        t_9 = (t_13, t_9)
        t_7 = t_9 + t_7
        t_9 = t_7[slice(None, 2, None)]
        t_9 = t_9[0]
        t_13 = t_7[2]
        t_7 = t_7[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2784
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2786
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2788
        return list(flatten((x0, t_6, t_9, t_13, t_7)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition9(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:9'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.3.layer.0.layer_norm',
                        'l_1': 'decoder.block.3.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.3.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.3.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.3.layer.0.SelfAttention.o',
                        'l_5': 'decoder.block.3.layer.0.dropout',
                        'l_6': 'decoder.block.3.layer.1.layer_norm',
                        'l_7': 'decoder.block.3.layer.1.EncDecAttention.q',
                        'l_8': 'decoder.block.3.layer.1.EncDecAttention.k',
                        'l_9': 'decoder.block.3.layer.1.EncDecAttention.v',
                        'l_10': 'decoder.block.3.layer.1.EncDecAttention.o',
                        'l_11': 'decoder.block.3.layer.1.dropout',
                        'l_12': 'decoder.block.3.layer.2.layer_norm',
                        'l_13': 'decoder.block.3.layer.2.DenseReluDense.wi',
                        'l_14': 'decoder.block.3.layer.2.DenseReluDense.dropout',
                        'l_15': 'decoder.block.3.layer.2.DenseReluDense.wo',
                        'l_16': 'decoder.block.3.layer.2.dropout',
                        'l_17': 'decoder.block.4.layer.0.layer_norm',
                        'l_18': 'decoder.block.4.layer.0.SelfAttention.q',
                        'l_19': 'decoder.block.4.layer.0.SelfAttention.k',
                        'l_20': 'decoder.block.4.layer.0.SelfAttention.v',
                        'l_21': 'decoder.block.4.layer.0.SelfAttention.o',
                        'l_22': 'decoder.block.4.layer.0.dropout',
                        'l_23': 'decoder.block.4.layer.1.layer_norm',
                        'l_24': 'decoder.block.4.layer.1.EncDecAttention.q',
                        'l_25': 'decoder.block.4.layer.1.EncDecAttention.k',
                        'l_26': 'decoder.block.4.layer.1.EncDecAttention.v',
                        'l_27': 'decoder.block.4.layer.1.EncDecAttention.o',
                        'l_28': 'decoder.block.4.layer.1.dropout',
                        'l_29': 'decoder.block.4.layer.2.layer_norm',
                        'l_30': 'decoder.block.4.layer.2.DenseReluDense.wi',
                        'l_31': 'decoder.block.4.layer.2.DenseReluDense.dropout',
                        'l_32': 'decoder.block.4.layer.2.DenseReluDense.wo',
                        'l_33': 'decoder.block.4.layer.2.dropout',
                        'l_34': 'decoder.block.5.layer.0.layer_norm',
                        'l_35': 'decoder.block.5.layer.0.SelfAttention.q',
                        'l_36': 'decoder.block.5.layer.0.SelfAttention.k',
                        'l_37': 'decoder.block.5.layer.0.SelfAttention.v',
                        'l_38': 'decoder.block.5.layer.0.SelfAttention.o',
                        'l_39': 'decoder.block.5.layer.0.dropout',
                        'l_40': 'decoder.block.5.layer.1.layer_norm',
                        'l_41': 'decoder.block.5.layer.1.EncDecAttention.q',
                        'l_42': 'decoder.block.5.layer.1.EncDecAttention.k',
                        'l_43': 'decoder.block.5.layer.1.EncDecAttention.v',
                        'l_44': 'decoder.block.5.layer.1.EncDecAttention.o',
                        'l_45': 'decoder.block.5.layer.1.dropout',
                        'l_46': 'decoder.block.5.layer.2.layer_norm',
                        'l_47': 'decoder.block.5.layer.2.DenseReluDense.wi',
                        'l_48': 'decoder.block.5.layer.2.DenseReluDense.dropout',
                        'l_49': 'decoder.block.5.layer.2.DenseReluDense.wo',
                        'l_50': 'decoder.block.5.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_50
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2784 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2786 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2788 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_8(x0)
        t_1 = self.l_9(x0)
        t_2 = self.l_25(x0)
        t_3 = self.l_26(x0)
        t_4 = self.l_42(x0)
        t_5 = self.l_43(x0)
        t_6 = self.l_0(x1)
        t_7 = self.l_1(t_6)
        t_8 = self.l_2(t_6)
        t_9 = self.l_3(t_6)
        t_6 = t_6.shape
        t_6 = t_6[slice(None, 2, None)]
        t_6 = t_6[0]
        t_7 = t_7.view(t_6, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_8 = t_8.view(t_6, -1, 32, 128)
        t_8 = t_8.transpose(1, 2)
        t_9 = t_9.view(t_6, -1, 32, 128)
        t_9 = t_9.transpose(1, 2)
        t_8 = t_8.transpose(3, 2)
        t_8 = torch.matmul(t_7, t_8)
        t_8 += x2
        t_7 = t_8.float()
        t_7 = torch.nn.functional.softmax(t_7, dim=-1, _stacklevel=3, dtype=None)
        t_8 = t_7.type_as(t_8)
        t_8 = torch.nn.functional.dropout(t_8, p=0.1, training=True, inplace=False)
        t_9 = torch.matmul(t_8, t_9)
        t_9 = t_9.transpose(1, 2)
        t_9 = t_9.contiguous()
        t_6 = t_9.view(t_6, -1, 4096)
        t_6 = self.l_4(t_6)
        t_9 = self.l_5(t_6)
        t_9 = x1 + t_9
        t_6 = (t_6, None, x2)
        t_8 = t_6[0]
        t_9 = (t_9,)
        t_6 = t_6[slice(1, None, None)]
        t_6 = t_9 + t_6
        t_9 = t_6[slice(None, 2, None)]
        t_7 = t_9[0]
        t_10 = self.l_6(t_7)
        t_9 = t_9[1]
        t_6 = t_6[slice(2, None, None)]
        t_11 = self.l_7(t_10)
        t_10 = t_10.shape
        t_10 = t_10[slice(None, 2, None)]
        t_10 = t_10[0]
        t_11 = t_11.view(t_10, -1, 32, 128)
        t_11 = t_11.transpose(1, 2)
        t_0 = t_0.view(t_10, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_10, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_11, t_0)
        t_0 += x3
        t_11 = t_0.float()
        t_11 = torch.nn.functional.softmax(t_11, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_11.type_as(t_0)
        t_0 = torch.nn.functional.dropout(t_0, p=0.1, training=True, inplace=False)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_10 = t_1.view(t_10, -1, 4096)
        t_10 = self.l_10(t_10)
        t_1 = self.l_11(t_10)
        t_1 = t_7 + t_1
        t_10 = (t_10, None, x3)
        t_7 = t_10[0]
        t_1 = (t_1,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_1 + t_10
        t_1 = t_10[0]
        t_0 = self.l_12(t_1)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_6 + t_10
        t_0 = self.l_13(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_14(t_0)
        t_0 = self.l_15(t_0)
        t_0 = self.l_16(t_0)
        t_0 = t_1 + t_0
        t_9 = (t_0, t_9)
        t_10 = t_9 + t_10
        t_9 = t_10[slice(None, 2, None)]
        t_9 = t_9[0]
        t_0 = self.l_17(t_9)
        t_1 = t_10[2]
        t_10 = t_10[3]
        t_6 = self.l_18(t_0)
        t_11 = self.l_19(t_0)
        t_12 = self.l_20(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_6 = t_6.view(t_0, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_11 = t_11.view(t_0, -1, 32, 128)
        t_11 = t_11.transpose(1, 2)
        t_12 = t_12.view(t_0, -1, 32, 128)
        t_12 = t_12.transpose(1, 2)
        t_11 = t_11.transpose(3, 2)
        t_11 = torch.matmul(t_6, t_11)
        t_11 += t_1
        t_6 = t_11.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_11 = t_6.type_as(t_11)
        t_11 = torch.nn.functional.dropout(t_11, p=0.1, training=True, inplace=False)
        t_12 = torch.matmul(t_11, t_12)
        t_12 = t_12.transpose(1, 2)
        t_12 = t_12.contiguous()
        t_0 = t_12.view(t_0, -1, 4096)
        t_0 = self.l_21(t_0)
        t_12 = self.l_22(t_0)
        t_12 = t_9 + t_12
        t_1 = (t_0, None, t_1)
        t_0 = t_1[0]
        t_12 = (t_12,)
        t_1 = t_1[slice(1, None, None)]
        t_1 = t_12 + t_1
        t_12 = t_1[slice(None, 2, None)]
        t_9 = t_12[0]
        t_11 = self.l_23(t_9)
        t_12 = t_12[1]
        t_1 = t_1[slice(2, None, None)]
        t_6 = self.l_24(t_11)
        t_11 = t_11.shape
        t_11 = t_11[slice(None, 2, None)]
        t_11 = t_11[0]
        t_6 = t_6.view(t_11, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_2 = t_2.view(t_11, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_11, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_6, t_2)
        t_2 += t_10
        t_6 = t_2.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_6.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_11 = t_3.view(t_11, -1, 4096)
        t_11 = self.l_27(t_11)
        t_3 = self.l_28(t_11)
        t_3 = t_9 + t_3
        t_10 = (t_11, None, t_10)
        t_11 = t_10[0]
        t_3 = (t_3,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_3 + t_10
        t_3 = t_10[0]
        t_9 = self.l_29(t_3)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_1 + t_10
        t_9 = self.l_30(t_9)
        t_9 = torch.nn.functional.relu(t_9, inplace=False)
        t_9 = self.l_31(t_9)
        t_9 = self.l_32(t_9)
        t_9 = self.l_33(t_9)
        t_9 = t_3 + t_9
        t_12 = (t_9, t_12)
        t_10 = t_12 + t_10
        t_12 = t_10[slice(None, 2, None)]
        t_12 = t_12[0]
        t_9 = self.l_34(t_12)
        t_3 = t_10[2]
        t_10 = t_10[3]
        t_1 = self.l_35(t_9)
        t_2 = self.l_36(t_9)
        t_6 = self.l_37(t_9)
        t_9 = t_9.shape
        t_9 = t_9[slice(None, 2, None)]
        t_9 = t_9[0]
        t_1 = t_1.view(t_9, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_9, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_6 = t_6.view(t_9, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += t_3
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_6 = torch.matmul(t_2, t_6)
        t_6 = t_6.transpose(1, 2)
        t_6 = t_6.contiguous()
        t_9 = t_6.view(t_9, -1, 4096)
        t_9 = self.l_38(t_9)
        t_6 = self.l_39(t_9)
        t_6 = t_12 + t_6
        t_3 = (t_9, None, t_3)
        t_9 = t_3[0]
        t_6 = (t_6,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_6 + t_3
        t_6 = t_3[slice(None, 2, None)]
        t_12 = t_6[0]
        t_2 = self.l_40(t_12)
        t_6 = t_6[1]
        t_3 = t_3[slice(2, None, None)]
        t_1 = self.l_41(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_1, t_4)
        t_4 += t_10
        t_1 = t_4.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_1.type_as(t_4)
        t_4 = torch.nn.functional.dropout(t_4, p=0.1, training=True, inplace=False)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_44(t_2)
        t_5 = self.l_45(t_2)
        t_5 = t_12 + t_5
        t_10 = (t_2, None, t_10)
        t_2 = t_10[0]
        t_5 = (t_5,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_5 + t_10
        t_5 = t_10[0]
        t_12 = self.l_46(t_5)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_3 + t_10
        t_12 = self.l_47(t_12)
        t_12 = torch.nn.functional.relu(t_12, inplace=False)
        t_12 = self.l_48(t_12)
        t_12 = self.l_49(t_12)
        t_12 = self.l_50(t_12)
        t_12 = t_5 + t_12
        t_6 = (t_12, t_6)
        t_10 = t_6 + t_10
        t_6 = t_10[slice(None, 2, None)]
        t_6 = t_6[0]
        t_12 = t_10[2]
        t_10 = t_10[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3267
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3269
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3271
        return list(flatten((x0, t_6, t_12, t_10)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition10(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:10'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.6.layer.0.layer_norm',
                        'l_1': 'decoder.block.6.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.6.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.6.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.6.layer.0.SelfAttention.o',
                        'l_5': 'decoder.block.6.layer.0.dropout',
                        'l_6': 'decoder.block.6.layer.1.layer_norm',
                        'l_7': 'decoder.block.6.layer.1.EncDecAttention.q',
                        'l_8': 'decoder.block.6.layer.1.EncDecAttention.k',
                        'l_9': 'decoder.block.6.layer.1.EncDecAttention.v',
                        'l_10': 'decoder.block.6.layer.1.EncDecAttention.o',
                        'l_11': 'decoder.block.6.layer.1.dropout',
                        'l_12': 'decoder.block.6.layer.2.layer_norm',
                        'l_13': 'decoder.block.6.layer.2.DenseReluDense.wi',
                        'l_14': 'decoder.block.6.layer.2.DenseReluDense.dropout',
                        'l_15': 'decoder.block.6.layer.2.DenseReluDense.wo',
                        'l_16': 'decoder.block.6.layer.2.dropout',
                        'l_17': 'decoder.block.7.layer.0.layer_norm',
                        'l_18': 'decoder.block.7.layer.0.SelfAttention.q',
                        'l_19': 'decoder.block.7.layer.0.SelfAttention.k',
                        'l_20': 'decoder.block.7.layer.0.SelfAttention.v',
                        'l_21': 'decoder.block.7.layer.0.SelfAttention.o',
                        'l_22': 'decoder.block.7.layer.0.dropout',
                        'l_23': 'decoder.block.7.layer.1.layer_norm',
                        'l_24': 'decoder.block.7.layer.1.EncDecAttention.q',
                        'l_25': 'decoder.block.7.layer.1.EncDecAttention.k',
                        'l_26': 'decoder.block.7.layer.1.EncDecAttention.v',
                        'l_27': 'decoder.block.7.layer.1.EncDecAttention.o',
                        'l_28': 'decoder.block.7.layer.1.dropout',
                        'l_29': 'decoder.block.7.layer.2.layer_norm',
                        'l_30': 'decoder.block.7.layer.2.DenseReluDense.wi',
                        'l_31': 'decoder.block.7.layer.2.DenseReluDense.dropout',
                        'l_32': 'decoder.block.7.layer.2.DenseReluDense.wo',
                        'l_33': 'decoder.block.7.layer.2.dropout',
                        'l_34': 'decoder.block.8.layer.0.layer_norm',
                        'l_35': 'decoder.block.8.layer.0.SelfAttention.q',
                        'l_36': 'decoder.block.8.layer.0.SelfAttention.k',
                        'l_37': 'decoder.block.8.layer.0.SelfAttention.v',
                        'l_38': 'decoder.block.8.layer.0.SelfAttention.o',
                        'l_39': 'decoder.block.8.layer.0.dropout',
                        'l_40': 'decoder.block.8.layer.1.layer_norm',
                        'l_41': 'decoder.block.8.layer.1.EncDecAttention.q',
                        'l_42': 'decoder.block.8.layer.1.EncDecAttention.k',
                        'l_43': 'decoder.block.8.layer.1.EncDecAttention.v',
                        'l_44': 'decoder.block.8.layer.1.EncDecAttention.o',
                        'l_45': 'decoder.block.8.layer.1.dropout',
                        'l_46': 'decoder.block.8.layer.2.layer_norm',
                        'l_47': 'decoder.block.8.layer.2.DenseReluDense.wi',
                        'l_48': 'decoder.block.8.layer.2.DenseReluDense.dropout',
                        'l_49': 'decoder.block.8.layer.2.DenseReluDense.wo',
                        'l_50': 'decoder.block.8.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_50
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3267 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3269 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3271 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_8(x0)
        t_1 = self.l_9(x0)
        t_2 = self.l_25(x0)
        t_3 = self.l_26(x0)
        t_4 = self.l_42(x0)
        t_5 = self.l_43(x0)
        t_6 = self.l_0(x1)
        t_7 = self.l_1(t_6)
        t_8 = self.l_2(t_6)
        t_9 = self.l_3(t_6)
        t_6 = t_6.shape
        t_6 = t_6[slice(None, 2, None)]
        t_6 = t_6[0]
        t_7 = t_7.view(t_6, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_8 = t_8.view(t_6, -1, 32, 128)
        t_8 = t_8.transpose(1, 2)
        t_9 = t_9.view(t_6, -1, 32, 128)
        t_9 = t_9.transpose(1, 2)
        t_8 = t_8.transpose(3, 2)
        t_8 = torch.matmul(t_7, t_8)
        t_8 += x2
        t_7 = t_8.float()
        t_7 = torch.nn.functional.softmax(t_7, dim=-1, _stacklevel=3, dtype=None)
        t_8 = t_7.type_as(t_8)
        t_8 = torch.nn.functional.dropout(t_8, p=0.1, training=True, inplace=False)
        t_9 = torch.matmul(t_8, t_9)
        t_9 = t_9.transpose(1, 2)
        t_9 = t_9.contiguous()
        t_6 = t_9.view(t_6, -1, 4096)
        t_6 = self.l_4(t_6)
        t_9 = self.l_5(t_6)
        t_9 = x1 + t_9
        t_6 = (t_6, None, x2)
        t_8 = t_6[0]
        t_9 = (t_9,)
        t_6 = t_6[slice(1, None, None)]
        t_6 = t_9 + t_6
        t_9 = t_6[slice(None, 2, None)]
        t_7 = t_9[0]
        t_10 = self.l_6(t_7)
        t_9 = t_9[1]
        t_6 = t_6[slice(2, None, None)]
        t_11 = self.l_7(t_10)
        t_10 = t_10.shape
        t_10 = t_10[slice(None, 2, None)]
        t_10 = t_10[0]
        t_11 = t_11.view(t_10, -1, 32, 128)
        t_11 = t_11.transpose(1, 2)
        t_0 = t_0.view(t_10, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_10, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_11, t_0)
        t_0 += x3
        t_11 = t_0.float()
        t_11 = torch.nn.functional.softmax(t_11, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_11.type_as(t_0)
        t_0 = torch.nn.functional.dropout(t_0, p=0.1, training=True, inplace=False)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_10 = t_1.view(t_10, -1, 4096)
        t_10 = self.l_10(t_10)
        t_1 = self.l_11(t_10)
        t_1 = t_7 + t_1
        t_10 = (t_10, None, x3)
        t_7 = t_10[0]
        t_1 = (t_1,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_1 + t_10
        t_1 = t_10[0]
        t_0 = self.l_12(t_1)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_6 + t_10
        t_0 = self.l_13(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_14(t_0)
        t_0 = self.l_15(t_0)
        t_0 = self.l_16(t_0)
        t_0 = t_1 + t_0
        t_9 = (t_0, t_9)
        t_10 = t_9 + t_10
        t_9 = t_10[slice(None, 2, None)]
        t_9 = t_9[0]
        t_0 = self.l_17(t_9)
        t_1 = t_10[2]
        t_10 = t_10[3]
        t_6 = self.l_18(t_0)
        t_11 = self.l_19(t_0)
        t_12 = self.l_20(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_6 = t_6.view(t_0, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_11 = t_11.view(t_0, -1, 32, 128)
        t_11 = t_11.transpose(1, 2)
        t_12 = t_12.view(t_0, -1, 32, 128)
        t_12 = t_12.transpose(1, 2)
        t_11 = t_11.transpose(3, 2)
        t_11 = torch.matmul(t_6, t_11)
        t_11 += t_1
        t_6 = t_11.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_11 = t_6.type_as(t_11)
        t_11 = torch.nn.functional.dropout(t_11, p=0.1, training=True, inplace=False)
        t_12 = torch.matmul(t_11, t_12)
        t_12 = t_12.transpose(1, 2)
        t_12 = t_12.contiguous()
        t_0 = t_12.view(t_0, -1, 4096)
        t_0 = self.l_21(t_0)
        t_12 = self.l_22(t_0)
        t_12 = t_9 + t_12
        t_1 = (t_0, None, t_1)
        t_0 = t_1[0]
        t_12 = (t_12,)
        t_1 = t_1[slice(1, None, None)]
        t_1 = t_12 + t_1
        t_12 = t_1[slice(None, 2, None)]
        t_9 = t_12[0]
        t_11 = self.l_23(t_9)
        t_12 = t_12[1]
        t_1 = t_1[slice(2, None, None)]
        t_6 = self.l_24(t_11)
        t_11 = t_11.shape
        t_11 = t_11[slice(None, 2, None)]
        t_11 = t_11[0]
        t_6 = t_6.view(t_11, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_2 = t_2.view(t_11, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_11, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_6, t_2)
        t_2 += t_10
        t_6 = t_2.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_6.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_11 = t_3.view(t_11, -1, 4096)
        t_11 = self.l_27(t_11)
        t_3 = self.l_28(t_11)
        t_3 = t_9 + t_3
        t_10 = (t_11, None, t_10)
        t_11 = t_10[0]
        t_3 = (t_3,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_3 + t_10
        t_3 = t_10[0]
        t_9 = self.l_29(t_3)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_1 + t_10
        t_9 = self.l_30(t_9)
        t_9 = torch.nn.functional.relu(t_9, inplace=False)
        t_9 = self.l_31(t_9)
        t_9 = self.l_32(t_9)
        t_9 = self.l_33(t_9)
        t_9 = t_3 + t_9
        t_12 = (t_9, t_12)
        t_10 = t_12 + t_10
        t_12 = t_10[slice(None, 2, None)]
        t_12 = t_12[0]
        t_9 = self.l_34(t_12)
        t_3 = t_10[2]
        t_10 = t_10[3]
        t_1 = self.l_35(t_9)
        t_2 = self.l_36(t_9)
        t_6 = self.l_37(t_9)
        t_9 = t_9.shape
        t_9 = t_9[slice(None, 2, None)]
        t_9 = t_9[0]
        t_1 = t_1.view(t_9, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_9, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_6 = t_6.view(t_9, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += t_3
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_6 = torch.matmul(t_2, t_6)
        t_6 = t_6.transpose(1, 2)
        t_6 = t_6.contiguous()
        t_9 = t_6.view(t_9, -1, 4096)
        t_9 = self.l_38(t_9)
        t_6 = self.l_39(t_9)
        t_6 = t_12 + t_6
        t_3 = (t_9, None, t_3)
        t_9 = t_3[0]
        t_6 = (t_6,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_6 + t_3
        t_6 = t_3[slice(None, 2, None)]
        t_12 = t_6[0]
        t_2 = self.l_40(t_12)
        t_6 = t_6[1]
        t_3 = t_3[slice(2, None, None)]
        t_1 = self.l_41(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_1, t_4)
        t_4 += t_10
        t_1 = t_4.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_1.type_as(t_4)
        t_4 = torch.nn.functional.dropout(t_4, p=0.1, training=True, inplace=False)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_44(t_2)
        t_5 = self.l_45(t_2)
        t_5 = t_12 + t_5
        t_10 = (t_2, None, t_10)
        t_2 = t_10[0]
        t_5 = (t_5,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_5 + t_10
        t_5 = t_10[0]
        t_12 = self.l_46(t_5)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_3 + t_10
        t_12 = self.l_47(t_12)
        t_12 = torch.nn.functional.relu(t_12, inplace=False)
        t_12 = self.l_48(t_12)
        t_12 = self.l_49(t_12)
        t_12 = self.l_50(t_12)
        t_12 = t_5 + t_12
        t_6 = (t_12, t_6)
        t_10 = t_6 + t_10
        t_6 = t_10[slice(None, 2, None)]
        t_6 = t_6[0]
        t_12 = t_10[2]
        t_10 = t_10[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3750
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3752
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3754
        return list(flatten((x0, t_6, t_12, t_10)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition11(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:11'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.9.layer.0.layer_norm',
                        'l_1': 'decoder.block.9.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.9.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.9.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.9.layer.0.SelfAttention.o',
                        'l_5': 'decoder.block.9.layer.0.dropout',
                        'l_6': 'decoder.block.9.layer.1.layer_norm',
                        'l_7': 'decoder.block.9.layer.1.EncDecAttention.q',
                        'l_8': 'decoder.block.9.layer.1.EncDecAttention.k',
                        'l_9': 'decoder.block.9.layer.1.EncDecAttention.v',
                        'l_10': 'decoder.block.9.layer.1.EncDecAttention.o',
                        'l_11': 'decoder.block.9.layer.1.dropout',
                        'l_12': 'decoder.block.9.layer.2.layer_norm',
                        'l_13': 'decoder.block.9.layer.2.DenseReluDense.wi',
                        'l_14': 'decoder.block.9.layer.2.DenseReluDense.dropout',
                        'l_15': 'decoder.block.9.layer.2.DenseReluDense.wo',
                        'l_16': 'decoder.block.9.layer.2.dropout',
                        'l_17': 'decoder.block.10.layer.0.layer_norm',
                        'l_18': 'decoder.block.10.layer.0.SelfAttention.q',
                        'l_19': 'decoder.block.10.layer.0.SelfAttention.k',
                        'l_20': 'decoder.block.10.layer.0.SelfAttention.v',
                        'l_21': 'decoder.block.10.layer.0.SelfAttention.o',
                        'l_22': 'decoder.block.10.layer.0.dropout',
                        'l_23': 'decoder.block.10.layer.1.layer_norm',
                        'l_24': 'decoder.block.10.layer.1.EncDecAttention.q',
                        'l_25': 'decoder.block.10.layer.1.EncDecAttention.k',
                        'l_26': 'decoder.block.10.layer.1.EncDecAttention.v',
                        'l_27': 'decoder.block.10.layer.1.EncDecAttention.o',
                        'l_28': 'decoder.block.10.layer.1.dropout',
                        'l_29': 'decoder.block.10.layer.2.layer_norm',
                        'l_30': 'decoder.block.10.layer.2.DenseReluDense.wi',
                        'l_31': 'decoder.block.10.layer.2.DenseReluDense.dropout',
                        'l_32': 'decoder.block.10.layer.2.DenseReluDense.wo',
                        'l_33': 'decoder.block.10.layer.2.dropout',
                        'l_34': 'decoder.block.11.layer.0.layer_norm',
                        'l_35': 'decoder.block.11.layer.0.SelfAttention.q',
                        'l_36': 'decoder.block.11.layer.0.SelfAttention.k',
                        'l_37': 'decoder.block.11.layer.0.SelfAttention.v',
                        'l_38': 'decoder.block.11.layer.0.SelfAttention.o',
                        'l_39': 'decoder.block.11.layer.0.dropout',
                        'l_40': 'decoder.block.11.layer.1.layer_norm',
                        'l_41': 'decoder.block.11.layer.1.EncDecAttention.q',
                        'l_42': 'decoder.block.11.layer.1.EncDecAttention.k',
                        'l_43': 'decoder.block.11.layer.1.EncDecAttention.v',
                        'l_44': 'decoder.block.11.layer.1.EncDecAttention.o',
                        'l_45': 'decoder.block.11.layer.1.dropout',
                        'l_46': 'decoder.block.11.layer.2.layer_norm',
                        'l_47': 'decoder.block.11.layer.2.DenseReluDense.wi',
                        'l_48': 'decoder.block.11.layer.2.DenseReluDense.dropout',
                        'l_49': 'decoder.block.11.layer.2.DenseReluDense.wo',
                        'l_50': 'decoder.block.11.layer.2.dropout',
                        'l_51': 'decoder.block.12.layer.0.layer_norm'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_50
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_51
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3750 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3752 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3754 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_8(x0)
        t_1 = self.l_9(x0)
        t_2 = self.l_25(x0)
        t_3 = self.l_26(x0)
        t_4 = self.l_42(x0)
        t_5 = self.l_43(x0)
        t_6 = self.l_0(x1)
        t_7 = self.l_1(t_6)
        t_8 = self.l_2(t_6)
        t_9 = self.l_3(t_6)
        t_6 = t_6.shape
        t_6 = t_6[slice(None, 2, None)]
        t_6 = t_6[0]
        t_7 = t_7.view(t_6, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_8 = t_8.view(t_6, -1, 32, 128)
        t_8 = t_8.transpose(1, 2)
        t_9 = t_9.view(t_6, -1, 32, 128)
        t_9 = t_9.transpose(1, 2)
        t_8 = t_8.transpose(3, 2)
        t_8 = torch.matmul(t_7, t_8)
        t_8 += x2
        t_7 = t_8.float()
        t_7 = torch.nn.functional.softmax(t_7, dim=-1, _stacklevel=3, dtype=None)
        t_8 = t_7.type_as(t_8)
        t_8 = torch.nn.functional.dropout(t_8, p=0.1, training=True, inplace=False)
        t_9 = torch.matmul(t_8, t_9)
        t_9 = t_9.transpose(1, 2)
        t_9 = t_9.contiguous()
        t_6 = t_9.view(t_6, -1, 4096)
        t_6 = self.l_4(t_6)
        t_9 = self.l_5(t_6)
        t_9 = x1 + t_9
        t_6 = (t_6, None, x2)
        t_8 = t_6[0]
        t_9 = (t_9,)
        t_6 = t_6[slice(1, None, None)]
        t_6 = t_9 + t_6
        t_9 = t_6[slice(None, 2, None)]
        t_7 = t_9[0]
        t_10 = self.l_6(t_7)
        t_9 = t_9[1]
        t_6 = t_6[slice(2, None, None)]
        t_11 = self.l_7(t_10)
        t_10 = t_10.shape
        t_10 = t_10[slice(None, 2, None)]
        t_10 = t_10[0]
        t_11 = t_11.view(t_10, -1, 32, 128)
        t_11 = t_11.transpose(1, 2)
        t_0 = t_0.view(t_10, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_10, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_11, t_0)
        t_0 += x3
        t_11 = t_0.float()
        t_11 = torch.nn.functional.softmax(t_11, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_11.type_as(t_0)
        t_0 = torch.nn.functional.dropout(t_0, p=0.1, training=True, inplace=False)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_10 = t_1.view(t_10, -1, 4096)
        t_10 = self.l_10(t_10)
        t_1 = self.l_11(t_10)
        t_1 = t_7 + t_1
        t_10 = (t_10, None, x3)
        t_7 = t_10[0]
        t_1 = (t_1,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_1 + t_10
        t_1 = t_10[0]
        t_0 = self.l_12(t_1)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_6 + t_10
        t_0 = self.l_13(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_14(t_0)
        t_0 = self.l_15(t_0)
        t_0 = self.l_16(t_0)
        t_0 = t_1 + t_0
        t_9 = (t_0, t_9)
        t_10 = t_9 + t_10
        t_9 = t_10[slice(None, 2, None)]
        t_9 = t_9[0]
        t_0 = self.l_17(t_9)
        t_1 = t_10[2]
        t_10 = t_10[3]
        t_6 = self.l_18(t_0)
        t_11 = self.l_19(t_0)
        t_12 = self.l_20(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_6 = t_6.view(t_0, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_11 = t_11.view(t_0, -1, 32, 128)
        t_11 = t_11.transpose(1, 2)
        t_12 = t_12.view(t_0, -1, 32, 128)
        t_12 = t_12.transpose(1, 2)
        t_11 = t_11.transpose(3, 2)
        t_11 = torch.matmul(t_6, t_11)
        t_11 += t_1
        t_6 = t_11.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_11 = t_6.type_as(t_11)
        t_11 = torch.nn.functional.dropout(t_11, p=0.1, training=True, inplace=False)
        t_12 = torch.matmul(t_11, t_12)
        t_12 = t_12.transpose(1, 2)
        t_12 = t_12.contiguous()
        t_0 = t_12.view(t_0, -1, 4096)
        t_0 = self.l_21(t_0)
        t_12 = self.l_22(t_0)
        t_12 = t_9 + t_12
        t_1 = (t_0, None, t_1)
        t_0 = t_1[0]
        t_12 = (t_12,)
        t_1 = t_1[slice(1, None, None)]
        t_1 = t_12 + t_1
        t_12 = t_1[slice(None, 2, None)]
        t_9 = t_12[0]
        t_11 = self.l_23(t_9)
        t_12 = t_12[1]
        t_1 = t_1[slice(2, None, None)]
        t_6 = self.l_24(t_11)
        t_11 = t_11.shape
        t_11 = t_11[slice(None, 2, None)]
        t_11 = t_11[0]
        t_6 = t_6.view(t_11, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_2 = t_2.view(t_11, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_11, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_6, t_2)
        t_2 += t_10
        t_6 = t_2.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_6.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_11 = t_3.view(t_11, -1, 4096)
        t_11 = self.l_27(t_11)
        t_3 = self.l_28(t_11)
        t_3 = t_9 + t_3
        t_10 = (t_11, None, t_10)
        t_11 = t_10[0]
        t_3 = (t_3,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_3 + t_10
        t_3 = t_10[0]
        t_9 = self.l_29(t_3)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_1 + t_10
        t_9 = self.l_30(t_9)
        t_9 = torch.nn.functional.relu(t_9, inplace=False)
        t_9 = self.l_31(t_9)
        t_9 = self.l_32(t_9)
        t_9 = self.l_33(t_9)
        t_9 = t_3 + t_9
        t_12 = (t_9, t_12)
        t_10 = t_12 + t_10
        t_12 = t_10[slice(None, 2, None)]
        t_12 = t_12[0]
        t_9 = self.l_34(t_12)
        t_3 = t_10[2]
        t_10 = t_10[3]
        t_1 = self.l_35(t_9)
        t_2 = self.l_36(t_9)
        t_6 = self.l_37(t_9)
        t_9 = t_9.shape
        t_9 = t_9[slice(None, 2, None)]
        t_9 = t_9[0]
        t_1 = t_1.view(t_9, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_9, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_6 = t_6.view(t_9, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += t_3
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_6 = torch.matmul(t_2, t_6)
        t_6 = t_6.transpose(1, 2)
        t_6 = t_6.contiguous()
        t_9 = t_6.view(t_9, -1, 4096)
        t_9 = self.l_38(t_9)
        t_6 = self.l_39(t_9)
        t_6 = t_12 + t_6
        t_3 = (t_9, None, t_3)
        t_9 = t_3[0]
        t_6 = (t_6,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_6 + t_3
        t_6 = t_3[slice(None, 2, None)]
        t_12 = t_6[0]
        t_2 = self.l_40(t_12)
        t_6 = t_6[1]
        t_3 = t_3[slice(2, None, None)]
        t_1 = self.l_41(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_1, t_4)
        t_4 += t_10
        t_1 = t_4.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_1.type_as(t_4)
        t_4 = torch.nn.functional.dropout(t_4, p=0.1, training=True, inplace=False)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_44(t_2)
        t_5 = self.l_45(t_2)
        t_5 = t_12 + t_5
        t_10 = (t_2, None, t_10)
        t_2 = t_10[0]
        t_5 = (t_5,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_5 + t_10
        t_5 = t_10[0]
        t_12 = self.l_46(t_5)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_3 + t_10
        t_12 = self.l_47(t_12)
        t_12 = torch.nn.functional.relu(t_12, inplace=False)
        t_12 = self.l_48(t_12)
        t_12 = self.l_49(t_12)
        t_12 = self.l_50(t_12)
        t_12 = t_5 + t_12
        t_6 = (t_12, t_6)
        t_10 = t_6 + t_10
        t_6 = t_10[slice(None, 2, None)]
        t_6 = t_6[0]
        t_12 = self.l_51(t_6)
        t_5 = t_10[2]
        t_10 = t_10[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4233
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4235
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4237
        return list(flatten((x0, t_6, t_12, t_5, t_10)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition12(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:12'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.12.layer.0.SelfAttention.q',
                        'l_1': 'decoder.block.12.layer.0.SelfAttention.k',
                        'l_2': 'decoder.block.12.layer.0.SelfAttention.v',
                        'l_3': 'decoder.block.12.layer.0.SelfAttention.o',
                        'l_4': 'decoder.block.12.layer.0.dropout',
                        'l_5': 'decoder.block.12.layer.1.layer_norm',
                        'l_6': 'decoder.block.12.layer.1.EncDecAttention.q',
                        'l_7': 'decoder.block.12.layer.1.EncDecAttention.k',
                        'l_8': 'decoder.block.12.layer.1.EncDecAttention.v',
                        'l_9': 'decoder.block.12.layer.1.EncDecAttention.o',
                        'l_10': 'decoder.block.12.layer.1.dropout',
                        'l_11': 'decoder.block.12.layer.2.layer_norm',
                        'l_12': 'decoder.block.12.layer.2.DenseReluDense.wi',
                        'l_13': 'decoder.block.12.layer.2.DenseReluDense.dropout',
                        'l_14': 'decoder.block.12.layer.2.DenseReluDense.wo',
                        'l_15': 'decoder.block.12.layer.2.dropout',
                        'l_16': 'decoder.block.13.layer.0.layer_norm',
                        'l_17': 'decoder.block.13.layer.0.SelfAttention.q',
                        'l_18': 'decoder.block.13.layer.0.SelfAttention.k',
                        'l_19': 'decoder.block.13.layer.0.SelfAttention.v',
                        'l_20': 'decoder.block.13.layer.0.SelfAttention.o',
                        'l_21': 'decoder.block.13.layer.0.dropout',
                        'l_22': 'decoder.block.13.layer.1.layer_norm',
                        'l_23': 'decoder.block.13.layer.1.EncDecAttention.q',
                        'l_24': 'decoder.block.13.layer.1.EncDecAttention.k',
                        'l_25': 'decoder.block.13.layer.1.EncDecAttention.v',
                        'l_26': 'decoder.block.13.layer.1.EncDecAttention.o',
                        'l_27': 'decoder.block.13.layer.1.dropout',
                        'l_28': 'decoder.block.13.layer.2.layer_norm',
                        'l_29': 'decoder.block.13.layer.2.DenseReluDense.wi',
                        'l_30': 'decoder.block.13.layer.2.DenseReluDense.dropout',
                        'l_31': 'decoder.block.13.layer.2.DenseReluDense.wo',
                        'l_32': 'decoder.block.13.layer.2.dropout',
                        'l_33': 'decoder.block.14.layer.0.layer_norm',
                        'l_34': 'decoder.block.14.layer.0.SelfAttention.q',
                        'l_35': 'decoder.block.14.layer.0.SelfAttention.k',
                        'l_36': 'decoder.block.14.layer.0.SelfAttention.v',
                        'l_37': 'decoder.block.14.layer.0.SelfAttention.o',
                        'l_38': 'decoder.block.14.layer.0.dropout',
                        'l_39': 'decoder.block.14.layer.1.layer_norm',
                        'l_40': 'decoder.block.14.layer.1.EncDecAttention.q',
                        'l_41': 'decoder.block.14.layer.1.EncDecAttention.k',
                        'l_42': 'decoder.block.14.layer.1.EncDecAttention.v',
                        'l_43': 'decoder.block.14.layer.1.EncDecAttention.o',
                        'l_44': 'decoder.block.14.layer.1.dropout',
                        'l_45': 'decoder.block.14.layer.2.layer_norm',
                        'l_46': 'decoder.block.14.layer.2.DenseReluDense.wi',
                        'l_47': 'decoder.block.14.layer.2.DenseReluDense.dropout',
                        'l_48': 'decoder.block.14.layer.2.DenseReluDense.wo',
                        'l_49': 'decoder.block.14.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4233 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4235 <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4237 <=> x4
        x0, x1, x2, x3, x4 = unflatten(args, self.input_structure)
        t_0 = self.l_7(x0)
        t_1 = self.l_8(x0)
        t_2 = self.l_24(x0)
        t_3 = self.l_25(x0)
        t_4 = self.l_41(x0)
        t_5 = self.l_42(x0)
        t_6 = self.l_0(x2)
        t_7 = self.l_1(x2)
        t_8 = self.l_2(x2)
        t_9 = x2.shape
        t_9 = t_9[slice(None, 2, None)]
        t_9 = t_9[0]
        t_6 = t_6.view(t_9, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_7 = t_7.view(t_9, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_8 = t_8.view(t_9, -1, 32, 128)
        t_8 = t_8.transpose(1, 2)
        t_7 = t_7.transpose(3, 2)
        t_7 = torch.matmul(t_6, t_7)
        t_7 += x3
        t_6 = t_7.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_7 = t_6.type_as(t_7)
        t_7 = torch.nn.functional.dropout(t_7, p=0.1, training=True, inplace=False)
        t_8 = torch.matmul(t_7, t_8)
        t_8 = t_8.transpose(1, 2)
        t_8 = t_8.contiguous()
        t_9 = t_8.view(t_9, -1, 4096)
        t_9 = self.l_3(t_9)
        t_8 = self.l_4(t_9)
        t_8 = x1 + t_8
        t_9 = (t_9, None, x3)
        t_7 = t_9[0]
        t_8 = (t_8,)
        t_9 = t_9[slice(1, None, None)]
        t_9 = t_8 + t_9
        t_8 = t_9[slice(None, 2, None)]
        t_6 = t_8[0]
        t_10 = self.l_5(t_6)
        t_8 = t_8[1]
        t_9 = t_9[slice(2, None, None)]
        t_11 = self.l_6(t_10)
        t_10 = t_10.shape
        t_10 = t_10[slice(None, 2, None)]
        t_10 = t_10[0]
        t_11 = t_11.view(t_10, -1, 32, 128)
        t_11 = t_11.transpose(1, 2)
        t_0 = t_0.view(t_10, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_10, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_11, t_0)
        t_0 += x4
        t_11 = t_0.float()
        t_11 = torch.nn.functional.softmax(t_11, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_11.type_as(t_0)
        t_0 = torch.nn.functional.dropout(t_0, p=0.1, training=True, inplace=False)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_10 = t_1.view(t_10, -1, 4096)
        t_10 = self.l_9(t_10)
        t_1 = self.l_10(t_10)
        t_1 = t_6 + t_1
        t_10 = (t_10, None, x4)
        t_6 = t_10[0]
        t_1 = (t_1,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_1 + t_10
        t_1 = t_10[0]
        t_0 = self.l_11(t_1)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_9 + t_10
        t_0 = self.l_12(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_13(t_0)
        t_0 = self.l_14(t_0)
        t_0 = self.l_15(t_0)
        t_0 = t_1 + t_0
        t_8 = (t_0, t_8)
        t_10 = t_8 + t_10
        t_8 = t_10[slice(None, 2, None)]
        t_8 = t_8[0]
        t_0 = self.l_16(t_8)
        t_1 = t_10[2]
        t_10 = t_10[3]
        t_9 = self.l_17(t_0)
        t_11 = self.l_18(t_0)
        t_12 = self.l_19(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_9 = t_9.view(t_0, -1, 32, 128)
        t_9 = t_9.transpose(1, 2)
        t_11 = t_11.view(t_0, -1, 32, 128)
        t_11 = t_11.transpose(1, 2)
        t_12 = t_12.view(t_0, -1, 32, 128)
        t_12 = t_12.transpose(1, 2)
        t_11 = t_11.transpose(3, 2)
        t_11 = torch.matmul(t_9, t_11)
        t_11 += t_1
        t_9 = t_11.float()
        t_9 = torch.nn.functional.softmax(t_9, dim=-1, _stacklevel=3, dtype=None)
        t_11 = t_9.type_as(t_11)
        t_11 = torch.nn.functional.dropout(t_11, p=0.1, training=True, inplace=False)
        t_12 = torch.matmul(t_11, t_12)
        t_12 = t_12.transpose(1, 2)
        t_12 = t_12.contiguous()
        t_0 = t_12.view(t_0, -1, 4096)
        t_0 = self.l_20(t_0)
        t_12 = self.l_21(t_0)
        t_12 = t_8 + t_12
        t_1 = (t_0, None, t_1)
        t_0 = t_1[0]
        t_12 = (t_12,)
        t_1 = t_1[slice(1, None, None)]
        t_1 = t_12 + t_1
        t_12 = t_1[slice(None, 2, None)]
        t_8 = t_12[0]
        t_11 = self.l_22(t_8)
        t_12 = t_12[1]
        t_1 = t_1[slice(2, None, None)]
        t_9 = self.l_23(t_11)
        t_11 = t_11.shape
        t_11 = t_11[slice(None, 2, None)]
        t_11 = t_11[0]
        t_9 = t_9.view(t_11, -1, 32, 128)
        t_9 = t_9.transpose(1, 2)
        t_2 = t_2.view(t_11, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_11, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_9, t_2)
        t_2 += t_10
        t_9 = t_2.float()
        t_9 = torch.nn.functional.softmax(t_9, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_9.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_11 = t_3.view(t_11, -1, 4096)
        t_11 = self.l_26(t_11)
        t_3 = self.l_27(t_11)
        t_3 = t_8 + t_3
        t_10 = (t_11, None, t_10)
        t_11 = t_10[0]
        t_3 = (t_3,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_3 + t_10
        t_3 = t_10[0]
        t_8 = self.l_28(t_3)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_1 + t_10
        t_8 = self.l_29(t_8)
        t_8 = torch.nn.functional.relu(t_8, inplace=False)
        t_8 = self.l_30(t_8)
        t_8 = self.l_31(t_8)
        t_8 = self.l_32(t_8)
        t_8 = t_3 + t_8
        t_12 = (t_8, t_12)
        t_10 = t_12 + t_10
        t_12 = t_10[slice(None, 2, None)]
        t_12 = t_12[0]
        t_8 = self.l_33(t_12)
        t_3 = t_10[2]
        t_10 = t_10[3]
        t_1 = self.l_34(t_8)
        t_2 = self.l_35(t_8)
        t_9 = self.l_36(t_8)
        t_8 = t_8.shape
        t_8 = t_8[slice(None, 2, None)]
        t_8 = t_8[0]
        t_1 = t_1.view(t_8, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_8, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_9 = t_9.view(t_8, -1, 32, 128)
        t_9 = t_9.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += t_3
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_9 = torch.matmul(t_2, t_9)
        t_9 = t_9.transpose(1, 2)
        t_9 = t_9.contiguous()
        t_8 = t_9.view(t_8, -1, 4096)
        t_8 = self.l_37(t_8)
        t_9 = self.l_38(t_8)
        t_9 = t_12 + t_9
        t_3 = (t_8, None, t_3)
        t_8 = t_3[0]
        t_9 = (t_9,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_9 + t_3
        t_9 = t_3[slice(None, 2, None)]
        t_12 = t_9[0]
        t_2 = self.l_39(t_12)
        t_9 = t_9[1]
        t_3 = t_3[slice(2, None, None)]
        t_1 = self.l_40(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_1, t_4)
        t_4 += t_10
        t_1 = t_4.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_1.type_as(t_4)
        t_4 = torch.nn.functional.dropout(t_4, p=0.1, training=True, inplace=False)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_43(t_2)
        t_5 = self.l_44(t_2)
        t_5 = t_12 + t_5
        t_10 = (t_2, None, t_10)
        t_2 = t_10[0]
        t_5 = (t_5,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_5 + t_10
        t_5 = t_10[0]
        t_12 = self.l_45(t_5)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_3 + t_10
        t_12 = self.l_46(t_12)
        t_12 = torch.nn.functional.relu(t_12, inplace=False)
        t_12 = self.l_47(t_12)
        t_12 = self.l_48(t_12)
        t_12 = self.l_49(t_12)
        t_12 = t_5 + t_12
        t_9 = (t_12, t_9)
        t_10 = t_9 + t_10
        t_9 = t_10[slice(None, 2, None)]
        t_9 = t_9[0]
        t_12 = t_10[2]
        t_10 = t_10[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4716
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4718
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4720
        return list(flatten((x0, t_9, t_12, t_10)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition13(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:13'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.15.layer.0.layer_norm',
                        'l_1': 'decoder.block.15.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.15.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.15.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.15.layer.0.SelfAttention.o',
                        'l_5': 'decoder.block.15.layer.0.dropout',
                        'l_6': 'decoder.block.15.layer.1.layer_norm',
                        'l_7': 'decoder.block.15.layer.1.EncDecAttention.q',
                        'l_8': 'decoder.block.15.layer.1.EncDecAttention.k',
                        'l_9': 'decoder.block.15.layer.1.EncDecAttention.v',
                        'l_10': 'decoder.block.15.layer.1.EncDecAttention.o',
                        'l_11': 'decoder.block.15.layer.1.dropout',
                        'l_12': 'decoder.block.15.layer.2.layer_norm',
                        'l_13': 'decoder.block.15.layer.2.DenseReluDense.wi',
                        'l_14': 'decoder.block.15.layer.2.DenseReluDense.dropout',
                        'l_15': 'decoder.block.15.layer.2.DenseReluDense.wo',
                        'l_16': 'decoder.block.15.layer.2.dropout',
                        'l_17': 'decoder.block.16.layer.0.layer_norm',
                        'l_18': 'decoder.block.16.layer.0.SelfAttention.q',
                        'l_19': 'decoder.block.16.layer.0.SelfAttention.k',
                        'l_20': 'decoder.block.16.layer.0.SelfAttention.v',
                        'l_21': 'decoder.block.16.layer.0.SelfAttention.o',
                        'l_22': 'decoder.block.16.layer.0.dropout',
                        'l_23': 'decoder.block.16.layer.1.layer_norm',
                        'l_24': 'decoder.block.16.layer.1.EncDecAttention.q',
                        'l_25': 'decoder.block.16.layer.1.EncDecAttention.k',
                        'l_26': 'decoder.block.16.layer.1.EncDecAttention.v',
                        'l_27': 'decoder.block.16.layer.1.EncDecAttention.o',
                        'l_28': 'decoder.block.16.layer.1.dropout',
                        'l_29': 'decoder.block.16.layer.2.layer_norm',
                        'l_30': 'decoder.block.16.layer.2.DenseReluDense.wi',
                        'l_31': 'decoder.block.16.layer.2.DenseReluDense.dropout',
                        'l_32': 'decoder.block.16.layer.2.DenseReluDense.wo',
                        'l_33': 'decoder.block.16.layer.2.dropout',
                        'l_34': 'decoder.block.17.layer.0.layer_norm',
                        'l_35': 'decoder.block.17.layer.0.SelfAttention.q',
                        'l_36': 'decoder.block.17.layer.0.SelfAttention.k',
                        'l_37': 'decoder.block.17.layer.0.SelfAttention.v',
                        'l_38': 'decoder.block.17.layer.0.SelfAttention.o',
                        'l_39': 'decoder.block.17.layer.0.dropout',
                        'l_40': 'decoder.block.17.layer.1.layer_norm',
                        'l_41': 'decoder.block.17.layer.1.EncDecAttention.q',
                        'l_42': 'decoder.block.17.layer.1.EncDecAttention.k',
                        'l_43': 'decoder.block.17.layer.1.EncDecAttention.v',
                        'l_44': 'decoder.block.17.layer.1.EncDecAttention.o',
                        'l_45': 'decoder.block.17.layer.1.dropout',
                        'l_46': 'decoder.block.17.layer.2.layer_norm',
                        'l_47': 'decoder.block.17.layer.2.DenseReluDense.wi',
                        'l_48': 'decoder.block.17.layer.2.DenseReluDense.dropout',
                        'l_49': 'decoder.block.17.layer.2.DenseReluDense.wo',
                        'l_50': 'decoder.block.17.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_50
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4716 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4718 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4720 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_8(x0)
        t_1 = self.l_9(x0)
        t_2 = self.l_25(x0)
        t_3 = self.l_26(x0)
        t_4 = self.l_42(x0)
        t_5 = self.l_43(x0)
        t_6 = self.l_0(x1)
        t_7 = self.l_1(t_6)
        t_8 = self.l_2(t_6)
        t_9 = self.l_3(t_6)
        t_6 = t_6.shape
        t_6 = t_6[slice(None, 2, None)]
        t_6 = t_6[0]
        t_7 = t_7.view(t_6, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_8 = t_8.view(t_6, -1, 32, 128)
        t_8 = t_8.transpose(1, 2)
        t_9 = t_9.view(t_6, -1, 32, 128)
        t_9 = t_9.transpose(1, 2)
        t_8 = t_8.transpose(3, 2)
        t_8 = torch.matmul(t_7, t_8)
        t_8 += x2
        t_7 = t_8.float()
        t_7 = torch.nn.functional.softmax(t_7, dim=-1, _stacklevel=3, dtype=None)
        t_8 = t_7.type_as(t_8)
        t_8 = torch.nn.functional.dropout(t_8, p=0.1, training=True, inplace=False)
        t_9 = torch.matmul(t_8, t_9)
        t_9 = t_9.transpose(1, 2)
        t_9 = t_9.contiguous()
        t_6 = t_9.view(t_6, -1, 4096)
        t_6 = self.l_4(t_6)
        t_9 = self.l_5(t_6)
        t_9 = x1 + t_9
        t_6 = (t_6, None, x2)
        t_8 = t_6[0]
        t_9 = (t_9,)
        t_6 = t_6[slice(1, None, None)]
        t_6 = t_9 + t_6
        t_9 = t_6[slice(None, 2, None)]
        t_7 = t_9[0]
        t_10 = self.l_6(t_7)
        t_9 = t_9[1]
        t_6 = t_6[slice(2, None, None)]
        t_11 = self.l_7(t_10)
        t_10 = t_10.shape
        t_10 = t_10[slice(None, 2, None)]
        t_10 = t_10[0]
        t_11 = t_11.view(t_10, -1, 32, 128)
        t_11 = t_11.transpose(1, 2)
        t_0 = t_0.view(t_10, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_10, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_11, t_0)
        t_0 += x3
        t_11 = t_0.float()
        t_11 = torch.nn.functional.softmax(t_11, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_11.type_as(t_0)
        t_0 = torch.nn.functional.dropout(t_0, p=0.1, training=True, inplace=False)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_10 = t_1.view(t_10, -1, 4096)
        t_10 = self.l_10(t_10)
        t_1 = self.l_11(t_10)
        t_1 = t_7 + t_1
        t_10 = (t_10, None, x3)
        t_7 = t_10[0]
        t_1 = (t_1,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_1 + t_10
        t_1 = t_10[0]
        t_0 = self.l_12(t_1)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_6 + t_10
        t_0 = self.l_13(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_14(t_0)
        t_0 = self.l_15(t_0)
        t_0 = self.l_16(t_0)
        t_0 = t_1 + t_0
        t_9 = (t_0, t_9)
        t_10 = t_9 + t_10
        t_9 = t_10[slice(None, 2, None)]
        t_9 = t_9[0]
        t_0 = self.l_17(t_9)
        t_1 = t_10[2]
        t_10 = t_10[3]
        t_6 = self.l_18(t_0)
        t_11 = self.l_19(t_0)
        t_12 = self.l_20(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_6 = t_6.view(t_0, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_11 = t_11.view(t_0, -1, 32, 128)
        t_11 = t_11.transpose(1, 2)
        t_12 = t_12.view(t_0, -1, 32, 128)
        t_12 = t_12.transpose(1, 2)
        t_11 = t_11.transpose(3, 2)
        t_11 = torch.matmul(t_6, t_11)
        t_11 += t_1
        t_6 = t_11.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_11 = t_6.type_as(t_11)
        t_11 = torch.nn.functional.dropout(t_11, p=0.1, training=True, inplace=False)
        t_12 = torch.matmul(t_11, t_12)
        t_12 = t_12.transpose(1, 2)
        t_12 = t_12.contiguous()
        t_0 = t_12.view(t_0, -1, 4096)
        t_0 = self.l_21(t_0)
        t_12 = self.l_22(t_0)
        t_12 = t_9 + t_12
        t_1 = (t_0, None, t_1)
        t_0 = t_1[0]
        t_12 = (t_12,)
        t_1 = t_1[slice(1, None, None)]
        t_1 = t_12 + t_1
        t_12 = t_1[slice(None, 2, None)]
        t_9 = t_12[0]
        t_11 = self.l_23(t_9)
        t_12 = t_12[1]
        t_1 = t_1[slice(2, None, None)]
        t_6 = self.l_24(t_11)
        t_11 = t_11.shape
        t_11 = t_11[slice(None, 2, None)]
        t_11 = t_11[0]
        t_6 = t_6.view(t_11, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_2 = t_2.view(t_11, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_11, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_6, t_2)
        t_2 += t_10
        t_6 = t_2.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_6.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_11 = t_3.view(t_11, -1, 4096)
        t_11 = self.l_27(t_11)
        t_3 = self.l_28(t_11)
        t_3 = t_9 + t_3
        t_10 = (t_11, None, t_10)
        t_11 = t_10[0]
        t_3 = (t_3,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_3 + t_10
        t_3 = t_10[0]
        t_9 = self.l_29(t_3)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_1 + t_10
        t_9 = self.l_30(t_9)
        t_9 = torch.nn.functional.relu(t_9, inplace=False)
        t_9 = self.l_31(t_9)
        t_9 = self.l_32(t_9)
        t_9 = self.l_33(t_9)
        t_9 = t_3 + t_9
        t_12 = (t_9, t_12)
        t_10 = t_12 + t_10
        t_12 = t_10[slice(None, 2, None)]
        t_12 = t_12[0]
        t_9 = self.l_34(t_12)
        t_3 = t_10[2]
        t_10 = t_10[3]
        t_1 = self.l_35(t_9)
        t_2 = self.l_36(t_9)
        t_6 = self.l_37(t_9)
        t_9 = t_9.shape
        t_9 = t_9[slice(None, 2, None)]
        t_9 = t_9[0]
        t_1 = t_1.view(t_9, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_9, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_6 = t_6.view(t_9, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += t_3
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_6 = torch.matmul(t_2, t_6)
        t_6 = t_6.transpose(1, 2)
        t_6 = t_6.contiguous()
        t_9 = t_6.view(t_9, -1, 4096)
        t_9 = self.l_38(t_9)
        t_6 = self.l_39(t_9)
        t_6 = t_12 + t_6
        t_3 = (t_9, None, t_3)
        t_9 = t_3[0]
        t_6 = (t_6,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_6 + t_3
        t_6 = t_3[slice(None, 2, None)]
        t_12 = t_6[0]
        t_2 = self.l_40(t_12)
        t_6 = t_6[1]
        t_3 = t_3[slice(2, None, None)]
        t_1 = self.l_41(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_1, t_4)
        t_4 += t_10
        t_1 = t_4.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_1.type_as(t_4)
        t_4 = torch.nn.functional.dropout(t_4, p=0.1, training=True, inplace=False)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_44(t_2)
        t_5 = self.l_45(t_2)
        t_5 = t_12 + t_5
        t_10 = (t_2, None, t_10)
        t_2 = t_10[0]
        t_5 = (t_5,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_5 + t_10
        t_5 = t_10[0]
        t_12 = self.l_46(t_5)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_3 + t_10
        t_12 = self.l_47(t_12)
        t_12 = torch.nn.functional.relu(t_12, inplace=False)
        t_12 = self.l_48(t_12)
        t_12 = self.l_49(t_12)
        t_12 = self.l_50(t_12)
        t_12 = t_5 + t_12
        t_6 = (t_12, t_6)
        t_10 = t_6 + t_10
        t_6 = t_10[slice(None, 2, None)]
        t_6 = t_6[0]
        t_12 = t_10[2]
        t_10 = t_10[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5199
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5201
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5203
        return list(flatten((x0, t_6, t_12, t_10)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition14(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:14'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.18.layer.0.layer_norm',
                        'l_1': 'decoder.block.18.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.18.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.18.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.18.layer.0.SelfAttention.o',
                        'l_5': 'decoder.block.18.layer.0.dropout',
                        'l_6': 'decoder.block.18.layer.1.layer_norm',
                        'l_7': 'decoder.block.18.layer.1.EncDecAttention.q',
                        'l_8': 'decoder.block.18.layer.1.EncDecAttention.k',
                        'l_9': 'decoder.block.18.layer.1.EncDecAttention.v',
                        'l_10': 'decoder.block.18.layer.1.EncDecAttention.o',
                        'l_11': 'decoder.block.18.layer.1.dropout',
                        'l_12': 'decoder.block.18.layer.2.layer_norm',
                        'l_13': 'decoder.block.18.layer.2.DenseReluDense.wi',
                        'l_14': 'decoder.block.18.layer.2.DenseReluDense.dropout',
                        'l_15': 'decoder.block.18.layer.2.DenseReluDense.wo',
                        'l_16': 'decoder.block.18.layer.2.dropout',
                        'l_17': 'decoder.block.19.layer.0.layer_norm',
                        'l_18': 'decoder.block.19.layer.0.SelfAttention.q',
                        'l_19': 'decoder.block.19.layer.0.SelfAttention.k',
                        'l_20': 'decoder.block.19.layer.0.SelfAttention.v',
                        'l_21': 'decoder.block.19.layer.0.SelfAttention.o',
                        'l_22': 'decoder.block.19.layer.0.dropout',
                        'l_23': 'decoder.block.19.layer.1.layer_norm',
                        'l_24': 'decoder.block.19.layer.1.EncDecAttention.q',
                        'l_25': 'decoder.block.19.layer.1.EncDecAttention.k',
                        'l_26': 'decoder.block.19.layer.1.EncDecAttention.v',
                        'l_27': 'decoder.block.19.layer.1.EncDecAttention.o',
                        'l_28': 'decoder.block.19.layer.1.dropout',
                        'l_29': 'decoder.block.19.layer.2.layer_norm',
                        'l_30': 'decoder.block.19.layer.2.DenseReluDense.wi',
                        'l_31': 'decoder.block.19.layer.2.DenseReluDense.dropout',
                        'l_32': 'decoder.block.19.layer.2.DenseReluDense.wo',
                        'l_33': 'decoder.block.19.layer.2.dropout',
                        'l_34': 'decoder.block.20.layer.0.layer_norm',
                        'l_35': 'decoder.block.20.layer.0.SelfAttention.q',
                        'l_36': 'decoder.block.20.layer.0.SelfAttention.k',
                        'l_37': 'decoder.block.20.layer.0.SelfAttention.v',
                        'l_38': 'decoder.block.20.layer.0.SelfAttention.o',
                        'l_39': 'decoder.block.20.layer.0.dropout',
                        'l_40': 'decoder.block.20.layer.1.layer_norm',
                        'l_41': 'decoder.block.20.layer.1.EncDecAttention.q',
                        'l_42': 'decoder.block.20.layer.1.EncDecAttention.k',
                        'l_43': 'decoder.block.20.layer.1.EncDecAttention.v',
                        'l_44': 'decoder.block.20.layer.1.EncDecAttention.o',
                        'l_45': 'decoder.block.20.layer.1.dropout',
                        'l_46': 'decoder.block.20.layer.2.layer_norm',
                        'l_47': 'decoder.block.20.layer.2.DenseReluDense.wi',
                        'l_48': 'decoder.block.20.layer.2.DenseReluDense.dropout',
                        'l_49': 'decoder.block.20.layer.2.DenseReluDense.wo',
                        'l_50': 'decoder.block.20.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_50
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5199 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5201 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5203 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_8(x0)
        t_1 = self.l_9(x0)
        t_2 = self.l_25(x0)
        t_3 = self.l_26(x0)
        t_4 = self.l_42(x0)
        t_5 = self.l_43(x0)
        t_6 = self.l_0(x1)
        t_7 = self.l_1(t_6)
        t_8 = self.l_2(t_6)
        t_9 = self.l_3(t_6)
        t_6 = t_6.shape
        t_6 = t_6[slice(None, 2, None)]
        t_6 = t_6[0]
        t_7 = t_7.view(t_6, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_8 = t_8.view(t_6, -1, 32, 128)
        t_8 = t_8.transpose(1, 2)
        t_9 = t_9.view(t_6, -1, 32, 128)
        t_9 = t_9.transpose(1, 2)
        t_8 = t_8.transpose(3, 2)
        t_8 = torch.matmul(t_7, t_8)
        t_8 += x2
        t_7 = t_8.float()
        t_7 = torch.nn.functional.softmax(t_7, dim=-1, _stacklevel=3, dtype=None)
        t_8 = t_7.type_as(t_8)
        t_8 = torch.nn.functional.dropout(t_8, p=0.1, training=True, inplace=False)
        t_9 = torch.matmul(t_8, t_9)
        t_9 = t_9.transpose(1, 2)
        t_9 = t_9.contiguous()
        t_6 = t_9.view(t_6, -1, 4096)
        t_6 = self.l_4(t_6)
        t_9 = self.l_5(t_6)
        t_9 = x1 + t_9
        t_6 = (t_6, None, x2)
        t_8 = t_6[0]
        t_9 = (t_9,)
        t_6 = t_6[slice(1, None, None)]
        t_6 = t_9 + t_6
        t_9 = t_6[slice(None, 2, None)]
        t_7 = t_9[0]
        t_10 = self.l_6(t_7)
        t_9 = t_9[1]
        t_6 = t_6[slice(2, None, None)]
        t_11 = self.l_7(t_10)
        t_10 = t_10.shape
        t_10 = t_10[slice(None, 2, None)]
        t_10 = t_10[0]
        t_11 = t_11.view(t_10, -1, 32, 128)
        t_11 = t_11.transpose(1, 2)
        t_0 = t_0.view(t_10, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_10, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_11, t_0)
        t_0 += x3
        t_11 = t_0.float()
        t_11 = torch.nn.functional.softmax(t_11, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_11.type_as(t_0)
        t_0 = torch.nn.functional.dropout(t_0, p=0.1, training=True, inplace=False)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_10 = t_1.view(t_10, -1, 4096)
        t_10 = self.l_10(t_10)
        t_1 = self.l_11(t_10)
        t_1 = t_7 + t_1
        t_10 = (t_10, None, x3)
        t_7 = t_10[0]
        t_1 = (t_1,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_1 + t_10
        t_1 = t_10[0]
        t_0 = self.l_12(t_1)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_6 + t_10
        t_0 = self.l_13(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_14(t_0)
        t_0 = self.l_15(t_0)
        t_0 = self.l_16(t_0)
        t_0 = t_1 + t_0
        t_9 = (t_0, t_9)
        t_10 = t_9 + t_10
        t_9 = t_10[slice(None, 2, None)]
        t_9 = t_9[0]
        t_0 = self.l_17(t_9)
        t_1 = t_10[2]
        t_10 = t_10[3]
        t_6 = self.l_18(t_0)
        t_11 = self.l_19(t_0)
        t_12 = self.l_20(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_6 = t_6.view(t_0, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_11 = t_11.view(t_0, -1, 32, 128)
        t_11 = t_11.transpose(1, 2)
        t_12 = t_12.view(t_0, -1, 32, 128)
        t_12 = t_12.transpose(1, 2)
        t_11 = t_11.transpose(3, 2)
        t_11 = torch.matmul(t_6, t_11)
        t_11 += t_1
        t_6 = t_11.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_11 = t_6.type_as(t_11)
        t_11 = torch.nn.functional.dropout(t_11, p=0.1, training=True, inplace=False)
        t_12 = torch.matmul(t_11, t_12)
        t_12 = t_12.transpose(1, 2)
        t_12 = t_12.contiguous()
        t_0 = t_12.view(t_0, -1, 4096)
        t_0 = self.l_21(t_0)
        t_12 = self.l_22(t_0)
        t_12 = t_9 + t_12
        t_1 = (t_0, None, t_1)
        t_0 = t_1[0]
        t_12 = (t_12,)
        t_1 = t_1[slice(1, None, None)]
        t_1 = t_12 + t_1
        t_12 = t_1[slice(None, 2, None)]
        t_9 = t_12[0]
        t_11 = self.l_23(t_9)
        t_12 = t_12[1]
        t_1 = t_1[slice(2, None, None)]
        t_6 = self.l_24(t_11)
        t_11 = t_11.shape
        t_11 = t_11[slice(None, 2, None)]
        t_11 = t_11[0]
        t_6 = t_6.view(t_11, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_2 = t_2.view(t_11, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_11, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_6, t_2)
        t_2 += t_10
        t_6 = t_2.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_6.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_11 = t_3.view(t_11, -1, 4096)
        t_11 = self.l_27(t_11)
        t_3 = self.l_28(t_11)
        t_3 = t_9 + t_3
        t_10 = (t_11, None, t_10)
        t_11 = t_10[0]
        t_3 = (t_3,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_3 + t_10
        t_3 = t_10[0]
        t_9 = self.l_29(t_3)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_1 + t_10
        t_9 = self.l_30(t_9)
        t_9 = torch.nn.functional.relu(t_9, inplace=False)
        t_9 = self.l_31(t_9)
        t_9 = self.l_32(t_9)
        t_9 = self.l_33(t_9)
        t_9 = t_3 + t_9
        t_12 = (t_9, t_12)
        t_10 = t_12 + t_10
        t_12 = t_10[slice(None, 2, None)]
        t_12 = t_12[0]
        t_9 = self.l_34(t_12)
        t_3 = t_10[2]
        t_10 = t_10[3]
        t_1 = self.l_35(t_9)
        t_2 = self.l_36(t_9)
        t_6 = self.l_37(t_9)
        t_9 = t_9.shape
        t_9 = t_9[slice(None, 2, None)]
        t_9 = t_9[0]
        t_1 = t_1.view(t_9, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_9, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_6 = t_6.view(t_9, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += t_3
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = torch.nn.functional.dropout(t_2, p=0.1, training=True, inplace=False)
        t_6 = torch.matmul(t_2, t_6)
        t_6 = t_6.transpose(1, 2)
        t_6 = t_6.contiguous()
        t_9 = t_6.view(t_9, -1, 4096)
        t_9 = self.l_38(t_9)
        t_6 = self.l_39(t_9)
        t_6 = t_12 + t_6
        t_3 = (t_9, None, t_3)
        t_9 = t_3[0]
        t_6 = (t_6,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_6 + t_3
        t_6 = t_3[slice(None, 2, None)]
        t_12 = t_6[0]
        t_2 = self.l_40(t_12)
        t_6 = t_6[1]
        t_3 = t_3[slice(2, None, None)]
        t_1 = self.l_41(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_1, t_4)
        t_4 += t_10
        t_1 = t_4.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_1.type_as(t_4)
        t_4 = torch.nn.functional.dropout(t_4, p=0.1, training=True, inplace=False)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_44(t_2)
        t_5 = self.l_45(t_2)
        t_5 = t_12 + t_5
        t_10 = (t_2, None, t_10)
        t_2 = t_10[0]
        t_5 = (t_5,)
        t_10 = t_10[slice(1, None, None)]
        t_10 = t_5 + t_10
        t_5 = t_10[0]
        t_12 = self.l_46(t_5)
        t_10 = t_10[slice(2, None, None)]
        t_10 = t_3 + t_10
        t_12 = self.l_47(t_12)
        t_12 = torch.nn.functional.relu(t_12, inplace=False)
        t_12 = self.l_48(t_12)
        t_12 = self.l_49(t_12)
        t_12 = self.l_50(t_12)
        t_12 = t_5 + t_12
        t_6 = (t_12, t_6)
        t_10 = t_6 + t_10
        t_6 = t_10[slice(None, 2, None)]
        t_6 = t_6[0]
        t_12 = t_10[2]
        t_10 = t_10[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5682
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5684
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5686
        return list(flatten((x0, t_6, t_12, t_10)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition15(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5LayerNorm[final_layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/Linear[lm_head]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:15'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.21.layer.0.layer_norm',
                        'l_1': 'decoder.block.21.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.21.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.21.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.21.layer.0.SelfAttention.o',
                        'l_5': 'decoder.block.21.layer.0.dropout',
                        'l_6': 'decoder.block.21.layer.1.layer_norm',
                        'l_7': 'decoder.block.21.layer.1.EncDecAttention.q',
                        'l_8': 'decoder.block.21.layer.1.EncDecAttention.v',
                        'l_9': 'decoder.block.21.layer.1.EncDecAttention.o',
                        'l_10': 'decoder.block.21.layer.1.dropout',
                        'l_11': 'decoder.block.21.layer.2.layer_norm',
                        'l_12': 'decoder.block.21.layer.2.DenseReluDense.wi',
                        'l_13': 'decoder.block.21.layer.2.DenseReluDense.dropout',
                        'l_14': 'decoder.block.21.layer.2.DenseReluDense.wo',
                        'l_15': 'decoder.block.21.layer.2.dropout',
                        'l_16': 'decoder.block.22.layer.0.layer_norm',
                        'l_17': 'decoder.block.22.layer.0.SelfAttention.q',
                        'l_18': 'decoder.block.22.layer.0.SelfAttention.k',
                        'l_19': 'decoder.block.22.layer.0.SelfAttention.v',
                        'l_20': 'decoder.block.22.layer.0.SelfAttention.o',
                        'l_21': 'decoder.block.22.layer.0.dropout',
                        'l_22': 'decoder.block.22.layer.1.layer_norm',
                        'l_23': 'decoder.block.22.layer.1.EncDecAttention.q',
                        'l_24': 'decoder.block.22.layer.1.EncDecAttention.k',
                        'l_25': 'decoder.block.22.layer.1.EncDecAttention.v',
                        'l_26': 'decoder.block.22.layer.1.EncDecAttention.o',
                        'l_27': 'decoder.block.22.layer.1.dropout',
                        'l_28': 'decoder.block.22.layer.2.layer_norm',
                        'l_29': 'decoder.block.22.layer.2.DenseReluDense.wi',
                        'l_30': 'decoder.block.22.layer.2.DenseReluDense.dropout',
                        'l_31': 'decoder.block.22.layer.2.DenseReluDense.wo',
                        'l_32': 'decoder.block.22.layer.2.dropout',
                        'l_33': 'decoder.block.23.layer.0.layer_norm',
                        'l_34': 'decoder.block.23.layer.0.SelfAttention.q',
                        'l_35': 'decoder.block.23.layer.0.SelfAttention.k',
                        'l_36': 'decoder.block.23.layer.0.SelfAttention.v',
                        'l_37': 'decoder.block.23.layer.0.SelfAttention.o',
                        'l_38': 'decoder.block.23.layer.0.dropout',
                        'l_39': 'decoder.block.23.layer.1.layer_norm',
                        'l_40': 'decoder.block.23.layer.1.EncDecAttention.q',
                        'l_41': 'decoder.block.23.layer.1.EncDecAttention.k',
                        'l_42': 'decoder.block.23.layer.1.EncDecAttention.v',
                        'l_43': 'decoder.block.23.layer.1.EncDecAttention.o',
                        'l_44': 'decoder.block.23.layer.1.dropout',
                        'l_45': 'decoder.block.23.layer.2.layer_norm',
                        'l_46': 'decoder.block.23.layer.2.DenseReluDense.wi',
                        'l_47': 'decoder.block.23.layer.2.DenseReluDense.dropout',
                        'l_48': 'decoder.block.23.layer.2.DenseReluDense.wo',
                        'l_49': 'decoder.block.23.layer.2.dropout',
                        'l_50': 'decoder.final_layer_norm',
                        'l_51': 'decoder.dropout',
                        'l_52': 'lm_head'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[decoder]/T5LayerNorm[final_layer_norm] <=> self.l_50
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout] <=> self.l_51
        # T5ForConditionalGeneration/Linear[lm_head] <=> self.l_52
        # input4 <=> labels
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5682 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5684 <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5686 <=> x4
        labels, x0, x1, x2, x3, x4 = unflatten(args, self.input_structure)
        t_0 = self.l_8(x0)
        t_1 = self.l_24(x0)
        t_2 = self.l_25(x0)
        t_3 = self.l_41(x0)
        t_4 = self.l_42(x0)
        t_5 = self.l_0(x2)
        t_6 = self.l_1(t_5)
        t_7 = self.l_2(t_5)
        t_8 = self.l_3(t_5)
        t_5 = t_5.shape
        t_5 = t_5[slice(None, 2, None)]
        t_5 = t_5[0]
        t_6 = t_6.view(t_5, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_7 = t_7.view(t_5, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_8 = t_8.view(t_5, -1, 32, 128)
        t_8 = t_8.transpose(1, 2)
        t_7 = t_7.transpose(3, 2)
        t_7 = torch.matmul(t_6, t_7)
        t_7 += x3
        t_6 = t_7.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_7 = t_6.type_as(t_7)
        t_7 = torch.nn.functional.dropout(t_7, p=0.1, training=True, inplace=False)
        t_8 = torch.matmul(t_7, t_8)
        t_8 = t_8.transpose(1, 2)
        t_8 = t_8.contiguous()
        t_5 = t_8.view(t_5, -1, 4096)
        t_5 = self.l_4(t_5)
        t_8 = self.l_5(t_5)
        t_8 = x2 + t_8
        t_5 = (t_5, None, x3)
        t_7 = t_5[0]
        t_8 = (t_8,)
        t_5 = t_5[slice(1, None, None)]
        t_5 = t_8 + t_5
        t_8 = t_5[slice(None, 2, None)]
        t_6 = t_8[0]
        t_9 = self.l_6(t_6)
        t_8 = t_8[1]
        t_5 = t_5[slice(2, None, None)]
        t_10 = self.l_7(t_9)
        t_9 = t_9.shape
        t_9 = t_9[slice(None, 2, None)]
        t_9 = t_9[0]
        t_10 = t_10.view(t_9, -1, 32, 128)
        t_10 = t_10.transpose(1, 2)
        t_11 = x1.view(t_9, -1, 32, 128)
        t_11 = t_11.transpose(1, 2)
        t_0 = t_0.view(t_9, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_11 = t_11.transpose(3, 2)
        t_11 = torch.matmul(t_10, t_11)
        t_11 += x4
        t_10 = t_11.float()
        t_10 = torch.nn.functional.softmax(t_10, dim=-1, _stacklevel=3, dtype=None)
        t_11 = t_10.type_as(t_11)
        t_11 = torch.nn.functional.dropout(t_11, p=0.1, training=True, inplace=False)
        t_0 = torch.matmul(t_11, t_0)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.contiguous()
        t_9 = t_0.view(t_9, -1, 4096)
        t_9 = self.l_9(t_9)
        t_0 = self.l_10(t_9)
        t_0 = t_6 + t_0
        t_9 = (t_9, None, x4)
        t_6 = t_9[0]
        t_0 = (t_0,)
        t_9 = t_9[slice(1, None, None)]
        t_9 = t_0 + t_9
        t_0 = t_9[0]
        t_11 = self.l_11(t_0)
        t_9 = t_9[slice(2, None, None)]
        t_9 = t_5 + t_9
        t_11 = self.l_12(t_11)
        t_11 = torch.nn.functional.relu(t_11, inplace=False)
        t_11 = self.l_13(t_11)
        t_11 = self.l_14(t_11)
        t_11 = self.l_15(t_11)
        t_11 = t_0 + t_11
        t_8 = (t_11, t_8)
        t_9 = t_8 + t_9
        t_8 = t_9[slice(None, 2, None)]
        t_8 = t_8[0]
        t_11 = self.l_16(t_8)
        t_0 = t_9[2]
        t_9 = t_9[3]
        t_5 = self.l_17(t_11)
        t_10 = self.l_18(t_11)
        t_12 = self.l_19(t_11)
        t_11 = t_11.shape
        t_11 = t_11[slice(None, 2, None)]
        t_11 = t_11[0]
        t_5 = t_5.view(t_11, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_10 = t_10.view(t_11, -1, 32, 128)
        t_10 = t_10.transpose(1, 2)
        t_12 = t_12.view(t_11, -1, 32, 128)
        t_12 = t_12.transpose(1, 2)
        t_10 = t_10.transpose(3, 2)
        t_10 = torch.matmul(t_5, t_10)
        t_10 += t_0
        t_5 = t_10.float()
        t_5 = torch.nn.functional.softmax(t_5, dim=-1, _stacklevel=3, dtype=None)
        t_10 = t_5.type_as(t_10)
        t_10 = torch.nn.functional.dropout(t_10, p=0.1, training=True, inplace=False)
        t_12 = torch.matmul(t_10, t_12)
        t_12 = t_12.transpose(1, 2)
        t_12 = t_12.contiguous()
        t_11 = t_12.view(t_11, -1, 4096)
        t_11 = self.l_20(t_11)
        t_12 = self.l_21(t_11)
        t_12 = t_8 + t_12
        t_0 = (t_11, None, t_0)
        t_11 = t_0[0]
        t_12 = (t_12,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_12 + t_0
        t_12 = t_0[slice(None, 2, None)]
        t_8 = t_12[0]
        t_10 = self.l_22(t_8)
        t_12 = t_12[1]
        t_0 = t_0[slice(2, None, None)]
        t_5 = self.l_23(t_10)
        t_10 = t_10.shape
        t_10 = t_10[slice(None, 2, None)]
        t_10 = t_10[0]
        t_5 = t_5.view(t_10, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_1 = t_1.view(t_10, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_10, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_5, t_1)
        t_1 += t_9
        t_5 = t_1.float()
        t_5 = torch.nn.functional.softmax(t_5, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_5.type_as(t_1)
        t_1 = torch.nn.functional.dropout(t_1, p=0.1, training=True, inplace=False)
        t_2 = torch.matmul(t_1, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_10 = t_2.view(t_10, -1, 4096)
        t_10 = self.l_26(t_10)
        t_2 = self.l_27(t_10)
        t_2 = t_8 + t_2
        t_9 = (t_10, None, t_9)
        t_10 = t_9[0]
        t_2 = (t_2,)
        t_9 = t_9[slice(1, None, None)]
        t_9 = t_2 + t_9
        t_2 = t_9[0]
        t_8 = self.l_28(t_2)
        t_9 = t_9[slice(2, None, None)]
        t_9 = t_0 + t_9
        t_8 = self.l_29(t_8)
        t_8 = torch.nn.functional.relu(t_8, inplace=False)
        t_8 = self.l_30(t_8)
        t_8 = self.l_31(t_8)
        t_8 = self.l_32(t_8)
        t_8 = t_2 + t_8
        t_12 = (t_8, t_12)
        t_9 = t_12 + t_9
        t_12 = t_9[slice(None, 2, None)]
        t_12 = t_12[0]
        t_8 = self.l_33(t_12)
        t_2 = t_9[2]
        t_9 = t_9[3]
        t_0 = self.l_34(t_8)
        t_1 = self.l_35(t_8)
        t_5 = self.l_36(t_8)
        t_8 = t_8.shape
        t_8 = t_8[slice(None, 2, None)]
        t_8 = t_8[0]
        t_0 = t_0.view(t_8, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_8, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_5 = t_5.view(t_8, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_0, t_1)
        t_1 += t_2
        t_0 = t_1.float()
        t_0 = torch.nn.functional.softmax(t_0, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_0.type_as(t_1)
        t_1 = torch.nn.functional.dropout(t_1, p=0.1, training=True, inplace=False)
        t_5 = torch.matmul(t_1, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_8 = t_5.view(t_8, -1, 4096)
        t_8 = self.l_37(t_8)
        t_5 = self.l_38(t_8)
        t_5 = t_12 + t_5
        t_2 = (t_8, None, t_2)
        t_8 = t_2[0]
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_12 = t_5[0]
        t_1 = self.l_39(t_12)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_0 = self.l_40(t_1)
        t_1 = t_1.shape
        t_1 = t_1[slice(None, 2, None)]
        t_1 = t_1[0]
        t_0 = t_0.view(t_1, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_3 = t_3.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_1, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_0, t_3)
        t_3 += t_9
        t_0 = t_3.float()
        t_0 = torch.nn.functional.softmax(t_0, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_0.type_as(t_3)
        t_3 = torch.nn.functional.dropout(t_3, p=0.1, training=True, inplace=False)
        t_4 = torch.matmul(t_3, t_4)
        t_4 = t_4.transpose(1, 2)
        t_4 = t_4.contiguous()
        t_1 = t_4.view(t_1, -1, 4096)
        t_1 = self.l_43(t_1)
        t_4 = self.l_44(t_1)
        t_4 = t_12 + t_4
        t_9 = (t_1, None, t_9)
        t_1 = t_9[0]
        t_4 = (t_4,)
        t_9 = t_9[slice(1, None, None)]
        t_9 = t_4 + t_9
        t_4 = t_9[0]
        t_12 = self.l_45(t_4)
        t_9 = t_9[slice(2, None, None)]
        t_9 = t_2 + t_9
        t_12 = self.l_46(t_12)
        t_12 = torch.nn.functional.relu(t_12, inplace=False)
        t_12 = self.l_47(t_12)
        t_12 = self.l_48(t_12)
        t_12 = self.l_49(t_12)
        t_12 = t_4 + t_12
        t_5 = (t_12, t_5)
        t_9 = t_5 + t_9
        t_5 = t_9[slice(None, 2, None)]
        t_5 = t_5[0]
        t_5 = self.l_50(t_5)
        t_12 = t_9[2]
        t_9 = t_9[3]
        t_5 = self.l_51(t_5)
        t_5 = t_5 * 0.03125
        t_5 = self.l_52(t_5)
        t_4 = t_5.size(-1)
        t_4 = t_5.view(-1, t_4)
        t_5 = labels.view(-1)
        t_5 = torch.nn.functional.cross_entropy(t_4, t_5, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')
        # Returning:
        # T5ForConditionalGeneration/torch.nn.functional::cross_entropy_6186
        return (t_5,)

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


def traverse_model(module: nn.Module, depth: int, prefix: Optional[str] = None,
                   basic_blocks: Tuple[Type[nn.Module]] = (), full: bool = False) -> Iterator[
    Tuple[nn.Module, str, nn.Module, Optional[bool]]]:
    """
    iterate over model layers yielding the layer,layer_scope,encasing_module
    Parameters:
    -----------
    model:
        the model to iterate over
    depth:
        how far down in the model tree to go
    basic_blocks:
        a list of modules that if encountered will not be broken down
    full:
        whether to yield only layers specified by the depth and basic_block options or to yield all layers
    """
    if prefix is None:
        prefix = type(module).__name__

    for name, sub_module in module.named_children():
        scope = prefix + "/" + type(sub_module).__name__ + f"[{name}]"
        if len(list(sub_module.children())) == 0 or isinstance(sub_module, tuple(basic_blocks)) or depth == 0:
            if full:
                # TODO:
                # is_explicit_block_limit = len(list(sub_module.children())) != 0 and (isinstance(sub_module, tuple(basic_blocks)) or depth == 0)
                yield sub_module, scope, module, True

            else:
                yield sub_module, scope, module
        else:
            if full:
                yield sub_module, scope, module, False
            yield from traverse_model(sub_module, depth - 1, scope, basic_blocks, full)


def layerDict(model: nn.Module, depth=1000, basic_blocks=()) -> Dict[str, nn.Module]:
    return {s: l for l, s, _ in traverse_model(model, depth, basic_blocks=basic_blocks)}


def traverse_params_buffs(module: nn.Module, prefix: Optional[str] = None) -> Iterator[Tuple[torch.tensor, str]]:
    """
    iterate over model's buffers and parameters yielding obj,obj_scope

    Parameters:
    -----------
    model:
        the model to iterate over
    """
    if prefix is None:
        prefix = type(module).__name__

    # params
    for param_name, param in module.named_parameters(recurse=False):
        param_scope = f"{prefix}/{type(param).__name__}[{param_name}]"
        yield param, param_scope

    # buffs
    for buffer_name, buffer in module.named_buffers(recurse=False):
        buffer_scope = f"{prefix}/{type(buffer).__name__}[{buffer_name}]"
        yield buffer, buffer_scope

    # recurse
    for name, sub_module in module.named_children():
        yield from traverse_params_buffs(sub_module, prefix + "/" + type(sub_module).__name__ + f"[{name}]")


def tensorDict(model: nn.Module) -> OrderedDict[str, Tensor]:
    return collections.OrderedDict((s, t) for t, s in traverse_params_buffs(model))


def move_tensors(ts, device):
    def move(t):
        if isinstance(t, (nn.Module, Tensor)):
            return t.to(device)
        return t

    return nested_map(move, ts)


def nested_map(func, ts, full=False):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        return func(ts)
    elif isinstance(ts, (list, tuple, set)):
        return type(ts)(nested_map(func, t, full=full) for t in ts)
    elif isinstance(ts, dict):
        return {k: nested_map(func, v, full=full) for k, v in ts.items()}
    elif isinstance(ts, slice) and full:
        start = nested_map(func, ts.start, full=full)
        stop = nested_map(func, ts.stop, full=full)
        step = nested_map(func, ts.step, full=full)
        return slice(start, stop, step)
    return func(ts)


def flatten(ts):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        yield ts
    elif isinstance(ts, (list, tuple, set)):
        yield from chain(*[flatten(t) for t in ts])
    elif isinstance(ts, dict):
        yield from chain(*[flatten(t) for k, t in sorted(ts.items(), key=lambda t: t[0])])
    else:
        yield ts


def unflatten(xs, structure):
    return _unflatten(xs, structure)[0]


def _unflatten(xs, structure):
    if isinstance(structure, torch.Size):
        # torch.Size is subclass of tuple which is stupid
        return xs[0], 1

    if not isinstance(structure, (list, tuple, set, dict)):
        return xs[0], 1

    if isinstance(structure, (list, tuple, set)):
        offset = 0
        elements = []
        for s in structure:
            e, n = _unflatten(xs[offset:], s)
            elements.append(e)
            offset += n

        return type(structure)(elements), offset

    assert isinstance(structure, dict)
    offset = 0
    elements = dict()
    for k, v in sorted(structure.items(), key=lambda t: t[0]):
        e, n = _unflatten(xs[offset:], v)
        elements[k] = e
        offset += n

    return elements, offset


def state_dict(partition, *args, **kwargs):
    # we return the state dict of this part as it should be in the original model
    state = nn.Module.state_dict(partition, *args, **kwargs)
    lookup = partition.lookup
    result = dict()
    for k, v in state.items():
        if k in lookup:
            result[lookup[k]] = v
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            result[new_k] = v
    return result


def load_state_dict(partition, state_dict, strict=True):
    reverse_lookup = {v: k for k, v in partition.lookup.items()}
    device = partition.device
    keys = list(partition.state_dict(None).keys())
    new_state = dict()
    for k in keys:
        if k in reverse_lookup:
            new_state[reverse_lookup[k]] = state_dict[k].to(device)
            continue
        idx = k.rfind(".")
        to_replace = k[:idx]
        if to_replace in reverse_lookup:
            key = reverse_lookup[to_replace] + k[idx:]
            new_state[key] = state_dict[k].to(device)
    nn.Module.load_state_dict(partition, new_state, strict=strict)


def named_buffers(partition, prefix='', recurse=True):
    # we return the named buffers of this part as it should be in the original model
    params = nn.Module.named_buffers(partition, prefix=prefix, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield lookup[k], v
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield new_k, v


def named_parameters(partition, prefix='', recurse=True):
    # we return the named parameters of this part as it should be in the original model
    params = nn.Module.named_parameters(partition, prefix=prefix, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield lookup[k], v
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield new_k, v


def cpu(partition):
    partition.device = torch.device('cpu')
    return nn.Module.cpu(partition)


def cuda(partition, device=None):
    if device is None:
        device = torch.cuda.current_device()
    partition.device = torch.device(device)
    return nn.Module.cuda(partition, partition.device)


def to(partition, *args, **kwargs):
    device = None
    if 'device' in kwargs:
        device = kwargs['device']
    elif 'tensor' in kwargs:
        device = kwargs['tensor'].device
    if args:
        if isinstance(args[0], (torch.device, int, str)):
            device = args[0]
        if torch.is_tensor(args[0]):
            device = args[0].device
    if not (device is None):
        partition.device = torch.device(device)
    return nn.Module.to(partition, *args, **kwargs)

model_args = {'model_name_or_path': 't5-3b', 'max_seq_length': 64, 'answer_max_seq_length': 4, 'stateless_tied': True, 'lmhead': True, 'precompute_masks': False}

def op_t5_3b_tied_lmheads_64_4_8p_bw12_async_squad1_mpipe():
    return dict(model_type='t5_stateless',
                model_name_or_path='t5-3b',
                do_lower_case=False,
                output_past=False,
                stateless_tied=True,
                explicitly_set_dict={'return_dict': False, 'use_cache': False, 'output_only': True, 'output_attentions': False, 'precompute_masks': False, 'output_hidden_states': False},
                do_resize_token_embedding=True,
                )
    
"""analysis summary
-I- Printing Report
warnings:
Partition0 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___335 is not contiguous!
Partition1 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___602 is not contiguous!
Partition2 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___869 is not contiguous!
Partition3 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1136 is not contiguous!
Partition4 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1403 is not contiguous!
Partition5 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1670 is not contiguous!
Partition6 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1937 is not contiguous!
Partition8 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2786 is not contiguous!
Partition9 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3269 is not contiguous!
Partition10 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3752 is not contiguous!
Partition11 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4235 is not contiguous!
Partition12 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4718 is not contiguous!
Partition13 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5201 is not contiguous!
Partition14 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5684 is not contiguous!
Number of nodes in Computation Graph: 6187
Number of stages: 8
n_partitions:16, num_dummy_stages:8
unique_stages_on_same_gpu: [{0, 11}, {1, 13}, {2, 14}, {8, 3}, {9, 4}, {10, 5}, {12, 6}, {15, 7}]
"stage_to_device_map": [0, 1, 2, 3, 4, 5, 6, 7, 3, 4, 5, 0, 6, 1, 2, 7],
backward times include recomputation
Analysis for async_pipeline=True: last partition will not do recomputation.

Stage parameter count:
 {0: 183872512,
 1: 151001088,
 2: 151002112,
 3: 151000064,
 4: 151001088,
 5: 151001088,
 6: 151001088,
 7: 151002112,
 8: 205531136,
 9: 201335808,
 10: 201335808,
 11: 201336832,
 12: 201334784,
 13: 201335808,
 14: 201335808,
 15: 230012928,
 'total': 2884440064}

GPU parameter count:
 Number of Model Parameters 2884.4M
{0: 385209344,
 1: 352336896,
 2: 352337920,
 3: 356531200,
 4: 352336896,
 5: 352336896,
 6: 352335872,
 7: 381015040,
 'total': 2884440064}

real times are based on real measurements of execution time (with communication) of generated partitions ms
forward {0: 75.96, 1: 75.02, 2: 75.51, 3: 78.52, 4: 74.95, 5: 75.01, 6: 75.11, 7: 71.58}
backward {0: 210.51, 1: 210.47, 2: 210.95, 3: 214.6, 4: 210.09, 5: 210.66, 6: 210.73, 7: 200.73}

Analysis for T = (1-R)fwd + R*bwd:

Pipeline Slowdown: (compared to sequential execution with no communication, and same recompute policy)
forward 1.089
backward 1.038

Expected utilization by partition
forward {0: 0.93, 1: 0.92, 2: 0.91, 3: 0.93, 4: 0.91, 5: 0.92, 6: 0.92, 7: 0.9}
backward {0: 0.98, 1: 0.97, 2: 0.97, 3: 0.98, 4: 0.97, 5: 0.97, 6: 0.97, 7: 0.91}

worstcase: bwd: 214.603 fwd: 78.524
Expected speedup for 8 partitions is: 7.607
Assuming bandwidth of 12 GBps between GPUs

communication volumes size of activations of each partition
0: input size:'10.03 MB', recieve_time:'0.84 ms', out:'36.24 MB', send time:'3.02 ms'
1: input size:'35.19 MB', recieve_time:'2.93 ms', out:'35.19 MB', send time:'2.93 ms'
2: input size:'35.19 MB', recieve_time:'2.93 ms', out:'43.58 MB', send time:'3.63 ms'
3: input size:'42.47 MB', recieve_time:'3.54 ms', out:'68.75 MB', send time:'5.73 ms'
4: input size:'35.19 MB', recieve_time:'2.93 ms', out:'35.19 MB', send time:'2.93 ms'
5: input size:'35.19 MB', recieve_time:'2.93 ms', out:'35.19 MB', send time:'2.93 ms'
6: input size:'35.72 MB', recieve_time:'2.98 ms', out:'35.19 MB', send time:'2.93 ms'
7: input size:'68.75 MB', recieve_time:'5.73 ms', out:'8.39 MB', send time:'0.70 ms'

Compuatation Communication ratio (comp/(comp+comm)):
forward {0: 0.96, 1: 0.96, 2: 0.95, 3: 0.93, 4: 0.96, 5: 0.96, 6: 0.96, 7: 0.99} 
backward {0: 1.0, 1: 0.99, 2: 0.99, 3: 0.98, 4: 0.99, 5: 0.99, 6: 0.99, 7: 0.97}

Analysis for T = fwd + bwd:
 {'expected_compute_utilization': {0: 1.0,
                                  1: 0.99,
                                  2: 0.99,
                                  3: 1.0,
                                  4: 0.98,
                                  5: 0.99,
                                  6: 0.99,
                                  7: 0.94},
 'pipeline_no_comm': {0: 282.61,
                      1: 279.62,
                      2: 279.9,
                      3: 283.86,
                      4: 279.17,
                      5: 279.81,
                      6: 279.93,
                      7: 265.88,
                      'worstcase': 283.86},
 'pipeline_vs_seq_no_comm': 5.97,
 'pipeline_with_non_parallel_comm': {0: 286.47,
                                     1: 285.49,
                                     2: 286.46,
                                     3: 293.13,
                                     4: 285.04,
                                     5: 285.68,
                                     6: 285.84,
                                     7: 272.31,
                                     'worstcase': 293.13},
 'seq_no_comm_no_recomp': {0: 169.28,
                           1: 166.31,
                           2: 167.34,
                           3: 165.89,
                           4: 166.18,
                           5: 166.46,
                           6: 166.31,
                           7: 166.85,
                           8: 49.56,
                           9: 44.59,
                           10: 44.86,
                           11: 44.6,
                           12: 44.99,
                           13: 45.16,
                           14: 44.55,
                           15: 42.17}}

expected_speedup_compared_to_seq_no_recomp_no_comm: 5.783
Analysis max cuda memory used 2.76GB
"""
