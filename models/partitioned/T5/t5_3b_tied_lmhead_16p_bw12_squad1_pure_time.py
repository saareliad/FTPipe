"""AutoGenerated with:
python partitioning_script.py --partitioning_task t5 --auto_infer_node_bwd_to_fwd_ratio --bwd_to_fwd_ratio 1 --t5_task squad1 --lmhead --n_iter 10 --n_partitions 16 --analysis_batch_size 1 --partitioning_batch_size 1 --multilevel --precompute_masks --stateless_tied --model_name_or_path t5-3b --save_memory_mode --constraint time --objective stage_time --basic_blocks
"""
import torch
import torch.functional
import math
import torch.nn.functional
from torch import Tensor
import torch.nn as nn
from itertools import chain
from typing import Optional, Tuple, Iterator, Iterable, OrderedDict, Dict
import collections

from models.normal.NLP_models.modeling_t5 import T5LayerNorm
from models.normal.NLP_models.stateless import StatelessEmbedding
from torch.nn.modules.dropout import Dropout
from torch.nn.modules.loss import CrossEntropyLoss
from torch.nn.modules.linear import Linear
from torch.nn.modules.sparse import Embedding
# this is an auto generated file do not edit unless you know what you are doing


# partition adjacency
# model inputs {0, 11, 15}
# partition 0 {'inputs': {'input_ids', 'decoder_attention_mask', 'attention_mask', 'decoder_input_ids'}, 'outputs': {1, 2, 3, 4, 5, 6, 7, 8, 9}}
# partition 1 {'inputs': {0}, 'outputs': {2, 4, 8, 10, 11}}
# partition 2 {'inputs': {0, 1}, 'outputs': {3}}
# partition 3 {'inputs': {0, 2}, 'outputs': {4}}
# partition 4 {'inputs': {0, 1, 3}, 'outputs': {5, 6}}
# partition 5 {'inputs': {0, 4}, 'outputs': {10, 6}}
# partition 6 {'inputs': {0, 4, 5}, 'outputs': {10, 11, 7}}
# partition 7 {'inputs': {0, 6}, 'outputs': {8}}
# partition 8 {'inputs': {0, 1, 7}, 'outputs': {9, 11}}
# partition 9 {'inputs': {0, 8}, 'outputs': {10}}
# partition 10 {'inputs': {9, 5, 1, 6}, 'outputs': {11, 12, 13, 14, 15}}
# partition 11 {'inputs': {1, 6, 'inverted_encoder_attention_mask', 8, 10}, 'outputs': {12, 13, 14, 15}}
# partition 12 {'inputs': {10, 11}, 'outputs': {13}}
# partition 13 {'inputs': {10, 11, 12}, 'outputs': {14}}
# partition 14 {'inputs': {10, 11, 13}, 'outputs': {15}}
# partition 15 {'inputs': {10, 11, 'lm_labels', 14}, 'outputs': {'output'}}
# model outputs {15}


def create_pipeline_configuration(DEBUG=False, batch_size=1):
    config = {
        'batch_dim': 0,
        'depth': 10000,
        'basic_blocks': (T5LayerNorm,StatelessEmbedding,Dropout,CrossEntropyLoss,Linear,Embedding),
        'model_inputs': {
            'attention_mask': {
                'shape': torch.Size([1, 1, 1, 384]),
                'dtype': torch.float32,
                'is_batched': True,
                'used_by': [0]},
            'decoder_attention_mask': {
                'shape': torch.Size([1, 1, 32, 32]),
                'dtype': torch.float32,
                'is_batched': True,
                'used_by': [0]},
            'decoder_input_ids': {
                'shape': torch.Size([1, 32]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0]},
            'input_ids': {
                'shape': torch.Size([1, 384]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0]},
            'inverted_encoder_attention_mask': {
                'shape': torch.Size([1, 1, 1, 384]),
                'dtype': torch.float32,
                'is_batched': True,
                'used_by': [11]},
            'lm_labels': {
                'shape': torch.Size([1, 32]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [15]}},
        'model_outputs': {
            'T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]': {
                'shape': torch.Size([1]),
                'dtype': torch.float32,
                'is_batched': False,
                'created_by': 15}},
        'stages': {
            0: {
                'stage_cls': Partition0,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 384]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'decoder_input_ids': {
                        'shape': torch.Size([1, 32]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'input_ids': {
                        'shape': torch.Size([1, 384]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149': {
                        'shape': torch.Size([1, 32, 384, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1, 2, 3, 4, 5, 6, 7, 8, 9]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/Tensor::__add___200': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::__add___1603': {
                        'shape': torch.Size([1, 32, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::view_1619': {
                        'shape': torch.Size([1, 32, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]}},
                'devices': ['cpu' if DEBUG else 'cuda:0']},
            1: {
                'stage_cls': Partition1,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149': {
                        'shape': torch.Size([1, 32, 384, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/Tensor::__add___200': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::__add___1603': {
                        'shape': torch.Size([1, 32, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::view_1619': {
                        'shape': torch.Size([1, 32, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/Tensor::__add___326': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [2]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___330': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [2]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::transpose_354': {
                        'shape': torch.Size([1, 32, 384, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [2]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]': {
                        'shape': torch.Size([1, 32, 384, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [2]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/prim::TupleConstruct_1628_0': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [8]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/prim::TupleConstruct_1628_1': {
                        'shape': torch.Size([1, 32, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [8]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___1630': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [10, 4]}},
                'devices': ['cpu' if DEBUG else 'cuda:1']},
            2: {
                'stage_cls': Partition2,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149': {
                        'shape': torch.Size([1, 32, 384, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/Tensor::__add___326': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___330': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::transpose_354': {
                        'shape': torch.Size([1, 32, 384, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]': {
                        'shape': torch.Size([1, 32, 384, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/Tensor::__add___495': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [3]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]': {
                        'shape': torch.Size([1, 384, 16384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [3]}},
                'devices': ['cpu' if DEBUG else 'cuda:2']},
            3: {
                'stage_cls': Partition3,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149': {
                        'shape': torch.Size([1, 32, 384, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/Tensor::__add___495': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 2},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]': {
                        'shape': torch.Size([1, 384, 16384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 2}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/Tensor::__add___621': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [4]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___625': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [4]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [4]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [4]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [4]}},
                'devices': ['cpu' if DEBUG else 'cuda:3']},
            4: {
                'stage_cls': Partition4,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149': {
                        'shape': torch.Size([1, 32, 384, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/Tensor::__add___621': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 3},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___625': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 3},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 3},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 3},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 3},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/Tensor::__add___790': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [5]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 384, 16384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [5]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::size_1634': {
                        'shape': None,
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [6]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/torch::arange_1670': {
                        'shape': torch.Size([32]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [5]}},
                'devices': ['cpu' if DEBUG else 'cuda:4']},
            5: {
                'stage_cls': Partition5,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149': {
                        'shape': torch.Size([1, 32, 384, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/Tensor::__add___790': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 4},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 384, 16384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 4},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/torch::arange_1670': {
                        'shape': torch.Size([32]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 4}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/Tensor::__add___916': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [6]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [6]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___920': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [6]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::transpose_928': {
                        'shape': torch.Size([1, 32, 384, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [6]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::transpose_944': {
                        'shape': torch.Size([1, 32, 384, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [6]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::__getitem___1677': {
                        'shape': torch.Size([32, 1]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [10]}},
                'devices': ['cpu' if DEBUG else 'cuda:5']},
            6: {
                'stage_cls': Partition6,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149': {
                        'shape': torch.Size([1, 32, 384, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/Tensor::__add___916': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 5},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 5},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___920': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 5},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::transpose_928': {
                        'shape': torch.Size([1, 32, 384, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 5},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::transpose_944': {
                        'shape': torch.Size([1, 32, 384, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 5},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::size_1634': {
                        'shape': None,
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 4}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/Tensor::__add___1085': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [7]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 384, 16384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [7]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Size::__getitem___1636': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [10, 11]}},
                'devices': ['cpu' if DEBUG else 'cuda:6']},
            7: {
                'stage_cls': Partition7,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149': {
                        'shape': torch.Size([1, 32, 384, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/Tensor::__add___1085': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 6},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 384, 16384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 6}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/Tensor::__add___1211': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [8]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [8]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::size_1213': {
                        'shape': None,
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [8]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [8]}},
                'devices': ['cpu' if DEBUG else 'cuda:7']},
            8: {
                'stage_cls': Partition8,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149': {
                        'shape': torch.Size([1, 32, 384, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/Tensor::__add___1211': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 7},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 7},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::size_1213': {
                        'shape': None,
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 7},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 7},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/prim::TupleConstruct_1628_0': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/prim::TupleConstruct_1628_1': {
                        'shape': torch.Size([1, 32, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/Tensor::__add___1380': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [9]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [9]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___1632': {
                        'shape': torch.Size([1, 32, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]}},
                'devices': ['cpu' if DEBUG else 'cuda:8']},
            9: {
                'stage_cls': Partition9,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149': {
                        'shape': torch.Size([1, 32, 384, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/Tensor::__add___1380': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 8},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 8}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/Tensor::__add___1498': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [10]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [10]}},
                'devices': ['cpu' if DEBUG else 'cuda:9']},
            10: {
                'stage_cls': Partition10,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/Tensor::__add___1498': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 9},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 9},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Size::__getitem___1636': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 6},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::__getitem___1677': {
                        'shape': torch.Size([32, 1]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 5}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::transpose_1664': {
                        'shape': torch.Size([1, 32, 384, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/torch::matmul_1668': {
                        'shape': torch.Size([1, 32, 32, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/torch::max_1690': {
                        'shape': torch.Size([32, 384]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::__lt___1692': {
                        'shape': torch.Size([32, 384]),
                        'dtype': torch.bool,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/int::__add___1705': {
                        'shape': torch.Size([32, 384]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]}},
                'devices': ['cpu' if DEBUG else 'cuda:10']},
            11: {
                'stage_cls': Partition11,
                'inputs': {
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 384]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]': {
                        'shape': torch.Size([1, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___1630': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___1632': {
                        'shape': torch.Size([1, 32, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 8},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Size::__getitem___1636': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 6},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::transpose_1664': {
                        'shape': torch.Size([1, 32, 384, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/torch::matmul_1668': {
                        'shape': torch.Size([1, 32, 32, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/torch::max_1690': {
                        'shape': torch.Size([32, 384]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::__lt___1692': {
                        'shape': torch.Size([32, 384]),
                        'dtype': torch.bool,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/int::__add___1705': {
                        'shape': torch.Size([32, 384]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1764': {
                        'shape': torch.Size([1, 32, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12, 13, 14, 15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1766': {
                        'shape': torch.Size([1, 32, 32, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12, 13, 14, 15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/Tensor::__add___2147': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]}},
                'devices': ['cpu' if DEBUG else 'cuda:11']},
            12: {
                'stage_cls': Partition12,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1764': {
                        'shape': torch.Size([1, 32, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1766': {
                        'shape': torch.Size([1, 32, 32, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/Tensor::__add___2147': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/Tensor::__add___2646': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___2650': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]': {
                        'shape': torch.Size([1, 32, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]}},
                'devices': ['cpu' if DEBUG else 'cuda:12']},
            13: {
                'stage_cls': Partition13,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1764': {
                        'shape': torch.Size([1, 32, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1766': {
                        'shape': torch.Size([1, 32, 32, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/Tensor::__add___2646': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 12},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 12},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___2650': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 12},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]': {
                        'shape': torch.Size([1, 32, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 12},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/Tensor::__add___3188': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 32, 16384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]}},
                'devices': ['cpu' if DEBUG else 'cuda:13']},
            14: {
                'stage_cls': Partition14,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1764': {
                        'shape': torch.Size([1, 32, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1766': {
                        'shape': torch.Size([1, 32, 32, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/Tensor::__add___3188': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 13},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 32, 16384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 13},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/Tensor::__add___3746': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]': {
                        'shape': torch.Size([1, 32, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]}},
                'devices': ['cpu' if DEBUG else 'cuda:14']},
            15: {
                'stage_cls': Partition15,
                'inputs': {
                    'lm_labels': {
                        'shape': torch.Size([1, 32]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1764': {
                        'shape': torch.Size([1, 32, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1766': {
                        'shape': torch.Size([1, 32, 32, 384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/Tensor::__add___3746': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 32, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]': {
                        'shape': torch.Size([1, 32, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]': {
                        'shape': torch.Size([1, 384, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11}},
                'outputs': {
                    'T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]': {
                        'shape': torch.Size([1]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [-1]}},
                'devices': ['cpu' if DEBUG else 'cuda:15']}}}
    
    
    # switching batch size
    batch_dim = config['batch_dim']
    for d in chain(config['model_inputs'].values(),config['model_outputs'].values()):
        if d['is_batched']:
            shape = d['shape']
            d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    for s in config['stages'].values():
        for d in chain(s['inputs'].values(),s['outputs'].values()):
            if d['is_batched']:
                shape = d['shape']
                d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    return config

class Partition0(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/StatelessEmbedding[embed_tokens]',
            'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Embedding[relative_attention_bias]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/StatelessEmbedding[embed_tokens]',
            'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Embedding[relative_attention_bias]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
        ]
    TENSORS=[
            'T5ForConditionalGeneration/Parameter[shared_embed_weight]',
        ]
    def __init__(self, layers, tensors, device='cuda:0'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.embed_tokens',
                        'l_1': 'encoder.dropout',
                        'l_2': 'encoder.0.0.layer_norm',
                        'l_3': 'encoder.0.0.SelfAttention.q',
                        'l_4': 'encoder.0.0.SelfAttention.k',
                        'l_5': 'encoder.0.0.SelfAttention.v',
                        'l_6': 'encoder.0.0.SelfAttention.relative_attention_bias',
                        'l_7': 'encoder.0.0.SelfAttention.dropout',
                        'l_8': 'encoder.0.0.SelfAttention.o',
                        'l_9': 'encoder.0.0.dropout',
                        'l_10': 'encoder.0.1.layer_norm',
                        'l_11': 'encoder.0.1.DenseReluDense.wi',
                        'l_12': 'encoder.0.1.DenseReluDense.dropout',
                        'l_13': 'encoder.0.1.DenseReluDense.wo',
                        'l_14': 'encoder.0.1.dropout',
                        'l_15': 'encoder.1.0.layer_norm',
                        'l_16': 'encoder.1.0.SelfAttention.q',
                        'l_17': 'encoder.1.0.SelfAttention.k',
                        'l_18': 'encoder.1.0.SelfAttention.v',
                        'l_19': 'encoder.1.0.SelfAttention.dropout',
                        'l_20': 'encoder.1.0.SelfAttention.o',
                        'l_21': 'encoder.1.0.dropout',
                        'l_22': 'encoder.1.1.layer_norm',
                        'l_23': 'encoder.1.1.DenseReluDense.wi',
                        'l_24': 'encoder.1.1.DenseReluDense.dropout',
                        'l_25': 'encoder.1.1.DenseReluDense.wo',
                        'l_26': 'decoder.embed_tokens',
                        'l_27': 'decoder.dropout',
                        'l_28': 'decoder.0.0.layer_norm',
                        'l_29': 'decoder.0.0.SelfAttention.q',
                        'l_30': 'decoder.0.0.SelfAttention.k',
                        'l_31': 'decoder.0.0.SelfAttention.v',
                        'l_32': 'decoder.0.0.SelfAttention.relative_attention_bias',
                        'l_33': 'decoder.0.0.SelfAttention.dropout',
                        'p_0': 'shared_embed_weight'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/StatelessEmbedding[embed_tokens] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Embedding[relative_attention_bias] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/StatelessEmbedding[embed_tokens] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Embedding[relative_attention_bias] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_33
        # T5ForConditionalGeneration/Parameter[shared_embed_weight] <=> self.p_0
        # input0 <=> attention_mask
        # input1 <=> decoder_attention_mask
        # input2 <=> decoder_input_ids
        # input3 <=> input_ids

        # moving inputs to current device no op if already on the correct device
        attention_mask, decoder_attention_mask, decoder_input_ids, input_ids = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = input_ids.size()
        t_0 = t_0[-1]
        t_0 = input_ids.view(-1, t_0)
        t_0 = self.l_0(self.p_0, t_0)
        t_0 = self.l_1(t_0)
        t_1 = self.l_2(t_0)
        t_2 = t_1.size()
        t_3 = t_2[0]
        t_2 = t_2[1]
        t_4 = self.l_3(t_1)
        t_4 = t_4.view(t_3, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = self.l_4(t_1)
        t_5 = t_5.view(t_3, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_1 = self.l_5(t_1)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_5 = t_5.transpose(3, 2)
        t_5 = torch.matmul(t_4, t_5)
        t_4 = torch.arange(t_2, dtype=torch.int64, device = self.device)
        t_6 = slice(None, None, None)
        t_6 = (t_6, None)
        t_6 = t_4[t_6]
        t_2 = torch.arange(t_2, dtype=torch.int64, device = self.device)
        t_4 = slice(None, None, None)
        t_4 = (None, t_4)
        t_4 = t_2[t_4]
        t_6 = t_4 - t_6
        t_6 = -t_6
        t_4 = t_6 < 0
        t_4 = t_4.to(torch.int64)
        t_4 = t_4 * 16
        t_4 = 0 + t_4
        t_6 = torch.abs(t_6)
        t_2 = t_6 < 8
        t_7 = t_6.float()
        t_7 = t_7 / 8
        t_7 = torch.log(t_7)
        t_8 = math.log(16.0)
        t_8 = t_7 / t_8
        t_8 = t_8 * 8
        t_8 = t_8.to(torch.int64)
        t_8 = 8 + t_8
        t_7 = torch.full_like(t_8, 15)
        t_7 = torch.min(t_8, t_7)
        t_7 = torch.where(t_2, t_6, t_7)
        t_4 += t_7
        t_7 = t_4
        t_7 = t_7.to(self.device)
        t_7 = self.l_6(t_7)
        t_4 = [2, 0, 1]
        t_4 = t_7.permute(t_4)
        t_4 = t_4.unsqueeze(0)
        t_4 = t_4 + attention_mask
        t_5 += t_4
        t_7 = t_5.float()
        t_7 = torch.nn.functional.softmax(t_7, dim=-1, _stacklevel=3, dtype=None)
        t_5 = t_7.type_as(t_5)
        t_5 = self.l_7(t_5)
        t_1 = torch.matmul(t_5, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_8(t_3)
        t_4 = (t_3, t_4)
        t_3 = t_4[0]
        t_3 = self.l_9(t_3)
        t_3 = t_0 + t_3
        t_4 = t_4[1]
        t_4 = (t_3, t_4)
        t_3 = t_4[0]
        t_4 = t_4[1]
        t_0 = self.l_10(t_3)
        t_0 = self.l_11(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_0 = self.l_14(t_0)
        t_0 = t_3 + t_0
        t_4 = (t_0, t_4)
        t_0 = t_4[0]
        t_4 = t_4[1]
        t_3 = self.l_15(t_0)
        t_1 = t_3.size()
        t_1 = t_1[0]
        t_5 = self.l_16(t_3)
        t_5 = t_5.view(t_1, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_7 = self.l_17(t_3)
        t_7 = t_7.view(t_1, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_3 = self.l_18(t_3)
        t_3 = t_3.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_7 = t_7.transpose(3, 2)
        t_7 = torch.matmul(t_5, t_7)
        t_7 += t_4
        t_5 = t_7.float()
        t_5 = torch.nn.functional.softmax(t_5, dim=-1, _stacklevel=3, dtype=None)
        t_7 = t_5.type_as(t_7)
        t_7 = self.l_19(t_7)
        t_3 = torch.matmul(t_7, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_1 = t_3.view(t_1, -1, 4096)
        t_1 = self.l_20(t_1)
        t_1 = self.l_21(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_22(t_1)
        t_0 = self.l_23(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_24(t_0)
        t_0 = self.l_25(t_0)
        t_3 = decoder_input_ids.size()
        t_3 = t_3[-1]
        t_3 = decoder_input_ids.view(-1, t_3)
        t_3 = self.l_26(self.p_0, t_3)
        t_3 = self.l_27(t_3)
        t_7 = self.l_28(t_3)
        t_5 = t_7.size()
        t_6 = t_5[0]
        t_5 = t_5[1]
        t_2 = self.l_29(t_7)
        t_2 = t_2.view(t_6, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_8 = self.l_30(t_7)
        t_8 = t_8.view(t_6, -1, 32, 128)
        t_8 = t_8.transpose(1, 2)
        t_7 = self.l_31(t_7)
        t_7 = t_7.view(t_6, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_8 = t_8.transpose(3, 2)
        t_8 = torch.matmul(t_2, t_8)
        t_2 = torch.arange(t_5, dtype=torch.int64, device = self.device)
        t_9 = slice(None, None, None)
        t_9 = (t_9, None)
        t_9 = t_2[t_9]
        t_5 = torch.arange(t_5, dtype=torch.int64, device = self.device)
        t_2 = slice(None, None, None)
        t_2 = (None, t_2)
        t_2 = t_5[t_2]
        t_9 = t_2 - t_9
        t_9 = -t_9
        t_2 = torch.zeros_like(t_9)
        t_2 = torch.max(t_9, t_2)
        t_9 = t_2 < 16
        t_5 = t_2.float()
        t_5 = t_5 / 16
        t_5 = torch.log(t_5)
        t_10 = math.log(8.0)
        t_10 = t_5 / t_10
        t_10 = t_10 * 16
        t_10 = t_10.to(torch.int64)
        t_10 = 16 + t_10
        t_5 = torch.full_like(t_10, 31)
        t_5 = torch.min(t_10, t_5)
        t_5 = torch.where(t_9, t_2, t_5)
        t_5 = 0 + t_5
        t_5 = t_5.to(self.device)
        t_5 = self.l_32(t_5)
        t_2 = [2, 0, 1]
        t_2 = t_5.permute(t_2)
        t_2 = t_2.unsqueeze(0)
        t_2 = t_2 + decoder_attention_mask
        t_8 += t_2
        t_5 = t_8.float()
        t_5 = torch.nn.functional.softmax(t_5, dim=-1, _stacklevel=3, dtype=None)
        t_8 = t_5.type_as(t_8)
        t_8 = self.l_33(t_8)
        t_7 = torch.matmul(t_8, t_7)
        t_7 = t_7.transpose(1, 2)
        t_7 = t_7.contiguous()
        t_6 = t_7.view(t_6, -1, 4096)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/Tensor::__add___200
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::__add___1603
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::view_1619
        return list(flatten((t_4, t_1, t_0, t_3, t_2, t_6)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition1(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:1'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.1.1.dropout',
                        'l_1': 'encoder.2.0.layer_norm',
                        'l_2': 'encoder.2.0.SelfAttention.q',
                        'l_3': 'encoder.2.0.SelfAttention.k',
                        'l_4': 'encoder.2.0.SelfAttention.v',
                        'l_5': 'encoder.2.0.SelfAttention.dropout',
                        'l_6': 'encoder.2.0.SelfAttention.o',
                        'l_7': 'encoder.2.0.dropout',
                        'l_8': 'encoder.2.1.layer_norm',
                        'l_9': 'encoder.2.1.DenseReluDense.wi',
                        'l_10': 'encoder.2.1.DenseReluDense.dropout',
                        'l_11': 'encoder.2.1.DenseReluDense.wo',
                        'l_12': 'encoder.2.1.dropout',
                        'l_13': 'encoder.3.0.layer_norm',
                        'l_14': 'encoder.3.0.SelfAttention.q',
                        'l_15': 'encoder.3.0.SelfAttention.k',
                        'l_16': 'encoder.3.0.SelfAttention.v',
                        'l_17': 'encoder.3.0.SelfAttention.dropout',
                        'l_18': 'encoder.3.0.SelfAttention.o',
                        'l_19': 'encoder.3.0.dropout',
                        'l_20': 'encoder.3.1.layer_norm',
                        'l_21': 'encoder.3.1.DenseReluDense.wi',
                        'l_22': 'encoder.3.1.DenseReluDense.dropout',
                        'l_23': 'encoder.3.1.DenseReluDense.wo',
                        'l_24': 'encoder.3.1.dropout',
                        'l_25': 'encoder.4.0.layer_norm',
                        'l_26': 'encoder.4.0.SelfAttention.q',
                        'l_27': 'encoder.4.0.SelfAttention.k',
                        'l_28': 'encoder.4.0.SelfAttention.v',
                        'l_29': 'encoder.4.0.SelfAttention.dropout',
                        'l_30': 'decoder.0.0.SelfAttention.o',
                        'l_31': 'decoder.0.0.dropout',
                        'l_32': 'decoder.0.1.layer_norm'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/Dropout[dropout] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/Dropout[dropout] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/Dropout[dropout] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/Tensor::__add___200 <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout] <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::__add___1603 <=> x4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::view_1619 <=> x5

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x2)
        t_0 = x1 + t_0
        t_1 = self.l_1(t_0)
        t_2 = t_1.size()
        t_2 = t_2[0]
        t_3 = self.l_2(t_1)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = self.l_3(t_1)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_1 = self.l_4(t_1)
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x0
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_5(t_4)
        t_1 = torch.matmul(t_4, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_2 = t_1.view(t_2, -1, 4096)
        t_2 = self.l_6(t_2)
        t_2 = self.l_7(t_2)
        t_2 = t_0 + t_2
        t_0 = self.l_8(t_2)
        t_0 = self.l_9(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_10(t_0)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_13(t_0)
        t_1 = t_2.size()
        t_1 = t_1[0]
        t_4 = self.l_14(t_2)
        t_4 = t_4.view(t_1, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = self.l_15(t_2)
        t_3 = t_3.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = self.l_16(t_2)
        t_2 = t_2.view(t_1, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_4, t_3)
        t_3 += x0
        t_4 = t_3.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_4.type_as(t_3)
        t_3 = self.l_17(t_3)
        t_2 = torch.matmul(t_3, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_1 = t_2.view(t_1, -1, 4096)
        t_1 = self.l_18(t_1)
        t_1 = self.l_19(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_20(t_1)
        t_0 = self.l_21(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_22(t_0)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_25(t_0)
        t_2 = t_1.size()
        t_2 = t_2[0]
        t_3 = self.l_26(t_1)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = self.l_27(t_1)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_1 = self.l_28(t_1)
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x0
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_29(t_4)
        t_3 = self.l_30(x5)
        t_3 = (t_3, x4)
        t_5 = t_3[0]
        t_5 = self.l_31(t_5)
        t_5 = x3 + t_5
        t_3 = t_3[1]
        t_3 = (t_5, t_3)
        t_5 = t_3[0]
        t_6 = self.l_32(t_5)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/Tensor::__add___326
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___330
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::transpose_354
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/prim::TupleConstruct_1628
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___1630
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]
        return list(flatten((t_0, t_2, t_1, t_4, t_3, t_5, t_6)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition2(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:2'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.4.0.SelfAttention.o',
                        'l_1': 'encoder.4.0.dropout',
                        'l_2': 'encoder.4.1.layer_norm',
                        'l_3': 'encoder.4.1.DenseReluDense.wi',
                        'l_4': 'encoder.4.1.DenseReluDense.dropout',
                        'l_5': 'encoder.4.1.DenseReluDense.wo',
                        'l_6': 'encoder.4.1.dropout',
                        'l_7': 'encoder.5.0.layer_norm',
                        'l_8': 'encoder.5.0.SelfAttention.q',
                        'l_9': 'encoder.5.0.SelfAttention.k',
                        'l_10': 'encoder.5.0.SelfAttention.v',
                        'l_11': 'encoder.5.0.SelfAttention.dropout',
                        'l_12': 'encoder.5.0.SelfAttention.o',
                        'l_13': 'encoder.5.0.dropout',
                        'l_14': 'encoder.5.1.layer_norm',
                        'l_15': 'encoder.5.1.DenseReluDense.wi',
                        'l_16': 'encoder.5.1.DenseReluDense.dropout',
                        'l_17': 'encoder.5.1.DenseReluDense.wo',
                        'l_18': 'encoder.5.1.dropout',
                        'l_19': 'encoder.6.0.layer_norm',
                        'l_20': 'encoder.6.0.SelfAttention.q',
                        'l_21': 'encoder.6.0.SelfAttention.k',
                        'l_22': 'encoder.6.0.SelfAttention.v',
                        'l_23': 'encoder.6.0.SelfAttention.dropout',
                        'l_24': 'encoder.6.0.SelfAttention.o',
                        'l_25': 'encoder.6.0.dropout',
                        'l_26': 'encoder.6.1.layer_norm',
                        'l_27': 'encoder.6.1.DenseReluDense.wi',
                        'l_28': 'encoder.6.1.DenseReluDense.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/Tensor::__add___326 <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___330 <=> x2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::transpose_354 <=> x3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> x4

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = torch.matmul(x4, x3)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.contiguous()
        t_0 = t_0.view(x2, -1, 4096)
        t_0 = self.l_0(t_0)
        t_0 = self.l_1(t_0)
        t_0 = x1 + t_0
        t_1 = self.l_2(t_0)
        t_1 = self.l_3(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_7(t_1)
        t_2 = t_0.size()
        t_2 = t_2[0]
        t_3 = self.l_8(t_0)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = self.l_9(t_0)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_0 = self.l_10(t_0)
        t_0 = t_0.view(t_2, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x0
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_11(t_4)
        t_0 = torch.matmul(t_4, t_0)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.contiguous()
        t_2 = t_0.view(t_2, -1, 4096)
        t_2 = self.l_12(t_2)
        t_2 = self.l_13(t_2)
        t_2 = t_1 + t_2
        t_1 = self.l_14(t_2)
        t_1 = self.l_15(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_16(t_1)
        t_1 = self.l_17(t_1)
        t_1 = self.l_18(t_1)
        t_1 = t_2 + t_1
        t_2 = self.l_19(t_1)
        t_0 = t_2.size()
        t_0 = t_0[0]
        t_4 = self.l_20(t_2)
        t_4 = t_4.view(t_0, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = self.l_21(t_2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = self.l_22(t_2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_4, t_3)
        t_3 += x0
        t_4 = t_3.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_4.type_as(t_3)
        t_3 = self.l_23(t_3)
        t_2 = torch.matmul(t_3, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_0 = t_2.view(t_0, -1, 4096)
        t_0 = self.l_24(t_0)
        t_0 = self.l_25(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_26(t_0)
        t_1 = self.l_27(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_28(t_1)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/Tensor::__add___495
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]
        return list(flatten((t_0, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition3(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:3'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.6.1.DenseReluDense.wo',
                        'l_1': 'encoder.6.1.dropout',
                        'l_2': 'encoder.7.0.layer_norm',
                        'l_3': 'encoder.7.0.SelfAttention.q',
                        'l_4': 'encoder.7.0.SelfAttention.k',
                        'l_5': 'encoder.7.0.SelfAttention.v',
                        'l_6': 'encoder.7.0.SelfAttention.dropout',
                        'l_7': 'encoder.7.0.SelfAttention.o',
                        'l_8': 'encoder.7.0.dropout',
                        'l_9': 'encoder.7.1.layer_norm',
                        'l_10': 'encoder.7.1.DenseReluDense.wi',
                        'l_11': 'encoder.7.1.DenseReluDense.dropout',
                        'l_12': 'encoder.7.1.DenseReluDense.wo',
                        'l_13': 'encoder.7.1.dropout',
                        'l_14': 'encoder.8.0.layer_norm',
                        'l_15': 'encoder.8.0.SelfAttention.q',
                        'l_16': 'encoder.8.0.SelfAttention.k',
                        'l_17': 'encoder.8.0.SelfAttention.v',
                        'l_18': 'encoder.8.0.SelfAttention.dropout',
                        'l_19': 'encoder.8.0.SelfAttention.o',
                        'l_20': 'encoder.8.0.dropout',
                        'l_21': 'encoder.8.1.layer_norm',
                        'l_22': 'encoder.8.1.DenseReluDense.wi',
                        'l_23': 'encoder.8.1.DenseReluDense.dropout',
                        'l_24': 'encoder.8.1.DenseReluDense.wo',
                        'l_25': 'encoder.8.1.dropout',
                        'l_26': 'encoder.9.0.layer_norm',
                        'l_27': 'encoder.9.0.SelfAttention.q',
                        'l_28': 'encoder.9.0.SelfAttention.k',
                        'l_29': 'encoder.9.0.SelfAttention.v'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/Dropout[dropout] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/Tensor::__add___495 <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> x2

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x2)
        t_0 = self.l_1(t_0)
        t_0 = x1 + t_0
        t_1 = self.l_2(t_0)
        t_2 = t_1.size()
        t_2 = t_2[0]
        t_3 = self.l_3(t_1)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = self.l_4(t_1)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_1 = self.l_5(t_1)
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x0
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_6(t_4)
        t_1 = torch.matmul(t_4, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_2 = t_1.view(t_2, -1, 4096)
        t_2 = self.l_7(t_2)
        t_2 = self.l_8(t_2)
        t_2 = t_0 + t_2
        t_0 = self.l_9(t_2)
        t_0 = self.l_10(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_14(t_0)
        t_1 = t_2.size()
        t_1 = t_1[0]
        t_4 = self.l_15(t_2)
        t_4 = t_4.view(t_1, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = self.l_16(t_2)
        t_3 = t_3.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = self.l_17(t_2)
        t_2 = t_2.view(t_1, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_4, t_3)
        t_3 += x0
        t_4 = t_3.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_4.type_as(t_3)
        t_3 = self.l_18(t_3)
        t_2 = torch.matmul(t_3, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_1 = t_2.view(t_1, -1, 4096)
        t_1 = self.l_19(t_1)
        t_1 = self.l_20(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_21(t_1)
        t_0 = self.l_22(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = self.l_25(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_26(t_0)
        t_2 = t_1.size()
        t_2 = t_2[0]
        t_3 = self.l_27(t_1)
        t_4 = self.l_28(t_1)
        t_1 = self.l_29(t_1)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/Tensor::__add___621
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___625
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]
        return list(flatten((t_0, t_2, t_3, t_4, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition4(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:4'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.9.0.SelfAttention.dropout',
                        'l_1': 'encoder.9.0.SelfAttention.o',
                        'l_2': 'encoder.9.0.dropout',
                        'l_3': 'encoder.9.1.layer_norm',
                        'l_4': 'encoder.9.1.DenseReluDense.wi',
                        'l_5': 'encoder.9.1.DenseReluDense.dropout',
                        'l_6': 'encoder.9.1.DenseReluDense.wo',
                        'l_7': 'encoder.9.1.dropout',
                        'l_8': 'encoder.10.0.layer_norm',
                        'l_9': 'encoder.10.0.SelfAttention.q',
                        'l_10': 'encoder.10.0.SelfAttention.k',
                        'l_11': 'encoder.10.0.SelfAttention.v',
                        'l_12': 'encoder.10.0.SelfAttention.dropout',
                        'l_13': 'encoder.10.0.SelfAttention.o',
                        'l_14': 'encoder.10.0.dropout',
                        'l_15': 'encoder.10.1.layer_norm',
                        'l_16': 'encoder.10.1.DenseReluDense.wi',
                        'l_17': 'encoder.10.1.DenseReluDense.dropout',
                        'l_18': 'encoder.10.1.DenseReluDense.wo',
                        'l_19': 'encoder.10.1.dropout',
                        'l_20': 'encoder.11.0.layer_norm',
                        'l_21': 'encoder.11.0.SelfAttention.q',
                        'l_22': 'encoder.11.0.SelfAttention.k',
                        'l_23': 'encoder.11.0.SelfAttention.v',
                        'l_24': 'encoder.11.0.SelfAttention.dropout',
                        'l_25': 'encoder.11.0.SelfAttention.o',
                        'l_26': 'encoder.11.0.dropout',
                        'l_27': 'encoder.11.1.layer_norm',
                        'l_28': 'encoder.11.1.DenseReluDense.wi'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/Tensor::__add___621 <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___625 <=> x2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> x3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> x4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> x5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> x6

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5, x6 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = x3.view(x2, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = x4.view(x2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = x5.view(x2, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_0, t_1)
        t_1 += x0
        t_0 = t_1.float()
        t_0 = torch.nn.functional.softmax(t_0, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_0.type_as(t_1)
        t_1 = self.l_0(t_1)
        t_2 = torch.matmul(t_1, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_2 = t_2.view(x2, -1, 4096)
        t_2 = self.l_1(t_2)
        t_2 = self.l_2(t_2)
        t_2 = x1 + t_2
        t_1 = self.l_3(t_2)
        t_1 = self.l_4(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = self.l_7(t_1)
        t_1 = t_2 + t_1
        t_2 = self.l_8(t_1)
        t_0 = t_2.size()
        t_0 = t_0[0]
        t_3 = self.l_9(t_2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = self.l_10(t_2)
        t_4 = t_4.view(t_0, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_2 = self.l_11(t_2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x0
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_12(t_4)
        t_2 = torch.matmul(t_4, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_0 = t_2.view(t_0, -1, 4096)
        t_0 = self.l_13(t_0)
        t_0 = self.l_14(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_15(t_0)
        t_1 = self.l_16(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_17(t_1)
        t_1 = self.l_18(t_1)
        t_1 = self.l_19(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_20(t_1)
        t_2 = t_0.size()
        t_2 = t_2[0]
        t_4 = self.l_21(t_0)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = self.l_22(t_0)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_0 = self.l_23(t_0)
        t_0 = t_0.view(t_2, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_4, t_3)
        t_3 += x0
        t_4 = t_3.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_4.type_as(t_3)
        t_3 = self.l_24(t_3)
        t_0 = torch.matmul(t_3, t_0)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.contiguous()
        t_2 = t_0.view(t_2, -1, 4096)
        t_2 = self.l_25(t_2)
        t_2 = self.l_26(t_2)
        t_2 = t_1 + t_2
        t_1 = self.l_27(t_2)
        t_1 = self.l_28(t_1)
        t_0 = x6.size()
        t_3 = t_0[1]
        t_3 = torch.arange(t_3, dtype=torch.int64, device = self.device)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/Tensor::__add___790
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::size_1634
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/torch::arange_1670
        return list(flatten((t_2, t_1, t_0, t_3)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition5(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:5'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.11.1.DenseReluDense.dropout',
                        'l_1': 'encoder.11.1.DenseReluDense.wo',
                        'l_2': 'encoder.11.1.dropout',
                        'l_3': 'encoder.12.0.layer_norm',
                        'l_4': 'encoder.12.0.SelfAttention.q',
                        'l_5': 'encoder.12.0.SelfAttention.k',
                        'l_6': 'encoder.12.0.SelfAttention.v',
                        'l_7': 'encoder.12.0.SelfAttention.dropout',
                        'l_8': 'encoder.12.0.SelfAttention.o',
                        'l_9': 'encoder.12.0.dropout',
                        'l_10': 'encoder.12.1.layer_norm',
                        'l_11': 'encoder.12.1.DenseReluDense.wi',
                        'l_12': 'encoder.12.1.DenseReluDense.dropout',
                        'l_13': 'encoder.12.1.DenseReluDense.wo',
                        'l_14': 'encoder.12.1.dropout',
                        'l_15': 'encoder.13.0.layer_norm',
                        'l_16': 'encoder.13.0.SelfAttention.q',
                        'l_17': 'encoder.13.0.SelfAttention.k',
                        'l_18': 'encoder.13.0.SelfAttention.v',
                        'l_19': 'encoder.13.0.SelfAttention.dropout',
                        'l_20': 'encoder.13.0.SelfAttention.o',
                        'l_21': 'encoder.13.0.dropout',
                        'l_22': 'encoder.13.1.layer_norm',
                        'l_23': 'encoder.13.1.DenseReluDense.wi',
                        'l_24': 'encoder.13.1.DenseReluDense.dropout',
                        'l_25': 'encoder.13.1.DenseReluDense.wo',
                        'l_26': 'encoder.13.1.dropout',
                        'l_27': 'encoder.14.0.layer_norm',
                        'l_28': 'encoder.14.0.SelfAttention.q',
                        'l_29': 'encoder.14.0.SelfAttention.v'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/Dropout[dropout] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/Tensor::__add___790 <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/torch::arange_1670 <=> x3

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = torch.nn.functional.relu(x2, inplace=False)
        t_0 = self.l_0(t_0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = x1 + t_0
        t_1 = self.l_3(t_0)
        t_2 = t_1.size()
        t_2 = t_2[0]
        t_3 = self.l_4(t_1)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = self.l_5(t_1)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_1 = self.l_6(t_1)
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x0
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_7(t_4)
        t_1 = torch.matmul(t_4, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_2 = t_1.view(t_2, -1, 4096)
        t_2 = self.l_8(t_2)
        t_2 = self.l_9(t_2)
        t_2 = t_0 + t_2
        t_0 = self.l_10(t_2)
        t_0 = self.l_11(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_0 = self.l_14(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_15(t_0)
        t_1 = t_2.size()
        t_1 = t_1[0]
        t_4 = self.l_16(t_2)
        t_4 = t_4.view(t_1, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = self.l_17(t_2)
        t_3 = t_3.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = self.l_18(t_2)
        t_2 = t_2.view(t_1, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_4, t_3)
        t_3 += x0
        t_4 = t_3.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_4.type_as(t_3)
        t_3 = self.l_19(t_3)
        t_2 = torch.matmul(t_3, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_1 = t_2.view(t_1, -1, 4096)
        t_1 = self.l_20(t_1)
        t_1 = self.l_21(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_22(t_1)
        t_0 = self.l_23(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_24(t_0)
        t_0 = self.l_25(t_0)
        t_0 = self.l_26(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_27(t_0)
        t_2 = t_1.size()
        t_2 = t_2[0]
        t_3 = self.l_28(t_1)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = self.l_29(t_1)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = slice(None, None, None)
        t_5 = (t_5, None)
        t_5 = x3[t_5]
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/Tensor::__add___916
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___920
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::transpose_928
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::transpose_944
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::__getitem___1677
        return list(flatten((t_0, t_1, t_2, t_3, t_4, t_5)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition6(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:6'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.14.0.SelfAttention.k',
                        'l_1': 'encoder.14.0.SelfAttention.dropout',
                        'l_2': 'encoder.14.0.SelfAttention.o',
                        'l_3': 'encoder.14.0.dropout',
                        'l_4': 'encoder.14.1.layer_norm',
                        'l_5': 'encoder.14.1.DenseReluDense.wi',
                        'l_6': 'encoder.14.1.DenseReluDense.dropout',
                        'l_7': 'encoder.14.1.DenseReluDense.wo',
                        'l_8': 'encoder.14.1.dropout',
                        'l_9': 'encoder.15.0.layer_norm',
                        'l_10': 'encoder.15.0.SelfAttention.q',
                        'l_11': 'encoder.15.0.SelfAttention.k',
                        'l_12': 'encoder.15.0.SelfAttention.v',
                        'l_13': 'encoder.15.0.SelfAttention.dropout',
                        'l_14': 'encoder.15.0.SelfAttention.o',
                        'l_15': 'encoder.15.0.dropout',
                        'l_16': 'encoder.15.1.layer_norm',
                        'l_17': 'encoder.15.1.DenseReluDense.wi',
                        'l_18': 'encoder.15.1.DenseReluDense.dropout',
                        'l_19': 'encoder.15.1.DenseReluDense.wo',
                        'l_20': 'encoder.15.1.dropout',
                        'l_21': 'encoder.16.0.layer_norm',
                        'l_22': 'encoder.16.0.SelfAttention.q',
                        'l_23': 'encoder.16.0.SelfAttention.k',
                        'l_24': 'encoder.16.0.SelfAttention.v',
                        'l_25': 'encoder.16.0.SelfAttention.dropout',
                        'l_26': 'encoder.16.0.SelfAttention.o',
                        'l_27': 'encoder.16.0.dropout',
                        'l_28': 'encoder.16.1.layer_norm',
                        'l_29': 'encoder.16.1.DenseReluDense.wi'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/Dropout[dropout] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/Tensor::__add___916 <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> x2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___920 <=> x3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::transpose_928 <=> x4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::transpose_944 <=> x5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::size_1634 <=> x6

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5, x6 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x2)
        t_0 = t_0.view(x3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(x4, t_0)
        t_0 += x0
        t_1 = t_0.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_1.type_as(t_0)
        t_0 = self.l_1(t_0)
        t_0 = torch.matmul(t_0, x5)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.contiguous()
        t_0 = t_0.view(x3, -1, 4096)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = x1 + t_0
        t_1 = self.l_4(t_0)
        t_1 = self.l_5(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_6(t_1)
        t_1 = self.l_7(t_1)
        t_1 = self.l_8(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_9(t_1)
        t_2 = t_0.size()
        t_2 = t_2[0]
        t_3 = self.l_10(t_0)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = self.l_11(t_0)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_0 = self.l_12(t_0)
        t_0 = t_0.view(t_2, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x0
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_13(t_4)
        t_0 = torch.matmul(t_4, t_0)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.contiguous()
        t_2 = t_0.view(t_2, -1, 4096)
        t_2 = self.l_14(t_2)
        t_2 = self.l_15(t_2)
        t_2 = t_1 + t_2
        t_1 = self.l_16(t_2)
        t_1 = self.l_17(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_18(t_1)
        t_1 = self.l_19(t_1)
        t_1 = self.l_20(t_1)
        t_1 = t_2 + t_1
        t_2 = self.l_21(t_1)
        t_0 = t_2.size()
        t_0 = t_0[0]
        t_4 = self.l_22(t_2)
        t_4 = t_4.view(t_0, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = self.l_23(t_2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = self.l_24(t_2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_4, t_3)
        t_3 += x0
        t_4 = t_3.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_4.type_as(t_3)
        t_3 = self.l_25(t_3)
        t_2 = torch.matmul(t_3, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_0 = t_2.view(t_0, -1, 4096)
        t_0 = self.l_26(t_0)
        t_0 = self.l_27(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_28(t_0)
        t_1 = self.l_29(t_1)
        t_2 = x6[0]
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/Tensor::__add___1085
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Size::__getitem___1636
        return list(flatten((t_0, t_1, t_2)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition7(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:7'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.16.1.DenseReluDense.dropout',
                        'l_1': 'encoder.16.1.DenseReluDense.wo',
                        'l_2': 'encoder.16.1.dropout',
                        'l_3': 'encoder.17.0.layer_norm',
                        'l_4': 'encoder.17.0.SelfAttention.q',
                        'l_5': 'encoder.17.0.SelfAttention.k',
                        'l_6': 'encoder.17.0.SelfAttention.v',
                        'l_7': 'encoder.17.0.SelfAttention.dropout',
                        'l_8': 'encoder.17.0.SelfAttention.o',
                        'l_9': 'encoder.17.0.dropout',
                        'l_10': 'encoder.17.1.layer_norm',
                        'l_11': 'encoder.17.1.DenseReluDense.wi',
                        'l_12': 'encoder.17.1.DenseReluDense.dropout',
                        'l_13': 'encoder.17.1.DenseReluDense.wo',
                        'l_14': 'encoder.17.1.dropout',
                        'l_15': 'encoder.18.0.layer_norm',
                        'l_16': 'encoder.18.0.SelfAttention.q',
                        'l_17': 'encoder.18.0.SelfAttention.k',
                        'l_18': 'encoder.18.0.SelfAttention.v',
                        'l_19': 'encoder.18.0.SelfAttention.dropout',
                        'l_20': 'encoder.18.0.SelfAttention.o',
                        'l_21': 'encoder.18.0.dropout',
                        'l_22': 'encoder.18.1.layer_norm',
                        'l_23': 'encoder.18.1.DenseReluDense.wi',
                        'l_24': 'encoder.18.1.DenseReluDense.dropout',
                        'l_25': 'encoder.18.1.DenseReluDense.wo',
                        'l_26': 'encoder.18.1.dropout',
                        'l_27': 'encoder.19.0.layer_norm',
                        'l_28': 'encoder.19.0.SelfAttention.k'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/Dropout[dropout] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/Tensor::__add___1085 <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> x2

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = torch.nn.functional.relu(x2, inplace=False)
        t_0 = self.l_0(t_0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = x1 + t_0
        t_1 = self.l_3(t_0)
        t_2 = t_1.size()
        t_2 = t_2[0]
        t_3 = self.l_4(t_1)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = self.l_5(t_1)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_1 = self.l_6(t_1)
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x0
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_7(t_4)
        t_1 = torch.matmul(t_4, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_2 = t_1.view(t_2, -1, 4096)
        t_2 = self.l_8(t_2)
        t_2 = self.l_9(t_2)
        t_2 = t_0 + t_2
        t_0 = self.l_10(t_2)
        t_0 = self.l_11(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_0 = self.l_14(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_15(t_0)
        t_1 = t_2.size()
        t_1 = t_1[0]
        t_4 = self.l_16(t_2)
        t_4 = t_4.view(t_1, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = self.l_17(t_2)
        t_3 = t_3.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = self.l_18(t_2)
        t_2 = t_2.view(t_1, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_4, t_3)
        t_3 += x0
        t_4 = t_3.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_4.type_as(t_3)
        t_3 = self.l_19(t_3)
        t_2 = torch.matmul(t_3, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_1 = t_2.view(t_1, -1, 4096)
        t_1 = self.l_20(t_1)
        t_1 = self.l_21(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_22(t_1)
        t_0 = self.l_23(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_24(t_0)
        t_0 = self.l_25(t_0)
        t_0 = self.l_26(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_27(t_0)
        t_2 = t_1.size()
        t_3 = self.l_28(t_1)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/Tensor::__add___1211
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::size_1213
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]
        return list(flatten((t_0, t_1, t_2, t_3)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition8(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:8'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, (1, 1)]
        self.lookup = { 'l_0': 'encoder.19.0.SelfAttention.q',
                        'l_1': 'encoder.19.0.SelfAttention.v',
                        'l_2': 'encoder.19.0.SelfAttention.dropout',
                        'l_3': 'encoder.19.0.SelfAttention.o',
                        'l_4': 'encoder.19.0.dropout',
                        'l_5': 'encoder.19.1.layer_norm',
                        'l_6': 'encoder.19.1.DenseReluDense.wi',
                        'l_7': 'encoder.19.1.DenseReluDense.dropout',
                        'l_8': 'encoder.19.1.DenseReluDense.wo',
                        'l_9': 'encoder.19.1.dropout',
                        'l_10': 'encoder.20.0.layer_norm',
                        'l_11': 'encoder.20.0.SelfAttention.q',
                        'l_12': 'encoder.20.0.SelfAttention.k',
                        'l_13': 'encoder.20.0.SelfAttention.v',
                        'l_14': 'encoder.20.0.SelfAttention.dropout',
                        'l_15': 'encoder.20.0.SelfAttention.o',
                        'l_16': 'encoder.20.0.dropout',
                        'l_17': 'encoder.20.1.layer_norm',
                        'l_18': 'encoder.20.1.DenseReluDense.wi',
                        'l_19': 'encoder.20.1.DenseReluDense.dropout',
                        'l_20': 'encoder.20.1.DenseReluDense.wo',
                        'l_21': 'encoder.20.1.dropout',
                        'l_22': 'encoder.21.0.layer_norm',
                        'l_23': 'encoder.21.0.SelfAttention.q',
                        'l_24': 'encoder.21.0.SelfAttention.k',
                        'l_25': 'encoder.21.0.SelfAttention.v',
                        'l_26': 'encoder.21.0.SelfAttention.dropout',
                        'l_27': 'encoder.21.0.SelfAttention.o',
                        'l_28': 'encoder.21.0.dropout',
                        'l_29': 'encoder.21.1.layer_norm'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/Tensor::__add___1211 <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> x2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::size_1213 <=> x3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> x4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/prim::TupleConstruct_1628 <=> x5

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = x3[0]
        t_1 = self.l_0(x2)
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = x4.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = self.l_1(x2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x0
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_2(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x1 + t_0
        t_3 = self.l_5(t_0)
        t_3 = self.l_6(t_3)
        t_3 = torch.nn.functional.relu(t_3, inplace=False)
        t_3 = self.l_7(t_3)
        t_3 = self.l_8(t_3)
        t_3 = self.l_9(t_3)
        t_3 = t_0 + t_3
        t_0 = self.l_10(t_3)
        t_2 = t_0.size()
        t_2 = t_2[0]
        t_1 = self.l_11(t_0)
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = self.l_12(t_0)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_0 = self.l_13(t_0)
        t_0 = t_0.view(t_2, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_1, t_4)
        t_4 += x0
        t_1 = t_4.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_1.type_as(t_4)
        t_4 = self.l_14(t_4)
        t_0 = torch.matmul(t_4, t_0)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.contiguous()
        t_2 = t_0.view(t_2, -1, 4096)
        t_2 = self.l_15(t_2)
        t_2 = self.l_16(t_2)
        t_2 = t_3 + t_2
        t_3 = self.l_17(t_2)
        t_3 = self.l_18(t_3)
        t_3 = torch.nn.functional.relu(t_3, inplace=False)
        t_3 = self.l_19(t_3)
        t_3 = self.l_20(t_3)
        t_3 = self.l_21(t_3)
        t_3 = t_2 + t_3
        t_2 = self.l_22(t_3)
        t_0 = t_2.size()
        t_0 = t_0[0]
        t_4 = self.l_23(t_2)
        t_4 = t_4.view(t_0, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_1 = self.l_24(t_2)
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = self.l_25(t_2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_4, t_1)
        t_1 += x0
        t_4 = t_1.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_4.type_as(t_1)
        t_1 = self.l_26(t_1)
        t_2 = torch.matmul(t_1, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_0 = t_2.view(t_0, -1, 4096)
        t_0 = self.l_27(t_0)
        t_0 = self.l_28(t_0)
        t_0 = t_3 + t_0
        t_3 = self.l_29(t_0)
        t_2 = x5[1]
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/Tensor::__add___1380
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5LayerNorm[layer_norm]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___1632
        return list(flatten((t_0, t_3, t_2)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition9(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:9'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.21.1.DenseReluDense.wi',
                        'l_1': 'encoder.21.1.DenseReluDense.dropout',
                        'l_2': 'encoder.21.1.DenseReluDense.wo',
                        'l_3': 'encoder.21.1.dropout',
                        'l_4': 'encoder.22.0.layer_norm',
                        'l_5': 'encoder.22.0.SelfAttention.q',
                        'l_6': 'encoder.22.0.SelfAttention.k',
                        'l_7': 'encoder.22.0.SelfAttention.v',
                        'l_8': 'encoder.22.0.SelfAttention.dropout',
                        'l_9': 'encoder.22.0.SelfAttention.o',
                        'l_10': 'encoder.22.0.dropout',
                        'l_11': 'encoder.22.1.layer_norm',
                        'l_12': 'encoder.22.1.DenseReluDense.wi',
                        'l_13': 'encoder.22.1.DenseReluDense.dropout',
                        'l_14': 'encoder.22.1.DenseReluDense.wo',
                        'l_15': 'encoder.22.1.dropout',
                        'l_16': 'encoder.23.0.layer_norm',
                        'l_17': 'encoder.23.0.SelfAttention.q',
                        'l_18': 'encoder.23.0.SelfAttention.k',
                        'l_19': 'encoder.23.0.SelfAttention.v',
                        'l_20': 'encoder.23.0.SelfAttention.dropout',
                        'l_21': 'encoder.23.0.SelfAttention.o',
                        'l_22': 'encoder.23.0.dropout',
                        'l_23': 'encoder.23.1.layer_norm',
                        'l_24': 'encoder.23.1.DenseReluDense.wi',
                        'l_25': 'encoder.23.1.DenseReluDense.dropout',
                        'l_26': 'encoder.23.1.DenseReluDense.wo'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/Dropout[dropout] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/Dropout[dropout] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/Tensor::__add___1380 <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> x2

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x2)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = x1 + t_0
        t_1 = self.l_4(t_0)
        t_2 = t_1.size()
        t_2 = t_2[0]
        t_3 = self.l_5(t_1)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = self.l_6(t_1)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_1 = self.l_7(t_1)
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x0
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_8(t_4)
        t_1 = torch.matmul(t_4, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_2 = t_1.view(t_2, -1, 4096)
        t_2 = self.l_9(t_2)
        t_2 = self.l_10(t_2)
        t_2 = t_0 + t_2
        t_0 = self.l_11(t_2)
        t_0 = self.l_12(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_13(t_0)
        t_0 = self.l_14(t_0)
        t_0 = self.l_15(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_16(t_0)
        t_1 = t_2.size()
        t_1 = t_1[0]
        t_4 = self.l_17(t_2)
        t_4 = t_4.view(t_1, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = self.l_18(t_2)
        t_3 = t_3.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = self.l_19(t_2)
        t_2 = t_2.view(t_1, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_4, t_3)
        t_3 += x0
        t_4 = t_3.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_4.type_as(t_3)
        t_3 = self.l_20(t_3)
        t_2 = torch.matmul(t_3, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_1 = t_2.view(t_1, -1, 4096)
        t_1 = self.l_21(t_1)
        t_1 = self.l_22(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_23(t_1)
        t_0 = self.l_24(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_25(t_0)
        t_0 = self.l_26(t_0)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/Tensor::__add___1498
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]
        return list(flatten((t_1, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition10(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5LayerNorm[final_layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:10'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.23.1.dropout',
                        'l_1': 'encoder.final_layer_norm',
                        'l_2': 'encoder.dropout',
                        'l_3': 'decoder.0.1.EncDecAttention.q',
                        'l_4': 'decoder.0.1.EncDecAttention.k',
                        'l_5': 'decoder.0.1.EncDecAttention.v',
                        'l_6': 'decoder.1.1.EncDecAttention.k',
                        'l_7': 'decoder.1.1.EncDecAttention.v',
                        'l_8': 'decoder.2.1.EncDecAttention.k',
                        'l_9': 'decoder.2.1.EncDecAttention.v',
                        'l_10': 'decoder.3.1.EncDecAttention.k',
                        'l_11': 'decoder.3.1.EncDecAttention.v',
                        'l_12': 'decoder.4.1.EncDecAttention.k',
                        'l_13': 'decoder.5.1.EncDecAttention.k',
                        'l_14': 'decoder.5.1.EncDecAttention.v',
                        'l_15': 'decoder.6.1.EncDecAttention.k',
                        'l_16': 'decoder.6.1.EncDecAttention.v',
                        'l_17': 'decoder.7.1.EncDecAttention.v',
                        'l_18': 'decoder.8.1.EncDecAttention.v',
                        'l_19': 'decoder.9.1.EncDecAttention.k',
                        'l_20': 'decoder.10.1.EncDecAttention.k',
                        'l_21': 'decoder.10.1.EncDecAttention.v',
                        'l_22': 'decoder.11.1.EncDecAttention.k',
                        'l_23': 'decoder.12.1.EncDecAttention.v',
                        'l_24': 'decoder.13.1.EncDecAttention.k',
                        'l_25': 'decoder.13.1.EncDecAttention.v',
                        'l_26': 'decoder.14.1.EncDecAttention.k',
                        'l_27': 'decoder.14.1.EncDecAttention.v',
                        'l_28': 'decoder.15.1.EncDecAttention.k',
                        'l_29': 'decoder.16.1.EncDecAttention.k',
                        'l_30': 'decoder.16.1.EncDecAttention.v',
                        'l_31': 'decoder.17.1.EncDecAttention.k',
                        'l_32': 'decoder.17.1.EncDecAttention.v',
                        'l_33': 'decoder.18.1.EncDecAttention.k',
                        'l_34': 'decoder.18.1.EncDecAttention.v',
                        'l_35': 'decoder.19.1.EncDecAttention.k',
                        'l_36': 'decoder.19.1.EncDecAttention.v',
                        'l_37': 'decoder.20.1.EncDecAttention.k',
                        'l_38': 'decoder.20.1.EncDecAttention.v',
                        'l_39': 'decoder.21.1.EncDecAttention.k',
                        'l_40': 'decoder.21.1.EncDecAttention.v',
                        'l_41': 'decoder.22.1.EncDecAttention.k',
                        'l_42': 'decoder.22.1.EncDecAttention.v',
                        'l_43': 'decoder.23.1.EncDecAttention.k'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/Dropout[dropout] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5LayerNorm[final_layer_norm] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/Tensor::__add___1498 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Size::__getitem___1636 <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::__getitem___1677 <=> x4

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1)
        t_0 = x0 + t_0
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_1 = t_0.size(1)
        t_2 = self.l_3(x2)
        t_2 = t_2.view(x3, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = self.l_4(t_0)
        t_3 = t_3.view(x3, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = self.l_5(t_0)
        t_4 = t_4.view(x3, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_2, t_3)
        t_1 = torch.arange(t_1, dtype=torch.int64, device = self.device)
        t_2 = slice(None, None, None)
        t_2 = (None, t_2)
        t_2 = t_1[t_2]
        t_2 = t_2 - x4
        t_2 = -t_2
        t_1 = torch.zeros_like(t_2)
        t_1 = torch.max(t_2, t_1)
        t_2 = t_1 < 16
        t_5 = t_1.float()
        t_5 = t_5 / 16
        t_5 = torch.log(t_5)
        t_6 = math.log(8.0)
        t_6 = t_5 / t_6
        t_6 = t_6 * 16
        t_6 = t_6.to(torch.int64)
        t_6 = 16 + t_6
        t_5 = self.l_6(t_0)
        t_7 = self.l_7(t_0)
        t_8 = self.l_8(t_0)
        t_9 = self.l_9(t_0)
        t_10 = self.l_10(t_0)
        t_11 = self.l_11(t_0)
        t_12 = self.l_12(t_0)
        t_13 = self.l_13(t_0)
        t_14 = self.l_14(t_0)
        t_15 = self.l_15(t_0)
        t_16 = self.l_16(t_0)
        t_17 = self.l_17(t_0)
        t_18 = self.l_18(t_0)
        t_19 = self.l_19(t_0)
        t_20 = self.l_20(t_0)
        t_21 = self.l_21(t_0)
        t_22 = self.l_22(t_0)
        t_23 = self.l_23(t_0)
        t_24 = self.l_24(t_0)
        t_25 = self.l_25(t_0)
        t_26 = self.l_26(t_0)
        t_27 = self.l_27(t_0)
        t_28 = self.l_28(t_0)
        t_29 = self.l_29(t_0)
        t_30 = self.l_30(t_0)
        t_31 = self.l_31(t_0)
        t_32 = self.l_32(t_0)
        t_33 = self.l_33(t_0)
        t_34 = self.l_34(t_0)
        t_35 = self.l_35(t_0)
        t_36 = self.l_36(t_0)
        t_37 = self.l_37(t_0)
        t_38 = self.l_38(t_0)
        t_39 = self.l_39(t_0)
        t_40 = self.l_40(t_0)
        t_41 = self.l_41(t_0)
        t_42 = self.l_42(t_0)
        t_43 = self.l_43(t_0)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::transpose_1664
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/torch::matmul_1668
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/torch::max_1690
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::__lt___1692
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/int::__add___1705
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        return list(flatten((t_0, t_4, t_3, t_1, t_2, t_6, t_5, t_7, t_8, t_9, t_10, t_11, t_12, t_13, t_14, t_15, t_16, t_17, t_18, t_19, t_20, t_21, t_22, t_23, t_24, t_25, t_26, t_27, t_28, t_29, t_30, t_31, t_32, t_33, t_34, t_35, t_36, t_37, t_38, t_39, t_40, t_41, t_42, t_43)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition11(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Embedding[relative_attention_bias]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:11'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.0.1.EncDecAttention.relative_attention_bias',
                        'l_1': 'decoder.0.1.EncDecAttention.dropout',
                        'l_2': 'decoder.0.1.EncDecAttention.o',
                        'l_3': 'decoder.0.1.dropout',
                        'l_4': 'decoder.0.2.layer_norm',
                        'l_5': 'decoder.0.2.DenseReluDense.wi',
                        'l_6': 'decoder.0.2.DenseReluDense.dropout',
                        'l_7': 'decoder.0.2.DenseReluDense.wo',
                        'l_8': 'decoder.0.2.dropout',
                        'l_9': 'decoder.1.0.layer_norm',
                        'l_10': 'decoder.1.0.SelfAttention.q',
                        'l_11': 'decoder.1.0.SelfAttention.k',
                        'l_12': 'decoder.1.0.SelfAttention.v',
                        'l_13': 'decoder.1.0.SelfAttention.dropout',
                        'l_14': 'decoder.1.0.SelfAttention.o',
                        'l_15': 'decoder.1.0.dropout',
                        'l_16': 'decoder.1.1.layer_norm',
                        'l_17': 'decoder.1.1.EncDecAttention.q',
                        'l_18': 'decoder.1.1.EncDecAttention.dropout',
                        'l_19': 'decoder.1.1.EncDecAttention.o',
                        'l_20': 'decoder.1.1.dropout',
                        'l_21': 'decoder.1.2.layer_norm',
                        'l_22': 'decoder.1.2.DenseReluDense.wi',
                        'l_23': 'decoder.1.2.DenseReluDense.dropout',
                        'l_24': 'decoder.1.2.DenseReluDense.wo',
                        'l_25': 'decoder.1.2.dropout',
                        'l_26': 'decoder.2.0.layer_norm',
                        'l_27': 'decoder.2.0.SelfAttention.q',
                        'l_28': 'decoder.2.0.SelfAttention.k',
                        'l_29': 'decoder.2.0.SelfAttention.v',
                        'l_30': 'decoder.2.0.SelfAttention.dropout',
                        'l_31': 'decoder.2.0.SelfAttention.o',
                        'l_32': 'decoder.2.0.dropout',
                        'l_33': 'decoder.2.1.layer_norm',
                        'l_34': 'decoder.2.1.EncDecAttention.q',
                        'l_35': 'decoder.2.1.EncDecAttention.dropout',
                        'l_36': 'decoder.2.1.EncDecAttention.o',
                        'l_37': 'decoder.2.1.dropout',
                        'l_38': 'decoder.2.2.layer_norm',
                        'l_39': 'decoder.2.2.DenseReluDense.wi',
                        'l_40': 'decoder.2.2.DenseReluDense.dropout',
                        'l_41': 'decoder.2.2.DenseReluDense.wo',
                        'l_42': 'decoder.2.2.dropout',
                        'l_43': 'decoder.3.0.layer_norm',
                        'l_44': 'decoder.3.0.SelfAttention.q',
                        'l_45': 'decoder.3.0.SelfAttention.k',
                        'l_46': 'decoder.3.0.SelfAttention.v',
                        'l_47': 'decoder.3.0.SelfAttention.dropout',
                        'l_48': 'decoder.3.0.SelfAttention.o',
                        'l_49': 'decoder.3.0.dropout',
                        'l_50': 'decoder.3.1.layer_norm',
                        'l_51': 'decoder.3.1.EncDecAttention.q',
                        'l_52': 'decoder.3.1.EncDecAttention.dropout',
                        'l_53': 'decoder.3.1.EncDecAttention.o',
                        'l_54': 'decoder.3.1.dropout',
                        'l_55': 'decoder.3.2.layer_norm',
                        'l_56': 'decoder.3.2.DenseReluDense.wi',
                        'l_57': 'decoder.3.2.DenseReluDense.dropout',
                        'l_58': 'decoder.3.2.DenseReluDense.wo',
                        'l_59': 'decoder.3.2.dropout',
                        'l_60': 'decoder.4.0.layer_norm',
                        'l_61': 'decoder.4.0.SelfAttention.q',
                        'l_62': 'decoder.4.0.SelfAttention.k',
                        'l_63': 'decoder.4.0.SelfAttention.v',
                        'l_64': 'decoder.4.0.SelfAttention.dropout',
                        'l_65': 'decoder.4.0.SelfAttention.o',
                        'l_66': 'decoder.4.0.dropout',
                        'l_67': 'decoder.4.1.EncDecAttention.v',
                        'l_68': 'decoder.7.1.EncDecAttention.k',
                        'l_69': 'decoder.8.1.EncDecAttention.k',
                        'l_70': 'decoder.9.1.EncDecAttention.v',
                        'l_71': 'decoder.11.1.EncDecAttention.v',
                        'l_72': 'decoder.12.1.EncDecAttention.k',
                        'l_73': 'decoder.15.1.EncDecAttention.v',
                        'l_74': 'decoder.23.1.EncDecAttention.v'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Embedding[relative_attention_bias] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/Dropout[dropout] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/Dropout[dropout] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_50
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_51
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_52
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_53
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_54
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_55
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_56
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_57
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_58
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/Dropout[dropout] <=> self.l_59
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_60
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_61
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_62
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_63
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_64
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_65
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_66
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_67
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_68
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_69
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_70
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_71
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_72
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_73
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_74
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___1630 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___1632 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Size::__getitem___1636 <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::transpose_1664 <=> x4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/torch::matmul_1668 <=> x5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/torch::max_1690 <=> x6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::__lt___1692 <=> x7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/int::__add___1705 <=> x8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x9
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x10
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x11
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x12
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x13
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x14

        # moving inputs to current device no op if already on the correct device
        inverted_encoder_attention_mask, x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = torch.full_like(x8, 31)
        t_0 = torch.min(x8, t_0)
        t_0 = torch.where(x7, x6, t_0)
        t_0 = 0 + t_0
        t_0 = t_0.to(self.device)
        t_0 = self.l_0(t_0)
        t_1 = [2, 0, 1]
        t_1 = t_0.permute(t_1)
        t_1 = t_1.unsqueeze(0)
        t_1 = t_1 + inverted_encoder_attention_mask
        x5 = x5 + t_1
        t_0 = x5
        t_2 = t_0.float()
        t_2 = torch.nn.functional.softmax(t_2, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_2.type_as(t_0)
        t_0 = self.l_1(t_0)
        t_0 = torch.matmul(t_0, x4)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.contiguous()
        t_0 = t_0.view(x3, -1, 4096)
        t_0 = self.l_2(t_0)
        t_1 = (t_0, t_1)
        t_0 = t_1[0]
        t_0 = self.l_3(t_0)
        t_0 = x1 + t_0
        t_1 = t_1[1]
        t_1 = (t_0, t_1)
        t_0 = t_1[0]
        t_1 = t_1[1]
        t_2 = self.l_4(t_0)
        t_2 = self.l_5(t_2)
        t_2 = torch.nn.functional.relu(t_2, inplace=False)
        t_2 = self.l_6(t_2)
        t_2 = self.l_7(t_2)
        t_2 = self.l_8(t_2)
        t_2 = t_0 + t_2
        t_1 = (t_2, x2, t_1)
        t_2 = t_1[0]
        t_0 = t_1[1]
        t_1 = t_1[2]
        t_3 = self.l_9(t_2)
        t_4 = t_3.size()
        t_4 = t_4[0]
        t_5 = self.l_10(t_3)
        t_5 = t_5.view(t_4, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_6 = self.l_11(t_3)
        t_6 = t_6.view(t_4, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_3 = self.l_12(t_3)
        t_3 = t_3.view(t_4, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_6 = t_6.transpose(3, 2)
        t_6 = torch.matmul(t_5, t_6)
        t_6 += t_0
        t_5 = t_6.float()
        t_5 = torch.nn.functional.softmax(t_5, dim=-1, _stacklevel=3, dtype=None)
        t_6 = t_5.type_as(t_6)
        t_6 = self.l_13(t_6)
        t_3 = torch.matmul(t_6, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_4 = t_3.view(t_4, -1, 4096)
        t_4 = self.l_14(t_4)
        t_4 = self.l_15(t_4)
        t_4 = t_2 + t_4
        t_2 = self.l_16(t_4)
        t_3 = t_2.size()
        t_3 = t_3[0]
        t_2 = self.l_17(t_2)
        t_2 = t_2.view(t_3, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_6 = x9.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_5 = x10.view(t_3, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_6 = t_6.transpose(3, 2)
        t_6 = torch.matmul(t_2, t_6)
        t_6 += t_1
        t_2 = t_6.float()
        t_2 = torch.nn.functional.softmax(t_2, dim=-1, _stacklevel=3, dtype=None)
        t_6 = t_2.type_as(t_6)
        t_6 = self.l_18(t_6)
        t_5 = torch.matmul(t_6, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_3 = t_5.view(t_3, -1, 4096)
        t_3 = self.l_19(t_3)
        t_3 = self.l_20(t_3)
        t_3 = t_4 + t_3
        t_4 = self.l_21(t_3)
        t_4 = self.l_22(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_23(t_4)
        t_4 = self.l_24(t_4)
        t_4 = self.l_25(t_4)
        t_4 = t_3 + t_4
        t_3 = self.l_26(t_4)
        t_5 = t_3.size()
        t_5 = t_5[0]
        t_6 = self.l_27(t_3)
        t_6 = t_6.view(t_5, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_2 = self.l_28(t_3)
        t_2 = t_2.view(t_5, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = self.l_29(t_3)
        t_3 = t_3.view(t_5, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_6, t_2)
        t_2 += t_0
        t_6 = t_2.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_6.type_as(t_2)
        t_2 = self.l_30(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_5 = t_3.view(t_5, -1, 4096)
        t_5 = self.l_31(t_5)
        t_5 = self.l_32(t_5)
        t_5 = t_4 + t_5
        t_4 = self.l_33(t_5)
        t_3 = t_4.size()
        t_3 = t_3[0]
        t_4 = self.l_34(t_4)
        t_4 = t_4.view(t_3, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_2 = x11.view(t_3, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_6 = x12.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_4, t_2)
        t_2 += t_1
        t_4 = t_2.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_4.type_as(t_2)
        t_2 = self.l_35(t_2)
        t_6 = torch.matmul(t_2, t_6)
        t_6 = t_6.transpose(1, 2)
        t_6 = t_6.contiguous()
        t_3 = t_6.view(t_3, -1, 4096)
        t_3 = self.l_36(t_3)
        t_3 = self.l_37(t_3)
        t_3 = t_5 + t_3
        t_5 = self.l_38(t_3)
        t_5 = self.l_39(t_5)
        t_5 = torch.nn.functional.relu(t_5, inplace=False)
        t_5 = self.l_40(t_5)
        t_5 = self.l_41(t_5)
        t_5 = self.l_42(t_5)
        t_5 = t_3 + t_5
        t_3 = self.l_43(t_5)
        t_6 = t_3.size()
        t_6 = t_6[0]
        t_2 = self.l_44(t_3)
        t_2 = t_2.view(t_6, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_4 = self.l_45(t_3)
        t_4 = t_4.view(t_6, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = self.l_46(t_3)
        t_3 = t_3.view(t_6, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_2, t_4)
        t_4 += t_0
        t_2 = t_4.float()
        t_2 = torch.nn.functional.softmax(t_2, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_2.type_as(t_4)
        t_4 = self.l_47(t_4)
        t_3 = torch.matmul(t_4, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_6 = t_3.view(t_6, -1, 4096)
        t_6 = self.l_48(t_6)
        t_6 = self.l_49(t_6)
        t_6 = t_5 + t_6
        t_5 = self.l_50(t_6)
        t_3 = t_5.size()
        t_3 = t_3[0]
        t_5 = self.l_51(t_5)
        t_5 = t_5.view(t_3, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = x13.view(t_3, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_2 = x14.view(t_3, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_5, t_4)
        t_4 += t_1
        t_5 = t_4.float()
        t_5 = torch.nn.functional.softmax(t_5, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_5.type_as(t_4)
        t_4 = self.l_52(t_4)
        t_2 = torch.matmul(t_4, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_3 = t_2.view(t_3, -1, 4096)
        t_3 = self.l_53(t_3)
        t_3 = self.l_54(t_3)
        t_3 = t_6 + t_3
        t_6 = self.l_55(t_3)
        t_6 = self.l_56(t_6)
        t_6 = torch.nn.functional.relu(t_6, inplace=False)
        t_6 = self.l_57(t_6)
        t_6 = self.l_58(t_6)
        t_6 = self.l_59(t_6)
        t_6 = t_3 + t_6
        t_3 = self.l_60(t_6)
        t_2 = t_3.size()
        t_2 = t_2[0]
        t_4 = self.l_61(t_3)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = self.l_62(t_3)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_3 = self.l_63(t_3)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_5 = t_5.transpose(3, 2)
        t_5 = torch.matmul(t_4, t_5)
        t_5 += t_0
        t_4 = t_5.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_5 = t_4.type_as(t_5)
        t_5 = self.l_64(t_5)
        t_3 = torch.matmul(t_5, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_2 = t_3.view(t_2, -1, 4096)
        t_2 = self.l_65(t_2)
        t_2 = self.l_66(t_2)
        t_2 = t_6 + t_2
        t_6 = self.l_67(x0)
        t_3 = self.l_68(x0)
        t_5 = self.l_69(x0)
        t_4 = self.l_70(x0)
        t_7 = self.l_71(x0)
        t_8 = self.l_72(x0)
        t_9 = self.l_73(x0)
        t_10 = self.l_74(x0)
        # returning:
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1764
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1766
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/Tensor::__add___2147
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]
        return list(flatten((t_0, t_1, t_2, t_6, t_3, t_5, t_4, t_7, t_8, t_9, t_10)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition12(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:12'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.4.1.layer_norm',
                        'l_1': 'decoder.4.1.EncDecAttention.q',
                        'l_2': 'decoder.4.1.EncDecAttention.dropout',
                        'l_3': 'decoder.4.1.EncDecAttention.o',
                        'l_4': 'decoder.4.1.dropout',
                        'l_5': 'decoder.4.2.layer_norm',
                        'l_6': 'decoder.4.2.DenseReluDense.wi',
                        'l_7': 'decoder.4.2.DenseReluDense.dropout',
                        'l_8': 'decoder.4.2.DenseReluDense.wo',
                        'l_9': 'decoder.4.2.dropout',
                        'l_10': 'decoder.5.0.layer_norm',
                        'l_11': 'decoder.5.0.SelfAttention.q',
                        'l_12': 'decoder.5.0.SelfAttention.k',
                        'l_13': 'decoder.5.0.SelfAttention.v',
                        'l_14': 'decoder.5.0.SelfAttention.dropout',
                        'l_15': 'decoder.5.0.SelfAttention.o',
                        'l_16': 'decoder.5.0.dropout',
                        'l_17': 'decoder.5.1.layer_norm',
                        'l_18': 'decoder.5.1.EncDecAttention.q',
                        'l_19': 'decoder.5.1.EncDecAttention.dropout',
                        'l_20': 'decoder.5.1.EncDecAttention.o',
                        'l_21': 'decoder.5.1.dropout',
                        'l_22': 'decoder.5.2.layer_norm',
                        'l_23': 'decoder.5.2.DenseReluDense.wi',
                        'l_24': 'decoder.5.2.DenseReluDense.dropout',
                        'l_25': 'decoder.5.2.DenseReluDense.wo',
                        'l_26': 'decoder.5.2.dropout',
                        'l_27': 'decoder.6.0.layer_norm',
                        'l_28': 'decoder.6.0.SelfAttention.q',
                        'l_29': 'decoder.6.0.SelfAttention.k',
                        'l_30': 'decoder.6.0.SelfAttention.v',
                        'l_31': 'decoder.6.0.SelfAttention.dropout',
                        'l_32': 'decoder.6.0.SelfAttention.o',
                        'l_33': 'decoder.6.0.dropout',
                        'l_34': 'decoder.6.1.layer_norm',
                        'l_35': 'decoder.6.1.EncDecAttention.q',
                        'l_36': 'decoder.6.1.EncDecAttention.dropout',
                        'l_37': 'decoder.6.1.EncDecAttention.o',
                        'l_38': 'decoder.6.1.dropout',
                        'l_39': 'decoder.6.2.layer_norm',
                        'l_40': 'decoder.6.2.DenseReluDense.wi',
                        'l_41': 'decoder.6.2.DenseReluDense.dropout',
                        'l_42': 'decoder.6.2.DenseReluDense.wo',
                        'l_43': 'decoder.6.2.dropout',
                        'l_44': 'decoder.7.0.layer_norm',
                        'l_45': 'decoder.7.0.SelfAttention.q',
                        'l_46': 'decoder.7.0.SelfAttention.k',
                        'l_47': 'decoder.7.0.SelfAttention.v',
                        'l_48': 'decoder.7.0.SelfAttention.dropout',
                        'l_49': 'decoder.7.0.SelfAttention.o',
                        'l_50': 'decoder.7.0.dropout',
                        'l_51': 'decoder.7.1.layer_norm',
                        'l_52': 'decoder.7.1.EncDecAttention.q',
                        'l_53': 'decoder.7.1.EncDecAttention.dropout',
                        'l_54': 'decoder.7.1.EncDecAttention.o',
                        'l_55': 'decoder.7.1.dropout',
                        'l_56': 'decoder.7.2.layer_norm',
                        'l_57': 'decoder.7.2.DenseReluDense.wi',
                        'l_58': 'decoder.7.2.DenseReluDense.dropout',
                        'l_59': 'decoder.7.2.DenseReluDense.wo',
                        'l_60': 'decoder.7.2.dropout',
                        'l_61': 'decoder.8.0.layer_norm',
                        'l_62': 'decoder.8.0.SelfAttention.q',
                        'l_63': 'decoder.8.0.SelfAttention.k',
                        'l_64': 'decoder.8.0.SelfAttention.v',
                        'l_65': 'decoder.8.0.SelfAttention.dropout',
                        'l_66': 'decoder.8.0.SelfAttention.o',
                        'l_67': 'decoder.8.0.dropout',
                        'l_68': 'decoder.8.1.layer_norm',
                        'l_69': 'decoder.8.1.EncDecAttention.q',
                        'l_70': 'decoder.8.1.EncDecAttention.dropout',
                        'l_71': 'decoder.8.1.EncDecAttention.o',
                        'l_72': 'decoder.8.1.dropout',
                        'l_73': 'decoder.8.2.layer_norm',
                        'l_74': 'decoder.8.2.DenseReluDense.wi',
                        'l_75': 'decoder.8.2.DenseReluDense.dropout',
                        'l_76': 'decoder.8.2.DenseReluDense.wo',
                        'l_77': 'decoder.8.2.dropout',
                        'l_78': 'decoder.9.0.layer_norm',
                        'l_79': 'decoder.9.0.SelfAttention.k'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/Dropout[dropout] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/Dropout[dropout] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_50
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_51
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_52
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_53
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_54
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_55
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_56
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_57
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_58
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_59
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/Dropout[dropout] <=> self.l_60
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_61
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_62
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_63
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_64
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_65
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_66
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_67
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_68
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_69
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_70
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_71
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_72
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_73
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_74
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_75
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_76
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/Dropout[dropout] <=> self.l_77
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_78
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_79
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1764 <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1766 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/Tensor::__add___2147 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x9
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x10
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x11
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x12

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x2)
        t_1 = t_0.size()
        t_1 = t_1[0]
        t_0 = self.l_1(t_0)
        t_0 = t_0.view(t_1, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_2 = x3.view(t_1, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = x4.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_0, t_2)
        t_2 += x1
        t_0 = t_2.float()
        t_0 = torch.nn.functional.softmax(t_0, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_0.type_as(t_2)
        t_2 = self.l_2(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_1 = t_3.view(t_1, -1, 4096)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = x2 + t_1
        t_3 = self.l_5(t_1)
        t_3 = self.l_6(t_3)
        t_3 = torch.nn.functional.relu(t_3, inplace=False)
        t_3 = self.l_7(t_3)
        t_3 = self.l_8(t_3)
        t_3 = self.l_9(t_3)
        t_3 = t_1 + t_3
        t_1 = self.l_10(t_3)
        t_2 = t_1.size()
        t_2 = t_2[0]
        t_0 = self.l_11(t_1)
        t_0 = t_0.view(t_2, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_4 = self.l_12(t_1)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_1 = self.l_13(t_1)
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_0, t_4)
        t_4 += x0
        t_0 = t_4.float()
        t_0 = torch.nn.functional.softmax(t_0, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_0.type_as(t_4)
        t_4 = self.l_14(t_4)
        t_1 = torch.matmul(t_4, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_2 = t_1.view(t_2, -1, 4096)
        t_2 = self.l_15(t_2)
        t_2 = self.l_16(t_2)
        t_2 = t_3 + t_2
        t_3 = self.l_17(t_2)
        t_1 = t_3.size()
        t_1 = t_1[0]
        t_3 = self.l_18(t_3)
        t_3 = t_3.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = x5.view(t_1, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_0 = x6.view(t_1, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x1
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_19(t_4)
        t_0 = torch.matmul(t_4, t_0)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.contiguous()
        t_1 = t_0.view(t_1, -1, 4096)
        t_1 = self.l_20(t_1)
        t_1 = self.l_21(t_1)
        t_1 = t_2 + t_1
        t_2 = self.l_22(t_1)
        t_2 = self.l_23(t_2)
        t_2 = torch.nn.functional.relu(t_2, inplace=False)
        t_2 = self.l_24(t_2)
        t_2 = self.l_25(t_2)
        t_2 = self.l_26(t_2)
        t_2 = t_1 + t_2
        t_1 = self.l_27(t_2)
        t_0 = t_1.size()
        t_0 = t_0[0]
        t_4 = self.l_28(t_1)
        t_4 = t_4.view(t_0, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = self.l_29(t_1)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_1 = self.l_30(t_1)
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_4, t_3)
        t_3 += x0
        t_4 = t_3.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_4.type_as(t_3)
        t_3 = self.l_31(t_3)
        t_1 = torch.matmul(t_3, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_0 = t_1.view(t_0, -1, 4096)
        t_0 = self.l_32(t_0)
        t_0 = self.l_33(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_34(t_0)
        t_1 = t_2.size()
        t_1 = t_1[0]
        t_2 = self.l_35(t_2)
        t_2 = t_2.view(t_1, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = x7.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = x8.view(t_1, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 += x1
        t_2 = t_3.float()
        t_2 = torch.nn.functional.softmax(t_2, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_2.type_as(t_3)
        t_3 = self.l_36(t_3)
        t_4 = torch.matmul(t_3, t_4)
        t_4 = t_4.transpose(1, 2)
        t_4 = t_4.contiguous()
        t_1 = t_4.view(t_1, -1, 4096)
        t_1 = self.l_37(t_1)
        t_1 = self.l_38(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_39(t_1)
        t_0 = self.l_40(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_41(t_0)
        t_0 = self.l_42(t_0)
        t_0 = self.l_43(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_44(t_0)
        t_4 = t_1.size()
        t_4 = t_4[0]
        t_3 = self.l_45(t_1)
        t_3 = t_3.view(t_4, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = self.l_46(t_1)
        t_2 = t_2.view(t_4, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_1 = self.l_47(t_1)
        t_1 = t_1.view(t_4, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_3, t_2)
        t_2 += x0
        t_3 = t_2.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_3.type_as(t_2)
        t_2 = self.l_48(t_2)
        t_1 = torch.matmul(t_2, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_4 = t_1.view(t_4, -1, 4096)
        t_4 = self.l_49(t_4)
        t_4 = self.l_50(t_4)
        t_4 = t_0 + t_4
        t_0 = self.l_51(t_4)
        t_1 = t_0.size()
        t_1 = t_1[0]
        t_0 = self.l_52(t_0)
        t_0 = t_0.view(t_1, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_2 = x9.view(t_1, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = x10.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_0, t_2)
        t_2 += x1
        t_0 = t_2.float()
        t_0 = torch.nn.functional.softmax(t_0, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_0.type_as(t_2)
        t_2 = self.l_53(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_1 = t_3.view(t_1, -1, 4096)
        t_1 = self.l_54(t_1)
        t_1 = self.l_55(t_1)
        t_1 = t_4 + t_1
        t_4 = self.l_56(t_1)
        t_4 = self.l_57(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_58(t_4)
        t_4 = self.l_59(t_4)
        t_4 = self.l_60(t_4)
        t_4 = t_1 + t_4
        t_1 = self.l_61(t_4)
        t_3 = t_1.size()
        t_3 = t_3[0]
        t_2 = self.l_62(t_1)
        t_2 = t_2.view(t_3, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_0 = self.l_63(t_1)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = self.l_64(t_1)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_2, t_0)
        t_0 += x0
        t_2 = t_0.float()
        t_2 = torch.nn.functional.softmax(t_2, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_2.type_as(t_0)
        t_0 = self.l_65(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_66(t_3)
        t_3 = self.l_67(t_3)
        t_3 = t_4 + t_3
        t_4 = self.l_68(t_3)
        t_1 = t_4.size()
        t_1 = t_1[0]
        t_4 = self.l_69(t_4)
        t_4 = t_4.view(t_1, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_0 = x11.view(t_1, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_2 = x12.view(t_1, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_4, t_0)
        t_0 += x1
        t_4 = t_0.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_4.type_as(t_0)
        t_0 = self.l_70(t_0)
        t_2 = torch.matmul(t_0, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_1 = t_2.view(t_1, -1, 4096)
        t_1 = self.l_71(t_1)
        t_1 = self.l_72(t_1)
        t_1 = t_3 + t_1
        t_3 = self.l_73(t_1)
        t_3 = self.l_74(t_3)
        t_3 = torch.nn.functional.relu(t_3, inplace=False)
        t_3 = self.l_75(t_3)
        t_3 = self.l_76(t_3)
        t_3 = self.l_77(t_3)
        t_3 = t_1 + t_3
        t_1 = self.l_78(t_3)
        t_2 = t_1.size()
        t_2 = t_2[0]
        t_0 = self.l_79(t_1)
        # returning:
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/Tensor::__add___2646
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___2650
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]
        return list(flatten((t_3, t_1, t_2, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition13(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:13'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.9.0.SelfAttention.q',
                        'l_1': 'decoder.9.0.SelfAttention.v',
                        'l_2': 'decoder.9.0.SelfAttention.dropout',
                        'l_3': 'decoder.9.0.SelfAttention.o',
                        'l_4': 'decoder.9.0.dropout',
                        'l_5': 'decoder.9.1.layer_norm',
                        'l_6': 'decoder.9.1.EncDecAttention.q',
                        'l_7': 'decoder.9.1.EncDecAttention.dropout',
                        'l_8': 'decoder.9.1.EncDecAttention.o',
                        'l_9': 'decoder.9.1.dropout',
                        'l_10': 'decoder.9.2.layer_norm',
                        'l_11': 'decoder.9.2.DenseReluDense.wi',
                        'l_12': 'decoder.9.2.DenseReluDense.dropout',
                        'l_13': 'decoder.9.2.DenseReluDense.wo',
                        'l_14': 'decoder.9.2.dropout',
                        'l_15': 'decoder.10.0.layer_norm',
                        'l_16': 'decoder.10.0.SelfAttention.q',
                        'l_17': 'decoder.10.0.SelfAttention.k',
                        'l_18': 'decoder.10.0.SelfAttention.v',
                        'l_19': 'decoder.10.0.SelfAttention.dropout',
                        'l_20': 'decoder.10.0.SelfAttention.o',
                        'l_21': 'decoder.10.0.dropout',
                        'l_22': 'decoder.10.1.layer_norm',
                        'l_23': 'decoder.10.1.EncDecAttention.q',
                        'l_24': 'decoder.10.1.EncDecAttention.dropout',
                        'l_25': 'decoder.10.1.EncDecAttention.o',
                        'l_26': 'decoder.10.1.dropout',
                        'l_27': 'decoder.10.2.layer_norm',
                        'l_28': 'decoder.10.2.DenseReluDense.wi',
                        'l_29': 'decoder.10.2.DenseReluDense.dropout',
                        'l_30': 'decoder.10.2.DenseReluDense.wo',
                        'l_31': 'decoder.10.2.dropout',
                        'l_32': 'decoder.11.0.layer_norm',
                        'l_33': 'decoder.11.0.SelfAttention.q',
                        'l_34': 'decoder.11.0.SelfAttention.k',
                        'l_35': 'decoder.11.0.SelfAttention.v',
                        'l_36': 'decoder.11.0.SelfAttention.dropout',
                        'l_37': 'decoder.11.0.SelfAttention.o',
                        'l_38': 'decoder.11.0.dropout',
                        'l_39': 'decoder.11.1.layer_norm',
                        'l_40': 'decoder.11.1.EncDecAttention.q',
                        'l_41': 'decoder.11.1.EncDecAttention.dropout',
                        'l_42': 'decoder.11.1.EncDecAttention.o',
                        'l_43': 'decoder.11.1.dropout',
                        'l_44': 'decoder.11.2.layer_norm',
                        'l_45': 'decoder.11.2.DenseReluDense.wi',
                        'l_46': 'decoder.11.2.DenseReluDense.dropout',
                        'l_47': 'decoder.11.2.DenseReluDense.wo',
                        'l_48': 'decoder.11.2.dropout',
                        'l_49': 'decoder.12.0.layer_norm',
                        'l_50': 'decoder.12.0.SelfAttention.q',
                        'l_51': 'decoder.12.0.SelfAttention.k',
                        'l_52': 'decoder.12.0.SelfAttention.v',
                        'l_53': 'decoder.12.0.SelfAttention.dropout',
                        'l_54': 'decoder.12.0.SelfAttention.o',
                        'l_55': 'decoder.12.0.dropout',
                        'l_56': 'decoder.12.1.layer_norm',
                        'l_57': 'decoder.12.1.EncDecAttention.q',
                        'l_58': 'decoder.12.1.EncDecAttention.dropout',
                        'l_59': 'decoder.12.1.EncDecAttention.o',
                        'l_60': 'decoder.12.1.dropout',
                        'l_61': 'decoder.12.2.layer_norm',
                        'l_62': 'decoder.12.2.DenseReluDense.wi',
                        'l_63': 'decoder.12.2.DenseReluDense.dropout',
                        'l_64': 'decoder.12.2.DenseReluDense.wo',
                        'l_65': 'decoder.12.2.dropout',
                        'l_66': 'decoder.13.0.layer_norm',
                        'l_67': 'decoder.13.0.SelfAttention.q',
                        'l_68': 'decoder.13.0.SelfAttention.k',
                        'l_69': 'decoder.13.0.SelfAttention.v',
                        'l_70': 'decoder.13.0.SelfAttention.dropout',
                        'l_71': 'decoder.13.0.SelfAttention.o',
                        'l_72': 'decoder.13.0.dropout',
                        'l_73': 'decoder.13.1.layer_norm',
                        'l_74': 'decoder.13.1.EncDecAttention.q',
                        'l_75': 'decoder.13.1.EncDecAttention.dropout',
                        'l_76': 'decoder.13.1.EncDecAttention.o',
                        'l_77': 'decoder.13.1.dropout',
                        'l_78': 'decoder.13.2.layer_norm',
                        'l_79': 'decoder.13.2.DenseReluDense.wi'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/Dropout[dropout] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_50
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_51
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_52
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_53
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_54
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_55
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_56
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_57
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_58
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_59
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_60
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_61
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_62
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_63
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_64
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/Dropout[dropout] <=> self.l_65
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_66
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_67
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_68
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_69
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_70
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_71
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_72
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_73
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_74
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_75
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_76
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_77
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_78
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_79
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1764 <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1766 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/Tensor::__add___2646 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Size::__getitem___2650 <=> x4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> x5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x9
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x10
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x11
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x12
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x13
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x14
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x15

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3)
        t_0 = t_0.view(x4, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = x5.view(x4, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = self.l_1(x3)
        t_2 = t_2.view(x4, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_0, t_1)
        t_1 += x0
        t_0 = t_1.float()
        t_0 = torch.nn.functional.softmax(t_0, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_0.type_as(t_1)
        t_1 = self.l_2(t_1)
        t_2 = torch.matmul(t_1, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_2 = t_2.view(x4, -1, 4096)
        t_2 = self.l_3(t_2)
        t_2 = self.l_4(t_2)
        t_2 = x2 + t_2
        t_1 = self.l_5(t_2)
        t_0 = t_1.size()
        t_0 = t_0[0]
        t_1 = self.l_6(t_1)
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_3 = x6.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = x7.view(t_0, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_1, t_3)
        t_3 += x1
        t_1 = t_3.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_1.type_as(t_3)
        t_3 = self.l_7(t_3)
        t_4 = torch.matmul(t_3, t_4)
        t_4 = t_4.transpose(1, 2)
        t_4 = t_4.contiguous()
        t_0 = t_4.view(t_0, -1, 4096)
        t_0 = self.l_8(t_0)
        t_0 = self.l_9(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_10(t_0)
        t_2 = self.l_11(t_2)
        t_2 = torch.nn.functional.relu(t_2, inplace=False)
        t_2 = self.l_12(t_2)
        t_2 = self.l_13(t_2)
        t_2 = self.l_14(t_2)
        t_2 = t_0 + t_2
        t_0 = self.l_15(t_2)
        t_4 = t_0.size()
        t_4 = t_4[0]
        t_3 = self.l_16(t_0)
        t_3 = t_3.view(t_4, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_1 = self.l_17(t_0)
        t_1 = t_1.view(t_4, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = self.l_18(t_0)
        t_0 = t_0.view(t_4, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_3, t_1)
        t_1 += x0
        t_3 = t_1.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_3.type_as(t_1)
        t_1 = self.l_19(t_1)
        t_0 = torch.matmul(t_1, t_0)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.contiguous()
        t_4 = t_0.view(t_4, -1, 4096)
        t_4 = self.l_20(t_4)
        t_4 = self.l_21(t_4)
        t_4 = t_2 + t_4
        t_2 = self.l_22(t_4)
        t_0 = t_2.size()
        t_0 = t_0[0]
        t_2 = self.l_23(t_2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_1 = x8.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_3 = x9.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_2, t_1)
        t_1 += x1
        t_2 = t_1.float()
        t_2 = torch.nn.functional.softmax(t_2, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_2.type_as(t_1)
        t_1 = self.l_24(t_1)
        t_3 = torch.matmul(t_1, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_25(t_0)
        t_0 = self.l_26(t_0)
        t_0 = t_4 + t_0
        t_4 = self.l_27(t_0)
        t_4 = self.l_28(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_29(t_4)
        t_4 = self.l_30(t_4)
        t_4 = self.l_31(t_4)
        t_4 = t_0 + t_4
        t_0 = self.l_32(t_4)
        t_3 = t_0.size()
        t_3 = t_3[0]
        t_1 = self.l_33(t_0)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = self.l_34(t_0)
        t_2 = t_2.view(t_3, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_0 = self.l_35(t_0)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x0
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_36(t_2)
        t_0 = torch.matmul(t_2, t_0)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.contiguous()
        t_3 = t_0.view(t_3, -1, 4096)
        t_3 = self.l_37(t_3)
        t_3 = self.l_38(t_3)
        t_3 = t_4 + t_3
        t_4 = self.l_39(t_3)
        t_0 = t_4.size()
        t_0 = t_0[0]
        t_4 = self.l_40(t_4)
        t_4 = t_4.view(t_0, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_2 = x10.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_1 = x11.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_4, t_2)
        t_2 += x1
        t_4 = t_2.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_4.type_as(t_2)
        t_2 = self.l_41(t_2)
        t_1 = torch.matmul(t_2, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_0 = t_1.view(t_0, -1, 4096)
        t_0 = self.l_42(t_0)
        t_0 = self.l_43(t_0)
        t_0 = t_3 + t_0
        t_3 = self.l_44(t_0)
        t_3 = self.l_45(t_3)
        t_3 = torch.nn.functional.relu(t_3, inplace=False)
        t_3 = self.l_46(t_3)
        t_3 = self.l_47(t_3)
        t_3 = self.l_48(t_3)
        t_3 = t_0 + t_3
        t_0 = self.l_49(t_3)
        t_1 = t_0.size()
        t_1 = t_1[0]
        t_2 = self.l_50(t_0)
        t_2 = t_2.view(t_1, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_4 = self.l_51(t_0)
        t_4 = t_4.view(t_1, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_0 = self.l_52(t_0)
        t_0 = t_0.view(t_1, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_2, t_4)
        t_4 += x0
        t_2 = t_4.float()
        t_2 = torch.nn.functional.softmax(t_2, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_2.type_as(t_4)
        t_4 = self.l_53(t_4)
        t_0 = torch.matmul(t_4, t_0)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.contiguous()
        t_1 = t_0.view(t_1, -1, 4096)
        t_1 = self.l_54(t_1)
        t_1 = self.l_55(t_1)
        t_1 = t_3 + t_1
        t_3 = self.l_56(t_1)
        t_0 = t_3.size()
        t_0 = t_0[0]
        t_3 = self.l_57(t_3)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = x12.view(t_0, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_2 = x13.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x1
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_58(t_4)
        t_2 = torch.matmul(t_4, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_0 = t_2.view(t_0, -1, 4096)
        t_0 = self.l_59(t_0)
        t_0 = self.l_60(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_61(t_0)
        t_1 = self.l_62(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_63(t_1)
        t_1 = self.l_64(t_1)
        t_1 = self.l_65(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_66(t_1)
        t_2 = t_0.size()
        t_2 = t_2[0]
        t_4 = self.l_67(t_0)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = self.l_68(t_0)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_0 = self.l_69(t_0)
        t_0 = t_0.view(t_2, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_4, t_3)
        t_3 += x0
        t_4 = t_3.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_4.type_as(t_3)
        t_3 = self.l_70(t_3)
        t_0 = torch.matmul(t_3, t_0)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.contiguous()
        t_2 = t_0.view(t_2, -1, 4096)
        t_2 = self.l_71(t_2)
        t_2 = self.l_72(t_2)
        t_2 = t_1 + t_2
        t_1 = self.l_73(t_2)
        t_0 = t_1.size()
        t_0 = t_0[0]
        t_1 = self.l_74(t_1)
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_3 = x14.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = x15.view(t_0, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_1, t_3)
        t_3 += x1
        t_1 = t_3.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_1.type_as(t_3)
        t_3 = self.l_75(t_3)
        t_4 = torch.matmul(t_3, t_4)
        t_4 = t_4.transpose(1, 2)
        t_4 = t_4.contiguous()
        t_0 = t_4.view(t_0, -1, 4096)
        t_0 = self.l_76(t_0)
        t_0 = self.l_77(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_78(t_0)
        t_2 = self.l_79(t_2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/Tensor::__add___3188
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]
        return list(flatten((t_0, t_2)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition14(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:14'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.13.2.DenseReluDense.dropout',
                        'l_1': 'decoder.13.2.DenseReluDense.wo',
                        'l_2': 'decoder.13.2.dropout',
                        'l_3': 'decoder.14.0.layer_norm',
                        'l_4': 'decoder.14.0.SelfAttention.q',
                        'l_5': 'decoder.14.0.SelfAttention.k',
                        'l_6': 'decoder.14.0.SelfAttention.v',
                        'l_7': 'decoder.14.0.SelfAttention.dropout',
                        'l_8': 'decoder.14.0.SelfAttention.o',
                        'l_9': 'decoder.14.0.dropout',
                        'l_10': 'decoder.14.1.layer_norm',
                        'l_11': 'decoder.14.1.EncDecAttention.q',
                        'l_12': 'decoder.14.1.EncDecAttention.dropout',
                        'l_13': 'decoder.14.1.EncDecAttention.o',
                        'l_14': 'decoder.14.1.dropout',
                        'l_15': 'decoder.14.2.layer_norm',
                        'l_16': 'decoder.14.2.DenseReluDense.wi',
                        'l_17': 'decoder.14.2.DenseReluDense.dropout',
                        'l_18': 'decoder.14.2.DenseReluDense.wo',
                        'l_19': 'decoder.14.2.dropout',
                        'l_20': 'decoder.15.0.layer_norm',
                        'l_21': 'decoder.15.0.SelfAttention.q',
                        'l_22': 'decoder.15.0.SelfAttention.k',
                        'l_23': 'decoder.15.0.SelfAttention.v',
                        'l_24': 'decoder.15.0.SelfAttention.dropout',
                        'l_25': 'decoder.15.0.SelfAttention.o',
                        'l_26': 'decoder.15.0.dropout',
                        'l_27': 'decoder.15.1.layer_norm',
                        'l_28': 'decoder.15.1.EncDecAttention.q',
                        'l_29': 'decoder.15.1.EncDecAttention.dropout',
                        'l_30': 'decoder.15.1.EncDecAttention.o',
                        'l_31': 'decoder.15.1.dropout',
                        'l_32': 'decoder.15.2.layer_norm',
                        'l_33': 'decoder.15.2.DenseReluDense.wi',
                        'l_34': 'decoder.15.2.DenseReluDense.dropout',
                        'l_35': 'decoder.15.2.DenseReluDense.wo',
                        'l_36': 'decoder.15.2.dropout',
                        'l_37': 'decoder.16.0.layer_norm',
                        'l_38': 'decoder.16.0.SelfAttention.q',
                        'l_39': 'decoder.16.0.SelfAttention.k',
                        'l_40': 'decoder.16.0.SelfAttention.v',
                        'l_41': 'decoder.16.0.SelfAttention.dropout',
                        'l_42': 'decoder.16.0.SelfAttention.o',
                        'l_43': 'decoder.16.0.dropout',
                        'l_44': 'decoder.16.1.layer_norm',
                        'l_45': 'decoder.16.1.EncDecAttention.q',
                        'l_46': 'decoder.16.1.EncDecAttention.dropout',
                        'l_47': 'decoder.16.1.EncDecAttention.o',
                        'l_48': 'decoder.16.1.dropout',
                        'l_49': 'decoder.16.2.layer_norm',
                        'l_50': 'decoder.16.2.DenseReluDense.wi',
                        'l_51': 'decoder.16.2.DenseReluDense.dropout',
                        'l_52': 'decoder.16.2.DenseReluDense.wo',
                        'l_53': 'decoder.16.2.dropout',
                        'l_54': 'decoder.17.0.layer_norm',
                        'l_55': 'decoder.17.0.SelfAttention.q',
                        'l_56': 'decoder.17.0.SelfAttention.k',
                        'l_57': 'decoder.17.0.SelfAttention.v',
                        'l_58': 'decoder.17.0.SelfAttention.dropout',
                        'l_59': 'decoder.17.0.SelfAttention.o',
                        'l_60': 'decoder.17.0.dropout',
                        'l_61': 'decoder.17.1.layer_norm',
                        'l_62': 'decoder.17.1.EncDecAttention.q',
                        'l_63': 'decoder.17.1.EncDecAttention.dropout',
                        'l_64': 'decoder.17.1.EncDecAttention.o',
                        'l_65': 'decoder.17.1.dropout',
                        'l_66': 'decoder.17.2.layer_norm',
                        'l_67': 'decoder.17.2.DenseReluDense.wi',
                        'l_68': 'decoder.17.2.DenseReluDense.dropout',
                        'l_69': 'decoder.17.2.DenseReluDense.wo',
                        'l_70': 'decoder.17.2.dropout',
                        'l_71': 'decoder.18.0.layer_norm',
                        'l_72': 'decoder.18.0.SelfAttention.q',
                        'l_73': 'decoder.18.0.SelfAttention.k',
                        'l_74': 'decoder.18.0.SelfAttention.v',
                        'l_75': 'decoder.18.0.SelfAttention.dropout',
                        'l_76': 'decoder.18.0.SelfAttention.o',
                        'l_77': 'decoder.18.0.dropout',
                        'l_78': 'decoder.18.1.layer_norm',
                        'l_79': 'decoder.18.1.EncDecAttention.q',
                        'l_80': 'decoder.18.1.EncDecAttention.dropout',
                        'l_81': 'decoder.18.1.EncDecAttention.o',
                        'l_82': 'decoder.18.1.dropout',
                        'l_83': 'decoder.18.2.layer_norm',
                        'l_84': 'decoder.18.2.DenseReluDense.wi',
                        'l_85': 'decoder.18.2.DenseReluDense.dropout',
                        'l_86': 'decoder.18.2.DenseReluDense.wo',
                        'l_87': 'decoder.18.2.dropout',
                        'l_88': 'decoder.19.0.layer_norm',
                        'l_89': 'decoder.19.0.SelfAttention.v'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/Dropout[dropout] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_50
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_51
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_52
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/Dropout[dropout] <=> self.l_53
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_54
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_55
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_56
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_57
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_58
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_59
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_60
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_61
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_62
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_63
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_64
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_65
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_66
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_67
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_68
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_69
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/Dropout[dropout] <=> self.l_70
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_71
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_72
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_73
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_74
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_75
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_76
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_77
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_78
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_79
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_80
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_81
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_82
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_83
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_84
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_85
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_86
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/Dropout[dropout] <=> self.l_87
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_88
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_89
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1764 <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1766 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/Tensor::__add___3188 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x9
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x10
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x11
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x12
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x13

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = torch.nn.functional.relu(x3, inplace=False)
        t_0 = self.l_0(t_0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = x2 + t_0
        t_1 = self.l_3(t_0)
        t_2 = t_1.size()
        t_2 = t_2[0]
        t_3 = self.l_4(t_1)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = self.l_5(t_1)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_1 = self.l_6(t_1)
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x0
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_7(t_4)
        t_1 = torch.matmul(t_4, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_2 = t_1.view(t_2, -1, 4096)
        t_2 = self.l_8(t_2)
        t_2 = self.l_9(t_2)
        t_2 = t_0 + t_2
        t_0 = self.l_10(t_2)
        t_1 = t_0.size()
        t_1 = t_1[0]
        t_0 = self.l_11(t_0)
        t_0 = t_0.view(t_1, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_4 = x4.view(t_1, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = x5.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_0, t_4)
        t_4 += x1
        t_0 = t_4.float()
        t_0 = torch.nn.functional.softmax(t_0, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_0.type_as(t_4)
        t_4 = self.l_12(t_4)
        t_3 = torch.matmul(t_4, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_1 = t_3.view(t_1, -1, 4096)
        t_1 = self.l_13(t_1)
        t_1 = self.l_14(t_1)
        t_1 = t_2 + t_1
        t_2 = self.l_15(t_1)
        t_2 = self.l_16(t_2)
        t_2 = torch.nn.functional.relu(t_2, inplace=False)
        t_2 = self.l_17(t_2)
        t_2 = self.l_18(t_2)
        t_2 = self.l_19(t_2)
        t_2 = t_1 + t_2
        t_1 = self.l_20(t_2)
        t_3 = t_1.size()
        t_3 = t_3[0]
        t_4 = self.l_21(t_1)
        t_4 = t_4.view(t_3, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_0 = self.l_22(t_1)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = self.l_23(t_1)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_4, t_0)
        t_0 += x0
        t_4 = t_0.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_4.type_as(t_0)
        t_0 = self.l_24(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_25(t_3)
        t_3 = self.l_26(t_3)
        t_3 = t_2 + t_3
        t_2 = self.l_27(t_3)
        t_1 = t_2.size()
        t_1 = t_1[0]
        t_2 = self.l_28(t_2)
        t_2 = t_2.view(t_1, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_0 = x6.view(t_1, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_4 = x7.view(t_1, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_2, t_0)
        t_0 += x1
        t_2 = t_0.float()
        t_2 = torch.nn.functional.softmax(t_2, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_2.type_as(t_0)
        t_0 = self.l_29(t_0)
        t_4 = torch.matmul(t_0, t_4)
        t_4 = t_4.transpose(1, 2)
        t_4 = t_4.contiguous()
        t_1 = t_4.view(t_1, -1, 4096)
        t_1 = self.l_30(t_1)
        t_1 = self.l_31(t_1)
        t_1 = t_3 + t_1
        t_3 = self.l_32(t_1)
        t_3 = self.l_33(t_3)
        t_3 = torch.nn.functional.relu(t_3, inplace=False)
        t_3 = self.l_34(t_3)
        t_3 = self.l_35(t_3)
        t_3 = self.l_36(t_3)
        t_3 = t_1 + t_3
        t_1 = self.l_37(t_3)
        t_4 = t_1.size()
        t_4 = t_4[0]
        t_0 = self.l_38(t_1)
        t_0 = t_0.view(t_4, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_2 = self.l_39(t_1)
        t_2 = t_2.view(t_4, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_1 = self.l_40(t_1)
        t_1 = t_1.view(t_4, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_0, t_2)
        t_2 += x0
        t_0 = t_2.float()
        t_0 = torch.nn.functional.softmax(t_0, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_0.type_as(t_2)
        t_2 = self.l_41(t_2)
        t_1 = torch.matmul(t_2, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_4 = t_1.view(t_4, -1, 4096)
        t_4 = self.l_42(t_4)
        t_4 = self.l_43(t_4)
        t_4 = t_3 + t_4
        t_3 = self.l_44(t_4)
        t_1 = t_3.size()
        t_1 = t_1[0]
        t_3 = self.l_45(t_3)
        t_3 = t_3.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = x8.view(t_1, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_0 = x9.view(t_1, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_3, t_2)
        t_2 += x1
        t_3 = t_2.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_3.type_as(t_2)
        t_2 = self.l_46(t_2)
        t_0 = torch.matmul(t_2, t_0)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.contiguous()
        t_1 = t_0.view(t_1, -1, 4096)
        t_1 = self.l_47(t_1)
        t_1 = self.l_48(t_1)
        t_1 = t_4 + t_1
        t_4 = self.l_49(t_1)
        t_4 = self.l_50(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_51(t_4)
        t_4 = self.l_52(t_4)
        t_4 = self.l_53(t_4)
        t_4 = t_1 + t_4
        t_1 = self.l_54(t_4)
        t_0 = t_1.size()
        t_0 = t_0[0]
        t_2 = self.l_55(t_1)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = self.l_56(t_1)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_1 = self.l_57(t_1)
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 += x0
        t_2 = t_3.float()
        t_2 = torch.nn.functional.softmax(t_2, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_2.type_as(t_3)
        t_3 = self.l_58(t_3)
        t_1 = torch.matmul(t_3, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_0 = t_1.view(t_0, -1, 4096)
        t_0 = self.l_59(t_0)
        t_0 = self.l_60(t_0)
        t_0 = t_4 + t_0
        t_4 = self.l_61(t_0)
        t_1 = t_4.size()
        t_1 = t_1[0]
        t_4 = self.l_62(t_4)
        t_4 = t_4.view(t_1, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = x10.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = x11.view(t_1, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_4, t_3)
        t_3 += x1
        t_4 = t_3.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_4.type_as(t_3)
        t_3 = self.l_63(t_3)
        t_2 = torch.matmul(t_3, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_1 = t_2.view(t_1, -1, 4096)
        t_1 = self.l_64(t_1)
        t_1 = self.l_65(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_66(t_1)
        t_0 = self.l_67(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_68(t_0)
        t_0 = self.l_69(t_0)
        t_0 = self.l_70(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_71(t_0)
        t_2 = t_1.size()
        t_2 = t_2[0]
        t_3 = self.l_72(t_1)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = self.l_73(t_1)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_1 = self.l_74(t_1)
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x0
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_75(t_4)
        t_1 = torch.matmul(t_4, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_2 = t_1.view(t_2, -1, 4096)
        t_2 = self.l_76(t_2)
        t_2 = self.l_77(t_2)
        t_2 = t_0 + t_2
        t_0 = self.l_78(t_2)
        t_1 = t_0.size()
        t_1 = t_1[0]
        t_0 = self.l_79(t_0)
        t_0 = t_0.view(t_1, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_4 = x12.view(t_1, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_3 = x13.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_0, t_4)
        t_4 += x1
        t_0 = t_4.float()
        t_0 = torch.nn.functional.softmax(t_0, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_0.type_as(t_4)
        t_4 = self.l_80(t_4)
        t_3 = torch.matmul(t_4, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_1 = t_3.view(t_1, -1, 4096)
        t_1 = self.l_81(t_1)
        t_1 = self.l_82(t_1)
        t_1 = t_2 + t_1
        t_2 = self.l_83(t_1)
        t_2 = self.l_84(t_2)
        t_2 = torch.nn.functional.relu(t_2, inplace=False)
        t_2 = self.l_85(t_2)
        t_2 = self.l_86(t_2)
        t_2 = self.l_87(t_2)
        t_2 = t_1 + t_2
        t_1 = self.l_88(t_2)
        t_3 = self.l_89(t_1)
        # returning:
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/Tensor::__add___3746
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]
        return list(flatten((t_2, t_1, t_3)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition15(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5LayerNorm[final_layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/Linear[lm_head]',
            'T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:15'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.19.0.SelfAttention.q',
                        'l_1': 'decoder.19.0.SelfAttention.k',
                        'l_2': 'decoder.19.0.SelfAttention.dropout',
                        'l_3': 'decoder.19.0.SelfAttention.o',
                        'l_4': 'decoder.19.0.dropout',
                        'l_5': 'decoder.19.1.layer_norm',
                        'l_6': 'decoder.19.1.EncDecAttention.q',
                        'l_7': 'decoder.19.1.EncDecAttention.dropout',
                        'l_8': 'decoder.19.1.EncDecAttention.o',
                        'l_9': 'decoder.19.1.dropout',
                        'l_10': 'decoder.19.2.layer_norm',
                        'l_11': 'decoder.19.2.DenseReluDense.wi',
                        'l_12': 'decoder.19.2.DenseReluDense.dropout',
                        'l_13': 'decoder.19.2.DenseReluDense.wo',
                        'l_14': 'decoder.19.2.dropout',
                        'l_15': 'decoder.20.0.layer_norm',
                        'l_16': 'decoder.20.0.SelfAttention.q',
                        'l_17': 'decoder.20.0.SelfAttention.k',
                        'l_18': 'decoder.20.0.SelfAttention.v',
                        'l_19': 'decoder.20.0.SelfAttention.dropout',
                        'l_20': 'decoder.20.0.SelfAttention.o',
                        'l_21': 'decoder.20.0.dropout',
                        'l_22': 'decoder.20.1.layer_norm',
                        'l_23': 'decoder.20.1.EncDecAttention.q',
                        'l_24': 'decoder.20.1.EncDecAttention.dropout',
                        'l_25': 'decoder.20.1.EncDecAttention.o',
                        'l_26': 'decoder.20.1.dropout',
                        'l_27': 'decoder.20.2.layer_norm',
                        'l_28': 'decoder.20.2.DenseReluDense.wi',
                        'l_29': 'decoder.20.2.DenseReluDense.dropout',
                        'l_30': 'decoder.20.2.DenseReluDense.wo',
                        'l_31': 'decoder.20.2.dropout',
                        'l_32': 'decoder.21.0.layer_norm',
                        'l_33': 'decoder.21.0.SelfAttention.q',
                        'l_34': 'decoder.21.0.SelfAttention.k',
                        'l_35': 'decoder.21.0.SelfAttention.v',
                        'l_36': 'decoder.21.0.SelfAttention.dropout',
                        'l_37': 'decoder.21.0.SelfAttention.o',
                        'l_38': 'decoder.21.0.dropout',
                        'l_39': 'decoder.21.1.layer_norm',
                        'l_40': 'decoder.21.1.EncDecAttention.q',
                        'l_41': 'decoder.21.1.EncDecAttention.dropout',
                        'l_42': 'decoder.21.1.EncDecAttention.o',
                        'l_43': 'decoder.21.1.dropout',
                        'l_44': 'decoder.21.2.layer_norm',
                        'l_45': 'decoder.21.2.DenseReluDense.wi',
                        'l_46': 'decoder.21.2.DenseReluDense.dropout',
                        'l_47': 'decoder.21.2.DenseReluDense.wo',
                        'l_48': 'decoder.21.2.dropout',
                        'l_49': 'decoder.22.0.layer_norm',
                        'l_50': 'decoder.22.0.SelfAttention.q',
                        'l_51': 'decoder.22.0.SelfAttention.k',
                        'l_52': 'decoder.22.0.SelfAttention.v',
                        'l_53': 'decoder.22.0.SelfAttention.dropout',
                        'l_54': 'decoder.22.0.SelfAttention.o',
                        'l_55': 'decoder.22.0.dropout',
                        'l_56': 'decoder.22.1.layer_norm',
                        'l_57': 'decoder.22.1.EncDecAttention.q',
                        'l_58': 'decoder.22.1.EncDecAttention.dropout',
                        'l_59': 'decoder.22.1.EncDecAttention.o',
                        'l_60': 'decoder.22.1.dropout',
                        'l_61': 'decoder.22.2.layer_norm',
                        'l_62': 'decoder.22.2.DenseReluDense.wi',
                        'l_63': 'decoder.22.2.DenseReluDense.dropout',
                        'l_64': 'decoder.22.2.DenseReluDense.wo',
                        'l_65': 'decoder.22.2.dropout',
                        'l_66': 'decoder.23.0.layer_norm',
                        'l_67': 'decoder.23.0.SelfAttention.q',
                        'l_68': 'decoder.23.0.SelfAttention.k',
                        'l_69': 'decoder.23.0.SelfAttention.v',
                        'l_70': 'decoder.23.0.SelfAttention.dropout',
                        'l_71': 'decoder.23.0.SelfAttention.o',
                        'l_72': 'decoder.23.0.dropout',
                        'l_73': 'decoder.23.1.layer_norm',
                        'l_74': 'decoder.23.1.EncDecAttention.q',
                        'l_75': 'decoder.23.1.EncDecAttention.dropout',
                        'l_76': 'decoder.23.1.EncDecAttention.o',
                        'l_77': 'decoder.23.1.dropout',
                        'l_78': 'decoder.23.2.layer_norm',
                        'l_79': 'decoder.23.2.DenseReluDense.wi',
                        'l_80': 'decoder.23.2.DenseReluDense.dropout',
                        'l_81': 'decoder.23.2.DenseReluDense.wo',
                        'l_82': 'decoder.23.2.dropout',
                        'l_83': 'decoder.final_layer_norm',
                        'l_84': 'decoder.dropout',
                        'l_85': 'lm_head',
                        'l_86': 'lm_loss'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/Dropout[dropout] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_50
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_51
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_52
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_53
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_54
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_55
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_56
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_57
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_58
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_59
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_60
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_61
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_62
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_63
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_64
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/Dropout[dropout] <=> self.l_65
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_66
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_67
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_68
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_69
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_70
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_71
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_72
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_73
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_74
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_75
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_76
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_77
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_78
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_79
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_80
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_81
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/Dropout[dropout] <=> self.l_82
        # T5ForConditionalGeneration/T5Stack[decoder]/T5LayerNorm[final_layer_norm] <=> self.l_83
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout] <=> self.l_84
        # T5ForConditionalGeneration/Linear[lm_head] <=> self.l_85
        # T5ForConditionalGeneration/CrossEntropyLoss[lm_loss] <=> self.l_86
        # input5 <=> lm_labels
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1764 <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1766 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/Tensor::__add___3746 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> x4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x9
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x10
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x11
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x12
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> x13
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> x14

        # moving inputs to current device no op if already on the correct device
        lm_labels, x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = x3.size()
        t_0 = t_0[0]
        t_1 = self.l_0(x3)
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = self.l_1(x3)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = x4.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x0
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_2(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x2 + t_0
        t_3 = self.l_5(t_0)
        t_2 = t_3.size()
        t_2 = t_2[0]
        t_3 = self.l_6(t_3)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_1 = x5.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = x6.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_3, t_1)
        t_1 += x1
        t_3 = t_1.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_3.type_as(t_1)
        t_1 = self.l_7(t_1)
        t_4 = torch.matmul(t_1, t_4)
        t_4 = t_4.transpose(1, 2)
        t_4 = t_4.contiguous()
        t_2 = t_4.view(t_2, -1, 4096)
        t_2 = self.l_8(t_2)
        t_2 = self.l_9(t_2)
        t_2 = t_0 + t_2
        t_0 = self.l_10(t_2)
        t_0 = self.l_11(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_0 = self.l_14(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_15(t_0)
        t_4 = t_2.size()
        t_4 = t_4[0]
        t_1 = self.l_16(t_2)
        t_1 = t_1.view(t_4, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_3 = self.l_17(t_2)
        t_3 = t_3.view(t_4, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = self.l_18(t_2)
        t_2 = t_2.view(t_4, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_1, t_3)
        t_3 += x0
        t_1 = t_3.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_1.type_as(t_3)
        t_3 = self.l_19(t_3)
        t_2 = torch.matmul(t_3, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_4 = t_2.view(t_4, -1, 4096)
        t_4 = self.l_20(t_4)
        t_4 = self.l_21(t_4)
        t_4 = t_0 + t_4
        t_0 = self.l_22(t_4)
        t_2 = t_0.size()
        t_2 = t_2[0]
        t_0 = self.l_23(t_0)
        t_0 = t_0.view(t_2, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_3 = x7.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_1 = x8.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_0, t_3)
        t_3 += x1
        t_0 = t_3.float()
        t_0 = torch.nn.functional.softmax(t_0, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_0.type_as(t_3)
        t_3 = self.l_24(t_3)
        t_1 = torch.matmul(t_3, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_2 = t_1.view(t_2, -1, 4096)
        t_2 = self.l_25(t_2)
        t_2 = self.l_26(t_2)
        t_2 = t_4 + t_2
        t_4 = self.l_27(t_2)
        t_4 = self.l_28(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_29(t_4)
        t_4 = self.l_30(t_4)
        t_4 = self.l_31(t_4)
        t_4 = t_2 + t_4
        t_2 = self.l_32(t_4)
        t_1 = t_2.size()
        t_1 = t_1[0]
        t_3 = self.l_33(t_2)
        t_3 = t_3.view(t_1, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_0 = self.l_34(t_2)
        t_0 = t_0.view(t_1, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_2 = self.l_35(t_2)
        t_2 = t_2.view(t_1, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_3, t_0)
        t_0 += x0
        t_3 = t_0.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_3.type_as(t_0)
        t_0 = self.l_36(t_0)
        t_2 = torch.matmul(t_0, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_1 = t_2.view(t_1, -1, 4096)
        t_1 = self.l_37(t_1)
        t_1 = self.l_38(t_1)
        t_1 = t_4 + t_1
        t_4 = self.l_39(t_1)
        t_2 = t_4.size()
        t_2 = t_2[0]
        t_4 = self.l_40(t_4)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_0 = x9.view(t_2, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_3 = x10.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_4, t_0)
        t_0 += x1
        t_4 = t_0.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_4.type_as(t_0)
        t_0 = self.l_41(t_0)
        t_3 = torch.matmul(t_0, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_2 = t_3.view(t_2, -1, 4096)
        t_2 = self.l_42(t_2)
        t_2 = self.l_43(t_2)
        t_2 = t_1 + t_2
        t_1 = self.l_44(t_2)
        t_1 = self.l_45(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_46(t_1)
        t_1 = self.l_47(t_1)
        t_1 = self.l_48(t_1)
        t_1 = t_2 + t_1
        t_2 = self.l_49(t_1)
        t_3 = t_2.size()
        t_3 = t_3[0]
        t_0 = self.l_50(t_2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_4 = self.l_51(t_2)
        t_4 = t_4.view(t_3, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_2 = self.l_52(t_2)
        t_2 = t_2.view(t_3, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_0, t_4)
        t_4 += x0
        t_0 = t_4.float()
        t_0 = torch.nn.functional.softmax(t_0, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_0.type_as(t_4)
        t_4 = self.l_53(t_4)
        t_2 = torch.matmul(t_4, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_3 = t_2.view(t_3, -1, 4096)
        t_3 = self.l_54(t_3)
        t_3 = self.l_55(t_3)
        t_3 = t_1 + t_3
        t_1 = self.l_56(t_3)
        t_2 = t_1.size()
        t_2 = t_2[0]
        t_1 = self.l_57(t_1)
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = x11.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_0 = x12.view(t_2, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_1, t_4)
        t_4 += x1
        t_1 = t_4.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_1.type_as(t_4)
        t_4 = self.l_58(t_4)
        t_0 = torch.matmul(t_4, t_0)
        t_0 = t_0.transpose(1, 2)
        t_0 = t_0.contiguous()
        t_2 = t_0.view(t_2, -1, 4096)
        t_2 = self.l_59(t_2)
        t_2 = self.l_60(t_2)
        t_2 = t_3 + t_2
        t_3 = self.l_61(t_2)
        t_3 = self.l_62(t_3)
        t_3 = torch.nn.functional.relu(t_3, inplace=False)
        t_3 = self.l_63(t_3)
        t_3 = self.l_64(t_3)
        t_3 = self.l_65(t_3)
        t_3 = t_2 + t_3
        t_2 = self.l_66(t_3)
        t_0 = t_2.size()
        t_0 = t_0[0]
        t_4 = self.l_67(t_2)
        t_4 = t_4.view(t_0, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_1 = self.l_68(t_2)
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = self.l_69(t_2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_4, t_1)
        t_1 += x0
        t_4 = t_1.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_4.type_as(t_1)
        t_1 = self.l_70(t_1)
        t_2 = torch.matmul(t_1, t_2)
        t_2 = t_2.transpose(1, 2)
        t_2 = t_2.contiguous()
        t_0 = t_2.view(t_0, -1, 4096)
        t_0 = self.l_71(t_0)
        t_0 = self.l_72(t_0)
        t_0 = t_3 + t_0
        t_3 = self.l_73(t_0)
        t_2 = t_3.size()
        t_2 = t_2[0]
        t_3 = self.l_74(t_3)
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_1 = x13.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_4 = x14.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_1 = t_1.transpose(3, 2)
        t_1 = torch.matmul(t_3, t_1)
        t_1 += x1
        t_3 = t_1.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_1 = t_3.type_as(t_1)
        t_1 = self.l_75(t_1)
        t_4 = torch.matmul(t_1, t_4)
        t_4 = t_4.transpose(1, 2)
        t_4 = t_4.contiguous()
        t_2 = t_4.view(t_2, -1, 4096)
        t_2 = self.l_76(t_2)
        t_2 = self.l_77(t_2)
        t_2 = t_0 + t_2
        t_0 = self.l_78(t_2)
        t_0 = self.l_79(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_80(t_0)
        t_0 = self.l_81(t_0)
        t_0 = self.l_82(t_0)
        t_0 = t_2 + t_0
        t_0 = self.l_83(t_0)
        t_0 = self.l_84(t_0)
        t_0 = t_0 * 0.03125
        t_0 = self.l_85(t_0)
        t_2 = t_0.size(-1)
        t_2 = t_0.view(-1, t_2)
        t_0 = lm_labels.view(-1)
        t_0 = self.l_86(t_2, t_0)
        # returning:
        # T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]
        return (t_0,)

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


def traverse_model(module: nn.Module, depth: int, prefix: Optional[str] = None,
                   basic_blocks: Tuple[nn.Module] = (), full: bool = False) -> Iterator[Tuple[nn.Module, str, nn.Module]]:
    '''
    iterate over model layers yielding the layer,layer_scope,encasing_module
    Parameters:
    -----------
    model:
        the model to iterate over
    depth:
        how far down in the model tree to go
    basic_blocks:
        a list of modules that if encountered will not be broken down
    full:
        whether to yield only layers specified by the depth and basick_block options or to yield all layers
    '''
    if prefix is None:
        prefix = type(module).__name__

    for name, sub_module in module.named_children():
        scope = prefix + "/" + type(sub_module).__name__ + f"[{name}]"
        if len(list(sub_module.children())) == 0 or isinstance(sub_module, tuple(basic_blocks)) or depth == 0:
            if full:
                yield sub_module, scope, module, True
            else:
                yield sub_module, scope, module
        else:
            if full:
                yield sub_module, scope, module, False
            yield from traverse_model(sub_module, depth - 1, scope, basic_blocks, full)


def layerDict(model: nn.Module, depth=1000, basic_blocks=()) -> Dict[str, nn.Module]:
    return {s: l for l, s, _ in traverse_model(model, depth, basic_blocks=basic_blocks)}


def traverse_params_buffs(module: nn.Module, prefix: Optional[str] = None) -> Iterator[Tuple[torch.tensor, str]]:
    '''
    iterate over model's buffers and parameters yielding obj,obj_scope

    Parameters:
    -----------
    model:
        the model to iterate over
    '''
    if prefix is None:
        prefix = type(module).__name__

    # params
    for param_name, param in module.named_parameters(recurse=False):
        param_scope = f"{prefix}/{type(param).__name__}[{param_name}]"
        yield param, param_scope

    # buffs
    for buffer_name, buffer in module.named_buffers(recurse=False):
        buffer_scope = f"{prefix}/{type(buffer).__name__}[{buffer_name}]"
        yield buffer, buffer_scope

    # recurse
    for name, sub_module in module.named_children():
        yield from traverse_params_buffs(sub_module, prefix + "/" + type(sub_module).__name__ + f"[{name}]")


def tensorDict(model: nn.Module) -> OrderedDict[str, Tensor]:
    return collections.OrderedDict((s, t)for t, s in traverse_params_buffs(model))


def move_tensors(ts, device):
    def move(t):
        if isinstance(t, (nn.Module, Tensor)):
            return t.to(device)
        return t

    return nested_map(move, ts)


def nested_map(func, ts,full=False):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        return func(ts)
    elif isinstance(ts, (list, tuple, set)):
        return type(ts)(nested_map(func, t,full=full) for t in ts)
    elif isinstance(ts, dict):
        return {k: nested_map(func, v,full=full) for k, v in ts.items()}
    elif isinstance(ts, slice) and full:
        start = nested_map(func, ts.start,full=full)
        stop = nested_map(func, ts.stop,full=full)
        step = nested_map(func, ts.step,full=full)
        return slice(start, stop, step)
    return func(ts)


def flatten(ts):
    if isinstance(ts,torch.Size):
        # size is inheriting from tuple which is stupid
        yield ts
    elif isinstance(ts, (list, tuple, set)):
        yield from chain(*[flatten(t) for t in ts])
    elif isinstance(ts, dict):
        yield from chain(*[flatten(t) for k,t in sorted(ts.items(),key=lambda t:t[0])])
    else:
        yield ts


def unflatten(xs,structure):
    return _unflatten(xs,structure)[0]


def _unflatten(xs,structure):
    if isinstance(structure,torch.Size):
        #torch.Size is subclass of tuple which is stupid
        return xs[0],1

    if not isinstance(structure,(list,tuple,set,dict)):
        return xs[0],1
    
    if isinstance(structure,(list,tuple,set)):
        offset=0
        elements = []
        for s in structure:
            e,n = _unflatten(xs[offset:],s)
            elements.append(e)
            offset += n
        
        return type(structure)(elements),offset
    
    assert isinstance(structure,dict)
    offset = 0
    elements = dict()
    for k,v in sorted(structure.items(),key=lambda t: t[0]):
        e,n = _unflatten(xs[offset:],v)
        elements[k] = e
        offset += n
    
    return elements,offset


def state_dict(partition, *args,**kwargs):
    # we return the state dict of this part as it should be in the original model
    state = nn.Module.state_dict(partition,*args,**kwargs)
    lookup = partition.lookup
    result = dict()
    for k, v in state.items():
        if k in lookup:
            result[lookup[k]] = v
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            result[new_k] = v
    return result


def load_state_dict(partition, state):
    reverse_lookup = {v: k for k, v in partition.lookup.items()}
    device = partition.device
    keys = list(partition.state_dict(None).keys())
    new_state = dict()
    for k in keys:
        if k in reverse_lookup:
            new_state[reverse_lookup[k]] = state[k].to(device)
            continue
        idx = k.rfind(".")
        to_replace = k[:idx]
        if to_replace in reverse_lookup:
            key = reverse_lookup[to_replace] + k[idx:]
            new_state[key] = state[k].to(device)
    nn.Module.load_state_dict(partition, new_state, strict=True)


def named_buffers(partition, recurse=True):
    # we return the named buffers of this part as it should be in the original model
    params = nn.Module.named_buffers(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def named_parameters(partition, recurse=True):
    # we return the named parameters of this part as it should be in the original model
    params = nn.Module.named_parameters(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def cpu(partition):
    partition.device = torch.device('cpu')
    return nn.Module.cpu(partition)


def cuda(partition, device=None):
    if device is None:
        device = torch.cuda.current_device()
    partition.device = torch.device(device)
    return nn.Module.cuda(partition, partition.device)


def to(partition, *args, **kwargs):
    device = None
    if 'device' in kwargs:
        device = kwargs['device']
    elif 'tensor' in kwargs:
        device = kwargs['tensor'].device
    if args:
        if isinstance(args[0], (torch.device, int, str)):
            device = args[0]
        if torch.is_tensor(args[0]):
            device = args[0].device
    if not (device is None):
        partition.device = torch.device(device)
    return nn.Module.to(partition, *args, **kwargs)

model_args = {'model_name_or_path': 't5-3b', 'max_seq_length': 384, 'answer_max_seq_length': 32, 'stateless_tied': True, 'lmhead': True, 'precompute_masks': True}
"""analysis summary
-I- Printing Report
warnings:
tensor T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149 sent to more than 1 target. Inaccurate (backward) communication time analysis
Partition0 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___149 is not contiguous!
Partition0 output:T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::__add___1603 is not contiguous!
Partition1 output:T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::transpose_354 is not contiguous!
Partition1 output:T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/prim::TupleConstruct_1628_1 is not contiguous!
tensor T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] sent to more than 1 target. Inaccurate (backward) communication time analysis
Partition5 output:T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::transpose_928 is not contiguous!
Partition5 output:T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::transpose_944 is not contiguous!
tensor T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Size::__getitem___1636 sent to more than 1 target. Inaccurate (backward) communication time analysis
Partition8 output:T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___1632 is not contiguous!
Partition10 output:T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::transpose_1664 is not contiguous!
tensor T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1764 sent to more than 1 target. Inaccurate (backward) communication time analysis
Partition11 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1764 is not contiguous!
tensor T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1766 sent to more than 1 target. Inaccurate (backward) communication time analysis
Partition11 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___1766 is not contiguous!
Number of stages: 16
backward times include recomputation

Stage parameter count:
 {0: 146152448,
 1: 117446656,
 2: 121639936,
 3: 130028544,
 4: 121639936,
 5: 125834240,
 6: 125834240,
 7: 121639936,
 8: 113251328,
 9: 134221824,
 10: 171967488,
 11: 264253440,
 12: 281033728,
 13: 272644096,
 14: 314589184,
 15: 322321408,
 'total': 2884498432}

real times are based on real measurements of execution time (with communication) of generated partitions ms
forward {0: 16.25, 1: 18.0, 2: 17.67, 3: 17.99, 4: 18.39, 5: 17.44, 6: 18.79, 7: 16.56, 8: 15.92, 9: 16.93, 10: 35.39, 11: 15.19, 12: 8.95, 13: 9.46, 14: 9.87, 15: 10.03}
backward {0: 38.65, 1: 40.11, 2: 42.11, 3: 43.44, 4: 43.15, 5: 43.15, 6: 43.59, 7: 42.09, 8: 40.62, 9: 42.56, 10: 41.3, 11: 31.12, 12: 33.21, 13: 32.18, 14: 36.25, 15: 34.85}

Assuming bandwidth of 12 GBps between GPUs

communication volumes size of activations of each partition
0: input size:'0.01 MB', recieve_time:'0.00 ms', out:'22.81 MB', send time:'1.90 ms'
1: input size:'22.81 MB', recieve_time:'1.90 ms', out:'27.26 MB', send time:'2.27 ms'
2: input size:'45.61 MB', recieve_time:'3.80 ms', out:'26.74 MB', send time:'2.23 ms'
3: input size:'45.61 MB', recieve_time:'3.80 ms', out:'20.45 MB', send time:'1.70 ms'
4: input size:'39.45 MB', recieve_time:'3.29 ms', out:'26.74 MB', send time:'2.23 ms'
5: input size:'45.61 MB', recieve_time:'3.80 ms', out:'15.73 MB', send time:'1.31 ms'
6: input size:'34.60 MB', recieve_time:'2.88 ms', out:'26.74 MB', send time:'2.23 ms'
7: input size:'45.61 MB', recieve_time:'3.80 ms', out:'9.44 MB', send time:'0.79 ms'
8: input size:'28.57 MB', recieve_time:'2.38 ms', out:'3.28 MB', send time:'0.27 ms'
9: input size:'22.02 MB', recieve_time:'1.84 ms', out:'3.15 MB', send time:'0.26 ms'
10: input size:'3.28 MB', recieve_time:'0.27 ms', out:'248.72 MB', send time:'20.73 ms'
11: input size:'47.66 MB', recieve_time:'3.97 ms', out:'52.17 MB', send time:'4.35 ms'
12: input size:'64.75 MB', recieve_time:'5.40 ms', out:'0.79 MB', send time:'0.07 ms'
13: input size:'65.40 MB', recieve_time:'5.45 ms', out:'2.23 MB', send time:'0.19 ms'
14: input size:'66.85 MB', recieve_time:'5.57 ms', out:'0.79 MB', send time:'0.07 ms'
15: input size:'65.41 MB', recieve_time:'5.45 ms', out:'0.00 MB', send time:'0.00 ms'

Compuatation Communication ratio (comp/(comp+comm)):
forward {0: 0.88, 1: 0.87, 2: 0.87, 3: 0.91, 4: 0.88, 5: 0.92, 6: 0.88, 7: 0.95, 8: 0.98, 9: 0.98, 10: 0.41, 11: 0.71, 12: 0.99, 13: 0.98, 14: 0.99, 15: 1.0} 
backward {0: 1.0, 1: 0.95, 2: 0.91, 3: 0.91, 4: 0.92, 5: 0.91, 6: 0.93, 7: 0.91, 8: 0.94, 9: 0.96, 10: 0.99, 11: 0.87, 12: 0.84, 13: 0.83, 14: 0.85, 15: 0.84}

Analysis for T = fwd + bwd:
 {'expected_compute_utilization': {0: 0.92,
                                  1: 0.94,
                                  2: 0.94,
                                  3: 0.97,
                                  4: 0.98,
                                  5: 0.97,
                                  6: 1.0,
                                  7: 0.94,
                                  8: 0.94,
                                  9: 1.0,
                                  10: 0.97,
                                  11: 0.66,
                                  12: 0.64,
                                  13: 0.63,
                                  14: 0.71,
                                  15: 0.69},
 'pipeline_no_comm': {0: 53.0,
                      1: 53.94,
                      2: 53.74,
                      3: 55.92,
                      4: 56.02,
                      5: 55.47,
                      6: 57.27,
                      7: 54.07,
                      8: 53.89,
                      9: 57.39,
                      10: 55.69,
                      11: 37.98,
                      12: 36.7,
                      13: 36.0,
                      14: 40.49,
                      15: 39.43,
                      'worstcase': 57.39},
 'pipeline_vs_seq_no_comm': 10.19,
 'pipeline_with_non_parallel_comm': {0: 54.9,
                                     1: 58.11,
                                     2: 59.77,
                                     3: 61.43,
                                     4: 61.54,
                                     5: 60.59,
                                     6: 62.38,
                                     7: 58.66,
                                     8: 56.54,
                                     9: 59.49,
                                     10: 76.69,
                                     11: 46.3,
                                     12: 42.16,
                                     13: 41.63,
                                     14: 46.12,
                                     15: 44.88,
                                     'worstcase': 76.69},
 'seq_no_comm_no_recomp': {0: 38.55,
                           1: 39.08,
                           2: 38.83,
                           3: 40.38,
                           4: 40.61,
                           5: 39.98,
                           6: 41.42,
                           7: 39.03,
                           8: 39.35,
                           9: 41.53,
                           10: 42.77,
                           11: 29.25,
                           12: 28.27,
                           13: 28.17,
                           14: 28.88,
                           15: 28.59}}

Analysis for T = (1-R)fwd + R*bwd:

Pipeline Slowdown: (compared to sequential executation with no communication, and same recompute policy)
forward 2.548
backward 1.213

Expected utilization by partition
forward {0: 0.41, 1: 0.45, 2: 0.44, 3: 0.46, 4: 0.46, 5: 0.45, 6: 0.47, 7: 0.45, 8: 0.44, 9: 0.47, 10: 0.41, 11: 0.31, 12: 0.25, 13: 0.26, 14: 0.28, 15: 0.28}
backward {0: 0.89, 1: 0.88, 2: 0.88, 3: 0.91, 4: 0.91, 5: 0.9, 6: 0.93, 7: 0.88, 8: 0.88, 9: 0.94, 10: 0.94, 11: 0.62, 12: 0.64, 13: 0.61, 14: 0.7, 15: 0.67}

worstcase: bwd: 43.591 fwd: 35.394
expected_speedup_compared_to_seq_no_recomp_no_comm: 7.402
Expected speedup for 16 partitions is: 8.832
max cuda memory used 2.71GB
"""