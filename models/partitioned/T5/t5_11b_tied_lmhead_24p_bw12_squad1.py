"""AutoGenerated with:
python partitioning_script.py --partitioning_task t5 --t5_task squad1 --lmhead --n_iter 10 --n_partitions 24 --analysis_batch_size 1 --partitioning_batch_size 1 --precompute_masks --stateless_tied --model_name_or_path t5-11b --save_memory_mode --max_seq_length 128 --answer_max_seq_length 16 --basic_blocks T5Block --constraint memory --objective stage_time --profiles_cache_name t5_11b_T5Block_cache
"""
import torch.functional
import math
import torch.nn.functional
import torch
from torch import Tensor
import torch.nn as nn
from itertools import chain
from typing import Optional, Tuple, Iterator, Iterable, OrderedDict, Dict
import collections

from models.normal.NLP_models.modeling_t5 import T5LayerNorm
from models.normal.NLP_models.modeling_t5 import T5Block
from torch.nn.modules.dropout import Dropout
from torch.nn.modules.linear import Linear
from models.normal.NLP_models.stateless import StatelessEmbedding
from torch.nn.modules.loss import CrossEntropyLoss
# this is an auto generated file do not edit unless you know what you are doing


# partition adjacency
# model inputs {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23}
# partition 0 {'inputs': {'decoder_input_ids', 'input_ids', 'attention_mask'}, 'outputs': {1, 2, 3, 4, 5, 6, 7, 8, 9}}
# partition 1 {'inputs': {0, 'attention_mask'}, 'outputs': {2}}
# partition 2 {'inputs': {0, 1, 'attention_mask'}, 'outputs': {10, 3}}
# partition 3 {'inputs': {0, 2, 'attention_mask'}, 'outputs': {4}}
# partition 4 {'inputs': {0, 3, 'attention_mask'}, 'outputs': {5}}
# partition 5 {'inputs': {0, 4, 'attention_mask'}, 'outputs': {6}}
# partition 6 {'inputs': {0, 5, 'attention_mask'}, 'outputs': {7}}
# partition 7 {'inputs': {0, 6, 'attention_mask'}, 'outputs': {8}}
# partition 8 {'inputs': {0, 7, 'attention_mask'}, 'outputs': {9}}
# partition 9 {'inputs': {0, 8, 'attention_mask'}, 'outputs': {10}}
# partition 10 {'inputs': {'inverted_encoder_attention_mask', 9, 2, 'decoder_attention_mask'}, 'outputs': {11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23}}
# partition 11 {'inputs': {'inverted_encoder_attention_mask', 10, 'decoder_attention_mask'}, 'outputs': {12}}
# partition 12 {'inputs': {'inverted_encoder_attention_mask', 11, 10, 'decoder_attention_mask'}, 'outputs': {13}}
# partition 13 {'inputs': {'inverted_encoder_attention_mask', 10, 'decoder_attention_mask', 12}, 'outputs': {14}}
# partition 14 {'inputs': {'inverted_encoder_attention_mask', 10, 'decoder_attention_mask', 13}, 'outputs': {15}}
# partition 15 {'inputs': {'inverted_encoder_attention_mask', 10, 'decoder_attention_mask', 14}, 'outputs': {16}}
# partition 16 {'inputs': {'inverted_encoder_attention_mask', 10, 'decoder_attention_mask', 15}, 'outputs': {17}}
# partition 17 {'inputs': {'inverted_encoder_attention_mask', 16, 10, 'decoder_attention_mask'}, 'outputs': {18}}
# partition 18 {'inputs': {'inverted_encoder_attention_mask', 17, 10, 'decoder_attention_mask'}, 'outputs': {19}}
# partition 19 {'inputs': {'inverted_encoder_attention_mask', 18, 10, 'decoder_attention_mask'}, 'outputs': {20}}
# partition 20 {'inputs': {'inverted_encoder_attention_mask', 19, 10, 'decoder_attention_mask'}, 'outputs': {21}}
# partition 21 {'inputs': {'inverted_encoder_attention_mask', 10, 'decoder_attention_mask', 20}, 'outputs': {22}}
# partition 22 {'inputs': {'inverted_encoder_attention_mask', 10, 'decoder_attention_mask', 21}, 'outputs': {23}}
# partition 23 {'inputs': {'lm_labels', 10, 'inverted_encoder_attention_mask', 22, 'decoder_attention_mask'}, 'outputs': {'output'}}
# model outputs {23}


def create_pipeline_configuration(DEBUG=False, batch_size=1):
    config = {
        'batch_dim': 0,
        'depth': 10000,
        'basic_blocks': (T5LayerNorm,T5Block,Dropout,Linear,StatelessEmbedding,CrossEntropyLoss),
        'model_inputs': {
            'attention_mask': {
                'shape': torch.Size([1, 1, 1, 128]),
                'dtype': torch.float32,
                'is_batched': True,
                'used_by': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]},
            'decoder_attention_mask': {
                'shape': torch.Size([1, 1, 16, 16]),
                'dtype': torch.float32,
                'is_batched': True,
                'used_by': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]},
            'decoder_input_ids': {
                'shape': torch.Size([1, 16]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0]},
            'input_ids': {
                'shape': torch.Size([1, 128]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0]},
            'inverted_encoder_attention_mask': {
                'shape': torch.Size([1, 1, 1, 128]),
                'dtype': torch.float32,
                'is_batched': True,
                'used_by': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]},
            'lm_labels': {
                'shape': torch.Size([1, 16]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [23]}},
        'model_outputs': {
            'T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]': {
                'shape': torch.Size([1]),
                'dtype': torch.float32,
                'is_batched': False,
                'created_by': 23}},
        'stages': {
            0: {
                'stage_cls': Partition0,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'decoder_input_ids': {
                        'shape': torch.Size([1, 16]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'input_ids': {
                        'shape': torch.Size([1, 128]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_1': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/StatelessEmbedding[embed_tokens]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [2]}},
                'devices': ['cpu' if DEBUG else 'cuda:0']},
            1: {
                'stage_cls': Partition1,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_1': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_2': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [2]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [2]}},
                'devices': ['cpu' if DEBUG else 'cuda:1']},
            2: {
                'stage_cls': Partition2,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_2': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1},
                    'T5ForConditionalGeneration/T5Stack[decoder]/StatelessEmbedding[embed_tokens]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_3': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [3]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [3]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [10]}},
                'devices': ['cpu' if DEBUG else 'cuda:2']},
            3: {
                'stage_cls': Partition3,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_3': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 2},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 2}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_4': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [4]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [4]}},
                'devices': ['cpu' if DEBUG else 'cuda:3']},
            4: {
                'stage_cls': Partition4,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_4': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 3},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 3}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_5': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [5]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [5]}},
                'devices': ['cpu' if DEBUG else 'cuda:4']},
            5: {
                'stage_cls': Partition5,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_5': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 4},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 4}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_6': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [6]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [6]}},
                'devices': ['cpu' if DEBUG else 'cuda:5']},
            6: {
                'stage_cls': Partition6,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_6': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 5},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 5}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_7': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [7]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [7]}},
                'devices': ['cpu' if DEBUG else 'cuda:6']},
            7: {
                'stage_cls': Partition7,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_7': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 6},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 6}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_8': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [8]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [8]}},
                'devices': ['cpu' if DEBUG else 'cuda:7']},
            8: {
                'stage_cls': Partition8,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_8': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 7},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 7}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_9': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [9]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [9]}},
                'devices': ['cpu' if DEBUG else 'cuda:8']},
            9: {
                'stage_cls': Partition9,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_9': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 8},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 8}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [10]}},
                'devices': ['cpu' if DEBUG else 'cuda:9']},
            10: {
                'stage_cls': Partition10,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 9},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 2}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_11': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_11': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_11': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]}},
                'devices': ['cpu' if DEBUG else 'cuda:10']},
            11: {
                'stage_cls': Partition11,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_11': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_11': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_11': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_12': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_12': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_12': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]}},
                'devices': ['cpu' if DEBUG else 'cuda:11']},
            12: {
                'stage_cls': Partition12,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_12': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_12': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_12': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_13': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_13': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_13': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]}},
                'devices': ['cpu' if DEBUG else 'cuda:12']},
            13: {
                'stage_cls': Partition13,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_13': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 12},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_13': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 12},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_13': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 12},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 12}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_14': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_14': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_14': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]}},
                'devices': ['cpu' if DEBUG else 'cuda:13']},
            14: {
                'stage_cls': Partition14,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_14': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 13},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_14': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 13},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_14': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 13},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 13}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_15': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_15': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_15': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]}},
                'devices': ['cpu' if DEBUG else 'cuda:14']},
            15: {
                'stage_cls': Partition15,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_15': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_15': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_15': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_16': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [16]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_16': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [16]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_16': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [16]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [16]}},
                'devices': ['cpu' if DEBUG else 'cuda:15']},
            16: {
                'stage_cls': Partition16,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_16': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 15},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_16': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 15},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_16': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 15},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 15}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_17': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [17]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_17': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [17]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_17': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [17]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [17]}},
                'devices': ['cpu' if DEBUG else 'cuda:16']},
            17: {
                'stage_cls': Partition17,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_17': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 16},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_17': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 16},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_17': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 16},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 16}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_18': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [18]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_18': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [18]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_18': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [18]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [18]}},
                'devices': ['cpu' if DEBUG else 'cuda:17']},
            18: {
                'stage_cls': Partition18,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_18': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 17},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_18': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 17},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_18': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 17},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 17}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_19': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [19]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_19': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [19]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_19': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [19]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [19]}},
                'devices': ['cpu' if DEBUG else 'cuda:18']},
            19: {
                'stage_cls': Partition19,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_19': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 18},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_19': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 18},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_19': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 18},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 18}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_20': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [20]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_20': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [20]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_20': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [20]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [20]}},
                'devices': ['cpu' if DEBUG else 'cuda:19']},
            20: {
                'stage_cls': Partition20,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_20': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 19},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_20': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 19},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_20': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 19},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 19}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_21': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [21]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_21': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [21]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_21': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [21]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [21]}},
                'devices': ['cpu' if DEBUG else 'cuda:20']},
            21: {
                'stage_cls': Partition21,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_21': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 20},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_21': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 20},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_21': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 20},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 20}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_22': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [22]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_22': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [22]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_22': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [22]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [22]}},
                'devices': ['cpu' if DEBUG else 'cuda:21']},
            22: {
                'stage_cls': Partition22,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_22': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 21},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_22': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 21},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_22': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 21},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 21}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_23': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [23]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_23': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [23]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_23': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [23]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [23]}},
                'devices': ['cpu' if DEBUG else 'cuda:22']},
            23: {
                'stage_cls': Partition23,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'lm_labels': {
                        'shape': torch.Size([1, 16]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_23': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 22},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_23': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 22},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_23': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 22},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 22}},
                'outputs': {
                    'T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]': {
                        'shape': torch.Size([1]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [-1]}},
                'devices': ['cpu' if DEBUG else 'cuda:23']}}}
    
    
    # switching batch size
    batch_dim = config['batch_dim']
    for d in chain(config['model_inputs'].values(),config['model_outputs'].values()):
        if d['is_batched']:
            shape = d['shape']
            d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    for s in config['stages'].values():
        for d in chain(s['inputs'].values(),s['outputs'].values()):
            if d['is_batched']:
                shape = d['shape']
                d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    return config

class Partition0(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/StatelessEmbedding[embed_tokens]',
            'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]',
            'T5ForConditionalGeneration/T5Stack[decoder]/StatelessEmbedding[embed_tokens]',
        ]
    TENSORS=[
            'T5ForConditionalGeneration/Parameter[shared_embed_weight]',
        ]
    def __init__(self, layers, tensors, device='cuda:0'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.embed_tokens',
                        'l_1': 'encoder.dropout',
                        'l_2': 'encoder.0',
                        'l_3': 'encoder.1',
                        'l_4': 'encoder.2',
                        'l_5': 'decoder.embed_tokens',
                        'p_0': 'shared_embed_weight'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/StatelessEmbedding[embed_tokens] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/StatelessEmbedding[embed_tokens] <=> self.l_5
        # T5ForConditionalGeneration/Parameter[shared_embed_weight] <=> self.p_0
        # input0 <=> attention_mask
        # input2 <=> decoder_input_ids
        # input3 <=> input_ids

        # moving inputs to current device no op if already on the correct device
        attention_mask, decoder_input_ids, input_ids = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = input_ids.size()
        t_0 = t_0[-1]
        t_0 = input_ids.view(-1, t_0)
        t_0 = self.l_0(self.p_0, t_0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0, attention_mask=attention_mask, position_bias=None, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_1 = t_0[0]
        t_0 = t_0[1]
        t_1 = self.l_3(t_1, attention_mask=attention_mask, position_bias=t_0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_1 = self.l_4(t_1, attention_mask=attention_mask, position_bias=t_0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_2 = decoder_input_ids.size()
        t_2 = t_2[-1]
        t_2 = decoder_input_ids.view(-1, t_2)
        t_2 = self.l_5(self.p_0, t_2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]
        # T5ForConditionalGeneration/T5Stack[decoder]/StatelessEmbedding[embed_tokens]
        return list(flatten((t_0, t_1, t_2)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition1(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:1'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.3',
                        'l_1': 'encoder.4'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4] <=> self.l_1
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2] <=> x1

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0 = self.l_1(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]
        return list(flatten((x0, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition2(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]',
            'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:2'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.5',
                        'l_1': 'encoder.6',
                        'l_2': 'decoder.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout] <=> self.l_2
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4] <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/StatelessEmbedding[embed_tokens] <=> x2

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0 = self.l_1(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_1 = self.l_2(x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]
        return list(flatten((x0, t_0, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition3(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:3'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.7',
                        'l_1': 'encoder.8',
                        'l_2': 'encoder.9'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9] <=> self.l_2
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6] <=> x1

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0 = self.l_1(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0 = self.l_2(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]
        return list(flatten((x0, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition4(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:4'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.10',
                        'l_1': 'encoder.11'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11] <=> self.l_1
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9] <=> x1

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0 = self.l_1(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]
        return list(flatten((x0, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition5(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:5'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.12',
                        'l_1': 'encoder.13'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13] <=> self.l_1
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11] <=> x1

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0 = self.l_1(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]
        return list(flatten((x0, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition6(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:6'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.14',
                        'l_1': 'encoder.15',
                        'l_2': 'encoder.16'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16] <=> self.l_2
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13] <=> x1

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0 = self.l_1(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0 = self.l_2(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]
        return list(flatten((x0, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition7(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:7'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.17',
                        'l_1': 'encoder.18'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18] <=> self.l_1
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16] <=> x1

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0 = self.l_1(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]
        return list(flatten((x0, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition8(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:8'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.19',
                        'l_1': 'encoder.20'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20] <=> self.l_1
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18] <=> x1

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0 = self.l_1(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]
        return list(flatten((x0, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition9(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:9'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.21',
                        'l_1': 'encoder.22',
                        'l_2': 'encoder.23'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23] <=> self.l_2
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20] <=> x1

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0 = self.l_1(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        t_0 = self.l_2(t_0, attention_mask=attention_mask, position_bias=x0, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]
        return (t_0,)

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition10(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5LayerNorm[final_layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:10'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.final_layer_norm',
                        'l_1': 'encoder.dropout',
                        'l_2': 'decoder.0',
                        'l_3': 'decoder.1'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5LayerNorm[final_layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1] <=> self.l_3
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout] <=> x1

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x0)
        t_0 = self.l_1(t_0)
        t_1 = self.l_2(x1, attention_mask=decoder_attention_mask, position_bias=None, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=None)
        t_2 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_2 = self.l_3(t_2, attention_mask=decoder_attention_mask, position_bias=t_3, encoder_hidden_states=t_0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=t_1)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]
        return list(flatten((t_0, t_3, t_1, t_2)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition11(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:11'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.2'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2] <=> self.l_0
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]
        return list(flatten((x0, x1, x2, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition12(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:12'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.3',
                        'l_1': 'decoder.4'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4] <=> self.l_1
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        t_0 = self.l_1(t_0, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]
        return list(flatten((x0, x1, x2, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition13(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:13'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.5',
                        'l_1': 'decoder.6'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6] <=> self.l_1
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        t_0 = self.l_1(t_0, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]
        return list(flatten((x0, x1, x2, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition14(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:14'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.7',
                        'l_1': 'decoder.8'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8] <=> self.l_1
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        t_0 = self.l_1(t_0, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]
        return list(flatten((x0, x1, x2, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition15(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:15'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.9',
                        'l_1': 'decoder.10'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10] <=> self.l_1
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        t_0 = self.l_1(t_0, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]
        return list(flatten((x0, x1, x2, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition16(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:16'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.11'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11] <=> self.l_0
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]
        return list(flatten((x0, x1, x2, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition17(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:17'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.12',
                        'l_1': 'decoder.13'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13] <=> self.l_1
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        t_0 = self.l_1(t_0, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]
        return list(flatten((x0, x1, x2, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition18(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:18'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.14',
                        'l_1': 'decoder.15'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15] <=> self.l_1
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        t_0 = self.l_1(t_0, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]
        return list(flatten((x0, x1, x2, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition19(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:19'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.16',
                        'l_1': 'decoder.17'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17] <=> self.l_1
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        t_0 = self.l_1(t_0, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]
        return list(flatten((x0, x1, x2, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition20(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:20'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.18'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18] <=> self.l_0
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]
        return list(flatten((x0, x1, x2, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition21(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:21'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.19',
                        'l_1': 'decoder.20'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20] <=> self.l_1
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        t_0 = self.l_1(t_0, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]
        return list(flatten((x0, x1, x2, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition22(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:22'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.21',
                        'l_1': 'decoder.22'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22] <=> self.l_1
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        t_0 = self.l_1(t_0, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]
        return list(flatten((x0, x1, x2, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition23(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5LayerNorm[final_layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/Linear[lm_head]',
            'T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:23'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.23',
                        'l_1': 'decoder.final_layer_norm',
                        'l_2': 'decoder.dropout',
                        'l_3': 'lm_head',
                        'l_4': 'lm_loss'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5LayerNorm[final_layer_norm] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/Linear[lm_head] <=> self.l_3
        # T5ForConditionalGeneration/CrossEntropyLoss[lm_loss] <=> self.l_4
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # input5 <=> lm_labels
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, lm_labels, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, attention_mask=decoder_attention_mask, position_bias=x1, encoder_hidden_states=x0, encoder_attention_mask=inverted_encoder_attention_mask, encoder_decoder_position_bias=x2)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = t_0 * 0.03125
        t_0 = self.l_3(t_0)
        t_1 = t_0.size(-1)
        t_1 = t_0.view(-1, t_1)
        t_0 = lm_labels.view(-1)
        t_0 = self.l_4(t_1, t_0)
        # returning:
        # T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]
        return (t_0,)

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


def traverse_model(module: nn.Module, depth: int, prefix: Optional[str] = None,
                   basic_blocks: Tuple[nn.Module] = (), full: bool = False) -> Iterator[Tuple[nn.Module, str, nn.Module]]:
    '''
    iterate over model layers yielding the layer,layer_scope,encasing_module
    Parameters:
    -----------
    model:
        the model to iterate over
    depth:
        how far down in the model tree to go
    basic_blocks:
        a list of modules that if encountered will not be broken down
    full:
        whether to yield only layers specified by the depth and basick_block options or to yield all layers
    '''
    if prefix is None:
        prefix = type(module).__name__

    for name, sub_module in module.named_children():
        scope = prefix + "/" + type(sub_module).__name__ + f"[{name}]"
        if len(list(sub_module.children())) == 0 or isinstance(sub_module, tuple(basic_blocks)) or depth == 0:
            if full:
                yield sub_module, scope, module, True
            else:
                yield sub_module, scope, module
        else:
            if full:
                yield sub_module, scope, module, False
            yield from traverse_model(sub_module, depth - 1, scope, basic_blocks, full)


def layerDict(model: nn.Module, depth=1000, basic_blocks=()) -> Dict[str, nn.Module]:
    return {s: l for l, s, _ in traverse_model(model, depth, basic_blocks=basic_blocks)}


def traverse_params_buffs(module: nn.Module, prefix: Optional[str] = None) -> Iterator[Tuple[torch.tensor, str]]:
    '''
    iterate over model's buffers and parameters yielding obj,obj_scope

    Parameters:
    -----------
    model:
        the model to iterate over
    '''
    if prefix is None:
        prefix = type(module).__name__

    # params
    for param_name, param in module.named_parameters(recurse=False):
        param_scope = f"{prefix}/{type(param).__name__}[{param_name}]"
        yield param, param_scope

    # buffs
    for buffer_name, buffer in module.named_buffers(recurse=False):
        buffer_scope = f"{prefix}/{type(buffer).__name__}[{buffer_name}]"
        yield buffer, buffer_scope

    # recurse
    for name, sub_module in module.named_children():
        yield from traverse_params_buffs(sub_module, prefix + "/" + type(sub_module).__name__ + f"[{name}]")


def tensorDict(model: nn.Module) -> OrderedDict[str, Tensor]:
    return collections.OrderedDict((s, t)for t, s in traverse_params_buffs(model))


def move_tensors(ts, device):
    def move(t):
        if isinstance(t, (nn.Module, Tensor)):
            return t.to(device)
        return t

    return nested_map(move, ts)


def nested_map(func, ts,full=False):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        return func(ts)
    elif isinstance(ts, (list, tuple, set)):
        return type(ts)(nested_map(func, t,full=full) for t in ts)
    elif isinstance(ts, dict):
        return {k: nested_map(func, v,full=full) for k, v in ts.items()}
    elif isinstance(ts, slice) and full:
        start = nested_map(func, ts.start,full=full)
        stop = nested_map(func, ts.stop,full=full)
        step = nested_map(func, ts.step,full=full)
        return slice(start, stop, step)
    return func(ts)


def flatten(ts):
    if isinstance(ts,torch.Size):
        # size is inheriting from tuple which is stupid
        yield ts
    elif isinstance(ts, (list, tuple, set)):
        yield from chain(*[flatten(t) for t in ts])
    elif isinstance(ts, dict):
        yield from chain(*[flatten(t) for k,t in sorted(ts.items(),key=lambda t:t[0])])
    else:
        yield ts


def unflatten(xs,structure):
    return _unflatten(xs,structure)[0]


def _unflatten(xs,structure):
    if isinstance(structure,torch.Size):
        #torch.Size is subclass of tuple which is stupid
        return xs[0],1

    if not isinstance(structure,(list,tuple,set,dict)):
        return xs[0],1
    
    if isinstance(structure,(list,tuple,set)):
        offset=0
        elements = []
        for s in structure:
            e,n = _unflatten(xs[offset:],s)
            elements.append(e)
            offset += n
        
        return type(structure)(elements),offset
    
    assert isinstance(structure,dict)
    offset = 0
    elements = dict()
    for k,v in sorted(structure.items(),key=lambda t: t[0]):
        e,n = _unflatten(xs[offset:],v)
        elements[k] = e
        offset += n
    
    return elements,offset


def state_dict(partition, *args,**kwargs):
    # we return the state dict of this part as it should be in the original model
    state = nn.Module.state_dict(partition,*args,**kwargs)
    lookup = partition.lookup
    result = dict()
    for k, v in state.items():
        if k in lookup:
            result[lookup[k]] = v
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            result[new_k] = v
    return result


def load_state_dict(partition, state):
    reverse_lookup = {v: k for k, v in partition.lookup.items()}
    device = partition.device
    keys = list(partition.state_dict(None).keys())
    new_state = dict()
    for k in keys:
        if k in reverse_lookup:
            new_state[reverse_lookup[k]] = state[k].to(device)
            continue
        idx = k.rfind(".")
        to_replace = k[:idx]
        if to_replace in reverse_lookup:
            key = reverse_lookup[to_replace] + k[idx:]
            new_state[key] = state[k].to(device)
    nn.Module.load_state_dict(partition, new_state, strict=True)


def named_buffers(partition, recurse=True):
    # we return the named buffers of this part as it should be in the original model
    params = nn.Module.named_buffers(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def named_parameters(partition, recurse=True):
    # we return the named parameters of this part as it should be in the original model
    params = nn.Module.named_parameters(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def cpu(partition):
    partition.device = torch.device('cpu')
    return nn.Module.cpu(partition)


def cuda(partition, device=None):
    if device is None:
        device = torch.cuda.current_device()
    partition.device = torch.device(device)
    return nn.Module.cuda(partition, partition.device)


def to(partition, *args, **kwargs):
    device = None
    if 'device' in kwargs:
        device = kwargs['device']
    elif 'tensor' in kwargs:
        device = kwargs['tensor'].device
    if args:
        if isinstance(args[0], (torch.device, int, str)):
            device = args[0]
        if torch.is_tensor(args[0]):
            device = args[0].device
    if not (device is None):
        partition.device = torch.device(device)
    return nn.Module.to(partition, *args, **kwargs)

model_args = {'model_name_or_path': 't5-11b', 'max_seq_length': 128, 'answer_max_seq_length': 16, 'stateless_tied': True, 'lmhead': True, 'precompute_masks': True}
"""analysis summary
-I- Printing Report
warnings:
Partition0 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_1 is not contiguous!
Partition1 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_2 is not contiguous!
Partition2 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_3 is not contiguous!
Partition3 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_4 is not contiguous!
Partition4 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_5 is not contiguous!
Partition5 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_6 is not contiguous!
Partition6 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_7 is not contiguous!
Partition7 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_8 is not contiguous!
Partition8 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___22_9 is not contiguous!
Partition10 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_11 is not contiguous!
Partition10 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_11 is not contiguous!
Partition11 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_12 is not contiguous!
Partition11 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_12 is not contiguous!
Partition12 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_13 is not contiguous!
Partition12 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_13 is not contiguous!
Partition13 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_14 is not contiguous!
Partition13 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_14 is not contiguous!
Partition14 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_15 is not contiguous!
Partition14 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_15 is not contiguous!
Partition15 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_16 is not contiguous!
Partition15 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_16 is not contiguous!
Partition16 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_17 is not contiguous!
Partition16 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_17 is not contiguous!
Partition17 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_18 is not contiguous!
Partition17 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_18 is not contiguous!
Partition18 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_19 is not contiguous!
Partition18 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_19 is not contiguous!
Partition19 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_20 is not contiguous!
Partition19 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_20 is not contiguous!
Partition20 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_21 is not contiguous!
Partition20 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_21 is not contiguous!
Partition21 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_22 is not contiguous!
Partition21 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_22 is not contiguous!
Partition22 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___130_23 is not contiguous!
Partition22 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___132_23 is not contiguous!
Number of stages: 24
backward times include recomputation

Stage parameter count:
 {0: 636889088,
 1: 402657280,
 2: 402657280,
 3: 603985920,
 4: 402657280,
 5: 402657280,
 6: 603985920,
 7: 402657280,
 8: 402657280,
 9: 603985920,
 10: 536886272,
 11: 268438528,
 12: 536877056,
 13: 536877056,
 14: 536877056,
 15: 536877056,
 16: 268438528,
 17: 536877056,
 18: 536877056,
 19: 536877056,
 20: 268438528,
 21: 536877056,
 22: 536877056,
 23: 301338624,
 'total': 11340224512}

real times are based on real measurements of execution time (with communication) of generated partitions ms
forward {0: 24.63, 1: 16.94, 2: 15.8, 3: 24.35, 4: 16.97, 5: 15.46, 6: 24.78, 7: 16.82, 8: 16.05, 9: 24.13, 10: 11.04, 11: 5.13, 12: 9.92, 13: 9.96, 14: 9.93, 15: 9.93, 16: 5.13, 17: 9.85, 18: 9.85, 19: 9.84, 20: 5.13, 21: 9.93, 22: 9.97, 23: 5.44}
backward {0: 68.1, 1: 45.09, 2: 40.82, 3: 65.67, 4: 44.94, 5: 40.01, 6: 67.41, 7: 45.61, 8: 41.82, 9: 67.2, 10: 27.62, 11: 13.33, 12: 26.25, 13: 26.25, 14: 26.25, 15: 26.25, 16: 13.34, 17: 26.01, 18: 26.01, 19: 26.02, 20: 13.34, 21: 26.46, 22: 26.26, 23: 14.59}

Assuming bandwidth of 12 GBps between GPUs

communication volumes size of activations of each partition
0: input size:'0.00 MB', recieve_time:'0.00 ms', out:'8.98 MB', send time:'0.75 ms'
1: input size:'8.91 MB', recieve_time:'0.74 ms', out:'8.91 MB', send time:'0.74 ms'
2: input size:'8.98 MB', recieve_time:'0.75 ms', out:'8.98 MB', send time:'0.75 ms'
3: input size:'8.91 MB', recieve_time:'0.74 ms', out:'8.91 MB', send time:'0.74 ms'
4: input size:'8.91 MB', recieve_time:'0.74 ms', out:'8.91 MB', send time:'0.74 ms'
5: input size:'8.91 MB', recieve_time:'0.74 ms', out:'8.91 MB', send time:'0.74 ms'
6: input size:'8.91 MB', recieve_time:'0.74 ms', out:'8.91 MB', send time:'0.74 ms'
7: input size:'8.91 MB', recieve_time:'0.74 ms', out:'8.91 MB', send time:'0.74 ms'
8: input size:'8.91 MB', recieve_time:'0.74 ms', out:'8.91 MB', send time:'0.74 ms'
9: input size:'8.91 MB', recieve_time:'0.74 ms', out:'0.52 MB', send time:'0.04 ms'
10: input size:'0.59 MB', recieve_time:'0.05 ms', out:'1.77 MB', send time:'0.15 ms'
11: input size:'1.77 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
12: input size:'1.77 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
13: input size:'1.77 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
14: input size:'1.77 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
15: input size:'1.77 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
16: input size:'1.77 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
17: input size:'1.77 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
18: input size:'1.77 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
19: input size:'1.77 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
20: input size:'1.77 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
21: input size:'1.77 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
22: input size:'1.77 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
23: input size:'1.77 MB', recieve_time:'0.15 ms', out:'0.00 MB', send time:'0.00 ms'

Compuatation Communication ratio (comp/(comp+comm)):
forward {0: 0.97, 1: 0.96, 2: 0.95, 3: 0.97, 4: 0.96, 5: 0.95, 6: 0.97, 7: 0.96, 8: 0.95, 9: 1.0, 10: 0.99, 11: 0.97, 12: 0.99, 13: 0.99, 14: 0.99, 15: 0.99, 16: 0.97, 17: 0.99, 18: 0.99, 19: 0.99, 20: 0.97, 21: 0.99, 22: 0.99, 23: 1.0} 
backward {0: 1.0, 1: 0.98, 2: 0.98, 3: 0.99, 4: 0.98, 5: 0.98, 6: 0.99, 7: 0.98, 8: 0.98, 9: 0.99, 10: 1.0, 11: 0.99, 12: 0.99, 13: 0.99, 14: 0.99, 15: 0.99, 16: 0.99, 17: 0.99, 18: 0.99, 19: 0.99, 20: 0.99, 21: 0.99, 22: 0.99, 23: 0.99}

Analysis for T = fwd + bwd:
 {'expected_compute_utilization': {0: 1.0,
                                  1: 0.66,
                                  2: 0.6,
                                  3: 0.96,
                                  4: 0.66,
                                  5: 0.59,
                                  6: 0.99,
                                  7: 0.66,
                                  8: 0.61,
                                  9: 0.98,
                                  10: 0.42,
                                  11: 0.2,
                                  12: 0.39,
                                  13: 0.39,
                                  14: 0.39,
                                  15: 0.39,
                                  16: 0.2,
                                  17: 0.39,
                                  18: 0.39,
                                  19: 0.39,
                                  20: 0.2,
                                  21: 0.39,
                                  22: 0.39,
                                  23: 0.22},
 'pipeline_no_comm': {0: 91.98,
                      1: 60.54,
                      2: 55.12,
                      3: 88.53,
                      4: 60.43,
                      5: 53.99,
                      6: 90.71,
                      7: 60.95,
                      8: 56.38,
                      9: 90.54,
                      10: 38.46,
                      11: 18.17,
                      12: 35.88,
                      13: 35.91,
                      14: 35.88,
                      15: 35.88,
                      16: 18.18,
                      17: 35.57,
                      18: 35.56,
                      19: 35.57,
                      20: 18.18,
                      21: 36.1,
                      22: 35.93,
                      23: 19.88,
                      'worstcase': 91.98},
 'pipeline_vs_seq_no_comm': 0.0,
 'pipeline_with_non_parallel_comm': {0: 92.73,
                                     1: 62.03,
                                     2: 56.62,
                                     3: 90.02,
                                     4: 61.92,
                                     5: 55.48,
                                     6: 92.2,
                                     7: 62.43,
                                     8: 57.87,
                                     9: 91.33,
                                     10: 38.66,
                                     11: 18.46,
                                     12: 36.17,
                                     13: 36.21,
                                     14: 36.18,
                                     15: 36.18,
                                     16: 18.47,
                                     17: 35.86,
                                     18: 35.85,
                                     19: 35.86,
                                     20: 18.47,
                                     21: 36.4,
                                     22: 36.23,
                                     23: 20.03,
                                     'worstcase': 92.73},
 'seq_no_comm_no_recomp': {}}

Analysis for T = (1-R)fwd + R*bwd:

Pipeline Slowdown: (compared to sequential executation with no communication, and same recompute policy)
forward 1.929
backward 1.955

Expected utilization by partition
forward {0: 0.96, 1: 0.65, 2: 0.61, 3: 0.95, 4: 0.65, 5: 0.59, 6: 0.97, 7: 0.65, 8: 0.62, 9: 0.97, 10: 0.44, 11: 0.2, 12: 0.39, 13: 0.39, 14: 0.39, 15: 0.39, 16: 0.2, 17: 0.39, 18: 0.39, 19: 0.39, 20: 0.2, 21: 0.39, 22: 0.39, 23: 0.22}
backward {0: 1.0, 1: 0.65, 2: 0.59, 3: 0.95, 4: 0.65, 5: 0.58, 6: 0.98, 7: 0.66, 8: 0.6, 9: 0.98, 10: 0.41, 11: 0.2, 12: 0.39, 13: 0.39, 14: 0.39, 15: 0.39, 16: 0.2, 17: 0.38, 18: 0.38, 19: 0.38, 20: 0.2, 21: 0.39, 22: 0.39, 23: 0.21}

worstcase: bwd: 68.102 fwd: 24.783
Expected speedup for 24 partitions is: 12.319
max cuda memory used 5.38GB
"""