"""AutoGenerated with:
python partition_gpt2_models.py --analysis_batch_size 1 --auto_file_name --block_size -1 --bwd_to_fwd_ratio 2 --lmhead --model_name_or_path gpt2-xl --model_type gpt2 --n_iter 50 --n_partitions 8 --output_file results/gpipe/gpt2xl_tied_p8/ --partitioning_batch_size 1 --seed 42 --stateless_tied --train_data_file wikitext-2-raw/wiki.train.raw
"""
import torch
from torch import Tensor
import torch.nn as nn
import torch.nn.functional as F
from itertools import chain
import operator
from typing import Optional, Tuple, Iterator, Iterable, OrderedDict, Dict
import collections
import os
from models.normal.NLP_models.stateless import StatelessEmbedding
from torch.nn.modules.sparse import Embedding
from torch.nn.modules.normalization import LayerNorm
from transformers.modeling_utils import Conv1D
from torch.nn.modules.dropout import Dropout
from models.normal.NLP_models.stateless import StatelessLinear
# this is an auto generated file do not edit unless you know what you are doing

# partition adjacency
# model inputs {0, 8}
# partition 0 {'inputs': {'input0'}, 'outputs': {8, 1}}
# partition 1 {'inputs': {0}, 'outputs': {2}}
# partition 2 {'inputs': {1}, 'outputs': {3}}
# partition 3 {'inputs': {2}, 'outputs': {4}}
# partition 4 {'inputs': {3}, 'outputs': {5}}
# partition 5 {'inputs': {4}, 'outputs': {6}}
# partition 6 {'inputs': {5}, 'outputs': {7}}
# partition 7 {'inputs': {6}, 'outputs': {8}}
# partition 8 {'inputs': {0, 'input1', 7}, 'outputs': {'output0'}}
# model outputs {8}


def create_pipeline_configuration(DEBUG=False):
    depth = 10000
    basic_blocks = (StatelessEmbedding, Embedding, LayerNorm, Conv1D, Dropout,
                    StatelessLinear)
    blocks_path = [
        'models.normal.NLP_models.stateless.StatelessEmbedding',
        'torch.nn.modules.sparse.Embedding',
        'torch.nn.modules.normalization.LayerNorm',
        'transformers.modeling_utils.Conv1D',
        'torch.nn.modules.dropout.Dropout',
        'models.normal.NLP_models.stateless.StatelessLinear'
    ]
    module_path = os.path.relpath(__file__).replace("/", ".")[:-3]

    # creating configuration
    stages = {
        0: {
            "inputs": {
                'input0': {
                    'shape': torch.Size([1, 1024]),
                    'dtype': torch.int64,
                    'is_batched': True,
                    'req_grad': False
                }
            },
            "outputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::mul19773':
                {
                    'shape': torch.Size([1, 1024, 6400]),
                    'dtype': torch.float32,
                    'is_batched': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/aten::add19753':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                },
                'GPT2LMHeadModel/Parameter[w_wte]': {
                    'shape': torch.Size([50257, 1600]),
                    'dtype': torch.float32,
                    'is_batched': False
                }
            }
        },
        1: {
            "inputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::mul19773':
                {
                    'shape': torch.Size([1, 1024, 6400]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/aten::add19753':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                }
            },
            "outputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj]':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/aten::add20857':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                }
            }
        },
        2: {
            "inputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj]':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/aten::add20857':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                }
            },
            "outputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/aten::add21985':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                }
            }
        },
        3: {
            "inputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/aten::add21985':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                }
            },
            "outputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/aten::add23089':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::permute23225':
                {
                    'shape': torch.Size([1, 1024, 25, 64]),
                    'dtype': torch.float32,
                    'is_batched': True
                }
            }
        },
        4: {
            "inputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/aten::add23089':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::permute23225':
                {
                    'shape': torch.Size([1, 1024, 25, 64]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                }
            },
            "outputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/aten::add24193':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::matmul24323':
                {
                    'shape': torch.Size([1, 25, 1024, 64]),
                    'dtype': torch.float32,
                    'is_batched': True
                }
            }
        },
        5: {
            "inputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/aten::add24193':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::matmul24323':
                {
                    'shape': torch.Size([1, 25, 1024, 64]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                }
            },
            "outputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/aten::add25481':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                }
            }
        },
        6: {
            "inputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/aten::add25481':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                }
            },
            "outputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/aten::add26561':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                }
            }
        },
        7: {
            "inputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/aten::add26561':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                }
            },
            "outputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]': {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                }
            }
        },
        8: {
            "inputs": {
                    'input1': {
                    'shape': torch.Size([1, 1024]),
                    'dtype': torch.int64,
                    'is_batched': True,
                    'req_grad': False
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]': {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                },
                'GPT2LMHeadModel/Parameter[w_wte]': {
                    'shape': torch.Size([50257, 1600]),
                    'dtype': torch.float32,
                    'is_batched': False,
                    'req_grad': True
                },

            },
            "outputs": {
                'GPT2LMHeadModel/aten::nll_loss16393': {
                    'shape': torch.Size([1]),
                    'dtype': torch.float32,
                    'is_batched': False
                }
            }
        }
    }

    stages[0]['stage_cls'] = module_path + '.Partition0'
    device = 'cpu' if DEBUG else 'cuda:0'
    stages[0]['devices'] = [device]

    stages[1]['stage_cls'] = module_path + '.Partition1'
    device = 'cpu' if DEBUG else 'cuda:1'
    stages[1]['devices'] = [device]

    stages[2]['stage_cls'] = module_path + '.Partition2'
    device = 'cpu' if DEBUG else 'cuda:2'
    stages[2]['devices'] = [device]

    stages[3]['stage_cls'] = module_path + '.Partition3'
    device = 'cpu' if DEBUG else 'cuda:3'
    stages[3]['devices'] = [device]

    stages[4]['stage_cls'] = module_path + '.Partition4'
    device = 'cpu' if DEBUG else 'cuda:4'
    stages[4]['devices'] = [device]

    stages[5]['stage_cls'] = module_path + '.Partition5'
    device = 'cpu' if DEBUG else 'cuda:5'
    stages[5]['devices'] = [device]

    stages[6]['stage_cls'] = module_path + '.Partition6'
    device = 'cpu' if DEBUG else 'cuda:6'
    stages[6]['devices'] = [device]

    stages[7]['stage_cls'] = module_path + '.Partition7'
    device = 'cpu' if DEBUG else 'cuda:7'
    stages[7]['devices'] = [device]

    stages[8]['stage_cls'] = module_path + '.Partition8'
    device = 'cpu' if DEBUG else 'cuda:0'
    stages[8]['devices'] = [device]

    config = dict()
    config['batch_dim'] = 0
    config['depth'] = depth
    config['basic_blocks'] = blocks_path
    config['model_inputs'] = {
        'input0': {
            "shape": torch.Size([1, 1024]),
            "dtype": torch.int64,
            "is_batched": True
        },
        'input1': {
            "shape": torch.Size([1, 1024]),
            "dtype": torch.int64,
            "is_batched": True
        }
    }
    config['model_outputs'] = {
        'GPT2LMHeadModel/aten::nll_loss16393': {
            "shape": torch.Size([1]),
            "dtype": torch.float32,
            "is_batched": False
        }
    }
    config['stages'] = stages

    return config


class Partition0(nn.Module):
    SCOPES = {
        'GPT2LMHeadModel/GPT2Model[transformer]/StatelessEmbedding[stateless_wte]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc]',
    }

    def __init__(self, layers, tensors):
        super(Partition0, self).__init__()
        # initializing partition layers
        self.scopes = []
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/StatelessEmbedding[stateless_wte]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/StatelessEmbedding[stateless_wte]'
        )
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe]')
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop]')
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1]')
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2]')
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1]')
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2]')
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1]')
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2]')
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1]')
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2]')
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1]')
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_43 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_44 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2]')
        self.l_45 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc]'
        )

        # initializing partition buffers
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_4', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Tensor[bias]']
        )

        # initializing partition parameters
        self.register_parameter('p_0',
                                tensors['GPT2LMHeadModel/Parameter[w_wte]'])

        self.device = torch.device('cuda:0')
        self.lookup = {
            'l_0': 'transformer.stateless_wte',
            'l_1': 'transformer.wpe',
            'l_2': 'transformer.drop',
            'l_3': 'transformer.blocks.0.ln_1',
            'l_4': 'transformer.blocks.0.attn.c_attn',
            'l_5': 'transformer.blocks.0.attn.attn_dropout',
            'l_6': 'transformer.blocks.0.attn.c_proj',
            'l_7': 'transformer.blocks.0.attn.resid_dropout',
            'l_8': 'transformer.blocks.0.ln_2',
            'l_9': 'transformer.blocks.0.mlp.c_fc',
            'l_10': 'transformer.blocks.0.mlp.c_proj',
            'l_11': 'transformer.blocks.0.mlp.dropout',
            'l_12': 'transformer.blocks.1.ln_1',
            'l_13': 'transformer.blocks.1.attn.c_attn',
            'l_14': 'transformer.blocks.1.attn.attn_dropout',
            'l_15': 'transformer.blocks.1.attn.c_proj',
            'l_16': 'transformer.blocks.1.attn.resid_dropout',
            'l_17': 'transformer.blocks.1.ln_2',
            'l_18': 'transformer.blocks.1.mlp.c_fc',
            'l_19': 'transformer.blocks.1.mlp.c_proj',
            'l_20': 'transformer.blocks.1.mlp.dropout',
            'l_21': 'transformer.blocks.2.ln_1',
            'l_22': 'transformer.blocks.2.attn.c_attn',
            'l_23': 'transformer.blocks.2.attn.attn_dropout',
            'l_24': 'transformer.blocks.2.attn.c_proj',
            'l_25': 'transformer.blocks.2.attn.resid_dropout',
            'l_26': 'transformer.blocks.2.ln_2',
            'l_27': 'transformer.blocks.2.mlp.c_fc',
            'l_28': 'transformer.blocks.2.mlp.c_proj',
            'l_29': 'transformer.blocks.2.mlp.dropout',
            'l_30': 'transformer.blocks.3.ln_1',
            'l_31': 'transformer.blocks.3.attn.c_attn',
            'l_32': 'transformer.blocks.3.attn.attn_dropout',
            'l_33': 'transformer.blocks.3.attn.c_proj',
            'l_34': 'transformer.blocks.3.attn.resid_dropout',
            'l_35': 'transformer.blocks.3.ln_2',
            'l_36': 'transformer.blocks.3.mlp.c_fc',
            'l_37': 'transformer.blocks.3.mlp.c_proj',
            'l_38': 'transformer.blocks.3.mlp.dropout',
            'l_39': 'transformer.blocks.4.ln_1',
            'l_40': 'transformer.blocks.4.attn.c_attn',
            'l_41': 'transformer.blocks.4.attn.attn_dropout',
            'l_42': 'transformer.blocks.4.attn.c_proj',
            'l_43': 'transformer.blocks.4.attn.resid_dropout',
            'l_44': 'transformer.blocks.4.ln_2',
            'l_45': 'transformer.blocks.4.mlp.c_fc',
            'b_0': 'transformer.blocks.0.attn.bias',
            'b_1': 'transformer.blocks.1.attn.bias',
            'b_2': 'transformer.blocks.2.attn.bias',
            'b_3': 'transformer.blocks.3.attn.bias',
            'b_4': 'transformer.blocks.4.attn.bias',
            'p_0': 'w_wte'
        }

    def forward(self, x0):
        # GPT2LMHeadModel/GPT2Model[transformer]/StatelessEmbedding[stateless_wte] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_attn] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[attn_dropout] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_proj] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[resid_dropout] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_attn] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[attn_dropout] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_proj] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[resid_dropout] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_attn] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[attn_dropout] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_proj] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[resid_dropout] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_attn] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[attn_dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_proj] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[resid_dropout] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_attn] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[attn_dropout] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_proj] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[resid_dropout] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/Parameter[w_wte] <=> self.p_0
        # input0 <=> x0

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)

        # calling Tensor.size with arguments:
        # input0
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18826
        t_2 = x0.size(dim=1)
        t_2 = [-1, t_2]
        # calling Tensor.view with arguments:
        # input0
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::ListConstruct18831
        t_2 = x0.view(size=t_2)
        del x0
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::view18832
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18833
        t_3 = t_2.size(dim=-1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::size18834
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18836
        t_3 = torch.add(input=t_3, other=0)
        # calling torch.arange with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18840
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::add18838
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18841
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18842
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18844
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18845
        t_3 = torch.arange(start=0,
                           end=t_3,
                           step=1,
                           dtype=torch.int64,
                           device=self.device,
                           requires_grad=False)
        # calling torch.unsqueeze with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::arange18846
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18847
        t_3 = t_3.unsqueeze(dim=0)
        # calling Tensor.expand_as with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::unsqueeze18848
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::view18832
        t_3 = t_3.expand_as(other=t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/StatelessEmbedding[stateless_wte] with arguments:
        # GPT2LMHeadModel/Parameter[w_wte]
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::view18832
        t_2 = self.l_0(self.p_0, t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::expand_as18849
        t_3 = self.l_1(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/StatelessEmbedding[stateless_wte]
        # GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe]
        t_3 = torch.add(input=t_2, other=t_3)
        del t_2
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::add18853
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18854
        t_3 = torch.add(input=t_3, other=0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::add18856
        t_3 = self.l_2(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop]
        t_2 = self.l_3(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1]
        t_2 = self.l_4(t_2)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18869
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18870
        t_2 = t_2.split(split_size=1600, dim=2)
        t_5 = t_2[0]
        t_6 = t_2[1]
        t_2 = t_2[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188720
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18875
        t_7 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188720
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18879
        t_8 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188720
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18883
        t_9 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size18884
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18886
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_7, t_8, 25, t_9]
        del t_8
        del t_7
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188720
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct18890
        t_9 = t_5.view(size=t_9)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::view18891
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct18896
        t_5 = t_9.permute(dims=t_5)
        del t_9
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188721
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18898
        t_9 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188721
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18902
        t_8 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188721
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18906
        t_7 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size18907
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18909
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_9, t_8, 25, t_7]
        del t_8
        del t_9
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188721
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct18913
        t_7 = t_6.view(size=t_7)
        del t_6
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::view18914
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct18919
        t_6 = t_7.permute(dims=t_6)
        del t_7
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188722
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18921
        t_7 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188722
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18925
        t_8 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188722
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18929
        t_9 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size18930
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18932
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_7, t_8, 25, t_9]
        del t_8
        del t_7
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188722
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct18936
        t_9 = t_2.view(size=t_9)
        del t_2
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::view18937
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct18942
        t_2 = t_9.permute(dims=t_2)
        del t_9
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::permute18897
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::permute18920
        t_6 = t_5.matmul(other=t_6)
        del t_5
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::matmul18944
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18945
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::div18946
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18947
        t_5 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::div18946
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18950
        t_9 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size18951
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size18948
        t_5 = torch.sub(input=t_9, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18958
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18959
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18960
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18961
        t_8 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::slice18962
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18963
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18964
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18965
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18966
        t_8 = t_8[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::slice18967
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18968
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::sub18956
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size18951
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18969
        t_5 = t_8[:, :, t_5:t_9:1]
        del t_8
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::slice18970
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18971
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18972
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size18951
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18973
        t_9 = t_5[:, :, :, 0:t_9:1]
        del t_5
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::div18946
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::slice18974
        t_6 = torch.mul(input=t_6, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::slice18974
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18976
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18977
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::rsub18978
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18979
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::mul18975
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::mul18980
        t_9 = torch.sub(input=t_6, other=t_9)
        del t_6
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::sub18982
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18983
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18984
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::softmax18985
        t_9 = self.l_5(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::permute18943
        t_2 = t_9.matmul(other=t_2)
        del t_9
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::matmul18987
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct18992
        t_9 = t_2.permute(dims=t_9)
        del t_2
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::permute18993
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::contiguous18995
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18996
        t_2 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::contiguous18995
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant19000
        t_6 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::contiguous18995
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant19004
        t_5 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::contiguous18995
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant19007
        t_8 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size19005
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size19008
        t_8 = torch.mul(input=t_5, other=t_8)
        del t_5
        t_8 = [t_2, t_6, t_8]
        del t_6
        del t_2
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::contiguous18995
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct19012
        t_8 = t_9.view(size=t_8)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::view19013
        t_8 = self.l_6(t_8)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_proj]
        t_8 = self.l_7(t_8)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[resid_dropout]
        t_8 = torch.add(input=t_3, other=t_8)
        del t_3
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/aten::add19017
        t_3 = self.l_8(t_8)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2]
        t_3 = self.l_9(t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/prim::Constant19023
        t_9 = torch.mul(input=t_3, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/prim::Constant19025
        t_6 = t_3.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::pow19026
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/prim::Constant19027
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::mul19028
        t_6 = torch.add(input=t_3, other=t_6)
        del t_3
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::add19030
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/prim::Constant19031
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::mul19032
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::tanh19033
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/prim::Constant19034
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::mul19024
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::add19036
        t_6 = torch.mul(input=t_9, other=t_6)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::mul19037
        t_6 = self.l_10(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj]
        t_6 = self.l_11(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/aten::add19017
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout]
        t_6 = torch.add(input=t_8, other=t_6)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/aten::add19041
        t_8 = self.l_12(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1]
        t_8 = self.l_13(t_8)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19053
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19054
        t_8 = t_8.split(split_size=1600, dim=2)
        t_3 = t_8[0]
        t_2 = t_8[1]
        t_8 = t_8[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190560
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19059
        t_5 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190560
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19063
        t_7 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190560
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19067
        t_10 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19068
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19070
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_5, t_7, 25, t_10]
        del t_7
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190560
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19074
        t_10 = t_3.view(size=t_10)
        del t_3
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::view19075
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19080
        t_3 = t_10.permute(dims=t_3)
        del t_10
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190561
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19082
        t_10 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190561
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19086
        t_7 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190561
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19090
        t_5 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19091
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19093
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_10, t_7, 25, t_5]
        del t_7
        del t_10
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190561
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19097
        t_5 = t_2.view(size=t_5)
        del t_2
        t_2 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::view19098
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19103
        t_2 = t_5.permute(dims=t_2)
        del t_5
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190562
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19105
        t_5 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190562
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19109
        t_7 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190562
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19113
        t_10 = t_8.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19114
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19116
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_5, t_7, 25, t_10]
        del t_7
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190562
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19120
        t_10 = t_8.view(size=t_10)
        del t_8
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::view19121
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19126
        t_8 = t_10.permute(dims=t_8)
        del t_10
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::permute19081
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::permute19104
        t_2 = t_3.matmul(other=t_2)
        del t_3
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::matmul19128
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19129
        t_2 = torch.div(input=t_2, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::div19130
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19131
        t_3 = t_2.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::div19130
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19134
        t_10 = t_2.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19135
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19132
        t_3 = torch.sub(input=t_10, other=t_3)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19142
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19143
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19144
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19145
        t_7 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::slice19146
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19147
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19148
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19149
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19150
        t_7 = t_7[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::slice19151
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19152
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::sub19140
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19135
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19153
        t_3 = t_7[:, :, t_3:t_10:1]
        del t_7
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::slice19154
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19155
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19156
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19135
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19157
        t_10 = t_3[:, :, :, 0:t_10:1]
        del t_3
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::div19130
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::slice19158
        t_2 = torch.mul(input=t_2, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::slice19158
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19160
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19161
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::rsub19162
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19163
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::mul19159
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::mul19164
        t_10 = torch.sub(input=t_2, other=t_10)
        del t_2
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::sub19166
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19167
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19168
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::softmax19169
        t_10 = self.l_14(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::permute19127
        t_8 = t_10.matmul(other=t_8)
        del t_10
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::matmul19171
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19176
        t_10 = t_8.permute(dims=t_10)
        del t_8
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::permute19177
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::contiguous19179
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19180
        t_8 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::contiguous19179
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19184
        t_2 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::contiguous19179
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19188
        t_3 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::contiguous19179
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19191
        t_7 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19189
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19192
        t_7 = torch.mul(input=t_3, other=t_7)
        del t_3
        t_7 = [t_8, t_2, t_7]
        del t_2
        del t_8
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::contiguous19179
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19196
        t_7 = t_10.view(size=t_7)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::view19197
        t_7 = self.l_15(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_proj]
        t_7 = self.l_16(t_7)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/aten::add19041
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[resid_dropout]
        t_7 = torch.add(input=t_6, other=t_7)
        del t_6
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/aten::add19201
        t_6 = self.l_17(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2]
        t_6 = self.l_18(t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/prim::Constant19207
        t_10 = torch.mul(input=t_6, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/prim::Constant19209
        t_2 = t_6.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::pow19210
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/prim::Constant19211
        t_2 = torch.mul(input=t_2, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::mul19212
        t_2 = torch.add(input=t_6, other=t_2)
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::add19214
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/prim::Constant19215
        t_2 = torch.mul(input=t_2, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::mul19216
        t_2 = t_2.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::tanh19217
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/prim::Constant19218
        t_2 = torch.add(input=t_2, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::mul19208
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::add19220
        t_2 = torch.mul(input=t_10, other=t_2)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::mul19221
        t_2 = self.l_19(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj]
        t_2 = self.l_20(t_2)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/aten::add19201
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout]
        t_2 = torch.add(input=t_7, other=t_2)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/aten::add19225
        t_7 = self.l_21(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1]
        t_7 = self.l_22(t_7)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19237
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19238
        t_7 = t_7.split(split_size=1600, dim=2)
        t_6 = t_7[0]
        t_8 = t_7[1]
        t_7 = t_7[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192400
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19243
        t_3 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192400
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19247
        t_5 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192400
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19251
        t_11 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19252
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19254
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_3, t_5, 25, t_11]
        del t_5
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192400
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19258
        t_11 = t_6.view(size=t_11)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::view19259
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19264
        t_6 = t_11.permute(dims=t_6)
        del t_11
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192401
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19266
        t_11 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192401
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19270
        t_5 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192401
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19274
        t_3 = t_8.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19275
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19277
        t_3 = torch.div(input=t_3, other=25)
        t_3 = [t_11, t_5, 25, t_3]
        del t_5
        del t_11
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192401
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19281
        t_3 = t_8.view(size=t_3)
        del t_8
        t_8 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::view19282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19287
        t_8 = t_3.permute(dims=t_8)
        del t_3
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192402
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19289
        t_3 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192402
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19293
        t_5 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192402
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19297
        t_11 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19298
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19300
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_3, t_5, 25, t_11]
        del t_5
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192402
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19304
        t_11 = t_7.view(size=t_11)
        del t_7
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::view19305
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19310
        t_7 = t_11.permute(dims=t_7)
        del t_11
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::permute19265
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::permute19288
        t_8 = t_6.matmul(other=t_8)
        del t_6
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::matmul19312
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19313
        t_8 = torch.div(input=t_8, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::div19314
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19315
        t_6 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::div19314
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19318
        t_11 = t_8.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19319
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19316
        t_6 = torch.sub(input=t_11, other=t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19326
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19327
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19328
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19329
        t_5 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::slice19330
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19331
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19332
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19333
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19334
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::slice19335
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19336
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::sub19324
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19319
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19337
        t_6 = t_5[:, :, t_6:t_11:1]
        del t_5
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::slice19338
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19339
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19340
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19319
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19341
        t_11 = t_6[:, :, :, 0:t_11:1]
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::div19314
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::slice19342
        t_8 = torch.mul(input=t_8, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::slice19342
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19344
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19345
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::rsub19346
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19347
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::mul19343
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::mul19348
        t_11 = torch.sub(input=t_8, other=t_11)
        del t_8
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::sub19350
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19351
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19352
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::softmax19353
        t_11 = self.l_23(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::permute19311
        t_7 = t_11.matmul(other=t_7)
        del t_11
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::matmul19355
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19360
        t_11 = t_7.permute(dims=t_11)
        del t_7
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::permute19361
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::contiguous19363
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19364
        t_7 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::contiguous19363
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19368
        t_8 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::contiguous19363
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19372
        t_6 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::contiguous19363
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19375
        t_5 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19373
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19376
        t_5 = torch.mul(input=t_6, other=t_5)
        del t_6
        t_5 = [t_7, t_8, t_5]
        del t_8
        del t_7
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::contiguous19363
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19380
        t_5 = t_11.view(size=t_5)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::view19381
        t_5 = self.l_24(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_25(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/aten::add19225
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=t_2, other=t_5)
        del t_2
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/aten::add19385
        t_2 = self.l_26(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2]
        t_2 = self.l_27(t_2)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/prim::Constant19391
        t_11 = torch.mul(input=t_2, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/prim::Constant19393
        t_8 = t_2.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::pow19394
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/prim::Constant19395
        t_8 = torch.mul(input=t_8, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::mul19396
        t_8 = torch.add(input=t_2, other=t_8)
        del t_2
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::add19398
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/prim::Constant19399
        t_8 = torch.mul(input=t_8, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::mul19400
        t_8 = t_8.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::tanh19401
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/prim::Constant19402
        t_8 = torch.add(input=t_8, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::mul19392
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::add19404
        t_8 = torch.mul(input=t_11, other=t_8)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::mul19405
        t_8 = self.l_28(t_8)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj]
        t_8 = self.l_29(t_8)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/aten::add19385
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout]
        t_8 = torch.add(input=t_5, other=t_8)
        del t_5
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/aten::add19409
        t_5 = self.l_30(t_8)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1]
        t_5 = self.l_31(t_5)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19421
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19422
        t_5 = t_5.split(split_size=1600, dim=2)
        t_2 = t_5[0]
        t_7 = t_5[1]
        t_5 = t_5[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194240
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19427
        t_6 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194240
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19431
        t_3 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194240
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19435
        t_12 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19436
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19438
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_3, 25, t_12]
        del t_3
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194240
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19442
        t_12 = t_2.view(size=t_12)
        del t_2
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::view19443
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19448
        t_2 = t_12.permute(dims=t_2)
        del t_12
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194241
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19450
        t_12 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194241
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19454
        t_3 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194241
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19458
        t_6 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19459
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19461
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_12, t_3, 25, t_6]
        del t_3
        del t_12
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194241
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19465
        t_6 = t_7.view(size=t_6)
        del t_7
        t_7 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::view19466
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19471
        t_7 = t_6.permute(dims=t_7)
        del t_6
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194242
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19473
        t_6 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194242
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19477
        t_3 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194242
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19481
        t_12 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19482
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19484
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_3, 25, t_12]
        del t_3
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194242
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19488
        t_12 = t_5.view(size=t_12)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::view19489
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19494
        t_5 = t_12.permute(dims=t_5)
        del t_12
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::permute19449
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::permute19472
        t_7 = t_2.matmul(other=t_7)
        del t_2
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::matmul19496
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19497
        t_7 = torch.div(input=t_7, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::div19498
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19499
        t_2 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::div19498
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19502
        t_12 = t_7.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19503
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19500
        t_2 = torch.sub(input=t_12, other=t_2)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19510
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19511
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19512
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19513
        t_3 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::slice19514
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19515
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19516
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19517
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19518
        t_3 = t_3[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::slice19519
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19520
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::sub19508
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19503
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19521
        t_2 = t_3[:, :, t_2:t_12:1]
        del t_3
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::slice19522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19523
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19524
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19503
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19525
        t_12 = t_2[:, :, :, 0:t_12:1]
        del t_2
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::div19498
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::slice19526
        t_7 = torch.mul(input=t_7, other=t_12)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::slice19526
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19528
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19529
        t_12 = torch.rsub(t_12, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::rsub19530
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19531
        t_12 = torch.mul(input=t_12, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::mul19527
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::mul19532
        t_12 = torch.sub(input=t_7, other=t_12)
        del t_7
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::sub19534
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19535
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19536
        t_12 = t_12.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::softmax19537
        t_12 = self.l_32(t_12)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::permute19495
        t_5 = t_12.matmul(other=t_5)
        del t_12
        t_12 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::matmul19539
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19544
        t_12 = t_5.permute(dims=t_12)
        del t_5
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::permute19545
        t_12 = t_12.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::contiguous19547
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19548
        t_5 = t_12.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::contiguous19547
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19552
        t_7 = t_12.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::contiguous19547
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19556
        t_2 = t_12.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::contiguous19547
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19559
        t_3 = t_12.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19557
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19560
        t_3 = torch.mul(input=t_2, other=t_3)
        del t_2
        t_3 = [t_5, t_7, t_3]
        del t_7
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::contiguous19547
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19564
        t_3 = t_12.view(size=t_3)
        del t_12
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::view19565
        t_3 = self.l_33(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_proj]
        t_3 = self.l_34(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/aten::add19409
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[resid_dropout]
        t_3 = torch.add(input=t_8, other=t_3)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/aten::add19569
        t_8 = self.l_35(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2]
        t_8 = self.l_36(t_8)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/prim::Constant19575
        t_12 = torch.mul(input=t_8, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/prim::Constant19577
        t_7 = t_8.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::pow19578
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/prim::Constant19579
        t_7 = torch.mul(input=t_7, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::mul19580
        t_7 = torch.add(input=t_8, other=t_7)
        del t_8
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::add19582
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/prim::Constant19583
        t_7 = torch.mul(input=t_7, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::mul19584
        t_7 = t_7.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::tanh19585
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/prim::Constant19586
        t_7 = torch.add(input=t_7, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::mul19576
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::add19588
        t_7 = torch.mul(input=t_12, other=t_7)
        del t_12
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::mul19589
        t_7 = self.l_37(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj]
        t_7 = self.l_38(t_7)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/aten::add19569
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout]
        t_7 = torch.add(input=t_3, other=t_7)
        del t_3
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/aten::add19593
        t_3 = self.l_39(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1]
        t_3 = self.l_40(t_3)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19605
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19606
        t_3 = t_3.split(split_size=1600, dim=2)
        t_8 = t_3[0]
        t_5 = t_3[1]
        t_3 = t_3[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196080
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19611
        t_2 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196080
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19615
        t_6 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196080
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19619
        t_13 = t_8.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19620
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19622
        t_13 = torch.div(input=t_13, other=25)
        t_13 = [t_2, t_6, 25, t_13]
        del t_6
        del t_2
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196080
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19626
        t_13 = t_8.view(size=t_13)
        del t_8
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::view19627
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19632
        t_8 = t_13.permute(dims=t_8)
        del t_13
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196081
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19634
        t_13 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196081
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19638
        t_6 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196081
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19642
        t_2 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19643
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19645
        t_2 = torch.div(input=t_2, other=25)
        t_2 = [t_13, t_6, 25, t_2]
        del t_6
        del t_13
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196081
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19649
        t_2 = t_5.view(size=t_2)
        del t_5
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::view19650
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19655
        t_5 = t_2.permute(dims=t_5)
        del t_2
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196082
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19657
        t_2 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196082
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19661
        t_6 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196082
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19665
        t_13 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19666
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19668
        t_13 = torch.div(input=t_13, other=25)
        t_13 = [t_2, t_6, 25, t_13]
        del t_6
        del t_2
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196082
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19672
        t_13 = t_3.view(size=t_13)
        del t_3
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::view19673
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19678
        t_3 = t_13.permute(dims=t_3)
        del t_13
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::permute19633
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::permute19656
        t_5 = t_8.matmul(other=t_5)
        del t_8
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::matmul19680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19681
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::div19682
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19683
        t_8 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::div19682
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19686
        t_13 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19687
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19684
        t_8 = torch.sub(input=t_13, other=t_8)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19694
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19695
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19696
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19697
        t_6 = self.b_4[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::slice19698
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19699
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19700
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19701
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19702
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::slice19703
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19704
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::sub19692
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19687
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19705
        t_8 = t_6[:, :, t_8:t_13:1]
        del t_6
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::slice19706
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19707
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19708
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19687
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19709
        t_13 = t_8[:, :, :, 0:t_13:1]
        del t_8
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::div19682
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::slice19710
        t_5 = torch.mul(input=t_5, other=t_13)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::slice19710
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19712
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19713
        t_13 = torch.rsub(t_13, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::rsub19714
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19715
        t_13 = torch.mul(input=t_13, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::mul19711
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::mul19716
        t_13 = torch.sub(input=t_5, other=t_13)
        del t_5
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::sub19718
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19719
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19720
        t_13 = t_13.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::softmax19721
        t_13 = self.l_41(t_13)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::permute19679
        t_3 = t_13.matmul(other=t_3)
        del t_13
        t_13 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::matmul19723
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19728
        t_13 = t_3.permute(dims=t_13)
        del t_3
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::permute19729
        t_13 = t_13.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::contiguous19731
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19732
        t_3 = t_13.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::contiguous19731
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19736
        t_5 = t_13.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::contiguous19731
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19740
        t_8 = t_13.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::contiguous19731
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19743
        t_6 = t_13.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19741
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19744
        t_6 = torch.mul(input=t_8, other=t_6)
        del t_8
        t_6 = [t_3, t_5, t_6]
        del t_5
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::contiguous19731
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19748
        t_6 = t_13.view(size=t_6)
        del t_13
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::view19749
        t_6 = self.l_42(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_43(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/aten::add19593
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_7, other=t_6)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/aten::add19753
        t_7 = self.l_44(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2]
        t_7 = self.l_45(t_7)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/prim::Constant19759
        t_13 = torch.mul(input=t_7, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/prim::Constant19761
        t_5 = t_7.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::pow19762
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/prim::Constant19763
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::mul19764
        t_5 = torch.add(input=t_7, other=t_5)
        del t_7
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::add19766
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/prim::Constant19767
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::mul19768
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::tanh19769
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/prim::Constant19770
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::mul19760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::add19772
        t_5 = torch.mul(input=t_13, other=t_5)
        del t_13

        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::mul19773
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/aten::add19753
        # GPT2LMHeadModel/Parameter[w_wte]
        return (t_5, t_6, self.p_0)

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition1(nn.Module):
    SCOPES = {
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj]',
    }

    def __init__(self, layers, tensors):
        super(Partition1, self).__init__()
        # initializing partition layers
        self.scopes = []
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1]')
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2]')
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1]')
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2]')
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1]')
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2]')
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1]')
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2]')
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1]')
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_43 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2]')
        self.l_44 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_45 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_46 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_47 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1]')
        self.l_48 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_49 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_50 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_51 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_52 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2]')
        self.l_53 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_54 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj]'
        )

        # initializing partition buffers
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_4', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_5', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Tensor[bias]']
        )

        self.device = torch.device('cuda:1')
        self.lookup = {
            'l_0': 'transformer.blocks.4.mlp.c_proj',
            'l_1': 'transformer.blocks.4.mlp.dropout',
            'l_2': 'transformer.blocks.5.ln_1',
            'l_3': 'transformer.blocks.5.attn.c_attn',
            'l_4': 'transformer.blocks.5.attn.attn_dropout',
            'l_5': 'transformer.blocks.5.attn.c_proj',
            'l_6': 'transformer.blocks.5.attn.resid_dropout',
            'l_7': 'transformer.blocks.5.ln_2',
            'l_8': 'transformer.blocks.5.mlp.c_fc',
            'l_9': 'transformer.blocks.5.mlp.c_proj',
            'l_10': 'transformer.blocks.5.mlp.dropout',
            'l_11': 'transformer.blocks.6.ln_1',
            'l_12': 'transformer.blocks.6.attn.c_attn',
            'l_13': 'transformer.blocks.6.attn.attn_dropout',
            'l_14': 'transformer.blocks.6.attn.c_proj',
            'l_15': 'transformer.blocks.6.attn.resid_dropout',
            'l_16': 'transformer.blocks.6.ln_2',
            'l_17': 'transformer.blocks.6.mlp.c_fc',
            'l_18': 'transformer.blocks.6.mlp.c_proj',
            'l_19': 'transformer.blocks.6.mlp.dropout',
            'l_20': 'transformer.blocks.7.ln_1',
            'l_21': 'transformer.blocks.7.attn.c_attn',
            'l_22': 'transformer.blocks.7.attn.attn_dropout',
            'l_23': 'transformer.blocks.7.attn.c_proj',
            'l_24': 'transformer.blocks.7.attn.resid_dropout',
            'l_25': 'transformer.blocks.7.ln_2',
            'l_26': 'transformer.blocks.7.mlp.c_fc',
            'l_27': 'transformer.blocks.7.mlp.c_proj',
            'l_28': 'transformer.blocks.7.mlp.dropout',
            'l_29': 'transformer.blocks.8.ln_1',
            'l_30': 'transformer.blocks.8.attn.c_attn',
            'l_31': 'transformer.blocks.8.attn.attn_dropout',
            'l_32': 'transformer.blocks.8.attn.c_proj',
            'l_33': 'transformer.blocks.8.attn.resid_dropout',
            'l_34': 'transformer.blocks.8.ln_2',
            'l_35': 'transformer.blocks.8.mlp.c_fc',
            'l_36': 'transformer.blocks.8.mlp.c_proj',
            'l_37': 'transformer.blocks.8.mlp.dropout',
            'l_38': 'transformer.blocks.9.ln_1',
            'l_39': 'transformer.blocks.9.attn.c_attn',
            'l_40': 'transformer.blocks.9.attn.attn_dropout',
            'l_41': 'transformer.blocks.9.attn.c_proj',
            'l_42': 'transformer.blocks.9.attn.resid_dropout',
            'l_43': 'transformer.blocks.9.ln_2',
            'l_44': 'transformer.blocks.9.mlp.c_fc',
            'l_45': 'transformer.blocks.9.mlp.c_proj',
            'l_46': 'transformer.blocks.9.mlp.dropout',
            'l_47': 'transformer.blocks.10.ln_1',
            'l_48': 'transformer.blocks.10.attn.c_attn',
            'l_49': 'transformer.blocks.10.attn.attn_dropout',
            'l_50': 'transformer.blocks.10.attn.c_proj',
            'l_51': 'transformer.blocks.10.attn.resid_dropout',
            'l_52': 'transformer.blocks.10.ln_2',
            'l_53': 'transformer.blocks.10.mlp.c_fc',
            'l_54': 'transformer.blocks.10.mlp.c_proj',
            'b_0': 'transformer.blocks.5.attn.bias',
            'b_1': 'transformer.blocks.6.attn.bias',
            'b_2': 'transformer.blocks.7.attn.bias',
            'b_3': 'transformer.blocks.8.attn.bias',
            'b_4': 'transformer.blocks.9.attn.bias',
            'b_5': 'transformer.blocks.10.attn.bias'
        }

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_attn] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[attn_dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_proj] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[resid_dropout] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_attn] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[attn_dropout] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_proj] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[resid_dropout] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_attn] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[attn_dropout] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_proj] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[resid_dropout] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_attn] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[attn_dropout] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_proj] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[resid_dropout] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_attn] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[attn_dropout] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_proj] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[resid_dropout] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_attn] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[attn_dropout] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_proj] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[resid_dropout] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::mul19773 <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/aten::add19753 <=> x1

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)

        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::mul19773
        t_0 = self.l_0(x0)
        del x0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj]
        t_0 = self.l_1(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/aten::add19753
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout]
        t_0 = torch.add(input=x1, other=t_0)
        del x1
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/aten::add19777
        t_1 = self.l_2(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1]
        t_1 = self.l_3(t_1)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19789
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19790
        t_1 = t_1.split(split_size=1600, dim=2)
        t_3 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197920
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19795
        t_5 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197920
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19799
        t_6 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197920
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19803
        t_7 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19804
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19806
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        del t_6
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197920
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19810
        t_7 = t_3.view(size=t_7)
        del t_3
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::view19811
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19816
        t_3 = t_7.permute(dims=t_3)
        del t_7
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19818
        t_7 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19822
        t_6 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19826
        t_5 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19827
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19829
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_7, t_6, 25, t_5]
        del t_6
        del t_7
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19833
        t_5 = t_4.view(size=t_5)
        del t_4
        t_4 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::view19834
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19839
        t_4 = t_5.permute(dims=t_4)
        del t_5
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197922
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19841
        t_5 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197922
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19845
        t_6 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197922
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19849
        t_7 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19850
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19852
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        del t_6
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197922
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19856
        t_7 = t_1.view(size=t_7)
        del t_1
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::view19857
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19862
        t_1 = t_7.permute(dims=t_1)
        del t_7
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::permute19817
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::permute19840
        t_4 = t_3.matmul(other=t_4)
        del t_3
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::matmul19864
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19865
        t_4 = torch.div(input=t_4, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::div19866
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19867
        t_3 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::div19866
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19870
        t_7 = t_4.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19871
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19868
        t_3 = torch.sub(input=t_7, other=t_3)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19878
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19879
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19880
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19881
        t_6 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::slice19882
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19883
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19884
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19885
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19886
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::slice19887
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19888
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::sub19876
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19871
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19889
        t_3 = t_6[:, :, t_3:t_7:1]
        del t_6
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::slice19890
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19891
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19892
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19871
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19893
        t_7 = t_3[:, :, :, 0:t_7:1]
        del t_3
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::div19866
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::slice19894
        t_4 = torch.mul(input=t_4, other=t_7)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::slice19894
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19896
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19897
        t_7 = torch.rsub(t_7, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::rsub19898
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19899
        t_7 = torch.mul(input=t_7, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::mul19895
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::mul19900
        t_7 = torch.sub(input=t_4, other=t_7)
        del t_4
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::sub19902
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19903
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19904
        t_7 = t_7.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::softmax19905
        t_7 = self.l_4(t_7)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::permute19863
        t_1 = t_7.matmul(other=t_1)
        del t_7
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::matmul19907
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19912
        t_7 = t_1.permute(dims=t_7)
        del t_1
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::permute19913
        t_7 = t_7.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::contiguous19915
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19916
        t_1 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::contiguous19915
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19920
        t_4 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::contiguous19915
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19924
        t_3 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::contiguous19915
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19927
        t_6 = t_7.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19925
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19928
        t_6 = torch.mul(input=t_3, other=t_6)
        del t_3
        t_6 = [t_1, t_4, t_6]
        del t_4
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::contiguous19915
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19932
        t_6 = t_7.view(size=t_6)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::view19933
        t_6 = self.l_5(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_6(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/aten::add19777
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_0, other=t_6)
        del t_0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/aten::add19937
        t_0 = self.l_7(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2]
        t_0 = self.l_8(t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/prim::Constant19943
        t_7 = torch.mul(input=t_0, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/prim::Constant19945
        t_4 = t_0.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::pow19946
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/prim::Constant19947
        t_4 = torch.mul(input=t_4, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::mul19948
        t_4 = torch.add(input=t_0, other=t_4)
        del t_0
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::add19950
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/prim::Constant19951
        t_4 = torch.mul(input=t_4, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::mul19952
        t_4 = t_4.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::tanh19953
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/prim::Constant19954
        t_4 = torch.add(input=t_4, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::mul19944
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::add19956
        t_4 = torch.mul(input=t_7, other=t_4)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::mul19957
        t_4 = self.l_9(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj]
        t_4 = self.l_10(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/aten::add19937
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout]
        t_4 = torch.add(input=t_6, other=t_4)
        del t_6
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/aten::add19961
        t_6 = self.l_11(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1]
        t_6 = self.l_12(t_6)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant19973
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant19974
        t_6 = t_6.split(split_size=1600, dim=2)
        t_0 = t_6[0]
        t_1 = t_6[1]
        t_6 = t_6[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant19979
        t_3 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant19983
        t_5 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant19987
        t_8 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size19988
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant19990
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        del t_5
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct19994
        t_8 = t_0.view(size=t_8)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::view19995
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct20000
        t_0 = t_8.permute(dims=t_0)
        del t_8
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20002
        t_8 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20006
        t_5 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20010
        t_3 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20011
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20013
        t_3 = torch.div(input=t_3, other=25)
        t_3 = [t_8, t_5, 25, t_3]
        del t_5
        del t_8
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct20017
        t_3 = t_1.view(size=t_3)
        del t_1
        t_1 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::view20018
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct20023
        t_1 = t_3.permute(dims=t_1)
        del t_3
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199762
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20025
        t_3 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199762
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20029
        t_5 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199762
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20033
        t_8 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20034
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20036
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        del t_5
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199762
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct20040
        t_8 = t_6.view(size=t_8)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::view20041
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct20046
        t_6 = t_8.permute(dims=t_6)
        del t_8
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::permute20001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::permute20024
        t_1 = t_0.matmul(other=t_1)
        del t_0
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::matmul20048
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20049
        t_1 = torch.div(input=t_1, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::div20050
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20051
        t_0 = t_1.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::div20050
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20054
        t_8 = t_1.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20055
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20052
        t_0 = torch.sub(input=t_8, other=t_0)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20062
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20063
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20064
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20065
        t_5 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::slice20066
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20067
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20068
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20069
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20070
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::slice20071
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20072
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::sub20060
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20055
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20073
        t_0 = t_5[:, :, t_0:t_8:1]
        del t_5
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::slice20074
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20075
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20076
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20055
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20077
        t_8 = t_0[:, :, :, 0:t_8:1]
        del t_0
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::div20050
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::slice20078
        t_1 = torch.mul(input=t_1, other=t_8)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::slice20078
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20080
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20081
        t_8 = torch.rsub(t_8, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::rsub20082
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20083
        t_8 = torch.mul(input=t_8, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::mul20079
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::mul20084
        t_8 = torch.sub(input=t_1, other=t_8)
        del t_1
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::sub20086
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20087
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20088
        t_8 = t_8.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::softmax20089
        t_8 = self.l_13(t_8)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::permute20047
        t_6 = t_8.matmul(other=t_6)
        del t_8
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::matmul20091
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct20096
        t_8 = t_6.permute(dims=t_8)
        del t_6
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::permute20097
        t_8 = t_8.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::contiguous20099
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20100
        t_6 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::contiguous20099
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20104
        t_1 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::contiguous20099
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20108
        t_0 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::contiguous20099
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20111
        t_5 = t_8.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20109
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20112
        t_5 = torch.mul(input=t_0, other=t_5)
        del t_0
        t_5 = [t_6, t_1, t_5]
        del t_1
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::contiguous20099
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct20116
        t_5 = t_8.view(size=t_5)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::view20117
        t_5 = self.l_14(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_15(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/aten::add19961
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=t_4, other=t_5)
        del t_4
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/aten::add20121
        t_4 = self.l_16(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2]
        t_4 = self.l_17(t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/prim::Constant20127
        t_8 = torch.mul(input=t_4, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/prim::Constant20129
        t_1 = t_4.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::pow20130
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/prim::Constant20131
        t_1 = torch.mul(input=t_1, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::mul20132
        t_1 = torch.add(input=t_4, other=t_1)
        del t_4
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::add20134
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/prim::Constant20135
        t_1 = torch.mul(input=t_1, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::mul20136
        t_1 = t_1.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::tanh20137
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/prim::Constant20138
        t_1 = torch.add(input=t_1, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::mul20128
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::add20140
        t_1 = torch.mul(input=t_8, other=t_1)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::mul20141
        t_1 = self.l_18(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj]
        t_1 = self.l_19(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/aten::add20121
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout]
        t_1 = torch.add(input=t_5, other=t_1)
        del t_5
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/aten::add20145
        t_5 = self.l_20(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1]
        t_5 = self.l_21(t_5)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20157
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20158
        t_5 = t_5.split(split_size=1600, dim=2)
        t_4 = t_5[0]
        t_6 = t_5[1]
        t_5 = t_5[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20163
        t_0 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20167
        t_3 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20171
        t_9 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20172
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20174
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_0, t_3, 25, t_9]
        del t_3
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20178
        t_9 = t_4.view(size=t_9)
        del t_4
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::view20179
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20184
        t_4 = t_9.permute(dims=t_4)
        del t_9
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20186
        t_9 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20190
        t_3 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20194
        t_0 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20195
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20197
        t_0 = torch.div(input=t_0, other=25)
        t_0 = [t_9, t_3, 25, t_0]
        del t_3
        del t_9
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20201
        t_0 = t_6.view(size=t_0)
        del t_6
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::view20202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20207
        t_6 = t_0.permute(dims=t_6)
        del t_0
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201602
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20209
        t_0 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201602
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20213
        t_3 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201602
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20217
        t_9 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20218
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20220
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_0, t_3, 25, t_9]
        del t_3
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201602
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20224
        t_9 = t_5.view(size=t_9)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::view20225
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20230
        t_5 = t_9.permute(dims=t_5)
        del t_9
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::permute20185
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::permute20208
        t_6 = t_4.matmul(other=t_6)
        del t_4
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::matmul20232
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20233
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::div20234
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20235
        t_4 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::div20234
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20238
        t_9 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20239
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20236
        t_4 = torch.sub(input=t_9, other=t_4)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20246
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20247
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20248
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20249
        t_3 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::slice20250
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20251
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20252
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20253
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20254
        t_3 = t_3[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::slice20255
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20256
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::sub20244
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20239
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20257
        t_4 = t_3[:, :, t_4:t_9:1]
        del t_3
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::slice20258
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20259
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20260
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20239
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20261
        t_9 = t_4[:, :, :, 0:t_9:1]
        del t_4
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::div20234
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::slice20262
        t_6 = torch.mul(input=t_6, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::slice20262
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20264
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20265
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::rsub20266
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20267
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::mul20263
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::mul20268
        t_9 = torch.sub(input=t_6, other=t_9)
        del t_6
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::sub20270
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20271
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20272
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::softmax20273
        t_9 = self.l_22(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::permute20231
        t_5 = t_9.matmul(other=t_5)
        del t_9
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::matmul20275
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20280
        t_9 = t_5.permute(dims=t_9)
        del t_5
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::permute20281
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::contiguous20283
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20284
        t_5 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::contiguous20283
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20288
        t_6 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::contiguous20283
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20292
        t_4 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::contiguous20283
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20295
        t_3 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20293
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20296
        t_3 = torch.mul(input=t_4, other=t_3)
        del t_4
        t_3 = [t_5, t_6, t_3]
        del t_6
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::contiguous20283
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20300
        t_3 = t_9.view(size=t_3)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::view20301
        t_3 = self.l_23(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_proj]
        t_3 = self.l_24(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/aten::add20145
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[resid_dropout]
        t_3 = torch.add(input=t_1, other=t_3)
        del t_1
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/aten::add20305
        t_1 = self.l_25(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2]
        t_1 = self.l_26(t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/prim::Constant20311
        t_9 = torch.mul(input=t_1, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/prim::Constant20313
        t_6 = t_1.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::pow20314
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/prim::Constant20315
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::mul20316
        t_6 = torch.add(input=t_1, other=t_6)
        del t_1
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::add20318
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/prim::Constant20319
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::mul20320
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::tanh20321
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/prim::Constant20322
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::mul20312
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::add20324
        t_6 = torch.mul(input=t_9, other=t_6)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::mul20325
        t_6 = self.l_27(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj]
        t_6 = self.l_28(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/aten::add20305
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout]
        t_6 = torch.add(input=t_3, other=t_6)
        del t_3
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/aten::add20329
        t_3 = self.l_29(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1]
        t_3 = self.l_30(t_3)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20341
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20342
        t_3 = t_3.split(split_size=1600, dim=2)
        t_1 = t_3[0]
        t_5 = t_3[1]
        t_3 = t_3[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203440
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20347
        t_4 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203440
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20351
        t_0 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203440
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20355
        t_10 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20356
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20358
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_0, 25, t_10]
        del t_0
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203440
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20362
        t_10 = t_1.view(size=t_10)
        del t_1
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::view20363
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20368
        t_1 = t_10.permute(dims=t_1)
        del t_10
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20370
        t_10 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20374
        t_0 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20378
        t_4 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20379
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20381
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_10, t_0, 25, t_4]
        del t_0
        del t_10
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20385
        t_4 = t_5.view(size=t_4)
        del t_5
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::view20386
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20391
        t_5 = t_4.permute(dims=t_5)
        del t_4
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20393
        t_4 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20397
        t_0 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20401
        t_10 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20402
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20404
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_0, 25, t_10]
        del t_0
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20408
        t_10 = t_3.view(size=t_10)
        del t_3
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::view20409
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20414
        t_3 = t_10.permute(dims=t_3)
        del t_10
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::permute20369
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::permute20392
        t_5 = t_1.matmul(other=t_5)
        del t_1
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::matmul20416
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20417
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::div20418
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20419
        t_1 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::div20418
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20422
        t_10 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20423
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20420
        t_1 = torch.sub(input=t_10, other=t_1)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20430
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20431
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20432
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20433
        t_0 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::slice20434
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20435
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20436
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20437
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20438
        t_0 = t_0[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::slice20439
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20440
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::sub20428
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20423
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20441
        t_1 = t_0[:, :, t_1:t_10:1]
        del t_0
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::slice20442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20443
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20444
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20423
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20445
        t_10 = t_1[:, :, :, 0:t_10:1]
        del t_1
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::div20418
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::slice20446
        t_5 = torch.mul(input=t_5, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::slice20446
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20448
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20449
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::rsub20450
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20451
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::mul20447
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::mul20452
        t_10 = torch.sub(input=t_5, other=t_10)
        del t_5
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::sub20454
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20455
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20456
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::softmax20457
        t_10 = self.l_31(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::permute20415
        t_3 = t_10.matmul(other=t_3)
        del t_10
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::matmul20459
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20464
        t_10 = t_3.permute(dims=t_10)
        del t_3
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::permute20465
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::contiguous20467
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20468
        t_3 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::contiguous20467
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20472
        t_5 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::contiguous20467
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20476
        t_1 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::contiguous20467
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20479
        t_0 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20477
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20480
        t_0 = torch.mul(input=t_1, other=t_0)
        del t_1
        t_0 = [t_3, t_5, t_0]
        del t_5
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::contiguous20467
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20484
        t_0 = t_10.view(size=t_0)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::view20485
        t_0 = self.l_32(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_proj]
        t_0 = self.l_33(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/aten::add20329
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[resid_dropout]
        t_0 = torch.add(input=t_6, other=t_0)
        del t_6
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/aten::add20489
        t_6 = self.l_34(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2]
        t_6 = self.l_35(t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/prim::Constant20495
        t_10 = torch.mul(input=t_6, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/prim::Constant20497
        t_5 = t_6.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::pow20498
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/prim::Constant20499
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::mul20500
        t_5 = torch.add(input=t_6, other=t_5)
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::add20502
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/prim::Constant20503
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::mul20504
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::tanh20505
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/prim::Constant20506
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::mul20496
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::add20508
        t_5 = torch.mul(input=t_10, other=t_5)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::mul20509
        t_5 = self.l_36(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj]
        t_5 = self.l_37(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/aten::add20489
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout]
        t_5 = torch.add(input=t_0, other=t_5)
        del t_0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/aten::add20513
        t_0 = self.l_38(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1]
        t_0 = self.l_39(t_0)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20525
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20526
        t_0 = t_0.split(split_size=1600, dim=2)
        t_6 = t_0[0]
        t_3 = t_0[1]
        t_0 = t_0[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205280
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20531
        t_1 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205280
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20535
        t_4 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205280
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20539
        t_11 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20540
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20542
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_1, t_4, 25, t_11]
        del t_4
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205280
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20546
        t_11 = t_6.view(size=t_11)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::view20547
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20552
        t_6 = t_11.permute(dims=t_6)
        del t_11
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20554
        t_11 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20558
        t_4 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20562
        t_1 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20563
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20565
        t_1 = torch.div(input=t_1, other=25)
        t_1 = [t_11, t_4, 25, t_1]
        del t_4
        del t_11
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20569
        t_1 = t_3.view(size=t_1)
        del t_3
        t_3 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::view20570
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20575
        t_3 = t_1.permute(dims=t_3)
        del t_1
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20577
        t_1 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20581
        t_4 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20585
        t_11 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20586
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20588
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_1, t_4, 25, t_11]
        del t_4
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20592
        t_11 = t_0.view(size=t_11)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::view20593
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20598
        t_0 = t_11.permute(dims=t_0)
        del t_11
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::permute20553
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::permute20576
        t_3 = t_6.matmul(other=t_3)
        del t_6
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::matmul20600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20601
        t_3 = torch.div(input=t_3, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::div20602
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20603
        t_6 = t_3.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::div20602
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20606
        t_11 = t_3.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20607
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20604
        t_6 = torch.sub(input=t_11, other=t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20614
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20615
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20616
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20617
        t_4 = self.b_4[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::slice20618
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20619
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20620
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20621
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20622
        t_4 = t_4[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::slice20623
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20624
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::sub20612
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20607
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20625
        t_6 = t_4[:, :, t_6:t_11:1]
        del t_4
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::slice20626
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20627
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20628
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20607
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20629
        t_11 = t_6[:, :, :, 0:t_11:1]
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::div20602
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::slice20630
        t_3 = torch.mul(input=t_3, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::slice20630
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20632
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20633
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::rsub20634
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20635
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::mul20631
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::mul20636
        t_11 = torch.sub(input=t_3, other=t_11)
        del t_3
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::sub20638
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20639
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20640
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::softmax20641
        t_11 = self.l_40(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::permute20599
        t_0 = t_11.matmul(other=t_0)
        del t_11
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::matmul20643
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20648
        t_11 = t_0.permute(dims=t_11)
        del t_0
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::permute20649
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::contiguous20651
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20652
        t_0 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::contiguous20651
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20656
        t_3 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::contiguous20651
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20660
        t_6 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::contiguous20651
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20663
        t_4 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20661
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20664
        t_4 = torch.mul(input=t_6, other=t_4)
        del t_6
        t_4 = [t_0, t_3, t_4]
        del t_3
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::contiguous20651
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20668
        t_4 = t_11.view(size=t_4)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::view20669
        t_4 = self.l_41(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_42(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/aten::add20513
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=t_5, other=t_4)
        del t_5
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/aten::add20673
        t_5 = self.l_43(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2]
        t_5 = self.l_44(t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/prim::Constant20679
        t_11 = torch.mul(input=t_5, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/prim::Constant20681
        t_3 = t_5.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::pow20682
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/prim::Constant20683
        t_3 = torch.mul(input=t_3, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::mul20684
        t_3 = torch.add(input=t_5, other=t_3)
        del t_5
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::add20686
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/prim::Constant20687
        t_3 = torch.mul(input=t_3, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::mul20688
        t_3 = t_3.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::tanh20689
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/prim::Constant20690
        t_3 = torch.add(input=t_3, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::mul20680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::add20692
        t_3 = torch.mul(input=t_11, other=t_3)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::mul20693
        t_3 = self.l_45(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj]
        t_3 = self.l_46(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/aten::add20673
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout]
        t_3 = torch.add(input=t_4, other=t_3)
        del t_4
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/aten::add20697
        t_4 = self.l_47(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1]
        t_4 = self.l_48(t_4)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20709
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20710
        t_4 = t_4.split(split_size=1600, dim=2)
        t_5 = t_4[0]
        t_0 = t_4[1]
        t_4 = t_4[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207120
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20715
        t_6 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207120
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20719
        t_1 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207120
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20723
        t_12 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20724
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20726
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_1, 25, t_12]
        del t_1
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207120
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20730
        t_12 = t_5.view(size=t_12)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::view20731
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20736
        t_5 = t_12.permute(dims=t_5)
        del t_12
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207121
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20738
        t_12 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207121
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20742
        t_1 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207121
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20746
        t_6 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20747
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20749
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_12, t_1, 25, t_6]
        del t_1
        del t_12
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207121
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20753
        t_6 = t_0.view(size=t_6)
        del t_0
        t_0 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::view20754
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20759
        t_0 = t_6.permute(dims=t_0)
        del t_6
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20761
        t_6 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20765
        t_1 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20769
        t_12 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20770
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20772
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_1, 25, t_12]
        del t_1
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20776
        t_12 = t_4.view(size=t_12)
        del t_4
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::view20777
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20782
        t_4 = t_12.permute(dims=t_4)
        del t_12
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::permute20737
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::permute20760
        t_0 = t_5.matmul(other=t_0)
        del t_5
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::matmul20784
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20785
        t_0 = torch.div(input=t_0, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::div20786
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20787
        t_5 = t_0.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::div20786
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20790
        t_12 = t_0.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20791
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20788
        t_5 = torch.sub(input=t_12, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20798
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20799
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20800
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20801
        t_1 = self.b_5[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::slice20802
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20803
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20804
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20805
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20806
        t_1 = t_1[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::slice20807
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20808
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::sub20796
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20791
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20809
        t_5 = t_1[:, :, t_5:t_12:1]
        del t_1
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::slice20810
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20811
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20812
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20791
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20813
        t_12 = t_5[:, :, :, 0:t_12:1]
        del t_5
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::div20786
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::slice20814
        t_0 = torch.mul(input=t_0, other=t_12)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::slice20814
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20816
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20817
        t_12 = torch.rsub(t_12, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::rsub20818
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20819
        t_12 = torch.mul(input=t_12, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::mul20815
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::mul20820
        t_12 = torch.sub(input=t_0, other=t_12)
        del t_0
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::sub20822
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20823
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20824
        t_12 = t_12.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::softmax20825
        t_12 = self.l_49(t_12)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::permute20783
        t_4 = t_12.matmul(other=t_4)
        del t_12
        t_12 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::matmul20827
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20832
        t_12 = t_4.permute(dims=t_12)
        del t_4
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::permute20833
        t_12 = t_12.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::contiguous20835
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20836
        t_4 = t_12.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::contiguous20835
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20840
        t_0 = t_12.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::contiguous20835
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20844
        t_5 = t_12.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::contiguous20835
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20847
        t_1 = t_12.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20845
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20848
        t_1 = torch.mul(input=t_5, other=t_1)
        del t_5
        t_1 = [t_4, t_0, t_1]
        del t_0
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::contiguous20835
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20852
        t_1 = t_12.view(size=t_1)
        del t_12
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::view20853
        t_1 = self.l_50(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_proj]
        t_1 = self.l_51(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/aten::add20697
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[resid_dropout]
        t_1 = torch.add(input=t_3, other=t_1)
        del t_3
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/aten::add20857
        t_3 = self.l_52(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2]
        t_3 = self.l_53(t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/prim::Constant20863
        t_12 = torch.mul(input=t_3, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/prim::Constant20865
        t_0 = t_3.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::pow20866
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/prim::Constant20867
        t_0 = torch.mul(input=t_0, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::mul20868
        t_0 = torch.add(input=t_3, other=t_0)
        del t_3
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::add20870
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/prim::Constant20871
        t_0 = torch.mul(input=t_0, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::mul20872
        t_0 = t_0.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::tanh20873
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/prim::Constant20874
        t_0 = torch.add(input=t_0, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::mul20864
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::add20876
        t_0 = torch.mul(input=t_12, other=t_0)
        del t_12
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::mul20877
        t_0 = self.l_54(t_0)

        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/aten::add20857
        return (t_0, t_1)

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition2(nn.Module):
    SCOPES = {
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout]',
    }

    def __init__(self, layers, tensors):
        super(Partition2, self).__init__()
        # initializing partition layers
        self.scopes = []
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1]')
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2]')
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1]')
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2]')
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1]')
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2]')
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1]')
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2]')
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1]')
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2]')
        self.l_43 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_44 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_45 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_46 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1]')
        self.l_47 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_48 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_49 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_50 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_51 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2]')
        self.l_52 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_53 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_54 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout]'
        )

        # initializing partition buffers
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_4', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_5', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Tensor[bias]']
        )

        self.device = torch.device('cuda:2')
        self.lookup = {
            'l_0': 'transformer.blocks.10.mlp.dropout',
            'l_1': 'transformer.blocks.11.ln_1',
            'l_2': 'transformer.blocks.11.attn.c_attn',
            'l_3': 'transformer.blocks.11.attn.attn_dropout',
            'l_4': 'transformer.blocks.11.attn.c_proj',
            'l_5': 'transformer.blocks.11.attn.resid_dropout',
            'l_6': 'transformer.blocks.11.ln_2',
            'l_7': 'transformer.blocks.11.mlp.c_fc',
            'l_8': 'transformer.blocks.11.mlp.c_proj',
            'l_9': 'transformer.blocks.11.mlp.dropout',
            'l_10': 'transformer.blocks.12.ln_1',
            'l_11': 'transformer.blocks.12.attn.c_attn',
            'l_12': 'transformer.blocks.12.attn.attn_dropout',
            'l_13': 'transformer.blocks.12.attn.c_proj',
            'l_14': 'transformer.blocks.12.attn.resid_dropout',
            'l_15': 'transformer.blocks.12.ln_2',
            'l_16': 'transformer.blocks.12.mlp.c_fc',
            'l_17': 'transformer.blocks.12.mlp.c_proj',
            'l_18': 'transformer.blocks.12.mlp.dropout',
            'l_19': 'transformer.blocks.13.ln_1',
            'l_20': 'transformer.blocks.13.attn.c_attn',
            'l_21': 'transformer.blocks.13.attn.attn_dropout',
            'l_22': 'transformer.blocks.13.attn.c_proj',
            'l_23': 'transformer.blocks.13.attn.resid_dropout',
            'l_24': 'transformer.blocks.13.ln_2',
            'l_25': 'transformer.blocks.13.mlp.c_fc',
            'l_26': 'transformer.blocks.13.mlp.c_proj',
            'l_27': 'transformer.blocks.13.mlp.dropout',
            'l_28': 'transformer.blocks.14.ln_1',
            'l_29': 'transformer.blocks.14.attn.c_attn',
            'l_30': 'transformer.blocks.14.attn.attn_dropout',
            'l_31': 'transformer.blocks.14.attn.c_proj',
            'l_32': 'transformer.blocks.14.attn.resid_dropout',
            'l_33': 'transformer.blocks.14.ln_2',
            'l_34': 'transformer.blocks.14.mlp.c_fc',
            'l_35': 'transformer.blocks.14.mlp.c_proj',
            'l_36': 'transformer.blocks.14.mlp.dropout',
            'l_37': 'transformer.blocks.15.ln_1',
            'l_38': 'transformer.blocks.15.attn.c_attn',
            'l_39': 'transformer.blocks.15.attn.attn_dropout',
            'l_40': 'transformer.blocks.15.attn.c_proj',
            'l_41': 'transformer.blocks.15.attn.resid_dropout',
            'l_42': 'transformer.blocks.15.ln_2',
            'l_43': 'transformer.blocks.15.mlp.c_fc',
            'l_44': 'transformer.blocks.15.mlp.c_proj',
            'l_45': 'transformer.blocks.15.mlp.dropout',
            'l_46': 'transformer.blocks.16.ln_1',
            'l_47': 'transformer.blocks.16.attn.c_attn',
            'l_48': 'transformer.blocks.16.attn.attn_dropout',
            'l_49': 'transformer.blocks.16.attn.c_proj',
            'l_50': 'transformer.blocks.16.attn.resid_dropout',
            'l_51': 'transformer.blocks.16.ln_2',
            'l_52': 'transformer.blocks.16.mlp.c_fc',
            'l_53': 'transformer.blocks.16.mlp.c_proj',
            'l_54': 'transformer.blocks.16.mlp.dropout',
            'b_0': 'transformer.blocks.11.attn.bias',
            'b_1': 'transformer.blocks.12.attn.bias',
            'b_2': 'transformer.blocks.13.attn.bias',
            'b_3': 'transformer.blocks.14.attn.bias',
            'b_4': 'transformer.blocks.15.attn.bias',
            'b_5': 'transformer.blocks.16.attn.bias'
        }

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_attn] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[attn_dropout] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_proj] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[resid_dropout] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_attn] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[attn_dropout] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_proj] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[resid_dropout] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_attn] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[attn_dropout] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_proj] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[resid_dropout] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_attn] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[attn_dropout] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_proj] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[resid_dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_attn] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[attn_dropout] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_proj] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[resid_dropout] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_attn] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[attn_dropout] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_proj] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[resid_dropout] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj] <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/aten::add20857 <=> x1

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)

        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj]
        t_0 = self.l_0(x0)
        del x0
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/aten::add20857
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout]
        t_0 = torch.add(input=x1, other=t_0)
        del x1
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/aten::add20881
        t_1 = self.l_1(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1]
        t_1 = self.l_2(t_1)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20893
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20894
        t_1 = t_1.split(split_size=1600, dim=2)
        t_3 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack208960
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20899
        t_5 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack208960
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20903
        t_6 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack208960
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20907
        t_7 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size20908
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20910
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        del t_6
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack208960
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct20914
        t_7 = t_3.view(size=t_7)
        del t_3
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::view20915
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct20920
        t_3 = t_7.permute(dims=t_3)
        del t_7
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack208961
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20922
        t_7 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack208961
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20926
        t_6 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack208961
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20930
        t_5 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size20931
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20933
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_7, t_6, 25, t_5]
        del t_6
        del t_7
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack208961
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct20937
        t_5 = t_4.view(size=t_5)
        del t_4
        t_4 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::view20938
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct20943
        t_4 = t_5.permute(dims=t_4)
        del t_5
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack208962
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20945
        t_5 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack208962
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20949
        t_6 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack208962
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20953
        t_7 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size20954
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20956
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        del t_6
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack208962
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct20960
        t_7 = t_1.view(size=t_7)
        del t_1
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::view20961
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct20966
        t_1 = t_7.permute(dims=t_1)
        del t_7
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::permute20921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::permute20944
        t_4 = t_3.matmul(other=t_4)
        del t_3
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::matmul20968
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20969
        t_4 = torch.div(input=t_4, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::div20970
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20971
        t_3 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::div20970
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20974
        t_7 = t_4.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size20975
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size20972
        t_3 = torch.sub(input=t_7, other=t_3)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20982
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20983
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20984
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20985
        t_6 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::slice20986
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20987
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20988
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20989
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20990
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::slice20991
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20992
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::sub20980
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size20975
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20993
        t_3 = t_6[:, :, t_3:t_7:1]
        del t_6
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::slice20994
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20995
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20996
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size20975
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20997
        t_7 = t_3[:, :, :, 0:t_7:1]
        del t_3
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::div20970
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::slice20998
        t_4 = torch.mul(input=t_4, other=t_7)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::slice20998
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21000
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21001
        t_7 = torch.rsub(t_7, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::rsub21002
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21003
        t_7 = torch.mul(input=t_7, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::mul20999
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::mul21004
        t_7 = torch.sub(input=t_4, other=t_7)
        del t_4
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::sub21006
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21007
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21008
        t_7 = t_7.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::softmax21009
        t_7 = self.l_3(t_7)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::permute20967
        t_1 = t_7.matmul(other=t_1)
        del t_7
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::matmul21011
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct21016
        t_7 = t_1.permute(dims=t_7)
        del t_1
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::permute21017
        t_7 = t_7.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::contiguous21019
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21020
        t_1 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::contiguous21019
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21024
        t_4 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::contiguous21019
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21028
        t_3 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::contiguous21019
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21031
        t_6 = t_7.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size21029
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size21032
        t_6 = torch.mul(input=t_3, other=t_6)
        del t_3
        t_6 = [t_1, t_4, t_6]
        del t_4
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::contiguous21019
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct21036
        t_6 = t_7.view(size=t_6)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::view21037
        t_6 = self.l_4(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_5(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/aten::add20881
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_0, other=t_6)
        del t_0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/aten::add21041
        t_0 = self.l_6(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2]
        t_0 = self.l_7(t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/prim::Constant21047
        t_7 = torch.mul(input=t_0, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/prim::Constant21049
        t_4 = t_0.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::pow21050
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/prim::Constant21051
        t_4 = torch.mul(input=t_4, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::mul21052
        t_4 = torch.add(input=t_0, other=t_4)
        del t_0
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::add21054
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/prim::Constant21055
        t_4 = torch.mul(input=t_4, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::mul21056
        t_4 = t_4.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::tanh21057
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/prim::Constant21058
        t_4 = torch.add(input=t_4, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::mul21048
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::add21060
        t_4 = torch.mul(input=t_7, other=t_4)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::mul21061
        t_4 = self.l_8(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj]
        t_4 = self.l_9(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/aten::add21041
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout]
        t_4 = torch.add(input=t_6, other=t_4)
        del t_6
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/aten::add21065
        t_6 = self.l_10(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1]
        t_6 = self.l_11(t_6)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21077
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21078
        t_6 = t_6.split(split_size=1600, dim=2)
        t_0 = t_6[0]
        t_1 = t_6[1]
        t_6 = t_6[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210800
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21083
        t_3 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210800
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21087
        t_5 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210800
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21091
        t_8 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21092
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21094
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        del t_5
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210800
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21098
        t_8 = t_0.view(size=t_8)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::view21099
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21104
        t_0 = t_8.permute(dims=t_0)
        del t_8
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210801
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21106
        t_8 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210801
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21110
        t_5 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210801
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21114
        t_3 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21115
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21117
        t_3 = torch.div(input=t_3, other=25)
        t_3 = [t_8, t_5, 25, t_3]
        del t_5
        del t_8
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210801
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21121
        t_3 = t_1.view(size=t_3)
        del t_1
        t_1 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::view21122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21127
        t_1 = t_3.permute(dims=t_1)
        del t_3
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210802
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21129
        t_3 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210802
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21133
        t_5 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210802
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21137
        t_8 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21138
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21140
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        del t_5
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210802
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21144
        t_8 = t_6.view(size=t_8)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::view21145
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21150
        t_6 = t_8.permute(dims=t_6)
        del t_8
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::permute21105
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::permute21128
        t_1 = t_0.matmul(other=t_1)
        del t_0
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::matmul21152
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21153
        t_1 = torch.div(input=t_1, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::div21154
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21155
        t_0 = t_1.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::div21154
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21158
        t_8 = t_1.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21159
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21156
        t_0 = torch.sub(input=t_8, other=t_0)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21166
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21167
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21168
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21169
        t_5 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::slice21170
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21171
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21172
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21173
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21174
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::slice21175
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21176
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::sub21164
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21159
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21177
        t_0 = t_5[:, :, t_0:t_8:1]
        del t_5
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::slice21178
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21179
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21180
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21159
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21181
        t_8 = t_0[:, :, :, 0:t_8:1]
        del t_0
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::div21154
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::slice21182
        t_1 = torch.mul(input=t_1, other=t_8)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::slice21182
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21184
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21185
        t_8 = torch.rsub(t_8, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::rsub21186
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21187
        t_8 = torch.mul(input=t_8, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::mul21183
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::mul21188
        t_8 = torch.sub(input=t_1, other=t_8)
        del t_1
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::sub21190
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21191
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21192
        t_8 = t_8.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::softmax21193
        t_8 = self.l_12(t_8)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::permute21151
        t_6 = t_8.matmul(other=t_6)
        del t_8
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::matmul21195
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21200
        t_8 = t_6.permute(dims=t_8)
        del t_6
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::permute21201
        t_8 = t_8.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::contiguous21203
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21204
        t_6 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::contiguous21203
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21208
        t_1 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::contiguous21203
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21212
        t_0 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::contiguous21203
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21215
        t_5 = t_8.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21213
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21216
        t_5 = torch.mul(input=t_0, other=t_5)
        del t_0
        t_5 = [t_6, t_1, t_5]
        del t_1
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::contiguous21203
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21220
        t_5 = t_8.view(size=t_5)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::view21221
        t_5 = self.l_13(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_14(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/aten::add21065
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=t_4, other=t_5)
        del t_4
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/aten::add21225
        t_4 = self.l_15(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2]
        t_4 = self.l_16(t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/prim::Constant21231
        t_8 = torch.mul(input=t_4, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/prim::Constant21233
        t_1 = t_4.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::pow21234
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/prim::Constant21235
        t_1 = torch.mul(input=t_1, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::mul21236
        t_1 = torch.add(input=t_4, other=t_1)
        del t_4
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::add21238
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/prim::Constant21239
        t_1 = torch.mul(input=t_1, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::mul21240
        t_1 = t_1.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::tanh21241
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/prim::Constant21242
        t_1 = torch.add(input=t_1, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::mul21232
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::add21244
        t_1 = torch.mul(input=t_8, other=t_1)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::mul21245
        t_1 = self.l_17(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj]
        t_1 = self.l_18(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/aten::add21225
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout]
        t_1 = torch.add(input=t_5, other=t_1)
        del t_5
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/aten::add21249
        t_5 = self.l_19(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1]
        t_5 = self.l_20(t_5)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21261
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21262
        t_5 = t_5.split(split_size=1600, dim=2)
        t_4 = t_5[0]
        t_6 = t_5[1]
        t_5 = t_5[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212640
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21267
        t_0 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212640
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21271
        t_3 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212640
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21275
        t_9 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21276
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21278
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_0, t_3, 25, t_9]
        del t_3
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212640
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21282
        t_9 = t_4.view(size=t_9)
        del t_4
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::view21283
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21288
        t_4 = t_9.permute(dims=t_4)
        del t_9
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212641
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21290
        t_9 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212641
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21294
        t_3 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212641
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21298
        t_0 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21299
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21301
        t_0 = torch.div(input=t_0, other=25)
        t_0 = [t_9, t_3, 25, t_0]
        del t_3
        del t_9
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212641
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21305
        t_0 = t_6.view(size=t_0)
        del t_6
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::view21306
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21311
        t_6 = t_0.permute(dims=t_6)
        del t_0
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212642
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21313
        t_0 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212642
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21317
        t_3 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212642
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21321
        t_9 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21322
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21324
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_0, t_3, 25, t_9]
        del t_3
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212642
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21328
        t_9 = t_5.view(size=t_9)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::view21329
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21334
        t_5 = t_9.permute(dims=t_5)
        del t_9
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::permute21289
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::permute21312
        t_6 = t_4.matmul(other=t_6)
        del t_4
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::matmul21336
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21337
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::div21338
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21339
        t_4 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::div21338
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21342
        t_9 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21343
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21340
        t_4 = torch.sub(input=t_9, other=t_4)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21350
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21351
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21352
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21353
        t_3 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::slice21354
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21355
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21356
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21357
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21358
        t_3 = t_3[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::slice21359
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21360
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::sub21348
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21343
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21361
        t_4 = t_3[:, :, t_4:t_9:1]
        del t_3
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::slice21362
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21363
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21364
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21343
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21365
        t_9 = t_4[:, :, :, 0:t_9:1]
        del t_4
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::div21338
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::slice21366
        t_6 = torch.mul(input=t_6, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::slice21366
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21368
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21369
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::rsub21370
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21371
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::mul21367
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::mul21372
        t_9 = torch.sub(input=t_6, other=t_9)
        del t_6
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::sub21374
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21375
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21376
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::softmax21377
        t_9 = self.l_21(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::permute21335
        t_5 = t_9.matmul(other=t_5)
        del t_9
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::matmul21379
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21384
        t_9 = t_5.permute(dims=t_9)
        del t_5
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::permute21385
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::contiguous21387
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21388
        t_5 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::contiguous21387
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21392
        t_6 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::contiguous21387
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21396
        t_4 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::contiguous21387
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21399
        t_3 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21397
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21400
        t_3 = torch.mul(input=t_4, other=t_3)
        del t_4
        t_3 = [t_5, t_6, t_3]
        del t_6
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::contiguous21387
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21404
        t_3 = t_9.view(size=t_3)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::view21405
        t_3 = self.l_22(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_proj]
        t_3 = self.l_23(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/aten::add21249
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[resid_dropout]
        t_3 = torch.add(input=t_1, other=t_3)
        del t_1
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/aten::add21409
        t_1 = self.l_24(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2]
        t_1 = self.l_25(t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/prim::Constant21415
        t_9 = torch.mul(input=t_1, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/prim::Constant21417
        t_6 = t_1.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::pow21418
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/prim::Constant21419
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::mul21420
        t_6 = torch.add(input=t_1, other=t_6)
        del t_1
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::add21422
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/prim::Constant21423
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::mul21424
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::tanh21425
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/prim::Constant21426
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::mul21416
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::add21428
        t_6 = torch.mul(input=t_9, other=t_6)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::mul21429
        t_6 = self.l_26(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj]
        t_6 = self.l_27(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/aten::add21409
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout]
        t_6 = torch.add(input=t_3, other=t_6)
        del t_3
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/aten::add21433
        t_3 = self.l_28(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1]
        t_3 = self.l_29(t_3)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21445
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21446
        t_3 = t_3.split(split_size=1600, dim=2)
        t_1 = t_3[0]
        t_5 = t_3[1]
        t_3 = t_3[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214480
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21451
        t_4 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214480
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21455
        t_0 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214480
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21459
        t_10 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21460
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21462
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_0, 25, t_10]
        del t_0
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214480
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21466
        t_10 = t_1.view(size=t_10)
        del t_1
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::view21467
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21472
        t_1 = t_10.permute(dims=t_1)
        del t_10
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214481
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21474
        t_10 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214481
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21478
        t_0 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214481
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21482
        t_4 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21483
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21485
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_10, t_0, 25, t_4]
        del t_0
        del t_10
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214481
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21489
        t_4 = t_5.view(size=t_4)
        del t_5
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::view21490
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21495
        t_5 = t_4.permute(dims=t_5)
        del t_4
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214482
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21497
        t_4 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214482
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21501
        t_0 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214482
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21505
        t_10 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21506
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21508
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_0, 25, t_10]
        del t_0
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214482
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21512
        t_10 = t_3.view(size=t_10)
        del t_3
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::view21513
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21518
        t_3 = t_10.permute(dims=t_3)
        del t_10
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::permute21473
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::permute21496
        t_5 = t_1.matmul(other=t_5)
        del t_1
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::matmul21520
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21521
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::div21522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21523
        t_1 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::div21522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21526
        t_10 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21527
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21524
        t_1 = torch.sub(input=t_10, other=t_1)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21534
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21535
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21536
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21537
        t_0 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::slice21538
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21539
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21540
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21541
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21542
        t_0 = t_0[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::slice21543
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21544
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::sub21532
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21527
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21545
        t_1 = t_0[:, :, t_1:t_10:1]
        del t_0
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::slice21546
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21547
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21548
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21527
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21549
        t_10 = t_1[:, :, :, 0:t_10:1]
        del t_1
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::div21522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::slice21550
        t_5 = torch.mul(input=t_5, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::slice21550
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21552
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21553
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::rsub21554
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21555
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::mul21551
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::mul21556
        t_10 = torch.sub(input=t_5, other=t_10)
        del t_5
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::sub21558
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21559
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21560
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::softmax21561
        t_10 = self.l_30(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::permute21519
        t_3 = t_10.matmul(other=t_3)
        del t_10
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::matmul21563
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21568
        t_10 = t_3.permute(dims=t_10)
        del t_3
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::permute21569
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::contiguous21571
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21572
        t_3 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::contiguous21571
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21576
        t_5 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::contiguous21571
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21580
        t_1 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::contiguous21571
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21583
        t_0 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21581
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21584
        t_0 = torch.mul(input=t_1, other=t_0)
        del t_1
        t_0 = [t_3, t_5, t_0]
        del t_5
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::contiguous21571
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21588
        t_0 = t_10.view(size=t_0)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::view21589
        t_0 = self.l_31(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_proj]
        t_0 = self.l_32(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/aten::add21433
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[resid_dropout]
        t_0 = torch.add(input=t_6, other=t_0)
        del t_6
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/aten::add21593
        t_6 = self.l_33(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2]
        t_6 = self.l_34(t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/prim::Constant21599
        t_10 = torch.mul(input=t_6, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/prim::Constant21601
        t_5 = t_6.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::pow21602
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/prim::Constant21603
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::mul21604
        t_5 = torch.add(input=t_6, other=t_5)
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::add21606
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/prim::Constant21607
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::mul21608
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::tanh21609
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/prim::Constant21610
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::mul21600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::add21612
        t_5 = torch.mul(input=t_10, other=t_5)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::mul21613
        t_5 = self.l_35(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj]
        t_5 = self.l_36(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/aten::add21593
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout]
        t_5 = torch.add(input=t_0, other=t_5)
        del t_0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/aten::add21617
        t_0 = self.l_37(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1]
        t_0 = self.l_38(t_0)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21629
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21630
        t_0 = t_0.split(split_size=1600, dim=2)
        t_6 = t_0[0]
        t_3 = t_0[1]
        t_0 = t_0[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216320
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21635
        t_1 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216320
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21639
        t_4 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216320
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21643
        t_11 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21644
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21646
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_1, t_4, 25, t_11]
        del t_4
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216320
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21650
        t_11 = t_6.view(size=t_11)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::view21651
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21656
        t_6 = t_11.permute(dims=t_6)
        del t_11
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216321
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21658
        t_11 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216321
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21662
        t_4 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216321
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21666
        t_1 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21667
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21669
        t_1 = torch.div(input=t_1, other=25)
        t_1 = [t_11, t_4, 25, t_1]
        del t_4
        del t_11
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216321
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21673
        t_1 = t_3.view(size=t_1)
        del t_3
        t_3 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::view21674
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21679
        t_3 = t_1.permute(dims=t_3)
        del t_1
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216322
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21681
        t_1 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216322
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21685
        t_4 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216322
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21689
        t_11 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21690
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21692
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_1, t_4, 25, t_11]
        del t_4
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216322
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21696
        t_11 = t_0.view(size=t_11)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::view21697
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21702
        t_0 = t_11.permute(dims=t_0)
        del t_11
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::permute21657
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::permute21680
        t_3 = t_6.matmul(other=t_3)
        del t_6
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::matmul21704
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21705
        t_3 = torch.div(input=t_3, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::div21706
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21707
        t_6 = t_3.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::div21706
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21710
        t_11 = t_3.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21711
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21708
        t_6 = torch.sub(input=t_11, other=t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21718
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21719
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21720
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21721
        t_4 = self.b_4[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::slice21722
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21723
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21724
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21725
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21726
        t_4 = t_4[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::slice21727
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21728
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::sub21716
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21711
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21729
        t_6 = t_4[:, :, t_6:t_11:1]
        del t_4
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::slice21730
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21731
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21732
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21711
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21733
        t_11 = t_6[:, :, :, 0:t_11:1]
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::div21706
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::slice21734
        t_3 = torch.mul(input=t_3, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::slice21734
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21736
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21737
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::rsub21738
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21739
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::mul21735
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::mul21740
        t_11 = torch.sub(input=t_3, other=t_11)
        del t_3
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::sub21742
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21743
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21744
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::softmax21745
        t_11 = self.l_39(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::permute21703
        t_0 = t_11.matmul(other=t_0)
        del t_11
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::matmul21747
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21752
        t_11 = t_0.permute(dims=t_11)
        del t_0
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::permute21753
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::contiguous21755
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21756
        t_0 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::contiguous21755
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21760
        t_3 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::contiguous21755
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21764
        t_6 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::contiguous21755
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21767
        t_4 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21765
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21768
        t_4 = torch.mul(input=t_6, other=t_4)
        del t_6
        t_4 = [t_0, t_3, t_4]
        del t_3
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::contiguous21755
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21772
        t_4 = t_11.view(size=t_4)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::view21773
        t_4 = self.l_40(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_41(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/aten::add21617
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=t_5, other=t_4)
        del t_5
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/aten::add21777
        t_5 = self.l_42(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2]
        t_5 = self.l_43(t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/prim::Constant21783
        t_11 = torch.mul(input=t_5, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/prim::Constant21785
        t_3 = t_5.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::pow21786
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/prim::Constant21787
        t_3 = torch.mul(input=t_3, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::mul21788
        t_3 = torch.add(input=t_5, other=t_3)
        del t_5
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::add21790
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/prim::Constant21791
        t_3 = torch.mul(input=t_3, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::mul21792
        t_3 = t_3.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::tanh21793
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/prim::Constant21794
        t_3 = torch.add(input=t_3, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::mul21784
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::add21796
        t_3 = torch.mul(input=t_11, other=t_3)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::mul21797
        t_3 = self.l_44(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj]
        t_3 = self.l_45(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/aten::add21777
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout]
        t_3 = torch.add(input=t_4, other=t_3)
        del t_4
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/aten::add21801
        t_4 = self.l_46(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1]
        t_4 = self.l_47(t_4)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21813
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21814
        t_4 = t_4.split(split_size=1600, dim=2)
        t_5 = t_4[0]
        t_0 = t_4[1]
        t_4 = t_4[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218160
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21819
        t_6 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218160
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21823
        t_1 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218160
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21827
        t_12 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21828
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21830
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_1, 25, t_12]
        del t_1
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218160
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21834
        t_12 = t_5.view(size=t_12)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::view21835
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21840
        t_5 = t_12.permute(dims=t_5)
        del t_12
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218161
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21842
        t_12 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218161
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21846
        t_1 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218161
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21850
        t_6 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21851
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21853
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_12, t_1, 25, t_6]
        del t_1
        del t_12
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218161
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21857
        t_6 = t_0.view(size=t_6)
        del t_0
        t_0 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::view21858
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21863
        t_0 = t_6.permute(dims=t_0)
        del t_6
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218162
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21865
        t_6 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218162
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21869
        t_1 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218162
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21873
        t_12 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21874
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21876
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_1, 25, t_12]
        del t_1
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218162
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21880
        t_12 = t_4.view(size=t_12)
        del t_4
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::view21881
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21886
        t_4 = t_12.permute(dims=t_4)
        del t_12
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::permute21841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::permute21864
        t_0 = t_5.matmul(other=t_0)
        del t_5
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::matmul21888
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21889
        t_0 = torch.div(input=t_0, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::div21890
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21891
        t_5 = t_0.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::div21890
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21894
        t_12 = t_0.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21895
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21892
        t_5 = torch.sub(input=t_12, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21902
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21903
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21904
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21905
        t_1 = self.b_5[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::slice21906
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21907
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21908
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21909
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21910
        t_1 = t_1[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::slice21911
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21912
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::sub21900
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21895
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21913
        t_5 = t_1[:, :, t_5:t_12:1]
        del t_1
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::slice21914
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21915
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21916
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21895
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21917
        t_12 = t_5[:, :, :, 0:t_12:1]
        del t_5
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::div21890
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::slice21918
        t_0 = torch.mul(input=t_0, other=t_12)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::slice21918
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21920
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21921
        t_12 = torch.rsub(t_12, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::rsub21922
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21923
        t_12 = torch.mul(input=t_12, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::mul21919
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::mul21924
        t_12 = torch.sub(input=t_0, other=t_12)
        del t_0
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::sub21926
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21927
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21928
        t_12 = t_12.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::softmax21929
        t_12 = self.l_48(t_12)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::permute21887
        t_4 = t_12.matmul(other=t_4)
        del t_12
        t_12 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::matmul21931
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21936
        t_12 = t_4.permute(dims=t_12)
        del t_4
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::permute21937
        t_12 = t_12.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::contiguous21939
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21940
        t_4 = t_12.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::contiguous21939
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21944
        t_0 = t_12.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::contiguous21939
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21948
        t_5 = t_12.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::contiguous21939
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21951
        t_1 = t_12.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21949
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21952
        t_1 = torch.mul(input=t_5, other=t_1)
        del t_5
        t_1 = [t_4, t_0, t_1]
        del t_0
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::contiguous21939
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21956
        t_1 = t_12.view(size=t_1)
        del t_12
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::view21957
        t_1 = self.l_49(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_proj]
        t_1 = self.l_50(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/aten::add21801
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[resid_dropout]
        t_1 = torch.add(input=t_3, other=t_1)
        del t_3
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/aten::add21961
        t_3 = self.l_51(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2]
        t_3 = self.l_52(t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/prim::Constant21967
        t_12 = torch.mul(input=t_3, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/prim::Constant21969
        t_0 = t_3.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::pow21970
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/prim::Constant21971
        t_0 = torch.mul(input=t_0, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::mul21972
        t_0 = torch.add(input=t_3, other=t_0)
        del t_3
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::add21974
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/prim::Constant21975
        t_0 = torch.mul(input=t_0, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::mul21976
        t_0 = t_0.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::tanh21977
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/prim::Constant21978
        t_0 = torch.add(input=t_0, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::mul21968
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::add21980
        t_0 = torch.mul(input=t_12, other=t_0)
        del t_12
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::mul21981
        t_0 = self.l_53(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj]
        t_0 = self.l_54(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/aten::add21961
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout]
        t_0 = torch.add(input=t_1, other=t_0)
        del t_1

        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/aten::add21985
        return (t_0, )

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition3(nn.Module):
    SCOPES = {
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[attn_dropout]',
    }

    def __init__(self, layers, tensors):
        super(Partition3, self).__init__()
        # initializing partition layers
        self.scopes = []
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1]')
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2]')
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1]')
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2]')
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1]')
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2]')
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1]')
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2]')
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1]')
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2]')
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_43 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_44 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_45 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1]')
        self.l_46 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_47 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_48 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_49 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_50 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2]')
        self.l_51 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_52 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_53 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_54 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1]')
        self.l_55 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_56 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[attn_dropout]'
        )

        # initializing partition buffers
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_4', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_5', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_6', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Tensor[bias]']
        )

        self.device = torch.device('cuda:3')
        self.lookup = {
            'l_0': 'transformer.blocks.17.ln_1',
            'l_1': 'transformer.blocks.17.attn.c_attn',
            'l_2': 'transformer.blocks.17.attn.attn_dropout',
            'l_3': 'transformer.blocks.17.attn.c_proj',
            'l_4': 'transformer.blocks.17.attn.resid_dropout',
            'l_5': 'transformer.blocks.17.ln_2',
            'l_6': 'transformer.blocks.17.mlp.c_fc',
            'l_7': 'transformer.blocks.17.mlp.c_proj',
            'l_8': 'transformer.blocks.17.mlp.dropout',
            'l_9': 'transformer.blocks.18.ln_1',
            'l_10': 'transformer.blocks.18.attn.c_attn',
            'l_11': 'transformer.blocks.18.attn.attn_dropout',
            'l_12': 'transformer.blocks.18.attn.c_proj',
            'l_13': 'transformer.blocks.18.attn.resid_dropout',
            'l_14': 'transformer.blocks.18.ln_2',
            'l_15': 'transformer.blocks.18.mlp.c_fc',
            'l_16': 'transformer.blocks.18.mlp.c_proj',
            'l_17': 'transformer.blocks.18.mlp.dropout',
            'l_18': 'transformer.blocks.19.ln_1',
            'l_19': 'transformer.blocks.19.attn.c_attn',
            'l_20': 'transformer.blocks.19.attn.attn_dropout',
            'l_21': 'transformer.blocks.19.attn.c_proj',
            'l_22': 'transformer.blocks.19.attn.resid_dropout',
            'l_23': 'transformer.blocks.19.ln_2',
            'l_24': 'transformer.blocks.19.mlp.c_fc',
            'l_25': 'transformer.blocks.19.mlp.c_proj',
            'l_26': 'transformer.blocks.19.mlp.dropout',
            'l_27': 'transformer.blocks.20.ln_1',
            'l_28': 'transformer.blocks.20.attn.c_attn',
            'l_29': 'transformer.blocks.20.attn.attn_dropout',
            'l_30': 'transformer.blocks.20.attn.c_proj',
            'l_31': 'transformer.blocks.20.attn.resid_dropout',
            'l_32': 'transformer.blocks.20.ln_2',
            'l_33': 'transformer.blocks.20.mlp.c_fc',
            'l_34': 'transformer.blocks.20.mlp.c_proj',
            'l_35': 'transformer.blocks.20.mlp.dropout',
            'l_36': 'transformer.blocks.21.ln_1',
            'l_37': 'transformer.blocks.21.attn.c_attn',
            'l_38': 'transformer.blocks.21.attn.attn_dropout',
            'l_39': 'transformer.blocks.21.attn.c_proj',
            'l_40': 'transformer.blocks.21.attn.resid_dropout',
            'l_41': 'transformer.blocks.21.ln_2',
            'l_42': 'transformer.blocks.21.mlp.c_fc',
            'l_43': 'transformer.blocks.21.mlp.c_proj',
            'l_44': 'transformer.blocks.21.mlp.dropout',
            'l_45': 'transformer.blocks.22.ln_1',
            'l_46': 'transformer.blocks.22.attn.c_attn',
            'l_47': 'transformer.blocks.22.attn.attn_dropout',
            'l_48': 'transformer.blocks.22.attn.c_proj',
            'l_49': 'transformer.blocks.22.attn.resid_dropout',
            'l_50': 'transformer.blocks.22.ln_2',
            'l_51': 'transformer.blocks.22.mlp.c_fc',
            'l_52': 'transformer.blocks.22.mlp.c_proj',
            'l_53': 'transformer.blocks.22.mlp.dropout',
            'l_54': 'transformer.blocks.23.ln_1',
            'l_55': 'transformer.blocks.23.attn.c_attn',
            'l_56': 'transformer.blocks.23.attn.attn_dropout',
            'b_0': 'transformer.blocks.17.attn.bias',
            'b_1': 'transformer.blocks.18.attn.bias',
            'b_2': 'transformer.blocks.19.attn.bias',
            'b_3': 'transformer.blocks.20.attn.bias',
            'b_4': 'transformer.blocks.21.attn.bias',
            'b_5': 'transformer.blocks.22.attn.bias',
            'b_6': 'transformer.blocks.23.attn.bias'
        }

    def forward(self, x0):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_attn] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[attn_dropout] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_proj] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[resid_dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_attn] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[attn_dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_proj] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[resid_dropout] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_attn] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[attn_dropout] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_proj] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[resid_dropout] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_attn] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[attn_dropout] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_proj] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[resid_dropout] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_attn] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[attn_dropout] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_proj] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[resid_dropout] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_attn] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[attn_dropout] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_proj] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[resid_dropout] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_attn] <=> self.l_55
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[attn_dropout] <=> self.l_56
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Tensor[bias] <=> self.b_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/aten::add21985 <=> x0

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)

        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/aten::add21985
        t_0 = self.l_0(x0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1]
        t_0 = self.l_1(t_0)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant21997
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant21998
        t_0 = t_0.split(split_size=1600, dim=2)
        t_2 = t_0[0]
        t_3 = t_0[1]
        t_0 = t_0[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220000
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22003
        t_4 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220000
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22007
        t_5 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220000
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22011
        t_6 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22012
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22014
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_4, t_5, 25, t_6]
        del t_5
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220000
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22018
        t_6 = t_2.view(size=t_6)
        del t_2
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::view22019
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22024
        t_2 = t_6.permute(dims=t_2)
        del t_6
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22026
        t_6 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22030
        t_5 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22034
        t_4 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22035
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22037
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_6, t_5, 25, t_4]
        del t_5
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22041
        t_4 = t_3.view(size=t_4)
        del t_3
        t_3 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::view22042
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22047
        t_3 = t_4.permute(dims=t_3)
        del t_4
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220002
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22049
        t_4 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220002
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22053
        t_5 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220002
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22057
        t_6 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22058
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22060
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_4, t_5, 25, t_6]
        del t_5
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220002
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22064
        t_6 = t_0.view(size=t_6)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::view22065
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22070
        t_0 = t_6.permute(dims=t_0)
        del t_6
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::permute22025
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::permute22048
        t_3 = t_2.matmul(other=t_3)
        del t_2
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::matmul22072
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22073
        t_3 = torch.div(input=t_3, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::div22074
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22075
        t_2 = t_3.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::div22074
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22078
        t_6 = t_3.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22079
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22076
        t_2 = torch.sub(input=t_6, other=t_2)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22086
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22087
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22088
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22089
        t_5 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::slice22090
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22091
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22092
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22093
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22094
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::slice22095
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22096
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::sub22084
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22079
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22097
        t_2 = t_5[:, :, t_2:t_6:1]
        del t_5
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::slice22098
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22099
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22100
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22079
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22101
        t_6 = t_2[:, :, :, 0:t_6:1]
        del t_2
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::div22074
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::slice22102
        t_3 = torch.mul(input=t_3, other=t_6)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::slice22102
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22104
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22105
        t_6 = torch.rsub(t_6, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::rsub22106
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22107
        t_6 = torch.mul(input=t_6, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::mul22103
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::mul22108
        t_6 = torch.sub(input=t_3, other=t_6)
        del t_3
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::sub22110
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22111
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22112
        t_6 = t_6.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::softmax22113
        t_6 = self.l_2(t_6)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::permute22071
        t_0 = t_6.matmul(other=t_0)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::matmul22115
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22120
        t_6 = t_0.permute(dims=t_6)
        del t_0
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::permute22121
        t_6 = t_6.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::contiguous22123
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22124
        t_0 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::contiguous22123
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22128
        t_3 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::contiguous22123
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22132
        t_2 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::contiguous22123
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22135
        t_5 = t_6.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22133
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22136
        t_5 = torch.mul(input=t_2, other=t_5)
        del t_2
        t_5 = [t_0, t_3, t_5]
        del t_3
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::contiguous22123
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22140
        t_5 = t_6.view(size=t_5)
        del t_6
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::view22141
        t_5 = self.l_3(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_4(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/aten::add21985
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=x0, other=t_5)
        del x0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/aten::add22145
        t_6 = self.l_5(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2]
        t_6 = self.l_6(t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/prim::Constant22151
        t_3 = torch.mul(input=t_6, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/prim::Constant22153
        t_0 = t_6.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::pow22154
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/prim::Constant22155
        t_0 = torch.mul(input=t_0, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::mul22156
        t_0 = torch.add(input=t_6, other=t_0)
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::add22158
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/prim::Constant22159
        t_0 = torch.mul(input=t_0, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::mul22160
        t_0 = t_0.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::tanh22161
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/prim::Constant22162
        t_0 = torch.add(input=t_0, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::mul22152
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::add22164
        t_0 = torch.mul(input=t_3, other=t_0)
        del t_3
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::mul22165
        t_0 = self.l_7(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj]
        t_0 = self.l_8(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/aten::add22145
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout]
        t_0 = torch.add(input=t_5, other=t_0)
        del t_5
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/aten::add22169
        t_5 = self.l_9(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1]
        t_5 = self.l_10(t_5)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22181
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22182
        t_5 = t_5.split(split_size=1600, dim=2)
        t_6 = t_5[0]
        t_2 = t_5[1]
        t_5 = t_5[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221840
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22187
        t_4 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221840
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22191
        t_7 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221840
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22195
        t_8 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22196
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22198
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_4, t_7, 25, t_8]
        del t_7
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221840
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22202
        t_8 = t_6.view(size=t_8)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22203
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22208
        t_6 = t_8.permute(dims=t_6)
        del t_8
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22210
        t_8 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22214
        t_7 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22218
        t_4 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22219
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22221
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_8, t_7, 25, t_4]
        del t_7
        del t_8
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22225
        t_4 = t_2.view(size=t_4)
        del t_2
        t_2 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22226
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22231
        t_2 = t_4.permute(dims=t_2)
        del t_4
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221842
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22233
        t_4 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221842
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22237
        t_7 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221842
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22241
        t_8 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22242
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22244
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_4, t_7, 25, t_8]
        del t_7
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221842
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22248
        t_8 = t_5.view(size=t_8)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22249
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22254
        t_5 = t_8.permute(dims=t_5)
        del t_8
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::permute22209
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::permute22232
        t_2 = t_6.matmul(other=t_2)
        del t_6
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::matmul22256
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22257
        t_2 = torch.div(input=t_2, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::div22258
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22259
        t_6 = t_2.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::div22258
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22262
        t_8 = t_2.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22263
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22260
        t_6 = torch.sub(input=t_8, other=t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22270
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22271
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22272
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22273
        t_7 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::slice22274
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22275
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22276
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22277
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22278
        t_7 = t_7[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::slice22279
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22280
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::sub22268
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22263
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22281
        t_6 = t_7[:, :, t_6:t_8:1]
        del t_7
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::slice22282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22283
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22284
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22263
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22285
        t_8 = t_6[:, :, :, 0:t_8:1]
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::div22258
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::slice22286
        t_2 = torch.mul(input=t_2, other=t_8)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::slice22286
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22288
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22289
        t_8 = torch.rsub(t_8, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::rsub22290
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22291
        t_8 = torch.mul(input=t_8, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::mul22287
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::mul22292
        t_8 = torch.sub(input=t_2, other=t_8)
        del t_2
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::sub22294
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22295
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22296
        t_8 = t_8.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::softmax22297
        t_8 = self.l_11(t_8)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::permute22255
        t_5 = t_8.matmul(other=t_5)
        del t_8
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::matmul22299
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22304
        t_8 = t_5.permute(dims=t_8)
        del t_5
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::permute22305
        t_8 = t_8.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::contiguous22307
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22308
        t_5 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::contiguous22307
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22312
        t_2 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::contiguous22307
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22316
        t_6 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::contiguous22307
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22319
        t_7 = t_8.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22317
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22320
        t_7 = torch.mul(input=t_6, other=t_7)
        del t_6
        t_7 = [t_5, t_2, t_7]
        del t_2
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::contiguous22307
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22324
        t_7 = t_8.view(size=t_7)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22325
        t_7 = self.l_12(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_proj]
        t_7 = self.l_13(t_7)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/aten::add22169
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[resid_dropout]
        t_7 = torch.add(input=t_0, other=t_7)
        del t_0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/aten::add22329
        t_0 = self.l_14(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2]
        t_0 = self.l_15(t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/prim::Constant22335
        t_8 = torch.mul(input=t_0, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/prim::Constant22337
        t_2 = t_0.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::pow22338
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/prim::Constant22339
        t_2 = torch.mul(input=t_2, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::mul22340
        t_2 = torch.add(input=t_0, other=t_2)
        del t_0
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::add22342
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/prim::Constant22343
        t_2 = torch.mul(input=t_2, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::mul22344
        t_2 = t_2.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::tanh22345
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/prim::Constant22346
        t_2 = torch.add(input=t_2, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::mul22336
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::add22348
        t_2 = torch.mul(input=t_8, other=t_2)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::mul22349
        t_2 = self.l_16(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj]
        t_2 = self.l_17(t_2)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/aten::add22329
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout]
        t_2 = torch.add(input=t_7, other=t_2)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/aten::add22353
        t_7 = self.l_18(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1]
        t_7 = self.l_19(t_7)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22365
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22366
        t_7 = t_7.split(split_size=1600, dim=2)
        t_0 = t_7[0]
        t_5 = t_7[1]
        t_7 = t_7[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22371
        t_6 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22375
        t_4 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22379
        t_9 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22380
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22382
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_6, t_4, 25, t_9]
        del t_4
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22386
        t_9 = t_0.view(size=t_9)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::view22387
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22392
        t_0 = t_9.permute(dims=t_0)
        del t_9
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22394
        t_9 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22398
        t_4 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22402
        t_6 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22403
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22405
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_9, t_4, 25, t_6]
        del t_4
        del t_9
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22409
        t_6 = t_5.view(size=t_6)
        del t_5
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::view22410
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22415
        t_5 = t_6.permute(dims=t_5)
        del t_6
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223682
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22417
        t_6 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223682
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22421
        t_4 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223682
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22425
        t_9 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22426
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22428
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_6, t_4, 25, t_9]
        del t_4
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223682
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22432
        t_9 = t_7.view(size=t_9)
        del t_7
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::view22433
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22438
        t_7 = t_9.permute(dims=t_7)
        del t_9
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::permute22393
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::permute22416
        t_5 = t_0.matmul(other=t_5)
        del t_0
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::matmul22440
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22441
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::div22442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22443
        t_0 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::div22442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22446
        t_9 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22447
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22444
        t_0 = torch.sub(input=t_9, other=t_0)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22454
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22455
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22456
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22457
        t_4 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::slice22458
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22459
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22460
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22461
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22462
        t_4 = t_4[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::slice22463
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22464
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::sub22452
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22447
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22465
        t_0 = t_4[:, :, t_0:t_9:1]
        del t_4
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::slice22466
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22467
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22468
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22447
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22469
        t_9 = t_0[:, :, :, 0:t_9:1]
        del t_0
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::div22442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::slice22470
        t_5 = torch.mul(input=t_5, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::slice22470
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22472
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22473
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::rsub22474
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22475
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::mul22471
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::mul22476
        t_9 = torch.sub(input=t_5, other=t_9)
        del t_5
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::sub22478
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22479
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22480
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::softmax22481
        t_9 = self.l_20(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::permute22439
        t_7 = t_9.matmul(other=t_7)
        del t_9
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::matmul22483
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22488
        t_9 = t_7.permute(dims=t_9)
        del t_7
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::permute22489
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::contiguous22491
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22492
        t_7 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::contiguous22491
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22496
        t_5 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::contiguous22491
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22500
        t_0 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::contiguous22491
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22503
        t_4 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22501
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22504
        t_4 = torch.mul(input=t_0, other=t_4)
        del t_0
        t_4 = [t_7, t_5, t_4]
        del t_5
        del t_7
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::contiguous22491
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22508
        t_4 = t_9.view(size=t_4)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::view22509
        t_4 = self.l_21(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_22(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/aten::add22353
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=t_2, other=t_4)
        del t_2
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/aten::add22513
        t_2 = self.l_23(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2]
        t_2 = self.l_24(t_2)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/prim::Constant22519
        t_9 = torch.mul(input=t_2, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/prim::Constant22521
        t_5 = t_2.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::pow22522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/prim::Constant22523
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::mul22524
        t_5 = torch.add(input=t_2, other=t_5)
        del t_2
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::add22526
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/prim::Constant22527
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::mul22528
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::tanh22529
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/prim::Constant22530
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::mul22520
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::add22532
        t_5 = torch.mul(input=t_9, other=t_5)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::mul22533
        t_5 = self.l_25(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj]
        t_5 = self.l_26(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/aten::add22513
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout]
        t_5 = torch.add(input=t_4, other=t_5)
        del t_4
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/aten::add22537
        t_4 = self.l_27(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1]
        t_4 = self.l_28(t_4)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22549
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22550
        t_4 = t_4.split(split_size=1600, dim=2)
        t_2 = t_4[0]
        t_7 = t_4[1]
        t_4 = t_4[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225520
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22555
        t_0 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225520
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22559
        t_6 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225520
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22563
        t_10 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22564
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22566
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_0, t_6, 25, t_10]
        del t_6
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225520
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22570
        t_10 = t_2.view(size=t_10)
        del t_2
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::view22571
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22576
        t_2 = t_10.permute(dims=t_2)
        del t_10
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22578
        t_10 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22582
        t_6 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22586
        t_0 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22587
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22589
        t_0 = torch.div(input=t_0, other=25)
        t_0 = [t_10, t_6, 25, t_0]
        del t_6
        del t_10
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22593
        t_0 = t_7.view(size=t_0)
        del t_7
        t_7 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::view22594
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22599
        t_7 = t_0.permute(dims=t_7)
        del t_0
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22601
        t_0 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22605
        t_6 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22609
        t_10 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22610
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22612
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_0, t_6, 25, t_10]
        del t_6
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22616
        t_10 = t_4.view(size=t_10)
        del t_4
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::view22617
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22622
        t_4 = t_10.permute(dims=t_4)
        del t_10
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::permute22577
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::permute22600
        t_7 = t_2.matmul(other=t_7)
        del t_2
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::matmul22624
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22625
        t_7 = torch.div(input=t_7, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::div22626
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22627
        t_2 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::div22626
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22630
        t_10 = t_7.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22631
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22628
        t_2 = torch.sub(input=t_10, other=t_2)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22638
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22639
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22640
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22641
        t_6 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::slice22642
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22643
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22644
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22645
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22646
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::slice22647
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22648
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::sub22636
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22631
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22649
        t_2 = t_6[:, :, t_2:t_10:1]
        del t_6
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::slice22650
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22651
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22652
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22631
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22653
        t_10 = t_2[:, :, :, 0:t_10:1]
        del t_2
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::div22626
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::slice22654
        t_7 = torch.mul(input=t_7, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::slice22654
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22656
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22657
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::rsub22658
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22659
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::mul22655
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::mul22660
        t_10 = torch.sub(input=t_7, other=t_10)
        del t_7
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::sub22662
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22663
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22664
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::softmax22665
        t_10 = self.l_29(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::permute22623
        t_4 = t_10.matmul(other=t_4)
        del t_10
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::matmul22667
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22672
        t_10 = t_4.permute(dims=t_10)
        del t_4
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::permute22673
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::contiguous22675
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22676
        t_4 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::contiguous22675
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22680
        t_7 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::contiguous22675
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22684
        t_2 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::contiguous22675
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22687
        t_6 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22685
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22688
        t_6 = torch.mul(input=t_2, other=t_6)
        del t_2
        t_6 = [t_4, t_7, t_6]
        del t_7
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::contiguous22675
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22692
        t_6 = t_10.view(size=t_6)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::view22693
        t_6 = self.l_30(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_31(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/aten::add22537
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_5, other=t_6)
        del t_5
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/aten::add22697
        t_5 = self.l_32(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2]
        t_5 = self.l_33(t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/prim::Constant22703
        t_10 = torch.mul(input=t_5, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/prim::Constant22705
        t_7 = t_5.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::pow22706
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/prim::Constant22707
        t_7 = torch.mul(input=t_7, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::mul22708
        t_7 = torch.add(input=t_5, other=t_7)
        del t_5
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::add22710
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/prim::Constant22711
        t_7 = torch.mul(input=t_7, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::mul22712
        t_7 = t_7.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::tanh22713
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/prim::Constant22714
        t_7 = torch.add(input=t_7, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::mul22704
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::add22716
        t_7 = torch.mul(input=t_10, other=t_7)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::mul22717
        t_7 = self.l_34(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj]
        t_7 = self.l_35(t_7)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/aten::add22697
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout]
        t_7 = torch.add(input=t_6, other=t_7)
        del t_6
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/aten::add22721
        t_6 = self.l_36(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1]
        t_6 = self.l_37(t_6)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22733
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22734
        t_6 = t_6.split(split_size=1600, dim=2)
        t_5 = t_6[0]
        t_4 = t_6[1]
        t_6 = t_6[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227360
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22739
        t_2 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227360
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22743
        t_0 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227360
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22747
        t_11 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22748
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22750
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_2, t_0, 25, t_11]
        del t_0
        del t_2
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227360
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22754
        t_11 = t_5.view(size=t_11)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::view22755
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22760
        t_5 = t_11.permute(dims=t_5)
        del t_11
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22762
        t_11 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22766
        t_0 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22770
        t_2 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22771
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22773
        t_2 = torch.div(input=t_2, other=25)
        t_2 = [t_11, t_0, 25, t_2]
        del t_0
        del t_11
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22777
        t_2 = t_4.view(size=t_2)
        del t_4
        t_4 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::view22778
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22783
        t_4 = t_2.permute(dims=t_4)
        del t_2
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227362
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22785
        t_2 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227362
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22789
        t_0 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227362
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22793
        t_11 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22794
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22796
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_2, t_0, 25, t_11]
        del t_0
        del t_2
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227362
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22800
        t_11 = t_6.view(size=t_11)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::view22801
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22806
        t_6 = t_11.permute(dims=t_6)
        del t_11
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::permute22761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::permute22784
        t_4 = t_5.matmul(other=t_4)
        del t_5
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::matmul22808
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22809
        t_4 = torch.div(input=t_4, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::div22810
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22811
        t_5 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::div22810
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22814
        t_11 = t_4.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22815
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22812
        t_5 = torch.sub(input=t_11, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22822
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22823
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22824
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22825
        t_0 = self.b_4[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::slice22826
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22827
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22828
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22829
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22830
        t_0 = t_0[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::slice22831
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22832
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::sub22820
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22815
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22833
        t_5 = t_0[:, :, t_5:t_11:1]
        del t_0
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::slice22834
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22835
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22836
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22815
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22837
        t_11 = t_5[:, :, :, 0:t_11:1]
        del t_5
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::div22810
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::slice22838
        t_4 = torch.mul(input=t_4, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::slice22838
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22840
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22841
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::rsub22842
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22843
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::mul22839
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::mul22844
        t_11 = torch.sub(input=t_4, other=t_11)
        del t_4
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::sub22846
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22847
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22848
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::softmax22849
        t_11 = self.l_38(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::permute22807
        t_6 = t_11.matmul(other=t_6)
        del t_11
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::matmul22851
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22856
        t_11 = t_6.permute(dims=t_11)
        del t_6
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::permute22857
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::contiguous22859
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22860
        t_6 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::contiguous22859
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22864
        t_4 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::contiguous22859
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22868
        t_5 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::contiguous22859
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22871
        t_0 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22869
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22872
        t_0 = torch.mul(input=t_5, other=t_0)
        del t_5
        t_0 = [t_6, t_4, t_0]
        del t_4
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::contiguous22859
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22876
        t_0 = t_11.view(size=t_0)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::view22877
        t_0 = self.l_39(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_proj]
        t_0 = self.l_40(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/aten::add22721
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[resid_dropout]
        t_0 = torch.add(input=t_7, other=t_0)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/aten::add22881
        t_7 = self.l_41(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2]
        t_7 = self.l_42(t_7)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/prim::Constant22887
        t_11 = torch.mul(input=t_7, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/prim::Constant22889
        t_4 = t_7.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::pow22890
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/prim::Constant22891
        t_4 = torch.mul(input=t_4, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::mul22892
        t_4 = torch.add(input=t_7, other=t_4)
        del t_7
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::add22894
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/prim::Constant22895
        t_4 = torch.mul(input=t_4, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::mul22896
        t_4 = t_4.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::tanh22897
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/prim::Constant22898
        t_4 = torch.add(input=t_4, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::mul22888
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::add22900
        t_4 = torch.mul(input=t_11, other=t_4)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::mul22901
        t_4 = self.l_43(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj]
        t_4 = self.l_44(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/aten::add22881
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout]
        t_4 = torch.add(input=t_0, other=t_4)
        del t_0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/aten::add22905
        t_0 = self.l_45(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1]
        t_0 = self.l_46(t_0)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22917
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22918
        t_0 = t_0.split(split_size=1600, dim=2)
        t_7 = t_0[0]
        t_6 = t_0[1]
        t_0 = t_0[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229200
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22923
        t_5 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229200
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22927
        t_2 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229200
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22931
        t_12 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size22932
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22934
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_5, t_2, 25, t_12]
        del t_2
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229200
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct22938
        t_12 = t_7.view(size=t_12)
        del t_7
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::view22939
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct22944
        t_7 = t_12.permute(dims=t_7)
        del t_12
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229201
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22946
        t_12 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229201
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22950
        t_2 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229201
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22954
        t_5 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size22955
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22957
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_12, t_2, 25, t_5]
        del t_2
        del t_12
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229201
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct22961
        t_5 = t_6.view(size=t_5)
        del t_6
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::view22962
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct22967
        t_6 = t_5.permute(dims=t_6)
        del t_5
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22969
        t_5 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22973
        t_2 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22977
        t_12 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size22978
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22980
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_5, t_2, 25, t_12]
        del t_2
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct22984
        t_12 = t_0.view(size=t_12)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::view22985
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct22990
        t_0 = t_12.permute(dims=t_0)
        del t_12
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::permute22945
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::permute22968
        t_6 = t_7.matmul(other=t_6)
        del t_7
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::matmul22992
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22993
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::div22994
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22995
        t_7 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::div22994
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22998
        t_12 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size22999
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size22996
        t_7 = torch.sub(input=t_12, other=t_7)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23006
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23007
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23008
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23009
        t_2 = self.b_5[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::slice23010
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23011
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23012
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23013
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23014
        t_2 = t_2[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::slice23015
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23016
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::sub23004
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size22999
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23017
        t_7 = t_2[:, :, t_7:t_12:1]
        del t_2
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::slice23018
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23019
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23020
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size22999
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23021
        t_12 = t_7[:, :, :, 0:t_12:1]
        del t_7
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::div22994
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::slice23022
        t_6 = torch.mul(input=t_6, other=t_12)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::slice23022
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23024
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23025
        t_12 = torch.rsub(t_12, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::rsub23026
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23027
        t_12 = torch.mul(input=t_12, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::mul23023
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::mul23028
        t_12 = torch.sub(input=t_6, other=t_12)
        del t_6
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::sub23030
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23031
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23032
        t_12 = t_12.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::softmax23033
        t_12 = self.l_47(t_12)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::permute22991
        t_0 = t_12.matmul(other=t_0)
        del t_12
        t_12 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::matmul23035
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct23040
        t_12 = t_0.permute(dims=t_12)
        del t_0
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::permute23041
        t_12 = t_12.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::contiguous23043
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23044
        t_0 = t_12.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::contiguous23043
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23048
        t_6 = t_12.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::contiguous23043
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23052
        t_7 = t_12.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::contiguous23043
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23055
        t_2 = t_12.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size23053
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size23056
        t_2 = torch.mul(input=t_7, other=t_2)
        del t_7
        t_2 = [t_0, t_6, t_2]
        del t_6
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::contiguous23043
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct23060
        t_2 = t_12.view(size=t_2)
        del t_12
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::view23061
        t_2 = self.l_48(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_proj]
        t_2 = self.l_49(t_2)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/aten::add22905
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[resid_dropout]
        t_2 = torch.add(input=t_4, other=t_2)
        del t_4
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/aten::add23065
        t_4 = self.l_50(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2]
        t_4 = self.l_51(t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/prim::Constant23071
        t_12 = torch.mul(input=t_4, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/prim::Constant23073
        t_6 = t_4.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::pow23074
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/prim::Constant23075
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::mul23076
        t_6 = torch.add(input=t_4, other=t_6)
        del t_4
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::add23078
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/prim::Constant23079
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::mul23080
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::tanh23081
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/prim::Constant23082
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::mul23072
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::add23084
        t_6 = torch.mul(input=t_12, other=t_6)
        del t_12
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::mul23085
        t_6 = self.l_52(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj]
        t_6 = self.l_53(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/aten::add23065
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout]
        t_6 = torch.add(input=t_2, other=t_6)
        del t_2
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/aten::add23089
        t_2 = self.l_54(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1]
        t_2 = self.l_55(t_2)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23101
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23102
        t_2 = t_2.split(split_size=1600, dim=2)
        t_4 = t_2[0]
        t_0 = t_2[1]
        t_2 = t_2[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231040
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23107
        t_7 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231040
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23111
        t_5 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231040
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23115
        t_13 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23116
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23118
        t_13 = torch.div(input=t_13, other=25)
        t_13 = [t_7, t_5, 25, t_13]
        del t_5
        del t_7
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231040
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23122
        t_13 = t_4.view(size=t_13)
        del t_4
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::view23123
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23128
        t_4 = t_13.permute(dims=t_4)
        del t_13
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231041
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23130
        t_13 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231041
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23134
        t_5 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231041
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23138
        t_7 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23139
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23141
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_13, t_5, 25, t_7]
        del t_5
        del t_13
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231041
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23145
        t_7 = t_0.view(size=t_7)
        del t_0
        t_0 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::view23146
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23151
        t_0 = t_7.permute(dims=t_0)
        del t_7
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231042
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23153
        t_7 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231042
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23157
        t_5 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231042
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23161
        t_13 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23162
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23164
        t_13 = torch.div(input=t_13, other=25)
        t_13 = [t_7, t_5, 25, t_13]
        del t_5
        del t_7
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231042
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23168
        t_13 = t_2.view(size=t_13)
        del t_2
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::view23169
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23174
        t_2 = t_13.permute(dims=t_2)
        del t_13
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::permute23129
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::permute23152
        t_0 = t_4.matmul(other=t_0)
        del t_4
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::matmul23176
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23177
        t_0 = torch.div(input=t_0, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::div23178
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23179
        t_4 = t_0.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::div23178
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23182
        t_13 = t_0.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23183
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23180
        t_4 = torch.sub(input=t_13, other=t_4)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23190
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23191
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23192
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23193
        t_5 = self.b_6[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::slice23194
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23195
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23196
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23197
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23198
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::slice23199
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23200
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::sub23188
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23183
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23201
        t_4 = t_5[:, :, t_4:t_13:1]
        del t_5
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::slice23202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23203
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23204
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23183
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23205
        t_13 = t_4[:, :, :, 0:t_13:1]
        del t_4
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::div23178
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::slice23206
        t_0 = torch.mul(input=t_0, other=t_13)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::slice23206
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23208
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23209
        t_13 = torch.rsub(t_13, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::rsub23210
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23211
        t_13 = torch.mul(input=t_13, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::mul23207
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::mul23212
        t_13 = torch.sub(input=t_0, other=t_13)
        del t_0
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::sub23214
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23215
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23216
        t_13 = t_13.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::softmax23217
        t_13 = self.l_56(t_13)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::permute23175
        t_2 = t_13.matmul(other=t_2)
        del t_13
        t_13 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::matmul23219
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23224
        t_13 = t_2.permute(dims=t_13)
        del t_2

        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/aten::add23089
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::permute23225
        return (t_6, t_13)

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition4(nn.Module):
    SCOPES = {
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[attn_dropout]',
    }

    def __init__(self, layers, tensors):
        super(Partition4, self).__init__()
        # initializing partition layers
        self.scopes = []
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2]')
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1]')
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2]')
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1]')
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2]')
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1]')
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2]')
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1]')
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2]')
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1]')
        self.l_43 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_44 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_45 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_46 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_47 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2]')
        self.l_48 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_49 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_50 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_51 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1]')
        self.l_52 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_53 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[attn_dropout]'
        )

        # initializing partition buffers
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_4', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_5', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Tensor[bias]']
        )

        self.device = torch.device('cuda:4')
        self.lookup = {
            'l_0': 'transformer.blocks.23.attn.c_proj',
            'l_1': 'transformer.blocks.23.attn.resid_dropout',
            'l_2': 'transformer.blocks.23.ln_2',
            'l_3': 'transformer.blocks.23.mlp.c_fc',
            'l_4': 'transformer.blocks.23.mlp.c_proj',
            'l_5': 'transformer.blocks.23.mlp.dropout',
            'l_6': 'transformer.blocks.24.ln_1',
            'l_7': 'transformer.blocks.24.attn.c_attn',
            'l_8': 'transformer.blocks.24.attn.attn_dropout',
            'l_9': 'transformer.blocks.24.attn.c_proj',
            'l_10': 'transformer.blocks.24.attn.resid_dropout',
            'l_11': 'transformer.blocks.24.ln_2',
            'l_12': 'transformer.blocks.24.mlp.c_fc',
            'l_13': 'transformer.blocks.24.mlp.c_proj',
            'l_14': 'transformer.blocks.24.mlp.dropout',
            'l_15': 'transformer.blocks.25.ln_1',
            'l_16': 'transformer.blocks.25.attn.c_attn',
            'l_17': 'transformer.blocks.25.attn.attn_dropout',
            'l_18': 'transformer.blocks.25.attn.c_proj',
            'l_19': 'transformer.blocks.25.attn.resid_dropout',
            'l_20': 'transformer.blocks.25.ln_2',
            'l_21': 'transformer.blocks.25.mlp.c_fc',
            'l_22': 'transformer.blocks.25.mlp.c_proj',
            'l_23': 'transformer.blocks.25.mlp.dropout',
            'l_24': 'transformer.blocks.26.ln_1',
            'l_25': 'transformer.blocks.26.attn.c_attn',
            'l_26': 'transformer.blocks.26.attn.attn_dropout',
            'l_27': 'transformer.blocks.26.attn.c_proj',
            'l_28': 'transformer.blocks.26.attn.resid_dropout',
            'l_29': 'transformer.blocks.26.ln_2',
            'l_30': 'transformer.blocks.26.mlp.c_fc',
            'l_31': 'transformer.blocks.26.mlp.c_proj',
            'l_32': 'transformer.blocks.26.mlp.dropout',
            'l_33': 'transformer.blocks.27.ln_1',
            'l_34': 'transformer.blocks.27.attn.c_attn',
            'l_35': 'transformer.blocks.27.attn.attn_dropout',
            'l_36': 'transformer.blocks.27.attn.c_proj',
            'l_37': 'transformer.blocks.27.attn.resid_dropout',
            'l_38': 'transformer.blocks.27.ln_2',
            'l_39': 'transformer.blocks.27.mlp.c_fc',
            'l_40': 'transformer.blocks.27.mlp.c_proj',
            'l_41': 'transformer.blocks.27.mlp.dropout',
            'l_42': 'transformer.blocks.28.ln_1',
            'l_43': 'transformer.blocks.28.attn.c_attn',
            'l_44': 'transformer.blocks.28.attn.attn_dropout',
            'l_45': 'transformer.blocks.28.attn.c_proj',
            'l_46': 'transformer.blocks.28.attn.resid_dropout',
            'l_47': 'transformer.blocks.28.ln_2',
            'l_48': 'transformer.blocks.28.mlp.c_fc',
            'l_49': 'transformer.blocks.28.mlp.c_proj',
            'l_50': 'transformer.blocks.28.mlp.dropout',
            'l_51': 'transformer.blocks.29.ln_1',
            'l_52': 'transformer.blocks.29.attn.c_attn',
            'l_53': 'transformer.blocks.29.attn.attn_dropout',
            'b_0': 'transformer.blocks.24.attn.bias',
            'b_1': 'transformer.blocks.25.attn.bias',
            'b_2': 'transformer.blocks.26.attn.bias',
            'b_3': 'transformer.blocks.27.attn.bias',
            'b_4': 'transformer.blocks.28.attn.bias',
            'b_5': 'transformer.blocks.29.attn.bias'
        }

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_proj] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[resid_dropout] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_attn] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[attn_dropout] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_proj] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[resid_dropout] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_attn] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[attn_dropout] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_proj] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[resid_dropout] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_attn] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[attn_dropout] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_proj] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[resid_dropout] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_attn] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[attn_dropout] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_proj] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[resid_dropout] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_attn] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[attn_dropout] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_proj] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[resid_dropout] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_attn] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[attn_dropout] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/aten::add23089 <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::permute23225 <=> x1

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)

        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::permute23225
        t_0 = x1.contiguous()
        del x1
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::contiguous23227
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23228
        t_1 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::contiguous23227
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23232
        t_2 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::contiguous23227
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23236
        t_3 = t_0.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::contiguous23227
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23239
        t_4 = t_0.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23237
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23240
        t_4 = torch.mul(input=t_3, other=t_4)
        del t_3
        t_4 = [t_1, t_2, t_4]
        del t_2
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::contiguous23227
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23244
        t_4 = t_0.view(size=t_4)
        del t_0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::view23245
        t_4 = self.l_0(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_1(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/aten::add23089
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=x0, other=t_4)
        del x0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/aten::add23249
        t_0 = self.l_2(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2]
        t_0 = self.l_3(t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/prim::Constant23255
        t_2 = torch.mul(input=t_0, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/prim::Constant23257
        t_1 = t_0.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::pow23258
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/prim::Constant23259
        t_1 = torch.mul(input=t_1, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::mul23260
        t_1 = torch.add(input=t_0, other=t_1)
        del t_0
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::add23262
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/prim::Constant23263
        t_1 = torch.mul(input=t_1, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::mul23264
        t_1 = t_1.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::tanh23265
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/prim::Constant23266
        t_1 = torch.add(input=t_1, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::mul23256
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::add23268
        t_1 = torch.mul(input=t_2, other=t_1)
        del t_2
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::mul23269
        t_1 = self.l_4(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj]
        t_1 = self.l_5(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/aten::add23249
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout]
        t_1 = torch.add(input=t_4, other=t_1)
        del t_4
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/aten::add23273
        t_4 = self.l_6(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1]
        t_4 = self.l_7(t_4)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23285
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23286
        t_4 = t_4.split(split_size=1600, dim=2)
        t_0 = t_4[0]
        t_3 = t_4[1]
        t_4 = t_4[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232880
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23291
        t_5 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232880
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23295
        t_6 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232880
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23299
        t_7 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23300
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23302
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        del t_6
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232880
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23306
        t_7 = t_0.view(size=t_7)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::view23307
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23312
        t_0 = t_7.permute(dims=t_0)
        del t_7
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232881
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23314
        t_7 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232881
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23318
        t_6 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232881
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23322
        t_5 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23323
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23325
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_7, t_6, 25, t_5]
        del t_6
        del t_7
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232881
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23329
        t_5 = t_3.view(size=t_5)
        del t_3
        t_3 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::view23330
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23335
        t_3 = t_5.permute(dims=t_3)
        del t_5
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232882
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23337
        t_5 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232882
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23341
        t_6 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232882
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23345
        t_7 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23346
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23348
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        del t_6
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232882
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23352
        t_7 = t_4.view(size=t_7)
        del t_4
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::view23353
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23358
        t_4 = t_7.permute(dims=t_4)
        del t_7
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::permute23313
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::permute23336
        t_3 = t_0.matmul(other=t_3)
        del t_0
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::matmul23360
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23361
        t_3 = torch.div(input=t_3, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::div23362
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23363
        t_0 = t_3.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::div23362
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23366
        t_7 = t_3.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23367
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23364
        t_0 = torch.sub(input=t_7, other=t_0)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23374
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23375
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23376
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23377
        t_6 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::slice23378
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23379
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23380
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23381
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23382
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::slice23383
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23384
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::sub23372
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23367
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23385
        t_0 = t_6[:, :, t_0:t_7:1]
        del t_6
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::slice23386
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23387
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23388
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23367
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23389
        t_7 = t_0[:, :, :, 0:t_7:1]
        del t_0
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::div23362
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::slice23390
        t_3 = torch.mul(input=t_3, other=t_7)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::slice23390
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23392
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23393
        t_7 = torch.rsub(t_7, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::rsub23394
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23395
        t_7 = torch.mul(input=t_7, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::mul23391
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::mul23396
        t_7 = torch.sub(input=t_3, other=t_7)
        del t_3
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::sub23398
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23399
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23400
        t_7 = t_7.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::softmax23401
        t_7 = self.l_8(t_7)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::permute23359
        t_4 = t_7.matmul(other=t_4)
        del t_7
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::matmul23403
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23408
        t_7 = t_4.permute(dims=t_7)
        del t_4
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::permute23409
        t_7 = t_7.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::contiguous23411
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23412
        t_4 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::contiguous23411
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23416
        t_3 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::contiguous23411
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23420
        t_0 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::contiguous23411
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23423
        t_6 = t_7.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23421
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23424
        t_6 = torch.mul(input=t_0, other=t_6)
        del t_0
        t_6 = [t_4, t_3, t_6]
        del t_3
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::contiguous23411
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23428
        t_6 = t_7.view(size=t_6)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::view23429
        t_6 = self.l_9(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_10(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/aten::add23273
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_1, other=t_6)
        del t_1
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/aten::add23433
        t_1 = self.l_11(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2]
        t_1 = self.l_12(t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/prim::Constant23439
        t_7 = torch.mul(input=t_1, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/prim::Constant23441
        t_3 = t_1.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::pow23442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/prim::Constant23443
        t_3 = torch.mul(input=t_3, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::mul23444
        t_3 = torch.add(input=t_1, other=t_3)
        del t_1
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::add23446
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/prim::Constant23447
        t_3 = torch.mul(input=t_3, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::mul23448
        t_3 = t_3.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::tanh23449
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/prim::Constant23450
        t_3 = torch.add(input=t_3, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::mul23440
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::add23452
        t_3 = torch.mul(input=t_7, other=t_3)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::mul23453
        t_3 = self.l_13(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj]
        t_3 = self.l_14(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/aten::add23433
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout]
        t_3 = torch.add(input=t_6, other=t_3)
        del t_6
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/aten::add23457
        t_6 = self.l_15(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1]
        t_6 = self.l_16(t_6)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23469
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23470
        t_6 = t_6.split(split_size=1600, dim=2)
        t_1 = t_6[0]
        t_4 = t_6[1]
        t_6 = t_6[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234720
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23475
        t_0 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234720
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23479
        t_5 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234720
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23483
        t_8 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23484
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23486
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_0, t_5, 25, t_8]
        del t_5
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234720
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23490
        t_8 = t_1.view(size=t_8)
        del t_1
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::view23491
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23496
        t_1 = t_8.permute(dims=t_1)
        del t_8
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234721
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23498
        t_8 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234721
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23502
        t_5 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234721
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23506
        t_0 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23507
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23509
        t_0 = torch.div(input=t_0, other=25)
        t_0 = [t_8, t_5, 25, t_0]
        del t_5
        del t_8
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234721
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23513
        t_0 = t_4.view(size=t_0)
        del t_4
        t_4 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::view23514
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23519
        t_4 = t_0.permute(dims=t_4)
        del t_0
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234722
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23521
        t_0 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234722
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23525
        t_5 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234722
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23529
        t_8 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23530
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23532
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_0, t_5, 25, t_8]
        del t_5
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234722
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23536
        t_8 = t_6.view(size=t_8)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::view23537
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23542
        t_6 = t_8.permute(dims=t_6)
        del t_8
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::permute23497
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::permute23520
        t_4 = t_1.matmul(other=t_4)
        del t_1
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::matmul23544
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23545
        t_4 = torch.div(input=t_4, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::div23546
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23547
        t_1 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::div23546
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23550
        t_8 = t_4.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23551
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23548
        t_1 = torch.sub(input=t_8, other=t_1)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23558
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23559
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23560
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23561
        t_5 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::slice23562
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23563
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23564
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23565
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23566
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::slice23567
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23568
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::sub23556
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23551
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23569
        t_1 = t_5[:, :, t_1:t_8:1]
        del t_5
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::slice23570
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23571
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23572
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23551
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23573
        t_8 = t_1[:, :, :, 0:t_8:1]
        del t_1
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::div23546
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::slice23574
        t_4 = torch.mul(input=t_4, other=t_8)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::slice23574
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23576
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23577
        t_8 = torch.rsub(t_8, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::rsub23578
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23579
        t_8 = torch.mul(input=t_8, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::mul23575
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::mul23580
        t_8 = torch.sub(input=t_4, other=t_8)
        del t_4
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::sub23582
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23583
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23584
        t_8 = t_8.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::softmax23585
        t_8 = self.l_17(t_8)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::permute23543
        t_6 = t_8.matmul(other=t_6)
        del t_8
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::matmul23587
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23592
        t_8 = t_6.permute(dims=t_8)
        del t_6
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::permute23593
        t_8 = t_8.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::contiguous23595
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23596
        t_6 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::contiguous23595
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23600
        t_4 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::contiguous23595
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23604
        t_1 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::contiguous23595
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23607
        t_5 = t_8.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23605
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23608
        t_5 = torch.mul(input=t_1, other=t_5)
        del t_1
        t_5 = [t_6, t_4, t_5]
        del t_4
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::contiguous23595
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23612
        t_5 = t_8.view(size=t_5)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::view23613
        t_5 = self.l_18(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_19(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/aten::add23457
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=t_3, other=t_5)
        del t_3
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/aten::add23617
        t_3 = self.l_20(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2]
        t_3 = self.l_21(t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/prim::Constant23623
        t_8 = torch.mul(input=t_3, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/prim::Constant23625
        t_4 = t_3.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::pow23626
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/prim::Constant23627
        t_4 = torch.mul(input=t_4, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::mul23628
        t_4 = torch.add(input=t_3, other=t_4)
        del t_3
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::add23630
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/prim::Constant23631
        t_4 = torch.mul(input=t_4, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::mul23632
        t_4 = t_4.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::tanh23633
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/prim::Constant23634
        t_4 = torch.add(input=t_4, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::mul23624
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::add23636
        t_4 = torch.mul(input=t_8, other=t_4)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::mul23637
        t_4 = self.l_22(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj]
        t_4 = self.l_23(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/aten::add23617
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout]
        t_4 = torch.add(input=t_5, other=t_4)
        del t_5
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/aten::add23641
        t_5 = self.l_24(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1]
        t_5 = self.l_25(t_5)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23653
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23654
        t_5 = t_5.split(split_size=1600, dim=2)
        t_3 = t_5[0]
        t_6 = t_5[1]
        t_5 = t_5[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236560
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23659
        t_1 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236560
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23663
        t_0 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236560
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23667
        t_9 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23668
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23670
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_1, t_0, 25, t_9]
        del t_0
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236560
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23674
        t_9 = t_3.view(size=t_9)
        del t_3
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::view23675
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23680
        t_3 = t_9.permute(dims=t_3)
        del t_9
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236561
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23682
        t_9 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236561
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23686
        t_0 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236561
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23690
        t_1 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23691
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23693
        t_1 = torch.div(input=t_1, other=25)
        t_1 = [t_9, t_0, 25, t_1]
        del t_0
        del t_9
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236561
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23697
        t_1 = t_6.view(size=t_1)
        del t_6
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::view23698
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23703
        t_6 = t_1.permute(dims=t_6)
        del t_1
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236562
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23705
        t_1 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236562
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23709
        t_0 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236562
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23713
        t_9 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23714
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23716
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_1, t_0, 25, t_9]
        del t_0
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236562
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23720
        t_9 = t_5.view(size=t_9)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::view23721
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23726
        t_5 = t_9.permute(dims=t_5)
        del t_9
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::permute23681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::permute23704
        t_6 = t_3.matmul(other=t_6)
        del t_3
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::matmul23728
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23729
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::div23730
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23731
        t_3 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::div23730
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23734
        t_9 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23735
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23732
        t_3 = torch.sub(input=t_9, other=t_3)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23742
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23743
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23744
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23745
        t_0 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::slice23746
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23747
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23748
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23749
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23750
        t_0 = t_0[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::slice23751
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23752
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::sub23740
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23735
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23753
        t_3 = t_0[:, :, t_3:t_9:1]
        del t_0
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::slice23754
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23755
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23756
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23735
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23757
        t_9 = t_3[:, :, :, 0:t_9:1]
        del t_3
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::div23730
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::slice23758
        t_6 = torch.mul(input=t_6, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::slice23758
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23761
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::rsub23762
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23763
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::mul23759
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::mul23764
        t_9 = torch.sub(input=t_6, other=t_9)
        del t_6
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::sub23766
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23767
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23768
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::softmax23769
        t_9 = self.l_26(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::permute23727
        t_5 = t_9.matmul(other=t_5)
        del t_9
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::matmul23771
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23776
        t_9 = t_5.permute(dims=t_9)
        del t_5
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::permute23777
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::contiguous23779
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23780
        t_5 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::contiguous23779
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23784
        t_6 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::contiguous23779
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23788
        t_3 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::contiguous23779
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23791
        t_0 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23789
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23792
        t_0 = torch.mul(input=t_3, other=t_0)
        del t_3
        t_0 = [t_5, t_6, t_0]
        del t_6
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::contiguous23779
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23796
        t_0 = t_9.view(size=t_0)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::view23797
        t_0 = self.l_27(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_proj]
        t_0 = self.l_28(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/aten::add23641
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[resid_dropout]
        t_0 = torch.add(input=t_4, other=t_0)
        del t_4
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/aten::add23801
        t_4 = self.l_29(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2]
        t_4 = self.l_30(t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/prim::Constant23807
        t_9 = torch.mul(input=t_4, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/prim::Constant23809
        t_6 = t_4.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::pow23810
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/prim::Constant23811
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::mul23812
        t_6 = torch.add(input=t_4, other=t_6)
        del t_4
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::add23814
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/prim::Constant23815
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::mul23816
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::tanh23817
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/prim::Constant23818
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::mul23808
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::add23820
        t_6 = torch.mul(input=t_9, other=t_6)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::mul23821
        t_6 = self.l_31(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj]
        t_6 = self.l_32(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/aten::add23801
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout]
        t_6 = torch.add(input=t_0, other=t_6)
        del t_0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/aten::add23825
        t_0 = self.l_33(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1]
        t_0 = self.l_34(t_0)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23837
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23838
        t_0 = t_0.split(split_size=1600, dim=2)
        t_4 = t_0[0]
        t_5 = t_0[1]
        t_0 = t_0[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238400
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23843
        t_3 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238400
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23847
        t_1 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238400
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23851
        t_10 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23852
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23854
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_3, t_1, 25, t_10]
        del t_1
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238400
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23858
        t_10 = t_4.view(size=t_10)
        del t_4
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::view23859
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23864
        t_4 = t_10.permute(dims=t_4)
        del t_10
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238401
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23866
        t_10 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238401
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23870
        t_1 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238401
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23874
        t_3 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23875
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23877
        t_3 = torch.div(input=t_3, other=25)
        t_3 = [t_10, t_1, 25, t_3]
        del t_1
        del t_10
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238401
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23881
        t_3 = t_5.view(size=t_3)
        del t_5
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::view23882
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23887
        t_5 = t_3.permute(dims=t_5)
        del t_3
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238402
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23889
        t_3 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238402
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23893
        t_1 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238402
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23897
        t_10 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23898
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23900
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_3, t_1, 25, t_10]
        del t_1
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238402
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23904
        t_10 = t_0.view(size=t_10)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::view23905
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23910
        t_0 = t_10.permute(dims=t_0)
        del t_10
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::permute23865
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::permute23888
        t_5 = t_4.matmul(other=t_5)
        del t_4
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::matmul23912
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23913
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::div23914
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23915
        t_4 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::div23914
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23918
        t_10 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23919
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23916
        t_4 = torch.sub(input=t_10, other=t_4)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23926
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23927
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23928
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23929
        t_1 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::slice23930
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23931
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23932
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23933
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23934
        t_1 = t_1[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::slice23935
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23936
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::sub23924
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23919
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23937
        t_4 = t_1[:, :, t_4:t_10:1]
        del t_1
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::slice23938
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23939
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23940
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23919
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23941
        t_10 = t_4[:, :, :, 0:t_10:1]
        del t_4
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::div23914
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::slice23942
        t_5 = torch.mul(input=t_5, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::slice23942
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23944
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23945
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::rsub23946
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23947
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::mul23943
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::mul23948
        t_10 = torch.sub(input=t_5, other=t_10)
        del t_5
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::sub23950
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23951
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23952
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::softmax23953
        t_10 = self.l_35(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::permute23911
        t_0 = t_10.matmul(other=t_0)
        del t_10
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::matmul23955
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23960
        t_10 = t_0.permute(dims=t_10)
        del t_0
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::permute23961
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::contiguous23963
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23964
        t_0 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::contiguous23963
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23968
        t_5 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::contiguous23963
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23972
        t_4 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::contiguous23963
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23975
        t_1 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23973
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23976
        t_1 = torch.mul(input=t_4, other=t_1)
        del t_4
        t_1 = [t_0, t_5, t_1]
        del t_5
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::contiguous23963
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23980
        t_1 = t_10.view(size=t_1)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::view23981
        t_1 = self.l_36(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_proj]
        t_1 = self.l_37(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/aten::add23825
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[resid_dropout]
        t_1 = torch.add(input=t_6, other=t_1)
        del t_6
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/aten::add23985
        t_6 = self.l_38(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2]
        t_6 = self.l_39(t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/prim::Constant23991
        t_10 = torch.mul(input=t_6, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/prim::Constant23993
        t_5 = t_6.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::pow23994
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/prim::Constant23995
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::mul23996
        t_5 = torch.add(input=t_6, other=t_5)
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::add23998
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/prim::Constant23999
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::mul24000
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::tanh24001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/prim::Constant24002
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::mul23992
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::add24004
        t_5 = torch.mul(input=t_10, other=t_5)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::mul24005
        t_5 = self.l_40(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj]
        t_5 = self.l_41(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/aten::add23985
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout]
        t_5 = torch.add(input=t_1, other=t_5)
        del t_1
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/aten::add24009
        t_1 = self.l_42(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1]
        t_1 = self.l_43(t_1)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24021
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24022
        t_1 = t_1.split(split_size=1600, dim=2)
        t_6 = t_1[0]
        t_0 = t_1[1]
        t_1 = t_1[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240240
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24027
        t_4 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240240
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24031
        t_3 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240240
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24035
        t_11 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24036
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24038
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_4, t_3, 25, t_11]
        del t_3
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240240
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24042
        t_11 = t_6.view(size=t_11)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::view24043
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24048
        t_6 = t_11.permute(dims=t_6)
        del t_11
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240241
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24050
        t_11 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240241
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24054
        t_3 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240241
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24058
        t_4 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24059
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24061
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_11, t_3, 25, t_4]
        del t_3
        del t_11
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240241
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24065
        t_4 = t_0.view(size=t_4)
        del t_0
        t_0 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::view24066
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24071
        t_0 = t_4.permute(dims=t_0)
        del t_4
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240242
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24073
        t_4 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240242
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24077
        t_3 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240242
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24081
        t_11 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24082
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24084
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_4, t_3, 25, t_11]
        del t_3
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240242
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24088
        t_11 = t_1.view(size=t_11)
        del t_1
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::view24089
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24094
        t_1 = t_11.permute(dims=t_1)
        del t_11
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::permute24049
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::permute24072
        t_0 = t_6.matmul(other=t_0)
        del t_6
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::matmul24096
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24097
        t_0 = torch.div(input=t_0, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::div24098
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24099
        t_6 = t_0.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::div24098
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24102
        t_11 = t_0.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24103
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24100
        t_6 = torch.sub(input=t_11, other=t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24110
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24111
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24112
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24113
        t_3 = self.b_4[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::slice24114
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24115
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24116
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24117
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24118
        t_3 = t_3[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::slice24119
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24120
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::sub24108
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24103
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24121
        t_6 = t_3[:, :, t_6:t_11:1]
        del t_3
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::slice24122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24123
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24124
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24103
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24125
        t_11 = t_6[:, :, :, 0:t_11:1]
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::div24098
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::slice24126
        t_0 = torch.mul(input=t_0, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::slice24126
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24128
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24129
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::rsub24130
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24131
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::mul24127
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::mul24132
        t_11 = torch.sub(input=t_0, other=t_11)
        del t_0
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::sub24134
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24135
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24136
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::softmax24137
        t_11 = self.l_44(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::permute24095
        t_1 = t_11.matmul(other=t_1)
        del t_11
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::matmul24139
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24144
        t_11 = t_1.permute(dims=t_11)
        del t_1
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::permute24145
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::contiguous24147
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24148
        t_1 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::contiguous24147
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24152
        t_0 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::contiguous24147
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24156
        t_6 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::contiguous24147
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24159
        t_3 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24157
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24160
        t_3 = torch.mul(input=t_6, other=t_3)
        del t_6
        t_3 = [t_1, t_0, t_3]
        del t_0
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::contiguous24147
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24164
        t_3 = t_11.view(size=t_3)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::view24165
        t_3 = self.l_45(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_proj]
        t_3 = self.l_46(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/aten::add24009
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[resid_dropout]
        t_3 = torch.add(input=t_5, other=t_3)
        del t_5
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/aten::add24169
        t_5 = self.l_47(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2]
        t_5 = self.l_48(t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/prim::Constant24175
        t_11 = torch.mul(input=t_5, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/prim::Constant24177
        t_0 = t_5.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::pow24178
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/prim::Constant24179
        t_0 = torch.mul(input=t_0, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::mul24180
        t_0 = torch.add(input=t_5, other=t_0)
        del t_5
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::add24182
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/prim::Constant24183
        t_0 = torch.mul(input=t_0, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::mul24184
        t_0 = t_0.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::tanh24185
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/prim::Constant24186
        t_0 = torch.add(input=t_0, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::mul24176
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::add24188
        t_0 = torch.mul(input=t_11, other=t_0)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::mul24189
        t_0 = self.l_49(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj]
        t_0 = self.l_50(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/aten::add24169
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout]
        t_0 = torch.add(input=t_3, other=t_0)
        del t_3
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/aten::add24193
        t_3 = self.l_51(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1]
        t_3 = self.l_52(t_3)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24205
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24206
        t_3 = t_3.split(split_size=1600, dim=2)
        t_5 = t_3[0]
        t_1 = t_3[1]
        t_3 = t_3[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242080
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24211
        t_6 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242080
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24215
        t_4 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242080
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24219
        t_12 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24220
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24222
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_4, 25, t_12]
        del t_4
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242080
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24226
        t_12 = t_5.view(size=t_12)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::view24227
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24232
        t_5 = t_12.permute(dims=t_5)
        del t_12
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242081
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24234
        t_12 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242081
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24238
        t_4 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242081
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24242
        t_6 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24243
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24245
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_12, t_4, 25, t_6]
        del t_4
        del t_12
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242081
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24249
        t_6 = t_1.view(size=t_6)
        del t_1
        t_1 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::view24250
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24255
        t_1 = t_6.permute(dims=t_1)
        del t_6
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242082
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24257
        t_6 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242082
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24261
        t_4 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242082
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24265
        t_12 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24266
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24268
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_4, 25, t_12]
        del t_4
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242082
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24272
        t_12 = t_3.view(size=t_12)
        del t_3
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::view24273
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24278
        t_3 = t_12.permute(dims=t_3)
        del t_12
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::permute24233
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::permute24256
        t_1 = t_5.matmul(other=t_1)
        del t_5
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::matmul24280
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24281
        t_1 = torch.div(input=t_1, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::div24282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24283
        t_5 = t_1.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::div24282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24286
        t_12 = t_1.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24287
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24284
        t_5 = torch.sub(input=t_12, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24294
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24295
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24296
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24297
        t_4 = self.b_5[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::slice24298
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24299
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24300
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24301
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24302
        t_4 = t_4[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::slice24303
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24304
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::sub24292
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24287
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24305
        t_5 = t_4[:, :, t_5:t_12:1]
        del t_4
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::slice24306
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24307
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24308
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24287
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24309
        t_12 = t_5[:, :, :, 0:t_12:1]
        del t_5
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::div24282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::slice24310
        t_1 = torch.mul(input=t_1, other=t_12)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::slice24310
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24312
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24313
        t_12 = torch.rsub(t_12, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::rsub24314
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24315
        t_12 = torch.mul(input=t_12, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::mul24311
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::mul24316
        t_12 = torch.sub(input=t_1, other=t_12)
        del t_1
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::sub24318
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24319
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24320
        t_12 = t_12.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::softmax24321
        t_12 = self.l_53(t_12)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::permute24279
        t_3 = t_12.matmul(other=t_3)
        del t_12

        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/aten::add24193
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::matmul24323
        return (t_0, t_3)

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition5(nn.Module):
    SCOPES = {
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout]',
    }

    def __init__(self, layers, tensors):
        super(Partition5, self).__init__()
        # initializing partition layers
        self.scopes = []
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2]')
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1]')
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2]')
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1]')
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2]')
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1]')
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2]')
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1]')
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2]')
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1]')
        self.l_43 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_44 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_45 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_46 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_47 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2]')
        self.l_48 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_49 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_50 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_51 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1]')
        self.l_52 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_53 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_54 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_55 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_56 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2]')
        self.l_57 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_58 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_59 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout]'
        )

        # initializing partition buffers
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_4', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_5', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Tensor[bias]']
        )

        self.device = torch.device('cuda:5')
        self.lookup = {
            'l_0': 'transformer.blocks.29.attn.c_proj',
            'l_1': 'transformer.blocks.29.attn.resid_dropout',
            'l_2': 'transformer.blocks.29.ln_2',
            'l_3': 'transformer.blocks.29.mlp.c_fc',
            'l_4': 'transformer.blocks.29.mlp.c_proj',
            'l_5': 'transformer.blocks.29.mlp.dropout',
            'l_6': 'transformer.blocks.30.ln_1',
            'l_7': 'transformer.blocks.30.attn.c_attn',
            'l_8': 'transformer.blocks.30.attn.attn_dropout',
            'l_9': 'transformer.blocks.30.attn.c_proj',
            'l_10': 'transformer.blocks.30.attn.resid_dropout',
            'l_11': 'transformer.blocks.30.ln_2',
            'l_12': 'transformer.blocks.30.mlp.c_fc',
            'l_13': 'transformer.blocks.30.mlp.c_proj',
            'l_14': 'transformer.blocks.30.mlp.dropout',
            'l_15': 'transformer.blocks.31.ln_1',
            'l_16': 'transformer.blocks.31.attn.c_attn',
            'l_17': 'transformer.blocks.31.attn.attn_dropout',
            'l_18': 'transformer.blocks.31.attn.c_proj',
            'l_19': 'transformer.blocks.31.attn.resid_dropout',
            'l_20': 'transformer.blocks.31.ln_2',
            'l_21': 'transformer.blocks.31.mlp.c_fc',
            'l_22': 'transformer.blocks.31.mlp.c_proj',
            'l_23': 'transformer.blocks.31.mlp.dropout',
            'l_24': 'transformer.blocks.32.ln_1',
            'l_25': 'transformer.blocks.32.attn.c_attn',
            'l_26': 'transformer.blocks.32.attn.attn_dropout',
            'l_27': 'transformer.blocks.32.attn.c_proj',
            'l_28': 'transformer.blocks.32.attn.resid_dropout',
            'l_29': 'transformer.blocks.32.ln_2',
            'l_30': 'transformer.blocks.32.mlp.c_fc',
            'l_31': 'transformer.blocks.32.mlp.c_proj',
            'l_32': 'transformer.blocks.32.mlp.dropout',
            'l_33': 'transformer.blocks.33.ln_1',
            'l_34': 'transformer.blocks.33.attn.c_attn',
            'l_35': 'transformer.blocks.33.attn.attn_dropout',
            'l_36': 'transformer.blocks.33.attn.c_proj',
            'l_37': 'transformer.blocks.33.attn.resid_dropout',
            'l_38': 'transformer.blocks.33.ln_2',
            'l_39': 'transformer.blocks.33.mlp.c_fc',
            'l_40': 'transformer.blocks.33.mlp.c_proj',
            'l_41': 'transformer.blocks.33.mlp.dropout',
            'l_42': 'transformer.blocks.34.ln_1',
            'l_43': 'transformer.blocks.34.attn.c_attn',
            'l_44': 'transformer.blocks.34.attn.attn_dropout',
            'l_45': 'transformer.blocks.34.attn.c_proj',
            'l_46': 'transformer.blocks.34.attn.resid_dropout',
            'l_47': 'transformer.blocks.34.ln_2',
            'l_48': 'transformer.blocks.34.mlp.c_fc',
            'l_49': 'transformer.blocks.34.mlp.c_proj',
            'l_50': 'transformer.blocks.34.mlp.dropout',
            'l_51': 'transformer.blocks.35.ln_1',
            'l_52': 'transformer.blocks.35.attn.c_attn',
            'l_53': 'transformer.blocks.35.attn.attn_dropout',
            'l_54': 'transformer.blocks.35.attn.c_proj',
            'l_55': 'transformer.blocks.35.attn.resid_dropout',
            'l_56': 'transformer.blocks.35.ln_2',
            'l_57': 'transformer.blocks.35.mlp.c_fc',
            'l_58': 'transformer.blocks.35.mlp.c_proj',
            'l_59': 'transformer.blocks.35.mlp.dropout',
            'b_0': 'transformer.blocks.30.attn.bias',
            'b_1': 'transformer.blocks.31.attn.bias',
            'b_2': 'transformer.blocks.32.attn.bias',
            'b_3': 'transformer.blocks.33.attn.bias',
            'b_4': 'transformer.blocks.34.attn.bias',
            'b_5': 'transformer.blocks.35.attn.bias'
        }

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_proj] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[resid_dropout] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_attn] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[attn_dropout] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_proj] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[resid_dropout] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_attn] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[attn_dropout] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_proj] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[resid_dropout] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_attn] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[attn_dropout] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_proj] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[resid_dropout] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_attn] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[attn_dropout] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_proj] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[resid_dropout] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_attn] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[attn_dropout] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_proj] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[resid_dropout] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_attn] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[attn_dropout] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_proj] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[resid_dropout] <=> self.l_55
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2] <=> self.l_56
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc] <=> self.l_57
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj] <=> self.l_58
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout] <=> self.l_59
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/aten::add24193 <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::matmul24323 <=> x1

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)

        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::matmul24323
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24328
        t_0 = x1.permute(dims=t_0)
        del x1
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::permute24329
        t_0 = t_0.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::contiguous24331
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24332
        t_1 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::contiguous24331
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24336
        t_2 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::contiguous24331
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24340
        t_3 = t_0.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::contiguous24331
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24343
        t_4 = t_0.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24341
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24344
        t_4 = torch.mul(input=t_3, other=t_4)
        del t_3
        t_4 = [t_1, t_2, t_4]
        del t_2
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::contiguous24331
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24348
        t_4 = t_0.view(size=t_4)
        del t_0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::view24349
        t_4 = self.l_0(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_1(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/aten::add24193
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=x0, other=t_4)
        del x0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/aten::add24353
        t_0 = self.l_2(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2]
        t_0 = self.l_3(t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/prim::Constant24359
        t_2 = torch.mul(input=t_0, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/prim::Constant24361
        t_1 = t_0.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::pow24362
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/prim::Constant24363
        t_1 = torch.mul(input=t_1, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::mul24364
        t_1 = torch.add(input=t_0, other=t_1)
        del t_0
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::add24366
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/prim::Constant24367
        t_1 = torch.mul(input=t_1, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::mul24368
        t_1 = t_1.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::tanh24369
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/prim::Constant24370
        t_1 = torch.add(input=t_1, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::mul24360
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::add24372
        t_1 = torch.mul(input=t_2, other=t_1)
        del t_2
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::mul24373
        t_1 = self.l_4(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj]
        t_1 = self.l_5(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/aten::add24353
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout]
        t_1 = torch.add(input=t_4, other=t_1)
        del t_4
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/aten::add24377
        t_4 = self.l_6(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1]
        t_4 = self.l_7(t_4)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24389
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24390
        t_4 = t_4.split(split_size=1600, dim=2)
        t_0 = t_4[0]
        t_3 = t_4[1]
        t_4 = t_4[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243920
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24395
        t_5 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243920
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24399
        t_6 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243920
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24403
        t_7 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24404
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24406
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        del t_6
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243920
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24410
        t_7 = t_0.view(size=t_7)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::view24411
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24416
        t_0 = t_7.permute(dims=t_0)
        del t_7
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24418
        t_7 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24422
        t_6 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24426
        t_5 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24427
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24429
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_7, t_6, 25, t_5]
        del t_6
        del t_7
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24433
        t_5 = t_3.view(size=t_5)
        del t_3
        t_3 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::view24434
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24439
        t_3 = t_5.permute(dims=t_3)
        del t_5
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243922
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24441
        t_5 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243922
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24445
        t_6 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243922
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24449
        t_7 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24450
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24452
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        del t_6
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243922
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24456
        t_7 = t_4.view(size=t_7)
        del t_4
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::view24457
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24462
        t_4 = t_7.permute(dims=t_4)
        del t_7
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::permute24417
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::permute24440
        t_3 = t_0.matmul(other=t_3)
        del t_0
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::matmul24464
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24465
        t_3 = torch.div(input=t_3, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::div24466
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24467
        t_0 = t_3.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::div24466
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24470
        t_7 = t_3.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24471
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24468
        t_0 = torch.sub(input=t_7, other=t_0)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24478
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24479
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24480
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24481
        t_6 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::slice24482
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24483
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24484
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24485
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24486
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::slice24487
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24488
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::sub24476
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24471
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24489
        t_0 = t_6[:, :, t_0:t_7:1]
        del t_6
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::slice24490
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24491
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24492
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24471
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24493
        t_7 = t_0[:, :, :, 0:t_7:1]
        del t_0
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::div24466
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::slice24494
        t_3 = torch.mul(input=t_3, other=t_7)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::slice24494
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24496
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24497
        t_7 = torch.rsub(t_7, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::rsub24498
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24499
        t_7 = torch.mul(input=t_7, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::mul24495
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::mul24500
        t_7 = torch.sub(input=t_3, other=t_7)
        del t_3
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::sub24502
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24503
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24504
        t_7 = t_7.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::softmax24505
        t_7 = self.l_8(t_7)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::permute24463
        t_4 = t_7.matmul(other=t_4)
        del t_7
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::matmul24507
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24512
        t_7 = t_4.permute(dims=t_7)
        del t_4
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::permute24513
        t_7 = t_7.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::contiguous24515
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24516
        t_4 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::contiguous24515
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24520
        t_3 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::contiguous24515
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24524
        t_0 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::contiguous24515
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24527
        t_6 = t_7.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24525
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24528
        t_6 = torch.mul(input=t_0, other=t_6)
        del t_0
        t_6 = [t_4, t_3, t_6]
        del t_3
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::contiguous24515
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24532
        t_6 = t_7.view(size=t_6)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::view24533
        t_6 = self.l_9(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_10(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/aten::add24377
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_1, other=t_6)
        del t_1
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/aten::add24537
        t_1 = self.l_11(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2]
        t_1 = self.l_12(t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/prim::Constant24543
        t_7 = torch.mul(input=t_1, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/prim::Constant24545
        t_3 = t_1.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::pow24546
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/prim::Constant24547
        t_3 = torch.mul(input=t_3, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::mul24548
        t_3 = torch.add(input=t_1, other=t_3)
        del t_1
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::add24550
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/prim::Constant24551
        t_3 = torch.mul(input=t_3, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::mul24552
        t_3 = t_3.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::tanh24553
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/prim::Constant24554
        t_3 = torch.add(input=t_3, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::mul24544
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::add24556
        t_3 = torch.mul(input=t_7, other=t_3)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::mul24557
        t_3 = self.l_13(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj]
        t_3 = self.l_14(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/aten::add24537
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout]
        t_3 = torch.add(input=t_6, other=t_3)
        del t_6
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/aten::add24561
        t_6 = self.l_15(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1]
        t_6 = self.l_16(t_6)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24573
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24574
        t_6 = t_6.split(split_size=1600, dim=2)
        t_1 = t_6[0]
        t_4 = t_6[1]
        t_6 = t_6[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24579
        t_0 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24583
        t_5 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24587
        t_8 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24588
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24590
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_0, t_5, 25, t_8]
        del t_5
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24594
        t_8 = t_1.view(size=t_8)
        del t_1
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::view24595
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24600
        t_1 = t_8.permute(dims=t_1)
        del t_8
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24602
        t_8 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24606
        t_5 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24610
        t_0 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24611
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24613
        t_0 = torch.div(input=t_0, other=25)
        t_0 = [t_8, t_5, 25, t_0]
        del t_5
        del t_8
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24617
        t_0 = t_4.view(size=t_0)
        del t_4
        t_4 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::view24618
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24623
        t_4 = t_0.permute(dims=t_4)
        del t_0
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245762
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24625
        t_0 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245762
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24629
        t_5 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245762
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24633
        t_8 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24634
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24636
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_0, t_5, 25, t_8]
        del t_5
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245762
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24640
        t_8 = t_6.view(size=t_8)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::view24641
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24646
        t_6 = t_8.permute(dims=t_6)
        del t_8
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::permute24601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::permute24624
        t_4 = t_1.matmul(other=t_4)
        del t_1
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::matmul24648
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24649
        t_4 = torch.div(input=t_4, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::div24650
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24651
        t_1 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::div24650
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24654
        t_8 = t_4.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24655
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24652
        t_1 = torch.sub(input=t_8, other=t_1)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24662
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24663
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24664
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24665
        t_5 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::slice24666
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24667
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24668
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24669
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24670
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::slice24671
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24672
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::sub24660
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24655
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24673
        t_1 = t_5[:, :, t_1:t_8:1]
        del t_5
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::slice24674
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24675
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24676
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24655
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24677
        t_8 = t_1[:, :, :, 0:t_8:1]
        del t_1
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::div24650
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::slice24678
        t_4 = torch.mul(input=t_4, other=t_8)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::slice24678
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24681
        t_8 = torch.rsub(t_8, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::rsub24682
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24683
        t_8 = torch.mul(input=t_8, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::mul24679
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::mul24684
        t_8 = torch.sub(input=t_4, other=t_8)
        del t_4
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::sub24686
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24687
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24688
        t_8 = t_8.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::softmax24689
        t_8 = self.l_17(t_8)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::permute24647
        t_6 = t_8.matmul(other=t_6)
        del t_8
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::matmul24691
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24696
        t_8 = t_6.permute(dims=t_8)
        del t_6
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::permute24697
        t_8 = t_8.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::contiguous24699
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24700
        t_6 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::contiguous24699
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24704
        t_4 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::contiguous24699
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24708
        t_1 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::contiguous24699
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24711
        t_5 = t_8.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24709
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24712
        t_5 = torch.mul(input=t_1, other=t_5)
        del t_1
        t_5 = [t_6, t_4, t_5]
        del t_4
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::contiguous24699
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24716
        t_5 = t_8.view(size=t_5)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::view24717
        t_5 = self.l_18(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_19(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/aten::add24561
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=t_3, other=t_5)
        del t_3
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/aten::add24721
        t_3 = self.l_20(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2]
        t_3 = self.l_21(t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/prim::Constant24727
        t_8 = torch.mul(input=t_3, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/prim::Constant24729
        t_4 = t_3.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::pow24730
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/prim::Constant24731
        t_4 = torch.mul(input=t_4, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::mul24732
        t_4 = torch.add(input=t_3, other=t_4)
        del t_3
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::add24734
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/prim::Constant24735
        t_4 = torch.mul(input=t_4, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::mul24736
        t_4 = t_4.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::tanh24737
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/prim::Constant24738
        t_4 = torch.add(input=t_4, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::mul24728
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::add24740
        t_4 = torch.mul(input=t_8, other=t_4)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::mul24741
        t_4 = self.l_22(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj]
        t_4 = self.l_23(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/aten::add24721
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout]
        t_4 = torch.add(input=t_5, other=t_4)
        del t_5
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/aten::add24745
        t_5 = self.l_24(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1]
        t_5 = self.l_25(t_5)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24757
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24758
        t_5 = t_5.split(split_size=1600, dim=2)
        t_3 = t_5[0]
        t_6 = t_5[1]
        t_5 = t_5[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24763
        t_1 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24767
        t_0 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24771
        t_9 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24772
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24774
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_1, t_0, 25, t_9]
        del t_0
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24778
        t_9 = t_3.view(size=t_9)
        del t_3
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::view24779
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24784
        t_3 = t_9.permute(dims=t_3)
        del t_9
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24786
        t_9 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24790
        t_0 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24794
        t_1 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24795
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24797
        t_1 = torch.div(input=t_1, other=25)
        t_1 = [t_9, t_0, 25, t_1]
        del t_0
        del t_9
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24801
        t_1 = t_6.view(size=t_1)
        del t_6
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::view24802
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24807
        t_6 = t_1.permute(dims=t_6)
        del t_1
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247602
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24809
        t_1 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247602
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24813
        t_0 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247602
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24817
        t_9 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24818
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24820
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_1, t_0, 25, t_9]
        del t_0
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247602
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24824
        t_9 = t_5.view(size=t_9)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::view24825
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24830
        t_5 = t_9.permute(dims=t_5)
        del t_9
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::permute24785
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::permute24808
        t_6 = t_3.matmul(other=t_6)
        del t_3
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::matmul24832
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24833
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::div24834
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24835
        t_3 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::div24834
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24838
        t_9 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24839
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24836
        t_3 = torch.sub(input=t_9, other=t_3)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24846
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24847
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24848
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24849
        t_0 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::slice24850
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24851
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24852
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24853
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24854
        t_0 = t_0[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::slice24855
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24856
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::sub24844
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24839
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24857
        t_3 = t_0[:, :, t_3:t_9:1]
        del t_0
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::slice24858
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24859
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24860
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24839
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24861
        t_9 = t_3[:, :, :, 0:t_9:1]
        del t_3
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::div24834
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::slice24862
        t_6 = torch.mul(input=t_6, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::slice24862
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24864
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24865
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::rsub24866
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24867
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::mul24863
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::mul24868
        t_9 = torch.sub(input=t_6, other=t_9)
        del t_6
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::sub24870
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24871
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24872
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::softmax24873
        t_9 = self.l_26(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::permute24831
        t_5 = t_9.matmul(other=t_5)
        del t_9
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::matmul24875
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24880
        t_9 = t_5.permute(dims=t_9)
        del t_5
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::permute24881
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::contiguous24883
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24884
        t_5 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::contiguous24883
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24888
        t_6 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::contiguous24883
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24892
        t_3 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::contiguous24883
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24895
        t_0 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24893
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24896
        t_0 = torch.mul(input=t_3, other=t_0)
        del t_3
        t_0 = [t_5, t_6, t_0]
        del t_6
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::contiguous24883
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24900
        t_0 = t_9.view(size=t_0)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::view24901
        t_0 = self.l_27(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_proj]
        t_0 = self.l_28(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/aten::add24745
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[resid_dropout]
        t_0 = torch.add(input=t_4, other=t_0)
        del t_4
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/aten::add24905
        t_4 = self.l_29(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2]
        t_4 = self.l_30(t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/prim::Constant24911
        t_9 = torch.mul(input=t_4, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/prim::Constant24913
        t_6 = t_4.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::pow24914
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/prim::Constant24915
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::mul24916
        t_6 = torch.add(input=t_4, other=t_6)
        del t_4
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::add24918
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/prim::Constant24919
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::mul24920
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::tanh24921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/prim::Constant24922
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::mul24912
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::add24924
        t_6 = torch.mul(input=t_9, other=t_6)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::mul24925
        t_6 = self.l_31(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj]
        t_6 = self.l_32(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/aten::add24905
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout]
        t_6 = torch.add(input=t_0, other=t_6)
        del t_0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/aten::add24929
        t_0 = self.l_33(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1]
        t_0 = self.l_34(t_0)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24941
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24942
        t_0 = t_0.split(split_size=1600, dim=2)
        t_4 = t_0[0]
        t_5 = t_0[1]
        t_0 = t_0[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249440
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24947
        t_3 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249440
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24951
        t_1 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249440
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24955
        t_10 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size24956
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24958
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_3, t_1, 25, t_10]
        del t_1
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249440
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct24962
        t_10 = t_4.view(size=t_10)
        del t_4
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::view24963
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct24968
        t_4 = t_10.permute(dims=t_4)
        del t_10
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24970
        t_10 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24974
        t_1 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24978
        t_3 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size24979
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24981
        t_3 = torch.div(input=t_3, other=25)
        t_3 = [t_10, t_1, 25, t_3]
        del t_1
        del t_10
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct24985
        t_3 = t_5.view(size=t_3)
        del t_5
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::view24986
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct24991
        t_5 = t_3.permute(dims=t_5)
        del t_3
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24993
        t_3 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24997
        t_1 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25001
        t_10 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size25002
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25004
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_3, t_1, 25, t_10]
        del t_1
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct25008
        t_10 = t_0.view(size=t_10)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::view25009
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct25014
        t_0 = t_10.permute(dims=t_0)
        del t_10
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::permute24969
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::permute24992
        t_5 = t_4.matmul(other=t_5)
        del t_4
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::matmul25016
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25017
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::div25018
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25019
        t_4 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::div25018
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25022
        t_10 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size25023
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size25020
        t_4 = torch.sub(input=t_10, other=t_4)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25030
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25031
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25032
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25033
        t_1 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::slice25034
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25035
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25036
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25037
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25038
        t_1 = t_1[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::slice25039
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25040
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::sub25028
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size25023
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25041
        t_4 = t_1[:, :, t_4:t_10:1]
        del t_1
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::slice25042
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25043
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25044
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size25023
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25045
        t_10 = t_4[:, :, :, 0:t_10:1]
        del t_4
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::div25018
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::slice25046
        t_5 = torch.mul(input=t_5, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::slice25046
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25048
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25049
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::rsub25050
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25051
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::mul25047
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::mul25052
        t_10 = torch.sub(input=t_5, other=t_10)
        del t_5
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::sub25054
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25055
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25056
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::softmax25057
        t_10 = self.l_35(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::permute25015
        t_0 = t_10.matmul(other=t_0)
        del t_10
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::matmul25059
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct25064
        t_10 = t_0.permute(dims=t_10)
        del t_0
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::permute25065
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::contiguous25067
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25068
        t_0 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::contiguous25067
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25072
        t_5 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::contiguous25067
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25076
        t_4 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::contiguous25067
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25079
        t_1 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size25077
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size25080
        t_1 = torch.mul(input=t_4, other=t_1)
        del t_4
        t_1 = [t_0, t_5, t_1]
        del t_5
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::contiguous25067
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct25084
        t_1 = t_10.view(size=t_1)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::view25085
        t_1 = self.l_36(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_proj]
        t_1 = self.l_37(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/aten::add24929
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[resid_dropout]
        t_1 = torch.add(input=t_6, other=t_1)
        del t_6
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/aten::add25089
        t_6 = self.l_38(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2]
        t_6 = self.l_39(t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/prim::Constant25095
        t_10 = torch.mul(input=t_6, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/prim::Constant25097
        t_5 = t_6.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::pow25098
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/prim::Constant25099
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::mul25100
        t_5 = torch.add(input=t_6, other=t_5)
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::add25102
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/prim::Constant25103
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::mul25104
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::tanh25105
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/prim::Constant25106
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::mul25096
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::add25108
        t_5 = torch.mul(input=t_10, other=t_5)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::mul25109
        t_5 = self.l_40(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj]
        t_5 = self.l_41(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/aten::add25089
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout]
        t_5 = torch.add(input=t_1, other=t_5)
        del t_1
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/aten::add25113
        t_1 = self.l_42(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1]
        t_1 = self.l_43(t_1)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25125
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25126
        t_1 = t_1.split(split_size=1600, dim=2)
        t_6 = t_1[0]
        t_0 = t_1[1]
        t_1 = t_1[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251280
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25131
        t_4 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251280
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25135
        t_3 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251280
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25139
        t_11 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25140
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25142
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_4, t_3, 25, t_11]
        del t_3
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251280
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25146
        t_11 = t_6.view(size=t_11)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::view25147
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25152
        t_6 = t_11.permute(dims=t_6)
        del t_11
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25154
        t_11 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25158
        t_3 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25162
        t_4 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25163
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25165
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_11, t_3, 25, t_4]
        del t_3
        del t_11
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25169
        t_4 = t_0.view(size=t_4)
        del t_0
        t_0 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::view25170
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25175
        t_0 = t_4.permute(dims=t_0)
        del t_4
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25177
        t_4 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25181
        t_3 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25185
        t_11 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25186
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25188
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_4, t_3, 25, t_11]
        del t_3
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25192
        t_11 = t_1.view(size=t_11)
        del t_1
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::view25193
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25198
        t_1 = t_11.permute(dims=t_1)
        del t_11
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::permute25153
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::permute25176
        t_0 = t_6.matmul(other=t_0)
        del t_6
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::matmul25200
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25201
        t_0 = torch.div(input=t_0, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::div25202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25203
        t_6 = t_0.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::div25202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25206
        t_11 = t_0.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25207
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25204
        t_6 = torch.sub(input=t_11, other=t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25214
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25215
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25216
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25217
        t_3 = self.b_4[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::slice25218
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25219
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25220
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25221
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25222
        t_3 = t_3[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::slice25223
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25224
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::sub25212
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25207
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25225
        t_6 = t_3[:, :, t_6:t_11:1]
        del t_3
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::slice25226
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25227
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25228
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25207
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25229
        t_11 = t_6[:, :, :, 0:t_11:1]
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::div25202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::slice25230
        t_0 = torch.mul(input=t_0, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::slice25230
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25232
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25233
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::rsub25234
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25235
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::mul25231
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::mul25236
        t_11 = torch.sub(input=t_0, other=t_11)
        del t_0
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::sub25238
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25239
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25240
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::softmax25241
        t_11 = self.l_44(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::permute25199
        t_1 = t_11.matmul(other=t_1)
        del t_11
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::matmul25243
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25248
        t_11 = t_1.permute(dims=t_11)
        del t_1
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::permute25249
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::contiguous25251
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25252
        t_1 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::contiguous25251
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25256
        t_0 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::contiguous25251
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25260
        t_6 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::contiguous25251
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25263
        t_3 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25261
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25264
        t_3 = torch.mul(input=t_6, other=t_3)
        del t_6
        t_3 = [t_1, t_0, t_3]
        del t_0
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::contiguous25251
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25268
        t_3 = t_11.view(size=t_3)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::view25269
        t_3 = self.l_45(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_proj]
        t_3 = self.l_46(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/aten::add25113
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[resid_dropout]
        t_3 = torch.add(input=t_5, other=t_3)
        del t_5
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/aten::add25273
        t_5 = self.l_47(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2]
        t_5 = self.l_48(t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/prim::Constant25279
        t_11 = torch.mul(input=t_5, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/prim::Constant25281
        t_0 = t_5.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::pow25282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/prim::Constant25283
        t_0 = torch.mul(input=t_0, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::mul25284
        t_0 = torch.add(input=t_5, other=t_0)
        del t_5
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::add25286
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/prim::Constant25287
        t_0 = torch.mul(input=t_0, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::mul25288
        t_0 = t_0.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::tanh25289
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/prim::Constant25290
        t_0 = torch.add(input=t_0, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::mul25280
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::add25292
        t_0 = torch.mul(input=t_11, other=t_0)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::mul25293
        t_0 = self.l_49(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj]
        t_0 = self.l_50(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/aten::add25273
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout]
        t_0 = torch.add(input=t_3, other=t_0)
        del t_3
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/aten::add25297
        t_3 = self.l_51(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1]
        t_3 = self.l_52(t_3)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25309
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25310
        t_3 = t_3.split(split_size=1600, dim=2)
        t_5 = t_3[0]
        t_1 = t_3[1]
        t_3 = t_3[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253120
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25315
        t_6 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253120
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25319
        t_4 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253120
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25323
        t_12 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25324
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25326
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_4, 25, t_12]
        del t_4
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253120
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25330
        t_12 = t_5.view(size=t_12)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::view25331
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25336
        t_5 = t_12.permute(dims=t_5)
        del t_12
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253121
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25338
        t_12 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253121
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25342
        t_4 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253121
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25346
        t_6 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25347
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25349
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_12, t_4, 25, t_6]
        del t_4
        del t_12
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253121
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25353
        t_6 = t_1.view(size=t_6)
        del t_1
        t_1 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::view25354
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25359
        t_1 = t_6.permute(dims=t_1)
        del t_6
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25361
        t_6 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25365
        t_4 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25369
        t_12 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25370
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25372
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_4, 25, t_12]
        del t_4
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25376
        t_12 = t_3.view(size=t_12)
        del t_3
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::view25377
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25382
        t_3 = t_12.permute(dims=t_3)
        del t_12
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::permute25337
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::permute25360
        t_1 = t_5.matmul(other=t_1)
        del t_5
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::matmul25384
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25385
        t_1 = torch.div(input=t_1, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::div25386
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25387
        t_5 = t_1.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::div25386
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25390
        t_12 = t_1.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25391
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25388
        t_5 = torch.sub(input=t_12, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25398
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25399
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25400
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25401
        t_4 = self.b_5[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::slice25402
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25403
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25404
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25405
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25406
        t_4 = t_4[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::slice25407
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25408
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::sub25396
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25391
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25409
        t_5 = t_4[:, :, t_5:t_12:1]
        del t_4
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::slice25410
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25411
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25412
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25391
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25413
        t_12 = t_5[:, :, :, 0:t_12:1]
        del t_5
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::div25386
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::slice25414
        t_1 = torch.mul(input=t_1, other=t_12)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::slice25414
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25416
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25417
        t_12 = torch.rsub(t_12, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::rsub25418
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25419
        t_12 = torch.mul(input=t_12, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::mul25415
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::mul25420
        t_12 = torch.sub(input=t_1, other=t_12)
        del t_1
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::sub25422
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25423
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25424
        t_12 = t_12.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::softmax25425
        t_12 = self.l_53(t_12)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::permute25383
        t_3 = t_12.matmul(other=t_3)
        del t_12
        t_12 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::matmul25427
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25432
        t_12 = t_3.permute(dims=t_12)
        del t_3
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::permute25433
        t_12 = t_12.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::contiguous25435
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25436
        t_3 = t_12.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::contiguous25435
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25440
        t_1 = t_12.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::contiguous25435
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25444
        t_5 = t_12.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::contiguous25435
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25447
        t_4 = t_12.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25445
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25448
        t_4 = torch.mul(input=t_5, other=t_4)
        del t_5
        t_4 = [t_3, t_1, t_4]
        del t_1
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::contiguous25435
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25452
        t_4 = t_12.view(size=t_4)
        del t_12
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::view25453
        t_4 = self.l_54(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_55(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/aten::add25297
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=t_0, other=t_4)
        del t_0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/aten::add25457
        t_0 = self.l_56(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2]
        t_0 = self.l_57(t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/prim::Constant25463
        t_12 = torch.mul(input=t_0, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/prim::Constant25465
        t_1 = t_0.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::pow25466
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/prim::Constant25467
        t_1 = torch.mul(input=t_1, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::mul25468
        t_1 = torch.add(input=t_0, other=t_1)
        del t_0
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::add25470
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/prim::Constant25471
        t_1 = torch.mul(input=t_1, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::mul25472
        t_1 = t_1.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::tanh25473
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/prim::Constant25474
        t_1 = torch.add(input=t_1, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::mul25464
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::add25476
        t_1 = torch.mul(input=t_12, other=t_1)
        del t_12
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::mul25477
        t_1 = self.l_58(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj]
        t_1 = self.l_59(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/aten::add25457
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout]
        t_1 = torch.add(input=t_4, other=t_1)
        del t_4

        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/aten::add25481
        return (t_1, )

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition6(nn.Module):
    SCOPES = {
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]',
    }

    def __init__(self, layers, tensors):
        super(Partition6, self).__init__()
        # initializing partition layers
        self.scopes = []
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1]')
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2]')
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1]')
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2]')
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1]')
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2]')
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1]')
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2]')
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1]')
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2]')
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_43 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_44 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_45 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1]')
        self.l_46 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_47 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_48 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_49 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_50 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2]')
        self.l_51 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_52 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]'
        )

        # initializing partition buffers
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_4', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_5', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Tensor[bias]']
        )

        self.device = torch.device('cuda:6')
        self.lookup = {
            'l_0': 'transformer.blocks.36.ln_1',
            'l_1': 'transformer.blocks.36.attn.c_attn',
            'l_2': 'transformer.blocks.36.attn.attn_dropout',
            'l_3': 'transformer.blocks.36.attn.c_proj',
            'l_4': 'transformer.blocks.36.attn.resid_dropout',
            'l_5': 'transformer.blocks.36.ln_2',
            'l_6': 'transformer.blocks.36.mlp.c_fc',
            'l_7': 'transformer.blocks.36.mlp.c_proj',
            'l_8': 'transformer.blocks.36.mlp.dropout',
            'l_9': 'transformer.blocks.37.ln_1',
            'l_10': 'transformer.blocks.37.attn.c_attn',
            'l_11': 'transformer.blocks.37.attn.attn_dropout',
            'l_12': 'transformer.blocks.37.attn.c_proj',
            'l_13': 'transformer.blocks.37.attn.resid_dropout',
            'l_14': 'transformer.blocks.37.ln_2',
            'l_15': 'transformer.blocks.37.mlp.c_fc',
            'l_16': 'transformer.blocks.37.mlp.c_proj',
            'l_17': 'transformer.blocks.37.mlp.dropout',
            'l_18': 'transformer.blocks.38.ln_1',
            'l_19': 'transformer.blocks.38.attn.c_attn',
            'l_20': 'transformer.blocks.38.attn.attn_dropout',
            'l_21': 'transformer.blocks.38.attn.c_proj',
            'l_22': 'transformer.blocks.38.attn.resid_dropout',
            'l_23': 'transformer.blocks.38.ln_2',
            'l_24': 'transformer.blocks.38.mlp.c_fc',
            'l_25': 'transformer.blocks.38.mlp.c_proj',
            'l_26': 'transformer.blocks.38.mlp.dropout',
            'l_27': 'transformer.blocks.39.ln_1',
            'l_28': 'transformer.blocks.39.attn.c_attn',
            'l_29': 'transformer.blocks.39.attn.attn_dropout',
            'l_30': 'transformer.blocks.39.attn.c_proj',
            'l_31': 'transformer.blocks.39.attn.resid_dropout',
            'l_32': 'transformer.blocks.39.ln_2',
            'l_33': 'transformer.blocks.39.mlp.c_fc',
            'l_34': 'transformer.blocks.39.mlp.c_proj',
            'l_35': 'transformer.blocks.39.mlp.dropout',
            'l_36': 'transformer.blocks.40.ln_1',
            'l_37': 'transformer.blocks.40.attn.c_attn',
            'l_38': 'transformer.blocks.40.attn.attn_dropout',
            'l_39': 'transformer.blocks.40.attn.c_proj',
            'l_40': 'transformer.blocks.40.attn.resid_dropout',
            'l_41': 'transformer.blocks.40.ln_2',
            'l_42': 'transformer.blocks.40.mlp.c_fc',
            'l_43': 'transformer.blocks.40.mlp.c_proj',
            'l_44': 'transformer.blocks.40.mlp.dropout',
            'l_45': 'transformer.blocks.41.ln_1',
            'l_46': 'transformer.blocks.41.attn.c_attn',
            'l_47': 'transformer.blocks.41.attn.attn_dropout',
            'l_48': 'transformer.blocks.41.attn.c_proj',
            'l_49': 'transformer.blocks.41.attn.resid_dropout',
            'l_50': 'transformer.blocks.41.ln_2',
            'l_51': 'transformer.blocks.41.mlp.c_fc',
            'l_52': 'transformer.blocks.41.mlp.c_proj',
            'b_0': 'transformer.blocks.36.attn.bias',
            'b_1': 'transformer.blocks.37.attn.bias',
            'b_2': 'transformer.blocks.38.attn.bias',
            'b_3': 'transformer.blocks.39.attn.bias',
            'b_4': 'transformer.blocks.40.attn.bias',
            'b_5': 'transformer.blocks.41.attn.bias'
        }

    def forward(self, x0):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_attn] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[attn_dropout] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_proj] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[resid_dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_attn] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[attn_dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_proj] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[resid_dropout] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_attn] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[attn_dropout] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_proj] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[resid_dropout] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_attn] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[attn_dropout] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_proj] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[resid_dropout] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_attn] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[attn_dropout] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_proj] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[resid_dropout] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_attn] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[attn_dropout] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_proj] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[resid_dropout] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/aten::add25481 <=> x0

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)

        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/aten::add25481
        t_0 = self.l_0(x0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1]
        t_0 = self.l_1(t_0)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25493
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25494
        t_0 = t_0.split(split_size=1600, dim=2)
        t_2 = t_0[0]
        t_3 = t_0[1]
        t_0 = t_0[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack254960
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25499
        t_4 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack254960
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25503
        t_5 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack254960
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25507
        t_6 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25508
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25510
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_4, t_5, 25, t_6]
        del t_5
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack254960
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25514
        t_6 = t_2.view(size=t_6)
        del t_2
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::view25515
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25520
        t_2 = t_6.permute(dims=t_2)
        del t_6
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack254961
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25522
        t_6 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack254961
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25526
        t_5 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack254961
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25530
        t_4 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25531
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25533
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_6, t_5, 25, t_4]
        del t_5
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack254961
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25537
        t_4 = t_3.view(size=t_4)
        del t_3
        t_3 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::view25538
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25543
        t_3 = t_4.permute(dims=t_3)
        del t_4
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack254962
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25545
        t_4 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack254962
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25549
        t_5 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack254962
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25553
        t_6 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25554
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25556
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_4, t_5, 25, t_6]
        del t_5
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack254962
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25560
        t_6 = t_0.view(size=t_6)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::view25561
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25566
        t_0 = t_6.permute(dims=t_0)
        del t_6
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::permute25521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::permute25544
        t_3 = t_2.matmul(other=t_3)
        del t_2
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::matmul25568
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25569
        t_3 = torch.div(input=t_3, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::div25570
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25571
        t_2 = t_3.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::div25570
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25574
        t_6 = t_3.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25575
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25572
        t_2 = torch.sub(input=t_6, other=t_2)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25582
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25583
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25584
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25585
        t_5 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::slice25586
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25587
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25588
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25589
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25590
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::slice25591
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25592
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::sub25580
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25575
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25593
        t_2 = t_5[:, :, t_2:t_6:1]
        del t_5
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::slice25594
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25595
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25596
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25575
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25597
        t_6 = t_2[:, :, :, 0:t_6:1]
        del t_2
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::div25570
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::slice25598
        t_3 = torch.mul(input=t_3, other=t_6)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::slice25598
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25601
        t_6 = torch.rsub(t_6, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::rsub25602
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25603
        t_6 = torch.mul(input=t_6, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::mul25599
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::mul25604
        t_6 = torch.sub(input=t_3, other=t_6)
        del t_3
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::sub25606
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25607
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25608
        t_6 = t_6.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::softmax25609
        t_6 = self.l_2(t_6)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::permute25567
        t_0 = t_6.matmul(other=t_0)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::matmul25611
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25616
        t_6 = t_0.permute(dims=t_6)
        del t_0
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::permute25617
        t_6 = t_6.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::contiguous25619
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25620
        t_0 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::contiguous25619
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25624
        t_3 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::contiguous25619
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25628
        t_2 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::contiguous25619
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25631
        t_5 = t_6.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25629
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25632
        t_5 = torch.mul(input=t_2, other=t_5)
        del t_2
        t_5 = [t_0, t_3, t_5]
        del t_3
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::contiguous25619
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25636
        t_5 = t_6.view(size=t_5)
        del t_6
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::view25637
        t_5 = self.l_3(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_4(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/aten::add25481
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=x0, other=t_5)
        del x0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/aten::add25641
        t_6 = self.l_5(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2]
        t_6 = self.l_6(t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/prim::Constant25647
        t_3 = torch.mul(input=t_6, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/prim::Constant25649
        t_0 = t_6.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::pow25650
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/prim::Constant25651
        t_0 = torch.mul(input=t_0, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::mul25652
        t_0 = torch.add(input=t_6, other=t_0)
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::add25654
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/prim::Constant25655
        t_0 = torch.mul(input=t_0, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::mul25656
        t_0 = t_0.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::tanh25657
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/prim::Constant25658
        t_0 = torch.add(input=t_0, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::mul25648
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::add25660
        t_0 = torch.mul(input=t_3, other=t_0)
        del t_3
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::mul25661
        t_0 = self.l_7(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj]
        t_0 = self.l_8(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/aten::add25641
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout]
        t_0 = torch.add(input=t_5, other=t_0)
        del t_5
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/aten::add25665
        t_5 = self.l_9(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1]
        t_5 = self.l_10(t_5)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25677
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25678
        t_5 = t_5.split(split_size=1600, dim=2)
        t_6 = t_5[0]
        t_2 = t_5[1]
        t_5 = t_5[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256800
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25683
        t_4 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256800
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25687
        t_7 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256800
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25691
        t_8 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25692
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25694
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_4, t_7, 25, t_8]
        del t_7
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256800
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25698
        t_8 = t_6.view(size=t_8)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::view25699
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25704
        t_6 = t_8.permute(dims=t_6)
        del t_8
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256801
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25706
        t_8 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256801
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25710
        t_7 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256801
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25714
        t_4 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25715
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25717
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_8, t_7, 25, t_4]
        del t_7
        del t_8
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256801
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25721
        t_4 = t_2.view(size=t_4)
        del t_2
        t_2 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::view25722
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25727
        t_2 = t_4.permute(dims=t_2)
        del t_4
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256802
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25729
        t_4 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256802
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25733
        t_7 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256802
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25737
        t_8 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25738
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25740
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_4, t_7, 25, t_8]
        del t_7
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256802
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25744
        t_8 = t_5.view(size=t_8)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::view25745
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25750
        t_5 = t_8.permute(dims=t_5)
        del t_8
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::permute25705
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::permute25728
        t_2 = t_6.matmul(other=t_2)
        del t_6
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::matmul25752
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25753
        t_2 = torch.div(input=t_2, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::div25754
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25755
        t_6 = t_2.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::div25754
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25758
        t_8 = t_2.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25759
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25756
        t_6 = torch.sub(input=t_8, other=t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25766
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25767
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25768
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25769
        t_7 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::slice25770
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25771
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25772
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25773
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25774
        t_7 = t_7[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::slice25775
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25776
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::sub25764
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25759
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25777
        t_6 = t_7[:, :, t_6:t_8:1]
        del t_7
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::slice25778
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25779
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25780
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25759
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25781
        t_8 = t_6[:, :, :, 0:t_8:1]
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::div25754
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::slice25782
        t_2 = torch.mul(input=t_2, other=t_8)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::slice25782
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25784
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25785
        t_8 = torch.rsub(t_8, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::rsub25786
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25787
        t_8 = torch.mul(input=t_8, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::mul25783
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::mul25788
        t_8 = torch.sub(input=t_2, other=t_8)
        del t_2
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::sub25790
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25791
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25792
        t_8 = t_8.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::softmax25793
        t_8 = self.l_11(t_8)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::permute25751
        t_5 = t_8.matmul(other=t_5)
        del t_8
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::matmul25795
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25800
        t_8 = t_5.permute(dims=t_8)
        del t_5
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::permute25801
        t_8 = t_8.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::contiguous25803
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25804
        t_5 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::contiguous25803
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25808
        t_2 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::contiguous25803
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25812
        t_6 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::contiguous25803
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25815
        t_7 = t_8.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25813
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25816
        t_7 = torch.mul(input=t_6, other=t_7)
        del t_6
        t_7 = [t_5, t_2, t_7]
        del t_2
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::contiguous25803
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25820
        t_7 = t_8.view(size=t_7)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::view25821
        t_7 = self.l_12(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_proj]
        t_7 = self.l_13(t_7)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/aten::add25665
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[resid_dropout]
        t_7 = torch.add(input=t_0, other=t_7)
        del t_0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/aten::add25825
        t_0 = self.l_14(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2]
        t_0 = self.l_15(t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/prim::Constant25831
        t_8 = torch.mul(input=t_0, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/prim::Constant25833
        t_2 = t_0.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::pow25834
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/prim::Constant25835
        t_2 = torch.mul(input=t_2, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::mul25836
        t_2 = torch.add(input=t_0, other=t_2)
        del t_0
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::add25838
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/prim::Constant25839
        t_2 = torch.mul(input=t_2, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::mul25840
        t_2 = t_2.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::tanh25841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/prim::Constant25842
        t_2 = torch.add(input=t_2, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::mul25832
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::add25844
        t_2 = torch.mul(input=t_8, other=t_2)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::mul25845
        t_2 = self.l_16(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj]
        t_2 = self.l_17(t_2)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/aten::add25825
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout]
        t_2 = torch.add(input=t_7, other=t_2)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/aten::add25849
        t_7 = self.l_18(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1]
        t_7 = self.l_19(t_7)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25861
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25862
        t_7 = t_7.split(split_size=1600, dim=2)
        t_0 = t_7[0]
        t_5 = t_7[1]
        t_7 = t_7[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258640
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25867
        t_6 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258640
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25871
        t_4 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258640
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25875
        t_9 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size25876
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25878
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_6, t_4, 25, t_9]
        del t_4
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258640
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct25882
        t_9 = t_0.view(size=t_9)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::view25883
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct25888
        t_0 = t_9.permute(dims=t_0)
        del t_9
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258641
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25890
        t_9 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258641
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25894
        t_4 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258641
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25898
        t_6 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size25899
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25901
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_9, t_4, 25, t_6]
        del t_4
        del t_9
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258641
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct25905
        t_6 = t_5.view(size=t_6)
        del t_5
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::view25906
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct25911
        t_5 = t_6.permute(dims=t_5)
        del t_6
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258642
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25913
        t_6 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258642
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25917
        t_4 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258642
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25921
        t_9 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size25922
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25924
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_6, t_4, 25, t_9]
        del t_4
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258642
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct25928
        t_9 = t_7.view(size=t_9)
        del t_7
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::view25929
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct25934
        t_7 = t_9.permute(dims=t_7)
        del t_9
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::permute25889
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::permute25912
        t_5 = t_0.matmul(other=t_5)
        del t_0
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::matmul25936
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25937
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::div25938
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25939
        t_0 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::div25938
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25942
        t_9 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size25943
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size25940
        t_0 = torch.sub(input=t_9, other=t_0)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25950
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25951
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25952
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25953
        t_4 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::slice25954
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25955
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25956
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25957
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25958
        t_4 = t_4[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::slice25959
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25960
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::sub25948
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size25943
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25961
        t_0 = t_4[:, :, t_0:t_9:1]
        del t_4
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::slice25962
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25963
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25964
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size25943
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25965
        t_9 = t_0[:, :, :, 0:t_9:1]
        del t_0
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::div25938
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::slice25966
        t_5 = torch.mul(input=t_5, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::slice25966
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25968
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25969
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::rsub25970
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25971
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::mul25967
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::mul25972
        t_9 = torch.sub(input=t_5, other=t_9)
        del t_5
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::sub25974
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25975
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25976
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::softmax25977
        t_9 = self.l_20(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::permute25935
        t_7 = t_9.matmul(other=t_7)
        del t_9
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::matmul25979
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct25984
        t_9 = t_7.permute(dims=t_9)
        del t_7
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::permute25985
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::contiguous25987
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25988
        t_7 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::contiguous25987
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25992
        t_5 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::contiguous25987
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25996
        t_0 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::contiguous25987
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25999
        t_4 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size25997
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size26000
        t_4 = torch.mul(input=t_0, other=t_4)
        del t_0
        t_4 = [t_7, t_5, t_4]
        del t_5
        del t_7
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::contiguous25987
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct26004
        t_4 = t_9.view(size=t_4)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::view26005
        t_4 = self.l_21(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_22(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/aten::add25849
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=t_2, other=t_4)
        del t_2
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/aten::add26009
        t_2 = self.l_23(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2]
        t_2 = self.l_24(t_2)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/prim::Constant26015
        t_9 = torch.mul(input=t_2, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/prim::Constant26017
        t_5 = t_2.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::pow26018
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/prim::Constant26019
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::mul26020
        t_5 = torch.add(input=t_2, other=t_5)
        del t_2
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::add26022
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/prim::Constant26023
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::mul26024
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::tanh26025
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/prim::Constant26026
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::mul26016
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::add26028
        t_5 = torch.mul(input=t_9, other=t_5)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::mul26029
        t_5 = self.l_25(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj]
        t_5 = self.l_26(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/aten::add26009
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout]
        t_5 = torch.add(input=t_4, other=t_5)
        del t_4
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/aten::add26033
        t_4 = self.l_27(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1]
        t_4 = self.l_28(t_4)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26045
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26046
        t_4 = t_4.split(split_size=1600, dim=2)
        t_2 = t_4[0]
        t_7 = t_4[1]
        t_4 = t_4[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260480
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26051
        t_0 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260480
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26055
        t_6 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260480
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26059
        t_10 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26060
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26062
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_0, t_6, 25, t_10]
        del t_6
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260480
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26066
        t_10 = t_2.view(size=t_10)
        del t_2
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::view26067
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26072
        t_2 = t_10.permute(dims=t_2)
        del t_10
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260481
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26074
        t_10 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260481
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26078
        t_6 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260481
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26082
        t_0 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26083
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26085
        t_0 = torch.div(input=t_0, other=25)
        t_0 = [t_10, t_6, 25, t_0]
        del t_6
        del t_10
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260481
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26089
        t_0 = t_7.view(size=t_0)
        del t_7
        t_7 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::view26090
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26095
        t_7 = t_0.permute(dims=t_7)
        del t_0
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260482
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26097
        t_0 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260482
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26101
        t_6 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260482
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26105
        t_10 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26106
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26108
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_0, t_6, 25, t_10]
        del t_6
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260482
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26112
        t_10 = t_4.view(size=t_10)
        del t_4
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::view26113
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26118
        t_4 = t_10.permute(dims=t_4)
        del t_10
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::permute26073
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::permute26096
        t_7 = t_2.matmul(other=t_7)
        del t_2
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::matmul26120
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26121
        t_7 = torch.div(input=t_7, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::div26122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26123
        t_2 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::div26122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26126
        t_10 = t_7.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26127
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26124
        t_2 = torch.sub(input=t_10, other=t_2)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26134
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26135
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26136
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26137
        t_6 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::slice26138
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26139
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26140
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26141
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26142
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::slice26143
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26144
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::sub26132
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26127
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26145
        t_2 = t_6[:, :, t_2:t_10:1]
        del t_6
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::slice26146
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26147
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26148
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26127
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26149
        t_10 = t_2[:, :, :, 0:t_10:1]
        del t_2
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::div26122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::slice26150
        t_7 = torch.mul(input=t_7, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::slice26150
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26152
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26153
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::rsub26154
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26155
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::mul26151
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::mul26156
        t_10 = torch.sub(input=t_7, other=t_10)
        del t_7
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::sub26158
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26159
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26160
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::softmax26161
        t_10 = self.l_29(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::permute26119
        t_4 = t_10.matmul(other=t_4)
        del t_10
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::matmul26163
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26168
        t_10 = t_4.permute(dims=t_10)
        del t_4
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::permute26169
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::contiguous26171
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26172
        t_4 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::contiguous26171
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26176
        t_7 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::contiguous26171
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26180
        t_2 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::contiguous26171
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26183
        t_6 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26181
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26184
        t_6 = torch.mul(input=t_2, other=t_6)
        del t_2
        t_6 = [t_4, t_7, t_6]
        del t_7
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::contiguous26171
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26188
        t_6 = t_10.view(size=t_6)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::view26189
        t_6 = self.l_30(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_31(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/aten::add26033
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_5, other=t_6)
        del t_5
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/aten::add26193
        t_5 = self.l_32(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2]
        t_5 = self.l_33(t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/prim::Constant26199
        t_10 = torch.mul(input=t_5, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/prim::Constant26201
        t_7 = t_5.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::pow26202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/prim::Constant26203
        t_7 = torch.mul(input=t_7, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::mul26204
        t_7 = torch.add(input=t_5, other=t_7)
        del t_5
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::add26206
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/prim::Constant26207
        t_7 = torch.mul(input=t_7, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::mul26208
        t_7 = t_7.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::tanh26209
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/prim::Constant26210
        t_7 = torch.add(input=t_7, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::mul26200
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::add26212
        t_7 = torch.mul(input=t_10, other=t_7)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::mul26213
        t_7 = self.l_34(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj]
        t_7 = self.l_35(t_7)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/aten::add26193
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout]
        t_7 = torch.add(input=t_6, other=t_7)
        del t_6
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/aten::add26217
        t_6 = self.l_36(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1]
        t_6 = self.l_37(t_6)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26229
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26230
        t_6 = t_6.split(split_size=1600, dim=2)
        t_5 = t_6[0]
        t_4 = t_6[1]
        t_6 = t_6[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262320
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26235
        t_2 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262320
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26239
        t_0 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262320
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26243
        t_11 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26244
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26246
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_2, t_0, 25, t_11]
        del t_0
        del t_2
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262320
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26250
        t_11 = t_5.view(size=t_11)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::view26251
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26256
        t_5 = t_11.permute(dims=t_5)
        del t_11
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262321
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26258
        t_11 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262321
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26262
        t_0 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262321
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26266
        t_2 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26267
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26269
        t_2 = torch.div(input=t_2, other=25)
        t_2 = [t_11, t_0, 25, t_2]
        del t_0
        del t_11
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262321
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26273
        t_2 = t_4.view(size=t_2)
        del t_4
        t_4 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::view26274
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26279
        t_4 = t_2.permute(dims=t_4)
        del t_2
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262322
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26281
        t_2 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262322
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26285
        t_0 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262322
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26289
        t_11 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26290
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26292
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_2, t_0, 25, t_11]
        del t_0
        del t_2
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262322
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26296
        t_11 = t_6.view(size=t_11)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::view26297
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26302
        t_6 = t_11.permute(dims=t_6)
        del t_11
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::permute26257
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::permute26280
        t_4 = t_5.matmul(other=t_4)
        del t_5
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::matmul26304
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26305
        t_4 = torch.div(input=t_4, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::div26306
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26307
        t_5 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::div26306
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26310
        t_11 = t_4.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26311
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26308
        t_5 = torch.sub(input=t_11, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26318
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26319
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26320
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26321
        t_0 = self.b_4[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::slice26322
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26323
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26324
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26325
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26326
        t_0 = t_0[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::slice26327
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26328
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::sub26316
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26311
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26329
        t_5 = t_0[:, :, t_5:t_11:1]
        del t_0
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::slice26330
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26331
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26332
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26311
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26333
        t_11 = t_5[:, :, :, 0:t_11:1]
        del t_5
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::div26306
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::slice26334
        t_4 = torch.mul(input=t_4, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::slice26334
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26336
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26337
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::rsub26338
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26339
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::mul26335
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::mul26340
        t_11 = torch.sub(input=t_4, other=t_11)
        del t_4
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::sub26342
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26343
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26344
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::softmax26345
        t_11 = self.l_38(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::permute26303
        t_6 = t_11.matmul(other=t_6)
        del t_11
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::matmul26347
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26352
        t_11 = t_6.permute(dims=t_11)
        del t_6
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::permute26353
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::contiguous26355
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26356
        t_6 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::contiguous26355
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26360
        t_4 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::contiguous26355
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26364
        t_5 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::contiguous26355
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26367
        t_0 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26365
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26368
        t_0 = torch.mul(input=t_5, other=t_0)
        del t_5
        t_0 = [t_6, t_4, t_0]
        del t_4
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::contiguous26355
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26372
        t_0 = t_11.view(size=t_0)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::view26373
        t_0 = self.l_39(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_proj]
        t_0 = self.l_40(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/aten::add26217
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[resid_dropout]
        t_0 = torch.add(input=t_7, other=t_0)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/aten::add26377
        t_7 = self.l_41(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2]
        t_7 = self.l_42(t_7)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/prim::Constant26383
        t_11 = torch.mul(input=t_7, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/prim::Constant26385
        t_4 = t_7.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::pow26386
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/prim::Constant26387
        t_4 = torch.mul(input=t_4, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::mul26388
        t_4 = torch.add(input=t_7, other=t_4)
        del t_7
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::add26390
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/prim::Constant26391
        t_4 = torch.mul(input=t_4, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::mul26392
        t_4 = t_4.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::tanh26393
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/prim::Constant26394
        t_4 = torch.add(input=t_4, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::mul26384
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::add26396
        t_4 = torch.mul(input=t_11, other=t_4)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::mul26397
        t_4 = self.l_43(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj]
        t_4 = self.l_44(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/aten::add26377
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout]
        t_4 = torch.add(input=t_0, other=t_4)
        del t_0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/aten::add26401
        t_0 = self.l_45(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1]
        t_0 = self.l_46(t_0)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26413
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26414
        t_0 = t_0.split(split_size=1600, dim=2)
        t_7 = t_0[0]
        t_6 = t_0[1]
        t_0 = t_0[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264160
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26419
        t_5 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264160
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26423
        t_2 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264160
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26427
        t_12 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26428
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26430
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_5, t_2, 25, t_12]
        del t_2
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264160
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26434
        t_12 = t_7.view(size=t_12)
        del t_7
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::view26435
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26440
        t_7 = t_12.permute(dims=t_7)
        del t_12
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264161
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26442
        t_12 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264161
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26446
        t_2 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264161
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26450
        t_5 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26451
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26453
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_12, t_2, 25, t_5]
        del t_2
        del t_12
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264161
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26457
        t_5 = t_6.view(size=t_5)
        del t_6
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::view26458
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26463
        t_6 = t_5.permute(dims=t_6)
        del t_5
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264162
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26465
        t_5 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264162
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26469
        t_2 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264162
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26473
        t_12 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26474
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26476
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_5, t_2, 25, t_12]
        del t_2
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264162
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26480
        t_12 = t_0.view(size=t_12)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::view26481
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26486
        t_0 = t_12.permute(dims=t_0)
        del t_12
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::permute26441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::permute26464
        t_6 = t_7.matmul(other=t_6)
        del t_7
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::matmul26488
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26489
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::div26490
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26491
        t_7 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::div26490
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26494
        t_12 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26495
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26492
        t_7 = torch.sub(input=t_12, other=t_7)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26502
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26503
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26504
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26505
        t_2 = self.b_5[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::slice26506
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26507
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26508
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26509
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26510
        t_2 = t_2[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::slice26511
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26512
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::sub26500
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26495
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26513
        t_7 = t_2[:, :, t_7:t_12:1]
        del t_2
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::slice26514
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26515
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26516
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26495
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26517
        t_12 = t_7[:, :, :, 0:t_12:1]
        del t_7
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::div26490
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::slice26518
        t_6 = torch.mul(input=t_6, other=t_12)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::slice26518
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26520
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26521
        t_12 = torch.rsub(t_12, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::rsub26522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26523
        t_12 = torch.mul(input=t_12, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::mul26519
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::mul26524
        t_12 = torch.sub(input=t_6, other=t_12)
        del t_6
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::sub26526
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26527
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26528
        t_12 = t_12.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::softmax26529
        t_12 = self.l_47(t_12)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::permute26487
        t_0 = t_12.matmul(other=t_0)
        del t_12
        t_12 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::matmul26531
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26536
        t_12 = t_0.permute(dims=t_12)
        del t_0
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::permute26537
        t_12 = t_12.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::contiguous26539
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26540
        t_0 = t_12.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::contiguous26539
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26544
        t_6 = t_12.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::contiguous26539
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26548
        t_7 = t_12.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::contiguous26539
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26551
        t_2 = t_12.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26549
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26552
        t_2 = torch.mul(input=t_7, other=t_2)
        del t_7
        t_2 = [t_0, t_6, t_2]
        del t_6
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::contiguous26539
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26556
        t_2 = t_12.view(size=t_2)
        del t_12
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::view26557
        t_2 = self.l_48(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_proj]
        t_2 = self.l_49(t_2)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/aten::add26401
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[resid_dropout]
        t_2 = torch.add(input=t_4, other=t_2)
        del t_4
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/aten::add26561
        t_4 = self.l_50(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2]
        t_4 = self.l_51(t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/prim::Constant26567
        t_12 = torch.mul(input=t_4, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/prim::Constant26569
        t_6 = t_4.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::pow26570
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/prim::Constant26571
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::mul26572
        t_6 = torch.add(input=t_4, other=t_6)
        del t_4
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::add26574
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/prim::Constant26575
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::mul26576
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::tanh26577
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/prim::Constant26578
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::mul26568
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::add26580
        t_6 = torch.mul(input=t_12, other=t_6)
        del t_12
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::mul26581
        t_6 = self.l_52(t_6)

        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/aten::add26561
        return (t_6, t_2)

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition7(nn.Module):
    SCOPES = {
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_attn]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[attn_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[resid_dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj]',
        'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout]',
        'GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]',
    }

    def __init__(self, layers, tensors):
        super(Partition7, self).__init__()
        # initializing partition layers
        self.scopes = []
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1]')
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2]')
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1]')
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2]')
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1]')
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2]')
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1]')
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2]')
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1]')
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2]')
        self.l_43 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_44 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_45 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_46 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1]')
        self.l_47 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_attn]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_attn]'
        )
        self.l_48 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[attn_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[attn_dropout]'
        )
        self.l_49 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_proj]'
        )
        self.l_50 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[resid_dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[resid_dropout]'
        )
        self.l_51 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2]')
        self.l_52 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc]'
        )
        self.l_53 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj]'
        )
        self.l_54 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout]'
        )
        self.l_55 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]']
        self.scopes.append(
            'GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]')

        # initializing partition buffers
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_4', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Tensor[bias]']
        )
        self.register_buffer(
            'b_5', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Tensor[bias]']
        )

        self.device = torch.device('cuda:7')
        self.lookup = {
            'l_0': 'transformer.blocks.41.mlp.dropout',
            'l_1': 'transformer.blocks.42.ln_1',
            'l_2': 'transformer.blocks.42.attn.c_attn',
            'l_3': 'transformer.blocks.42.attn.attn_dropout',
            'l_4': 'transformer.blocks.42.attn.c_proj',
            'l_5': 'transformer.blocks.42.attn.resid_dropout',
            'l_6': 'transformer.blocks.42.ln_2',
            'l_7': 'transformer.blocks.42.mlp.c_fc',
            'l_8': 'transformer.blocks.42.mlp.c_proj',
            'l_9': 'transformer.blocks.42.mlp.dropout',
            'l_10': 'transformer.blocks.43.ln_1',
            'l_11': 'transformer.blocks.43.attn.c_attn',
            'l_12': 'transformer.blocks.43.attn.attn_dropout',
            'l_13': 'transformer.blocks.43.attn.c_proj',
            'l_14': 'transformer.blocks.43.attn.resid_dropout',
            'l_15': 'transformer.blocks.43.ln_2',
            'l_16': 'transformer.blocks.43.mlp.c_fc',
            'l_17': 'transformer.blocks.43.mlp.c_proj',
            'l_18': 'transformer.blocks.43.mlp.dropout',
            'l_19': 'transformer.blocks.44.ln_1',
            'l_20': 'transformer.blocks.44.attn.c_attn',
            'l_21': 'transformer.blocks.44.attn.attn_dropout',
            'l_22': 'transformer.blocks.44.attn.c_proj',
            'l_23': 'transformer.blocks.44.attn.resid_dropout',
            'l_24': 'transformer.blocks.44.ln_2',
            'l_25': 'transformer.blocks.44.mlp.c_fc',
            'l_26': 'transformer.blocks.44.mlp.c_proj',
            'l_27': 'transformer.blocks.44.mlp.dropout',
            'l_28': 'transformer.blocks.45.ln_1',
            'l_29': 'transformer.blocks.45.attn.c_attn',
            'l_30': 'transformer.blocks.45.attn.attn_dropout',
            'l_31': 'transformer.blocks.45.attn.c_proj',
            'l_32': 'transformer.blocks.45.attn.resid_dropout',
            'l_33': 'transformer.blocks.45.ln_2',
            'l_34': 'transformer.blocks.45.mlp.c_fc',
            'l_35': 'transformer.blocks.45.mlp.c_proj',
            'l_36': 'transformer.blocks.45.mlp.dropout',
            'l_37': 'transformer.blocks.46.ln_1',
            'l_38': 'transformer.blocks.46.attn.c_attn',
            'l_39': 'transformer.blocks.46.attn.attn_dropout',
            'l_40': 'transformer.blocks.46.attn.c_proj',
            'l_41': 'transformer.blocks.46.attn.resid_dropout',
            'l_42': 'transformer.blocks.46.ln_2',
            'l_43': 'transformer.blocks.46.mlp.c_fc',
            'l_44': 'transformer.blocks.46.mlp.c_proj',
            'l_45': 'transformer.blocks.46.mlp.dropout',
            'l_46': 'transformer.blocks.47.ln_1',
            'l_47': 'transformer.blocks.47.attn.c_attn',
            'l_48': 'transformer.blocks.47.attn.attn_dropout',
            'l_49': 'transformer.blocks.47.attn.c_proj',
            'l_50': 'transformer.blocks.47.attn.resid_dropout',
            'l_51': 'transformer.blocks.47.ln_2',
            'l_52': 'transformer.blocks.47.mlp.c_fc',
            'l_53': 'transformer.blocks.47.mlp.c_proj',
            'l_54': 'transformer.blocks.47.mlp.dropout',
            'l_55': 'transformer.ln_f',
            'b_0': 'transformer.blocks.42.attn.bias',
            'b_1': 'transformer.blocks.43.attn.bias',
            'b_2': 'transformer.blocks.44.attn.bias',
            'b_3': 'transformer.blocks.45.attn.bias',
            'b_4': 'transformer.blocks.46.attn.bias',
            'b_5': 'transformer.blocks.47.attn.bias'
        }

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_attn] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[attn_dropout] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_proj] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[resid_dropout] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_attn] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[attn_dropout] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_proj] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[resid_dropout] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_attn] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[attn_dropout] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_proj] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[resid_dropout] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_attn] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[attn_dropout] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_proj] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[resid_dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_attn] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[attn_dropout] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_proj] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[resid_dropout] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_attn] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[attn_dropout] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_proj] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[resid_dropout] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f] <=> self.l_55
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj] <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/aten::add26561 <=> x1

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)

        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]
        t_0 = self.l_0(x0)
        del x0
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/aten::add26561
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout]
        t_0 = torch.add(input=x1, other=t_0)
        del x1
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/aten::add26585
        t_1 = self.l_1(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1]
        t_1 = self.l_2(t_1)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26597
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26598
        t_1 = t_1.split(split_size=1600, dim=2)
        t_3 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266000
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26603
        t_5 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266000
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26607
        t_6 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266000
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26611
        t_7 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26612
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26614
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        del t_6
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266000
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26618
        t_7 = t_3.view(size=t_7)
        del t_3
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::view26619
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26624
        t_3 = t_7.permute(dims=t_3)
        del t_7
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26626
        t_7 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26630
        t_6 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26634
        t_5 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26635
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26637
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_7, t_6, 25, t_5]
        del t_6
        del t_7
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26641
        t_5 = t_4.view(size=t_5)
        del t_4
        t_4 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::view26642
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26647
        t_4 = t_5.permute(dims=t_4)
        del t_5
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266002
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26649
        t_5 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266002
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26653
        t_6 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266002
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26657
        t_7 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26658
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26660
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        del t_6
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266002
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26664
        t_7 = t_1.view(size=t_7)
        del t_1
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::view26665
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26670
        t_1 = t_7.permute(dims=t_1)
        del t_7
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::permute26625
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::permute26648
        t_4 = t_3.matmul(other=t_4)
        del t_3
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::matmul26672
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26673
        t_4 = torch.div(input=t_4, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::div26674
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26675
        t_3 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::div26674
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26678
        t_7 = t_4.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26679
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26676
        t_3 = torch.sub(input=t_7, other=t_3)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26686
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26687
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26688
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26689
        t_6 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::slice26690
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26691
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26692
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26693
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26694
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::slice26695
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26696
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::sub26684
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26679
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26697
        t_3 = t_6[:, :, t_3:t_7:1]
        del t_6
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::slice26698
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26699
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26700
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26679
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26701
        t_7 = t_3[:, :, :, 0:t_7:1]
        del t_3
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::div26674
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::slice26702
        t_4 = torch.mul(input=t_4, other=t_7)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::slice26702
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26704
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26705
        t_7 = torch.rsub(t_7, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::rsub26706
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26707
        t_7 = torch.mul(input=t_7, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::mul26703
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::mul26708
        t_7 = torch.sub(input=t_4, other=t_7)
        del t_4
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::sub26710
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26711
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26712
        t_7 = t_7.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::softmax26713
        t_7 = self.l_3(t_7)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::permute26671
        t_1 = t_7.matmul(other=t_1)
        del t_7
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::matmul26715
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26720
        t_7 = t_1.permute(dims=t_7)
        del t_1
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::permute26721
        t_7 = t_7.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::contiguous26723
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26724
        t_1 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::contiguous26723
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26728
        t_4 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::contiguous26723
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26732
        t_3 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::contiguous26723
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26735
        t_6 = t_7.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26733
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26736
        t_6 = torch.mul(input=t_3, other=t_6)
        del t_3
        t_6 = [t_1, t_4, t_6]
        del t_4
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::contiguous26723
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26740
        t_6 = t_7.view(size=t_6)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::view26741
        t_6 = self.l_4(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_5(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/aten::add26585
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_0, other=t_6)
        del t_0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/aten::add26745
        t_0 = self.l_6(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2]
        t_0 = self.l_7(t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/prim::Constant26751
        t_7 = torch.mul(input=t_0, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/prim::Constant26753
        t_4 = t_0.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::pow26754
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/prim::Constant26755
        t_4 = torch.mul(input=t_4, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::mul26756
        t_4 = torch.add(input=t_0, other=t_4)
        del t_0
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::add26758
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/prim::Constant26759
        t_4 = torch.mul(input=t_4, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::mul26760
        t_4 = t_4.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::tanh26761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/prim::Constant26762
        t_4 = torch.add(input=t_4, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::mul26752
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::add26764
        t_4 = torch.mul(input=t_7, other=t_4)
        del t_7
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::mul26765
        t_4 = self.l_8(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj]
        t_4 = self.l_9(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/aten::add26745
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout]
        t_4 = torch.add(input=t_6, other=t_4)
        del t_6
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/aten::add26769
        t_6 = self.l_10(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1]
        t_6 = self.l_11(t_6)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26781
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26782
        t_6 = t_6.split(split_size=1600, dim=2)
        t_0 = t_6[0]
        t_1 = t_6[1]
        t_6 = t_6[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267840
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26787
        t_3 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267840
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26791
        t_5 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267840
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26795
        t_8 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26796
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26798
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        del t_5
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267840
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26802
        t_8 = t_0.view(size=t_8)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::view26803
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26808
        t_0 = t_8.permute(dims=t_0)
        del t_8
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26810
        t_8 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26814
        t_5 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26818
        t_3 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26819
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26821
        t_3 = torch.div(input=t_3, other=25)
        t_3 = [t_8, t_5, 25, t_3]
        del t_5
        del t_8
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26825
        t_3 = t_1.view(size=t_3)
        del t_1
        t_1 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::view26826
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26831
        t_1 = t_3.permute(dims=t_1)
        del t_3
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267842
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26833
        t_3 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267842
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26837
        t_5 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267842
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26841
        t_8 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26842
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26844
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        del t_5
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267842
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26848
        t_8 = t_6.view(size=t_8)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::view26849
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26854
        t_6 = t_8.permute(dims=t_6)
        del t_8
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::permute26809
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::permute26832
        t_1 = t_0.matmul(other=t_1)
        del t_0
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::matmul26856
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26857
        t_1 = torch.div(input=t_1, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::div26858
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26859
        t_0 = t_1.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::div26858
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26862
        t_8 = t_1.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26863
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26860
        t_0 = torch.sub(input=t_8, other=t_0)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26870
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26871
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26872
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26873
        t_5 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::slice26874
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26875
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26876
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26877
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26878
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::slice26879
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26880
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::sub26868
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26863
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26881
        t_0 = t_5[:, :, t_0:t_8:1]
        del t_5
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::slice26882
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26883
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26884
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26863
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26885
        t_8 = t_0[:, :, :, 0:t_8:1]
        del t_0
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::div26858
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::slice26886
        t_1 = torch.mul(input=t_1, other=t_8)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::slice26886
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26888
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26889
        t_8 = torch.rsub(t_8, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::rsub26890
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26891
        t_8 = torch.mul(input=t_8, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::mul26887
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::mul26892
        t_8 = torch.sub(input=t_1, other=t_8)
        del t_1
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::sub26894
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26895
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26896
        t_8 = t_8.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::softmax26897
        t_8 = self.l_12(t_8)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::permute26855
        t_6 = t_8.matmul(other=t_6)
        del t_8
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::matmul26899
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26904
        t_8 = t_6.permute(dims=t_8)
        del t_6
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::permute26905
        t_8 = t_8.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::contiguous26907
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26908
        t_6 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::contiguous26907
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26912
        t_1 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::contiguous26907
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26916
        t_0 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::contiguous26907
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26919
        t_5 = t_8.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26917
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26920
        t_5 = torch.mul(input=t_0, other=t_5)
        del t_0
        t_5 = [t_6, t_1, t_5]
        del t_1
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::contiguous26907
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26924
        t_5 = t_8.view(size=t_5)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::view26925
        t_5 = self.l_13(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_14(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/aten::add26769
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=t_4, other=t_5)
        del t_4
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/aten::add26929
        t_4 = self.l_15(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2]
        t_4 = self.l_16(t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/prim::Constant26935
        t_8 = torch.mul(input=t_4, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/prim::Constant26937
        t_1 = t_4.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::pow26938
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/prim::Constant26939
        t_1 = torch.mul(input=t_1, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::mul26940
        t_1 = torch.add(input=t_4, other=t_1)
        del t_4
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::add26942
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/prim::Constant26943
        t_1 = torch.mul(input=t_1, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::mul26944
        t_1 = t_1.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::tanh26945
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/prim::Constant26946
        t_1 = torch.add(input=t_1, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::mul26936
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::add26948
        t_1 = torch.mul(input=t_8, other=t_1)
        del t_8
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::mul26949
        t_1 = self.l_17(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj]
        t_1 = self.l_18(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/aten::add26929
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout]
        t_1 = torch.add(input=t_5, other=t_1)
        del t_5
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/aten::add26953
        t_5 = self.l_19(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1]
        t_5 = self.l_20(t_5)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant26965
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant26966
        t_5 = t_5.split(split_size=1600, dim=2)
        t_4 = t_5[0]
        t_6 = t_5[1]
        t_5 = t_5[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant26971
        t_0 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant26975
        t_3 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant26979
        t_9 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size26980
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant26982
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_0, t_3, 25, t_9]
        del t_3
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct26986
        t_9 = t_4.view(size=t_9)
        del t_4
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::view26987
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct26992
        t_4 = t_9.permute(dims=t_4)
        del t_9
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant26994
        t_9 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant26998
        t_3 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27002
        t_0 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27003
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27005
        t_0 = torch.div(input=t_0, other=25)
        t_0 = [t_9, t_3, 25, t_0]
        del t_3
        del t_9
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct27009
        t_0 = t_6.view(size=t_0)
        del t_6
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::view27010
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct27015
        t_6 = t_0.permute(dims=t_6)
        del t_0
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269682
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27017
        t_0 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269682
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27021
        t_3 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269682
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27025
        t_9 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27026
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27028
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_0, t_3, 25, t_9]
        del t_3
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269682
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct27032
        t_9 = t_5.view(size=t_9)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::view27033
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct27038
        t_5 = t_9.permute(dims=t_5)
        del t_9
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::permute26993
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::permute27016
        t_6 = t_4.matmul(other=t_6)
        del t_4
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::matmul27040
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27041
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::div27042
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27043
        t_4 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::div27042
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27046
        t_9 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27047
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27044
        t_4 = torch.sub(input=t_9, other=t_4)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27054
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27055
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27056
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27057
        t_3 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::slice27058
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27059
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27060
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27061
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27062
        t_3 = t_3[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::slice27063
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27064
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::sub27052
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27047
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27065
        t_4 = t_3[:, :, t_4:t_9:1]
        del t_3
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::slice27066
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27067
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27068
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27047
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27069
        t_9 = t_4[:, :, :, 0:t_9:1]
        del t_4
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::div27042
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::slice27070
        t_6 = torch.mul(input=t_6, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::slice27070
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27072
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27073
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::rsub27074
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27075
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::mul27071
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::mul27076
        t_9 = torch.sub(input=t_6, other=t_9)
        del t_6
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::sub27078
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27079
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27080
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::softmax27081
        t_9 = self.l_21(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::permute27039
        t_5 = t_9.matmul(other=t_5)
        del t_9
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::matmul27083
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct27088
        t_9 = t_5.permute(dims=t_9)
        del t_5
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::permute27089
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::contiguous27091
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27092
        t_5 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::contiguous27091
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27096
        t_6 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::contiguous27091
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27100
        t_4 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::contiguous27091
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27103
        t_3 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27101
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27104
        t_3 = torch.mul(input=t_4, other=t_3)
        del t_4
        t_3 = [t_5, t_6, t_3]
        del t_6
        del t_5
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::contiguous27091
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct27108
        t_3 = t_9.view(size=t_3)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::view27109
        t_3 = self.l_22(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_proj]
        t_3 = self.l_23(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/aten::add26953
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[resid_dropout]
        t_3 = torch.add(input=t_1, other=t_3)
        del t_1
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/aten::add27113
        t_1 = self.l_24(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2]
        t_1 = self.l_25(t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/prim::Constant27119
        t_9 = torch.mul(input=t_1, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/prim::Constant27121
        t_6 = t_1.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::pow27122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/prim::Constant27123
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::mul27124
        t_6 = torch.add(input=t_1, other=t_6)
        del t_1
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::add27126
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/prim::Constant27127
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::mul27128
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::tanh27129
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/prim::Constant27130
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::mul27120
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::add27132
        t_6 = torch.mul(input=t_9, other=t_6)
        del t_9
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::mul27133
        t_6 = self.l_26(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj]
        t_6 = self.l_27(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/aten::add27113
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout]
        t_6 = torch.add(input=t_3, other=t_6)
        del t_3
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/aten::add27137
        t_3 = self.l_28(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1]
        t_3 = self.l_29(t_3)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27149
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27150
        t_3 = t_3.split(split_size=1600, dim=2)
        t_1 = t_3[0]
        t_5 = t_3[1]
        t_3 = t_3[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271520
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27155
        t_4 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271520
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27159
        t_0 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271520
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27163
        t_10 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27164
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27166
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_0, 25, t_10]
        del t_0
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271520
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27170
        t_10 = t_1.view(size=t_10)
        del t_1
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::view27171
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27176
        t_1 = t_10.permute(dims=t_1)
        del t_10
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27178
        t_10 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27182
        t_0 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27186
        t_4 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27187
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27189
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_10, t_0, 25, t_4]
        del t_0
        del t_10
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27193
        t_4 = t_5.view(size=t_4)
        del t_5
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::view27194
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27199
        t_5 = t_4.permute(dims=t_5)
        del t_4
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27201
        t_4 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27205
        t_0 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27209
        t_10 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27210
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27212
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_0, 25, t_10]
        del t_0
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27216
        t_10 = t_3.view(size=t_10)
        del t_3
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::view27217
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27222
        t_3 = t_10.permute(dims=t_3)
        del t_10
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::permute27177
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::permute27200
        t_5 = t_1.matmul(other=t_5)
        del t_1
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::matmul27224
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27225
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::div27226
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27227
        t_1 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::div27226
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27230
        t_10 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27231
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27228
        t_1 = torch.sub(input=t_10, other=t_1)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27238
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27239
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27240
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27241
        t_0 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::slice27242
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27243
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27244
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27245
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27246
        t_0 = t_0[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::slice27247
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27248
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::sub27236
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27231
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27249
        t_1 = t_0[:, :, t_1:t_10:1]
        del t_0
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::slice27250
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27251
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27252
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27231
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27253
        t_10 = t_1[:, :, :, 0:t_10:1]
        del t_1
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::div27226
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::slice27254
        t_5 = torch.mul(input=t_5, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::slice27254
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27256
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27257
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::rsub27258
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27259
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::mul27255
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::mul27260
        t_10 = torch.sub(input=t_5, other=t_10)
        del t_5
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::sub27262
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27263
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27264
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::softmax27265
        t_10 = self.l_30(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::permute27223
        t_3 = t_10.matmul(other=t_3)
        del t_10
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::matmul27267
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27272
        t_10 = t_3.permute(dims=t_10)
        del t_3
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::permute27273
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::contiguous27275
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27276
        t_3 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::contiguous27275
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27280
        t_5 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::contiguous27275
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27284
        t_1 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::contiguous27275
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27287
        t_0 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27285
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27288
        t_0 = torch.mul(input=t_1, other=t_0)
        del t_1
        t_0 = [t_3, t_5, t_0]
        del t_5
        del t_3
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::contiguous27275
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27292
        t_0 = t_10.view(size=t_0)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::view27293
        t_0 = self.l_31(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_proj]
        t_0 = self.l_32(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/aten::add27137
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[resid_dropout]
        t_0 = torch.add(input=t_6, other=t_0)
        del t_6
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/aten::add27297
        t_6 = self.l_33(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2]
        t_6 = self.l_34(t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/prim::Constant27303
        t_10 = torch.mul(input=t_6, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/prim::Constant27305
        t_5 = t_6.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::pow27306
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/prim::Constant27307
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::mul27308
        t_5 = torch.add(input=t_6, other=t_5)
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::add27310
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/prim::Constant27311
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::mul27312
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::tanh27313
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/prim::Constant27314
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::mul27304
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::add27316
        t_5 = torch.mul(input=t_10, other=t_5)
        del t_10
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::mul27317
        t_5 = self.l_35(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj]
        t_5 = self.l_36(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/aten::add27297
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout]
        t_5 = torch.add(input=t_0, other=t_5)
        del t_0
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/aten::add27321
        t_0 = self.l_37(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1]
        t_0 = self.l_38(t_0)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27333
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27334
        t_0 = t_0.split(split_size=1600, dim=2)
        t_6 = t_0[0]
        t_3 = t_0[1]
        t_0 = t_0[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273360
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27339
        t_1 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273360
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27343
        t_4 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273360
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27347
        t_11 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27348
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27350
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_1, t_4, 25, t_11]
        del t_4
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273360
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27354
        t_11 = t_6.view(size=t_11)
        del t_6
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::view27355
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27360
        t_6 = t_11.permute(dims=t_6)
        del t_11
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27362
        t_11 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27366
        t_4 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27370
        t_1 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27371
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27373
        t_1 = torch.div(input=t_1, other=25)
        t_1 = [t_11, t_4, 25, t_1]
        del t_4
        del t_11
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27377
        t_1 = t_3.view(size=t_1)
        del t_3
        t_3 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::view27378
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27383
        t_3 = t_1.permute(dims=t_3)
        del t_1
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273362
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27385
        t_1 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273362
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27389
        t_4 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273362
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27393
        t_11 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27394
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27396
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_1, t_4, 25, t_11]
        del t_4
        del t_1
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273362
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27400
        t_11 = t_0.view(size=t_11)
        del t_0
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::view27401
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27406
        t_0 = t_11.permute(dims=t_0)
        del t_11
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::permute27361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::permute27384
        t_3 = t_6.matmul(other=t_3)
        del t_6
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::matmul27408
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27409
        t_3 = torch.div(input=t_3, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::div27410
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27411
        t_6 = t_3.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::div27410
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27414
        t_11 = t_3.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27415
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27412
        t_6 = torch.sub(input=t_11, other=t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27422
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27423
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27424
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27425
        t_4 = self.b_4[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::slice27426
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27427
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27428
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27429
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27430
        t_4 = t_4[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::slice27431
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27432
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::sub27420
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27415
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27433
        t_6 = t_4[:, :, t_6:t_11:1]
        del t_4
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::slice27434
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27435
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27436
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27415
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27437
        t_11 = t_6[:, :, :, 0:t_11:1]
        del t_6
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::div27410
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::slice27438
        t_3 = torch.mul(input=t_3, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::slice27438
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27440
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27441
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::rsub27442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27443
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::mul27439
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::mul27444
        t_11 = torch.sub(input=t_3, other=t_11)
        del t_3
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::sub27446
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27447
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27448
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::softmax27449
        t_11 = self.l_39(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::permute27407
        t_0 = t_11.matmul(other=t_0)
        del t_11
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::matmul27451
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27456
        t_11 = t_0.permute(dims=t_11)
        del t_0
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::permute27457
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::contiguous27459
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27460
        t_0 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::contiguous27459
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27464
        t_3 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::contiguous27459
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27468
        t_6 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::contiguous27459
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27471
        t_4 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27469
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27472
        t_4 = torch.mul(input=t_6, other=t_4)
        del t_6
        t_4 = [t_0, t_3, t_4]
        del t_3
        del t_0
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::contiguous27459
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27476
        t_4 = t_11.view(size=t_4)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::view27477
        t_4 = self.l_40(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_41(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/aten::add27321
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=t_5, other=t_4)
        del t_5
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/aten::add27481
        t_5 = self.l_42(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2]
        t_5 = self.l_43(t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/prim::Constant27487
        t_11 = torch.mul(input=t_5, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/prim::Constant27489
        t_3 = t_5.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::pow27490
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/prim::Constant27491
        t_3 = torch.mul(input=t_3, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::mul27492
        t_3 = torch.add(input=t_5, other=t_3)
        del t_5
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::add27494
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/prim::Constant27495
        t_3 = torch.mul(input=t_3, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::mul27496
        t_3 = t_3.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::tanh27497
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/prim::Constant27498
        t_3 = torch.add(input=t_3, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::mul27488
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::add27500
        t_3 = torch.mul(input=t_11, other=t_3)
        del t_11
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::mul27501
        t_3 = self.l_44(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj]
        t_3 = self.l_45(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/aten::add27481
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout]
        t_3 = torch.add(input=t_4, other=t_3)
        del t_4
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/aten::add27505
        t_4 = self.l_46(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1]
        t_4 = self.l_47(t_4)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27517
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27518
        t_4 = t_4.split(split_size=1600, dim=2)
        t_5 = t_4[0]
        t_0 = t_4[1]
        t_4 = t_4[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275200
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27523
        t_6 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275200
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27527
        t_1 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275200
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27531
        t_12 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27532
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27534
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_1, 25, t_12]
        del t_1
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275200
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27538
        t_12 = t_5.view(size=t_12)
        del t_5
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::view27539
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27544
        t_5 = t_12.permute(dims=t_5)
        del t_12
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275201
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27546
        t_12 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275201
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27550
        t_1 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275201
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27554
        t_6 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27555
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27557
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_12, t_1, 25, t_6]
        del t_1
        del t_12
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275201
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27561
        t_6 = t_0.view(size=t_6)
        del t_0
        t_0 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::view27562
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27567
        t_0 = t_6.permute(dims=t_0)
        del t_6
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27569
        t_6 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27573
        t_1 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27577
        t_12 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27578
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27580
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_1, 25, t_12]
        del t_1
        del t_6
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27584
        t_12 = t_4.view(size=t_12)
        del t_4
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::view27585
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27590
        t_4 = t_12.permute(dims=t_4)
        del t_12
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::permute27545
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::permute27568
        t_0 = t_5.matmul(other=t_0)
        del t_5
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::matmul27592
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27593
        t_0 = torch.div(input=t_0, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::div27594
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27595
        t_5 = t_0.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::div27594
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27598
        t_12 = t_0.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27599
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27596
        t_5 = torch.sub(input=t_12, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27606
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27607
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27608
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27609
        t_1 = self.b_5[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::slice27610
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27611
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27612
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27613
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27614
        t_1 = t_1[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::slice27615
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27616
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::sub27604
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27599
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27617
        t_5 = t_1[:, :, t_5:t_12:1]
        del t_1
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::slice27618
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27619
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27620
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27599
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27621
        t_12 = t_5[:, :, :, 0:t_12:1]
        del t_5
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::div27594
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::slice27622
        t_0 = torch.mul(input=t_0, other=t_12)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::slice27622
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27624
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27625
        t_12 = torch.rsub(t_12, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::rsub27626
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27627
        t_12 = torch.mul(input=t_12, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::mul27623
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::mul27628
        t_12 = torch.sub(input=t_0, other=t_12)
        del t_0
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::sub27630
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27631
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27632
        t_12 = t_12.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::softmax27633
        t_12 = self.l_48(t_12)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::permute27591
        t_4 = t_12.matmul(other=t_4)
        del t_12
        t_12 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::matmul27635
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27640
        t_12 = t_4.permute(dims=t_12)
        del t_4
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::permute27641
        t_12 = t_12.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::contiguous27643
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27644
        t_4 = t_12.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::contiguous27643
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27648
        t_0 = t_12.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::contiguous27643
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27652
        t_5 = t_12.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::contiguous27643
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27655
        t_1 = t_12.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27653
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27656
        t_1 = torch.mul(input=t_5, other=t_1)
        del t_5
        t_1 = [t_4, t_0, t_1]
        del t_0
        del t_4
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::contiguous27643
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27660
        t_1 = t_12.view(size=t_1)
        del t_12
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::view27661
        t_1 = self.l_49(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_proj]
        t_1 = self.l_50(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/aten::add27505
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[resid_dropout]
        t_1 = torch.add(input=t_3, other=t_1)
        del t_3
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/aten::add27665
        t_3 = self.l_51(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2]
        t_3 = self.l_52(t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/prim::Constant27671
        t_12 = torch.mul(input=t_3, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/prim::Constant27673
        t_0 = t_3.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::pow27674
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/prim::Constant27675
        t_0 = torch.mul(input=t_0, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::mul27676
        t_0 = torch.add(input=t_3, other=t_0)
        del t_3
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::add27678
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/prim::Constant27679
        t_0 = torch.mul(input=t_0, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::mul27680
        t_0 = t_0.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::tanh27681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/prim::Constant27682
        t_0 = torch.add(input=t_0, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::mul27672
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::add27684
        t_0 = torch.mul(input=t_12, other=t_0)
        del t_12
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::mul27685
        t_0 = self.l_53(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj]
        t_0 = self.l_54(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/aten::add27665
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout]
        t_0 = torch.add(input=t_1, other=t_0)
        del t_1
        # calling GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/aten::add27689
        t_0 = self.l_55(t_0)

        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]
        return (t_0, )

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition8(nn.Module):
    SCOPES = {
        'GPT2LMHeadModel/StatelessLinear[stateless_lm_head]',
    }

    def __init__(self, layers, tensors):
        super(Partition8, self).__init__()
        # initializing partition layers
        self.scopes = []
        self.l_0 = layers['GPT2LMHeadModel/StatelessLinear[stateless_lm_head]']
        self.scopes.append(
            'GPT2LMHeadModel/StatelessLinear[stateless_lm_head]')

        self.device = torch.device('cuda:0')
        self.lookup = {'l_0': 'stateless_lm_head'}

    def forward(self, x2, x0, x1):
        # GPT2LMHeadModel/StatelessLinear[stateless_lm_head] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f] <=> x0
        # GPT2LMHeadModel/Parameter[w_wte] <=> x1
        # input1 <=> x2

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)
        x2 = x2.to(self.device)

        # calling GPT2LMHeadModel/StatelessLinear[stateless_lm_head] with arguments:
        # GPT2LMHeadModel/Parameter[w_wte]
        # GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]
        t_1 = self.l_0(x1, x0)
        del x0
        del x1
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/StatelessLinear[stateless_lm_head]
        # GPT2LMHeadModel/prim::Constant16351
        # GPT2LMHeadModel/prim::Constant16352
        # GPT2LMHeadModel/prim::Constant16353
        # GPT2LMHeadModel/prim::Constant16354
        t_1 = t_1[:, 0:-1:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/aten::slice16355
        # GPT2LMHeadModel/prim::Constant16356
        # GPT2LMHeadModel/prim::Constant16357
        # GPT2LMHeadModel/prim::Constant16358
        # GPT2LMHeadModel/prim::Constant16359
        t_1 = t_1[:, :, 0:9223372036854775807:1]
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/aten::slice16360
        t_1 = t_1.contiguous()
        # calling Tensor.slice with arguments:
        # input1
        # GPT2LMHeadModel/prim::Constant16363
        # GPT2LMHeadModel/prim::Constant16364
        # GPT2LMHeadModel/prim::Constant16365
        # GPT2LMHeadModel/prim::Constant16366
        t_2 = x2[:, 1:9223372036854775807:1]
        del x2
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/aten::slice16367
        t_2 = t_2.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/aten::contiguous16362
        # GPT2LMHeadModel/prim::Constant16370
        t_3 = t_1.size(dim=-1)
        t_3 = [-1, t_3]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/aten::contiguous16362
        # GPT2LMHeadModel/prim::ListConstruct16375
        t_3 = t_1.view(size=t_3)
        del t_1
        t_1 = [-1]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/aten::contiguous16369
        # GPT2LMHeadModel/prim::ListConstruct16378
        t_1 = t_2.view(size=t_1)
        del t_2
        # calling torch.log_softmax with arguments:
        # GPT2LMHeadModel/aten::view16376
        # GPT2LMHeadModel/prim::Constant16380
        # GPT2LMHeadModel/prim::Constant16381
        t_3 = t_3.log_softmax(dim=1, dtype=None)
        # calling F.nll_loss with arguments:
        # GPT2LMHeadModel/aten::log_softmax16382
        # GPT2LMHeadModel/aten::view16379
        # GPT2LMHeadModel/prim::Constant16390
        # GPT2LMHeadModel/prim::Constant16391
        # GPT2LMHeadModel/prim::Constant16392
        t_1 = F.nll_loss(input=t_3,
                         target=t_1,
                         weight=None,
                         reduction='mean',
                         ignore_index=-100)
        del t_3

        # returning:
        # GPT2LMHeadModel/aten::nll_loss16393
        return (t_1, )

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


def traverse_model(
        module: nn.Module,
        depth: int,
        prefix: Optional[str] = None,
        basic_blocks: Tuple[nn.Module] = (),
        full: bool = False) -> Iterator[Tuple[nn.Module, str, nn.Module]]:
    '''
    iterate over model layers yielding the layer,layer_scope,encasing_module
    Parameters:
    -----------
    model:
        the model to iterate over
    depth:
        how far down in the model tree to go
    basic_blocks:
        a list of modules that if encountered will not be broken down
    full:
        whether to yield only layers specified by the depth and basick_block options or to yield all layers
    '''
    if prefix is None:
        prefix = type(module).__name__

    for name, sub_module in module.named_children():
        scope = prefix + "/" + type(sub_module).__name__ + f"[{name}]"
        if len(list(sub_module.children())) == 0 or isinstance(
                sub_module, tuple(basic_blocks)) or depth == 0:
            yield sub_module, scope, module
        else:
            if full:
                yield sub_module, scope, module
            yield from traverse_model(sub_module, depth - 1, scope,
                                      basic_blocks, full)


def layerDict(model: nn.Module,
              depth=1000,
              basic_blocks=None) -> Dict[str, nn.Module]:
    return {
        s: l
        for l, s, _ in traverse_model(model, depth, basic_blocks=basic_blocks)
    }


def traverse_params_buffs(
        module: nn.Module,
        prefix: Optional[str] = None) -> Iterator[Tuple[torch.tensor, str]]:
    '''
    iterate over model's buffers and parameters yielding obj,obj_scope

    Parameters:
    -----------
    model:
        the model to iterate over
    '''
    if prefix is None:
        prefix = type(module).__name__

    # params
    for param_name, param in module.named_parameters(recurse=False):
        param_scope = f"{prefix}/{type(param).__name__}[{param_name}]"
        yield param, param_scope

    # buffs
    for buffer_name, buffer in module.named_buffers(recurse=False):
        buffer_scope = f"{prefix}/{type(buffer).__name__}[{buffer_name}]"
        yield buffer, buffer_scope

    # recurse
    for name, sub_module in module.named_children():
        yield from traverse_params_buffs(
            sub_module, prefix + "/" + type(sub_module).__name__ + f"[{name}]")


def tensorDict(model: nn.Module) -> OrderedDict[str, Tensor]:
    return collections.OrderedDict(
        (s, t) for t, s in traverse_params_buffs(model))


def state_dict(partition, device=None):
    # we return the state dict of this part as it should be in the original model
    state = nn.Module.state_dict(partition)
    lookup = partition.lookup
    result = dict()
    for k, v in state.items():
        if k in lookup:
            result[lookup[k]] = v if device is None else v.to(device)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            result[new_k] = v if device is None else v.to(device)
    return result


def load_state_dict(partition, state):
    reverse_lookup = {v: k for k, v in partition.lookup.items()}
    device = partition.device
    keys = list(partition.state_dict(None).keys())
    new_state = dict()
    for k in keys:
        if k in reverse_lookup:
            new_state[reverse_lookup[k]] = state[k].to(device)
            continue
        idx = k.rfind(".")
        to_replace = k[:idx]
        if to_replace in reverse_lookup:
            key = reverse_lookup[to_replace] + k[idx:]
            new_state[key] = state[k].to(device)
    nn.Module.load_state_dict(partition, new_state, strict=True)


def named_buffers(partition, recurse=True):
    # we return the named buffers of this part as it should be in the original model
    params = nn.Module.named_buffers(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def named_parameters(partition, recurse=True):
    # we return the named parameters of this part as it should be in the original model
    params = nn.Module.named_parameters(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def cpu(partition):
    partition.device = torch.device('cpu')
    return nn.Module.cpu(partition)


def cuda(partition, device=None):
    if device is None:
        device = torch.cuda.current_device()
    partition.device = torch.device(device)
    return nn.Module.cuda(partition, partition.device)


def to(partition, *args, **kwargs):
    device = None
    if 'device' in kwargs:
        device = kwargs['device']
    elif 'tensor' in kwargs:
        device = kwargs['tensor'].device
    if args:
        if isinstance(args[0], (torch.device, int, str)):
            device = args[0]
        if torch.is_tensor(args[0]):
            device = args[0].device
    if not (device is None):
        partition.device = torch.device(device)
    return nn.Module.to(partition, *args, **kwargs)


"""analysis summary
-I- Printing Report
warnings:
Partition3 output:GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::permute23225 is not contiguous!
Number of stages: 8
n_partitions:9, num_dummy_stages:1
unique_stages_on_same_gpu: [{0, 8}]
cutting edges are edges between partitions
number of cutting edges: 16

backward times include recomputation

real times are based on real measurements of execution time of generated partitions ms
forward {0: 63.45, 1: 61.34, 2: 59.5, 3: 65.0, 4: 60.29, 5: 64.79, 6: 60.2, 7: 59.5}
backward {0: 180.04, 1: 169.81, 2: 162.95, 3: 174.08, 4: 162.68, 5: 177.84, 6: 162.28, 7: 163.07}

balance is ratio of computation time between fastest and slowest parts. (between 0 and 1 higher is better)

real balance:
forward 0.915
backward 0.901

Assuming bandwidth of 12 GBps between GPUs

communication volumes size of activations of each partition
0: input size:'6.57 MB', recieve_time:'0.55 ms', out:'32.77 MB', send time:'2.73 ms'
1: input size:'32.77 MB', recieve_time:'2.73 ms', out:'13.11 MB', send time:'1.09 ms'
2: input size:'13.11 MB', recieve_time:'1.09 ms', out:'6.55 MB', send time:'0.55 ms'
3: input size:'6.55 MB', recieve_time:'0.55 ms', out:'13.11 MB', send time:'1.09 ms'
4: input size:'13.11 MB', recieve_time:'1.09 ms', out:'13.11 MB', send time:'1.09 ms'
5: input size:'13.11 MB', recieve_time:'1.09 ms', out:'6.55 MB', send time:'0.55 ms'
6: input size:'6.55 MB', recieve_time:'0.55 ms', out:'13.11 MB', send time:'1.09 ms'
7: input size:'13.11 MB', recieve_time:'1.09 ms', out:'6.55 MB', send time:'0.55 ms'

Compuatation Communication ratio (comp/(comp+comm)):
forward {0: 0.96, 1: 0.98, 2: 0.99, 3: 0.98, 4: 0.98, 5: 0.99, 6: 0.98, 7: 0.99} 
backward {0: 1.0, 1: 0.98, 2: 0.99, 3: 1.0, 4: 0.99, 5: 0.99, 6: 1.0, 7: 0.99}

Pipeline Slowdown: (compared to sequential executation with no communication, and same recompute policy)
forward 1.072
backward 1.072

Expected utilization by partition
forward {0: 0.94, 1: 0.92, 2: 0.91, 3: 0.98, 4: 0.91, 5: 0.99, 6: 0.91, 7: 0.91}
backward {0: 1.0, 1: 0.92, 2: 0.9, 3: 0.97, 4: 0.89, 5: 0.98, 6: 0.9, 7: 0.9}

worstcase: bwd: 180.040 fwd: 65.004
expected_speedup_compared_to_seq_no_recomp_no_comm: 5.487
Expected speedup for 8 partitions is: 7.465
"""
