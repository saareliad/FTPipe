"""AutoGenerated with:
python partition_gpt2_models.py --model_type gpt2 --model_name_or_path gpt2-xl --train_data_file wikitext-2-raw/wiki.train.raw --n_partitions 8 --partitioning_batch_size 1 --analysis_batch_size 1 --block_size -1 --n_iter 1 --lmhead --async_pipeline --output_file gpt2_xl_8p_ratio_5 --overwrite_cache --bwd_to_fwd_ratio 5
"""
import torch
from torch import Tensor
import torch.nn as nn
import torch.nn.functional as F
from itertools import chain
import operator
from typing import Optional, Tuple, Iterator, Iterable, OrderedDict, Dict
import collections
from torch.nn.modules.sparse import Embedding
from torch.nn.modules.normalization import LayerNorm
from torch.nn.modules.dropout import Dropout
from torch.nn.modules.linear import Linear
from transformers.modeling_utils import Conv1D
# this is an auto generated file do not edit unless you know what you are doing

# partition adjacency
# model inputs {0, 7}
# partition 0 {'inputs': {'input0'}, 'outputs': {1}}
# partition 1 {'inputs': {0}, 'outputs': {2}}
# partition 2 {'inputs': {1}, 'outputs': {3}}
# partition 3 {'inputs': {2}, 'outputs': {4}}
# partition 4 {'inputs': {3}, 'outputs': {5}}
# partition 5 {'inputs': {4}, 'outputs': {6}}
# partition 6 {'inputs': {5}, 'outputs': {7}}
# partition 7 {'inputs': {'input1', 6}, 'outputs': {'output0'}}
# model outputs {7}


def create_pipeline_configuration(DEBUG=False):
    depth = 10000
    basic_blocks = (Embedding, LayerNorm, Dropout, Linear, Conv1D)
    blocks_path = [
        'torch.nn.modules.sparse.Embedding',
        'torch.nn.modules.normalization.LayerNorm',
        'torch.nn.modules.dropout.Dropout', 'torch.nn.modules.linear.Linear',
        'transformers.modeling_utils.Conv1D'
    ]
    module_path = 'models.partitioned.old_gpt2xl_8p_untied'

    # creating configuration
    stages = {
        0: {
            "inputs": {
                'input0': {
                    'shape': torch.Size([1, 1024]),
                    'dtype': torch.int64,
                    'is_batched': True,
                    'req_grad': False
                }
            },
            "outputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::mul19963':
                {
                    'shape': torch.Size([1, 1024, 6400]),
                    'dtype': torch.float32,
                    'is_batched': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/aten::add19943':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                }
            }
        },
        1: {
            "inputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::mul19963':
                {
                    'shape': torch.Size([1, 1024, 6400]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/aten::add19943':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                }
            },
            "outputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj]':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/aten::add21047':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                }
            }
        },
        2: {
            "inputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj]':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/aten::add21047':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                }
            },
            "outputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/aten::add22175':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::permute22238':
                {
                    'shape': torch.Size([1, 25, 64, 1024]),
                    'dtype': torch.float32,
                    'is_batched': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22209':
                {
                    'shape': torch.Size([1, 1024, 25, 64]),
                    'dtype': torch.float32,
                    'is_batched': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22255':
                {
                    'shape': torch.Size([1, 1024, 25, 64]),
                    'dtype': torch.float32,
                    'is_batched': True
                }
            }
        },
        3: {
            "inputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/aten::add22175':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::permute22238':
                {
                    'shape': torch.Size([1, 25, 64, 1024]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22209':
                {
                    'shape': torch.Size([1, 1024, 25, 64]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22255':
                {
                    'shape': torch.Size([1, 1024, 25, 64]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                }
            },
            "outputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2]':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/aten::add23439':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                }
            }
        },
        4: {
            "inputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2]':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/aten::add23439':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                }
            },
            "outputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]':
                {
                    'shape': torch.Size([1, 1024, 6400]),
                    'dtype': torch.float32,
                    'is_batched': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/aten::add24543':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                }
            }
        },
        5: {
            "inputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]':
                {
                    'shape': torch.Size([1, 1024, 6400]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/aten::add24543':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                }
            },
            "outputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout]':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/aten::add25647':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                }
            }
        },
        6: {
            "inputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout]':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/aten::add25647':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                }
            },
            "outputs": {
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/aten::add26775':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::view26931':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True
                }
            }
        },
        7: {
            "inputs": {
                'input1':
                {
                    'shape': torch.Size([1, 1024]),
                    'dtype': torch.int64,
                    'is_batched': True,
                    'req_grad': False
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/aten::add26775':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                },
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::view26931':
                {
                    'shape': torch.Size([1, 1024, 1600]),
                    'dtype': torch.float32,
                    'is_batched': True,
                    'req_grad': True
                }
            },
            "outputs": {
                'GPT2LMHeadModel/aten::nll_loss16398': {
                    'shape': torch.Size([1]),
                    'dtype': torch.float32,
                    'is_batched': False
                }
            }
        }
    }

    stages[0]['stage_cls'] = module_path + '.Partition0'
    device = 'cpu' if DEBUG else 'cuda:0'
    stages[0]['devices'] = [device]

    stages[1]['stage_cls'] = module_path + '.Partition1'
    device = 'cpu' if DEBUG else 'cuda:1'
    stages[1]['devices'] = [device]

    stages[2]['stage_cls'] = module_path + '.Partition2'
    device = 'cpu' if DEBUG else 'cuda:2'
    stages[2]['devices'] = [device]

    stages[3]['stage_cls'] = module_path + '.Partition3'
    device = 'cpu' if DEBUG else 'cuda:3'
    stages[3]['devices'] = [device]

    stages[4]['stage_cls'] = module_path + '.Partition4'
    device = 'cpu' if DEBUG else 'cuda:4'
    stages[4]['devices'] = [device]

    stages[5]['stage_cls'] = module_path + '.Partition5'
    device = 'cpu' if DEBUG else 'cuda:5'
    stages[5]['devices'] = [device]

    stages[6]['stage_cls'] = module_path + '.Partition6'
    device = 'cpu' if DEBUG else 'cuda:6'
    stages[6]['devices'] = [device]

    stages[7]['stage_cls'] = module_path + '.Partition7'
    device = 'cpu' if DEBUG else 'cuda:7'
    stages[7]['devices'] = [device]

    config = dict()
    config['batch_dim'] = 0
    config['depth'] = depth
    config['basic_blocks'] = blocks_path
    config['model_inputs'] = {
        'input0': {
            "shape": torch.Size([1, 1024]),
            "dtype": torch.int64,
            "is_batched": True
        },
        'input1': {
            "shape": torch.Size([1, 1024]),
            "dtype": torch.int64,
            "is_batched": True
        }
    }
    config['model_outputs'] = {
        'GPT2LMHeadModel/aten::nll_loss16398': {
            "shape": torch.Size([1]),
            "dtype": torch.float32,
            "is_batched": False
        }
    }
    config['stages'] = stages

    return config


class Partition0(nn.Module):
    def __init__(self, layers, tensors):
        super(Partition0, self).__init__()
        # initializing partition layers
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wte]']
        assert isinstance(
            self.l_0, Embedding
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wte]] is expected to be of type Embedding but was of type {type(self.l_0)}'
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe]']
        assert isinstance(
            self.l_1, Embedding
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe]] is expected to be of type Embedding but was of type {type(self.l_1)}'
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop]']
        assert isinstance(
            self.l_2, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop]] is expected to be of type Dropout but was of type {type(self.l_2)}'
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_3, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_3)}'
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_4, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_4)}'
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_5, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_5)}'
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_6, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_6)}'
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_7, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_7)}'
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_8, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_8)}'
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_9, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_9)}'
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_10, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_10)}'
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_11, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_11)}'
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_12, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_12)}'
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_13, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_13)}'
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_14, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_14)}'
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_15, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_15)}'
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_16, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_16)}'
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_17, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_17)}'
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_18, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_18)}'
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_19, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_19)}'
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_20, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_20)}'
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_21, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_21)}'
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_22, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_22)}'
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_23, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_23)}'
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_24, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_24)}'
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_25, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_25)}'
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_26, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_26)}'
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_27, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_27)}'
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_28, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_28)}'
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_29, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_29)}'
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_30, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_30)}'
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_31, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_31)}'
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_32, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_32)}'
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_33, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_33)}'
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_34, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_34)}'
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_35, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_35)}'
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_36, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_36)}'
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_37, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_37)}'
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_38, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_38)}'
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_39, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_39)}'
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_40, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_40)}'
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_41, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_41)}'
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_42, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_42)}'
        self.l_43 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_43, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_43)}'
        self.l_44 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_44, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_44)}'
        self.l_45 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_45, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_45)}'
        self.l_46 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_46, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_46)}'
        self.l_47 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_47, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_47)}'
        self.l_48 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_48, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_48)}'
        self.l_49 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_49, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_49)}'
        self.l_50 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_50, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_50)}'
        self.l_51 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_51, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_51)}'
        self.l_52 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_52, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_52)}'
        self.l_53 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_53, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_53)}'
        self.l_54 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_54, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_54)}'

        # initializing partition buffers
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_4', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_5', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Tensor[bias]']
        )

        # initializing partition parameters

        self.device = torch.device('cuda:0')
        self.lookup = {
            'l_0': 'transformer.wte',
            'l_1': 'transformer.wpe',
            'l_2': 'transformer.drop',
            'l_3': 'transformer.blocks.0.ln_1',
            'l_4': 'transformer.blocks.0.attn.c_attn',
            'l_5': 'transformer.blocks.0.attn.attn_dropout',
            'l_6': 'transformer.blocks.0.attn.c_proj',
            'l_7': 'transformer.blocks.0.attn.resid_dropout',
            'l_8': 'transformer.blocks.0.ln_2',
            'l_9': 'transformer.blocks.0.mlp.c_fc',
            'l_10': 'transformer.blocks.0.mlp.c_proj',
            'l_11': 'transformer.blocks.0.mlp.dropout',
            'l_12': 'transformer.blocks.1.ln_1',
            'l_13': 'transformer.blocks.1.attn.c_attn',
            'l_14': 'transformer.blocks.1.attn.attn_dropout',
            'l_15': 'transformer.blocks.1.attn.c_proj',
            'l_16': 'transformer.blocks.1.attn.resid_dropout',
            'l_17': 'transformer.blocks.1.ln_2',
            'l_18': 'transformer.blocks.1.mlp.c_fc',
            'l_19': 'transformer.blocks.1.mlp.c_proj',
            'l_20': 'transformer.blocks.1.mlp.dropout',
            'l_21': 'transformer.blocks.2.ln_1',
            'l_22': 'transformer.blocks.2.attn.c_attn',
            'l_23': 'transformer.blocks.2.attn.attn_dropout',
            'l_24': 'transformer.blocks.2.attn.c_proj',
            'l_25': 'transformer.blocks.2.attn.resid_dropout',
            'l_26': 'transformer.blocks.2.ln_2',
            'l_27': 'transformer.blocks.2.mlp.c_fc',
            'l_28': 'transformer.blocks.2.mlp.c_proj',
            'l_29': 'transformer.blocks.2.mlp.dropout',
            'l_30': 'transformer.blocks.3.ln_1',
            'l_31': 'transformer.blocks.3.attn.c_attn',
            'l_32': 'transformer.blocks.3.attn.attn_dropout',
            'l_33': 'transformer.blocks.3.attn.c_proj',
            'l_34': 'transformer.blocks.3.attn.resid_dropout',
            'l_35': 'transformer.blocks.3.ln_2',
            'l_36': 'transformer.blocks.3.mlp.c_fc',
            'l_37': 'transformer.blocks.3.mlp.c_proj',
            'l_38': 'transformer.blocks.3.mlp.dropout',
            'l_39': 'transformer.blocks.4.ln_1',
            'l_40': 'transformer.blocks.4.attn.c_attn',
            'l_41': 'transformer.blocks.4.attn.attn_dropout',
            'l_42': 'transformer.blocks.4.attn.c_proj',
            'l_43': 'transformer.blocks.4.attn.resid_dropout',
            'l_44': 'transformer.blocks.4.ln_2',
            'l_45': 'transformer.blocks.4.mlp.c_fc',
            'l_46': 'transformer.blocks.4.mlp.c_proj',
            'l_47': 'transformer.blocks.4.mlp.dropout',
            'l_48': 'transformer.blocks.5.ln_1',
            'l_49': 'transformer.blocks.5.attn.c_attn',
            'l_50': 'transformer.blocks.5.attn.attn_dropout',
            'l_51': 'transformer.blocks.5.attn.c_proj',
            'l_52': 'transformer.blocks.5.attn.resid_dropout',
            'l_53': 'transformer.blocks.5.ln_2',
            'l_54': 'transformer.blocks.5.mlp.c_fc',
            'b_0': 'transformer.blocks.0.attn.bias',
            'b_1': 'transformer.blocks.1.attn.bias',
            'b_2': 'transformer.blocks.2.attn.bias',
            'b_3': 'transformer.blocks.3.attn.bias',
            'b_4': 'transformer.blocks.4.attn.bias',
            'b_5': 'transformer.blocks.5.attn.bias'
        }

    def forward(self, x0):
        # GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wte] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_attn] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[attn_dropout] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_proj] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[resid_dropout] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_attn] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[attn_dropout] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_proj] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[resid_dropout] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_attn] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[attn_dropout] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_proj] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[resid_dropout] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_attn] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[attn_dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_proj] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[resid_dropout] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_attn] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[attn_dropout] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_proj] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[resid_dropout] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_attn] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[attn_dropout] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_proj] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[resid_dropout] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Tensor[bias] <=> self.b_5
        # input0 <=> x0

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)

        # calling Tensor.size with arguments:
        # input0
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18832
        t_1 = x0.size(dim=1)
        t_1 = [-1, t_1]
        # calling Tensor.view with arguments:
        # input0
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::ListConstruct18837
        t_1 = x0.view(size=t_1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::view18838
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18839
        t_2 = t_1.size(dim=-1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::size18840
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18842
        t_2 = torch.add(input=t_2, other=0)
        # calling torch.arange with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18846
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::add18844
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18847
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18848
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18850
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18851
        t_2 = torch.arange(start=0,
                           end=t_2,
                           step=1,
                           dtype=torch.int64,
                           device=self.device,
                           requires_grad=False)
        # calling torch.unsqueeze with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::arange18852
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18853
        t_2 = t_2.unsqueeze(dim=0)
        # calling Tensor.expand_as with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::unsqueeze18854
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::view18838
        t_2 = t_2.expand_as(other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wte] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::view18838
        t_1 = self.l_0(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::expand_as18855
        t_2 = self.l_1(t_2)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wte]
        # GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe]
        t_2 = torch.add(input=t_1, other=t_2)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::add18859
        # GPT2LMHeadModel/GPT2Model[transformer]/prim::Constant18860
        t_2 = torch.add(input=t_2, other=0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/aten::add18862
        t_2 = self.l_2(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop]
        t_1 = self.l_3(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1]
        t_1 = self.l_4(t_1)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18875
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18876
        t_1 = t_1.split(split_size=1600, dim=2)
        t_4 = t_1[0]
        t_5 = t_1[1]
        t_1 = t_1[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188780
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18881
        t_6 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188780
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18885
        t_7 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188780
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18889
        t_8 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size18890
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18892
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_6, t_7, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188780
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct18896
        t_8 = t_4.view(size=t_8)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::view18897
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct18902
        t_4 = t_8.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188781
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18904
        t_8 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188781
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18908
        t_7 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188781
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18912
        t_6 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size18913
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18915
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_8, t_7, 25, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188781
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct18919
        t_6 = t_5.view(size=t_6)
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::view18920
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct18925
        t_5 = t_6.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188782
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18927
        t_6 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188782
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18931
        t_7 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188782
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18935
        t_8 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size18936
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18938
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_6, t_7, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListUnpack188782
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct18942
        t_8 = t_1.view(size=t_8)
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::view18943
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct18948
        t_1 = t_8.permute(dims=t_1)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::permute18903
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::permute18926
        t_5 = t_4.matmul(other=t_5)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::matmul18950
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18951
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::div18952
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18953
        t_4 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::div18952
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18956
        t_8 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size18957
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size18954
        t_4 = torch.sub(input=t_8, other=t_4)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18964
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18965
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18966
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18967
        t_7 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::slice18968
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18969
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18970
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18971
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18972
        t_7 = t_7[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::slice18973
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18974
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::sub18962
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size18957
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18975
        t_4 = t_7[:, :, t_4:t_8:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::slice18976
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18977
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18978
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size18957
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18979
        t_8 = t_4[:, :, :, 0:t_8:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::div18952
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::slice18980
        t_5 = torch.mul(input=t_5, other=t_8)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::slice18980
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18982
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18983
        t_8 = torch.rsub(t_8, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::rsub18984
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18985
        t_8 = torch.mul(input=t_8, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::mul18981
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::mul18986
        t_8 = torch.sub(input=t_5, other=t_8)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::sub18988
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18989
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant18990
        t_8 = t_8.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::softmax18991
        t_8 = self.l_5(t_8)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::permute18949
        t_1 = t_8.matmul(other=t_1)
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::matmul18993
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct18998
        t_8 = t_1.permute(dims=t_8)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::permute18999
        t_8 = t_8.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::contiguous19001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant19002
        t_1 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::contiguous19001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant19006
        t_5 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::contiguous19001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant19010
        t_4 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::contiguous19001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::Constant19013
        t_7 = t_8.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size19011
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::size19014
        t_7 = torch.mul(input=t_4, other=t_7)
        t_7 = [t_1, t_5, t_7]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::contiguous19001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/prim::ListConstruct19018
        t_7 = t_8.view(size=t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/aten::view19019
        t_7 = self.l_6(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Conv1D[c_proj]
        t_7 = self.l_7(t_7)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Dropout[drop]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]/Dropout[resid_dropout]
        t_7 = torch.add(input=t_2, other=t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/aten::add19023
        t_2 = self.l_8(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2]
        t_2 = self.l_9(t_2)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/prim::Constant19029
        t_8 = torch.mul(input=t_2, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/prim::Constant19031
        t_5 = t_2.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::pow19032
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/prim::Constant19033
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::mul19034
        t_5 = torch.add(input=t_2, other=t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::add19036
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/prim::Constant19037
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::mul19038
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::tanh19039
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/prim::Constant19040
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::mul19030
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::add19042
        t_5 = torch.mul(input=t_8, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/aten::mul19043
        t_5 = self.l_10(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj]
        t_5 = self.l_11(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/aten::add19023
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout]
        t_5 = torch.add(input=t_7, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/aten::add19047
        t_7 = self.l_12(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1]
        t_7 = self.l_13(t_7)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19059
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19060
        t_7 = t_7.split(split_size=1600, dim=2)
        t_2 = t_7[0]
        t_1 = t_7[1]
        t_7 = t_7[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190620
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19065
        t_4 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190620
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19069
        t_6 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190620
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19073
        t_9 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19074
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19076
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_4, t_6, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190620
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19080
        t_9 = t_2.view(size=t_9)
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::view19081
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19086
        t_2 = t_9.permute(dims=t_2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190621
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19088
        t_9 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190621
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19092
        t_6 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190621
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19096
        t_4 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19097
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19099
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_9, t_6, 25, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190621
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19103
        t_4 = t_1.view(size=t_4)
        t_1 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::view19104
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19109
        t_1 = t_4.permute(dims=t_1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190622
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19111
        t_4 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190622
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19115
        t_6 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190622
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19119
        t_9 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19120
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19122
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_4, t_6, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListUnpack190622
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19126
        t_9 = t_7.view(size=t_9)
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::view19127
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19132
        t_7 = t_9.permute(dims=t_7)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::permute19087
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::permute19110
        t_1 = t_2.matmul(other=t_1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::matmul19134
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19135
        t_1 = torch.div(input=t_1, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::div19136
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19137
        t_2 = t_1.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::div19136
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19140
        t_9 = t_1.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19141
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19138
        t_2 = torch.sub(input=t_9, other=t_2)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19148
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19149
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19150
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19151
        t_6 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::slice19152
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19153
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19154
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19155
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19156
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::slice19157
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19158
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::sub19146
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19141
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19159
        t_2 = t_6[:, :, t_2:t_9:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::slice19160
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19161
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19162
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19141
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19163
        t_9 = t_2[:, :, :, 0:t_9:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::div19136
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::slice19164
        t_1 = torch.mul(input=t_1, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::slice19164
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19166
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19167
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::rsub19168
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19169
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::mul19165
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::mul19170
        t_9 = torch.sub(input=t_1, other=t_9)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::sub19172
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19173
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19174
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::softmax19175
        t_9 = self.l_14(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::permute19133
        t_7 = t_9.matmul(other=t_7)
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::matmul19177
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19182
        t_9 = t_7.permute(dims=t_9)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::permute19183
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::contiguous19185
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19186
        t_7 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::contiguous19185
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19190
        t_1 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::contiguous19185
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19194
        t_2 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::contiguous19185
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::Constant19197
        t_6 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19195
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::size19198
        t_6 = torch.mul(input=t_2, other=t_6)
        t_6 = [t_7, t_1, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::contiguous19185
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/prim::ListConstruct19202
        t_6 = t_9.view(size=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/aten::view19203
        t_6 = self.l_15(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_16(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/aten::add19047
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_5, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/aten::add19207
        t_5 = self.l_17(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2]
        t_5 = self.l_18(t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/prim::Constant19213
        t_9 = torch.mul(input=t_5, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/prim::Constant19215
        t_1 = t_5.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::pow19216
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/prim::Constant19217
        t_1 = torch.mul(input=t_1, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::mul19218
        t_1 = torch.add(input=t_5, other=t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::add19220
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/prim::Constant19221
        t_1 = torch.mul(input=t_1, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::mul19222
        t_1 = t_1.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::tanh19223
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/prim::Constant19224
        t_1 = torch.add(input=t_1, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::mul19214
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::add19226
        t_1 = torch.mul(input=t_9, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/aten::mul19227
        t_1 = self.l_19(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj]
        t_1 = self.l_20(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/aten::add19207
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout]
        t_1 = torch.add(input=t_6, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/aten::add19231
        t_6 = self.l_21(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1]
        t_6 = self.l_22(t_6)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19243
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19244
        t_6 = t_6.split(split_size=1600, dim=2)
        t_5 = t_6[0]
        t_7 = t_6[1]
        t_6 = t_6[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192460
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19249
        t_2 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192460
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19253
        t_4 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192460
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19257
        t_10 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19258
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19260
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_2, t_4, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192460
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19264
        t_10 = t_5.view(size=t_10)
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::view19265
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19270
        t_5 = t_10.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192461
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19272
        t_10 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192461
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19276
        t_4 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192461
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19280
        t_2 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19283
        t_2 = torch.div(input=t_2, other=25)
        t_2 = [t_10, t_4, 25, t_2]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192461
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19287
        t_2 = t_7.view(size=t_2)
        t_7 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::view19288
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19293
        t_7 = t_2.permute(dims=t_7)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192462
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19295
        t_2 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192462
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19299
        t_4 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192462
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19303
        t_10 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19304
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19306
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_2, t_4, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListUnpack192462
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19310
        t_10 = t_6.view(size=t_10)
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::view19311
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19316
        t_6 = t_10.permute(dims=t_6)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::permute19271
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::permute19294
        t_7 = t_5.matmul(other=t_7)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::matmul19318
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19319
        t_7 = torch.div(input=t_7, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::div19320
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19321
        t_5 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::div19320
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19324
        t_10 = t_7.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19325
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19322
        t_5 = torch.sub(input=t_10, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19332
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19333
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19334
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19335
        t_4 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::slice19336
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19337
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19338
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19339
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19340
        t_4 = t_4[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::slice19341
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19342
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::sub19330
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19325
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19343
        t_5 = t_4[:, :, t_5:t_10:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::slice19344
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19345
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19346
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19325
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19347
        t_10 = t_5[:, :, :, 0:t_10:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::div19320
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::slice19348
        t_7 = torch.mul(input=t_7, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::slice19348
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19350
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19351
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::rsub19352
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19353
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::mul19349
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::mul19354
        t_10 = torch.sub(input=t_7, other=t_10)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::sub19356
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19357
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19358
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::softmax19359
        t_10 = self.l_23(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::permute19317
        t_6 = t_10.matmul(other=t_6)
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::matmul19361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19366
        t_10 = t_6.permute(dims=t_10)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::permute19367
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::contiguous19369
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19370
        t_6 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::contiguous19369
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19374
        t_7 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::contiguous19369
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19378
        t_5 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::contiguous19369
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::Constant19381
        t_4 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19379
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::size19382
        t_4 = torch.mul(input=t_5, other=t_4)
        t_4 = [t_6, t_7, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::contiguous19369
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/prim::ListConstruct19386
        t_4 = t_10.view(size=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/aten::view19387
        t_4 = self.l_24(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_25(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/aten::add19231
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=t_1, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/aten::add19391
        t_1 = self.l_26(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2]
        t_1 = self.l_27(t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/prim::Constant19397
        t_10 = torch.mul(input=t_1, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/prim::Constant19399
        t_7 = t_1.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::pow19400
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/prim::Constant19401
        t_7 = torch.mul(input=t_7, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::mul19402
        t_7 = torch.add(input=t_1, other=t_7)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::add19404
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/prim::Constant19405
        t_7 = torch.mul(input=t_7, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::mul19406
        t_7 = t_7.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::tanh19407
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/prim::Constant19408
        t_7 = torch.add(input=t_7, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::mul19398
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::add19410
        t_7 = torch.mul(input=t_10, other=t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/aten::mul19411
        t_7 = self.l_28(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj]
        t_7 = self.l_29(t_7)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/aten::add19391
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout]
        t_7 = torch.add(input=t_4, other=t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/aten::add19415
        t_4 = self.l_30(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1]
        t_4 = self.l_31(t_4)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19427
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19428
        t_4 = t_4.split(split_size=1600, dim=2)
        t_1 = t_4[0]
        t_6 = t_4[1]
        t_4 = t_4[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194300
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19433
        t_5 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194300
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19437
        t_2 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194300
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19441
        t_11 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19444
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_5, t_2, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194300
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19448
        t_11 = t_1.view(size=t_11)
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::view19449
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19454
        t_1 = t_11.permute(dims=t_1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194301
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19456
        t_11 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194301
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19460
        t_2 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194301
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19464
        t_5 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19465
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19467
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_11, t_2, 25, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194301
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19471
        t_5 = t_6.view(size=t_5)
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::view19472
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19477
        t_6 = t_5.permute(dims=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194302
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19479
        t_5 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194302
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19483
        t_2 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194302
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19487
        t_11 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19488
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19490
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_5, t_2, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListUnpack194302
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19494
        t_11 = t_4.view(size=t_11)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::view19495
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19500
        t_4 = t_11.permute(dims=t_4)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::permute19455
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::permute19478
        t_6 = t_1.matmul(other=t_6)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::matmul19502
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19503
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::div19504
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19505
        t_1 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::div19504
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19508
        t_11 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19509
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19506
        t_1 = torch.sub(input=t_11, other=t_1)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19516
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19517
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19518
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19519
        t_2 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::slice19520
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19523
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19524
        t_2 = t_2[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::slice19525
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19526
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::sub19514
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19509
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19527
        t_1 = t_2[:, :, t_1:t_11:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::slice19528
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19529
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19530
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19509
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19531
        t_11 = t_1[:, :, :, 0:t_11:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::div19504
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::slice19532
        t_6 = torch.mul(input=t_6, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::slice19532
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19534
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19535
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::rsub19536
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19537
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::mul19533
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::mul19538
        t_11 = torch.sub(input=t_6, other=t_11)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::sub19540
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19541
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19542
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::softmax19543
        t_11 = self.l_32(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::permute19501
        t_4 = t_11.matmul(other=t_4)
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::matmul19545
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19550
        t_11 = t_4.permute(dims=t_11)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::permute19551
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::contiguous19553
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19554
        t_4 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::contiguous19553
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19558
        t_6 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::contiguous19553
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19562
        t_1 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::contiguous19553
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::Constant19565
        t_2 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19563
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::size19566
        t_2 = torch.mul(input=t_1, other=t_2)
        t_2 = [t_4, t_6, t_2]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::contiguous19553
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/prim::ListConstruct19570
        t_2 = t_11.view(size=t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/aten::view19571
        t_2 = self.l_33(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Conv1D[c_proj]
        t_2 = self.l_34(t_2)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/aten::add19415
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]/Dropout[resid_dropout]
        t_2 = torch.add(input=t_7, other=t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/aten::add19575
        t_7 = self.l_35(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2]
        t_7 = self.l_36(t_7)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/prim::Constant19581
        t_11 = torch.mul(input=t_7, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/prim::Constant19583
        t_6 = t_7.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::pow19584
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/prim::Constant19585
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::mul19586
        t_6 = torch.add(input=t_7, other=t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::add19588
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/prim::Constant19589
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::mul19590
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::tanh19591
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/prim::Constant19592
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::mul19582
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::add19594
        t_6 = torch.mul(input=t_11, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/aten::mul19595
        t_6 = self.l_37(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj]
        t_6 = self.l_38(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/aten::add19575
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout]
        t_6 = torch.add(input=t_2, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/aten::add19599
        t_2 = self.l_39(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1]
        t_2 = self.l_40(t_2)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19611
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19612
        t_2 = t_2.split(split_size=1600, dim=2)
        t_7 = t_2[0]
        t_4 = t_2[1]
        t_2 = t_2[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196140
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19617
        t_1 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196140
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19621
        t_5 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196140
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19625
        t_12 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19626
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19628
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_1, t_5, 25, t_12]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196140
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19632
        t_12 = t_7.view(size=t_12)
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::view19633
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19638
        t_7 = t_12.permute(dims=t_7)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196141
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19640
        t_12 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196141
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19644
        t_5 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196141
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19648
        t_1 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19649
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19651
        t_1 = torch.div(input=t_1, other=25)
        t_1 = [t_12, t_5, 25, t_1]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196141
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19655
        t_1 = t_4.view(size=t_1)
        t_4 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::view19656
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19661
        t_4 = t_1.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196142
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19663
        t_1 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196142
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19667
        t_5 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196142
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19671
        t_12 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19672
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19674
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_1, t_5, 25, t_12]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListUnpack196142
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19678
        t_12 = t_2.view(size=t_12)
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::view19679
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19684
        t_2 = t_12.permute(dims=t_2)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::permute19639
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::permute19662
        t_4 = t_7.matmul(other=t_4)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::matmul19686
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19687
        t_4 = torch.div(input=t_4, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::div19688
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19689
        t_7 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::div19688
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19692
        t_12 = t_4.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19693
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19690
        t_7 = torch.sub(input=t_12, other=t_7)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19700
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19701
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19702
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19703
        t_5 = self.b_4[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::slice19704
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19705
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19706
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19707
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19708
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::slice19709
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19710
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::sub19698
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19693
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19711
        t_7 = t_5[:, :, t_7:t_12:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::slice19712
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19713
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19714
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19693
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19715
        t_12 = t_7[:, :, :, 0:t_12:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::div19688
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::slice19716
        t_4 = torch.mul(input=t_4, other=t_12)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::slice19716
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19718
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19719
        t_12 = torch.rsub(t_12, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::rsub19720
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19721
        t_12 = torch.mul(input=t_12, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::mul19717
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::mul19722
        t_12 = torch.sub(input=t_4, other=t_12)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::sub19724
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19725
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19726
        t_12 = t_12.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::softmax19727
        t_12 = self.l_41(t_12)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::permute19685
        t_2 = t_12.matmul(other=t_2)
        t_12 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::matmul19729
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19734
        t_12 = t_2.permute(dims=t_12)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::permute19735
        t_12 = t_12.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::contiguous19737
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19738
        t_2 = t_12.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::contiguous19737
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19742
        t_4 = t_12.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::contiguous19737
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19746
        t_7 = t_12.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::contiguous19737
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::Constant19749
        t_5 = t_12.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19747
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::size19750
        t_5 = torch.mul(input=t_7, other=t_5)
        t_5 = [t_2, t_4, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::contiguous19737
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/prim::ListConstruct19754
        t_5 = t_12.view(size=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/aten::view19755
        t_5 = self.l_42(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_43(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/aten::add19599
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=t_6, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/aten::add19759
        t_6 = self.l_44(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2]
        t_6 = self.l_45(t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/prim::Constant19765
        t_12 = torch.mul(input=t_6, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/prim::Constant19767
        t_4 = t_6.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::pow19768
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/prim::Constant19769
        t_4 = torch.mul(input=t_4, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::mul19770
        t_4 = torch.add(input=t_6, other=t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::add19772
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/prim::Constant19773
        t_4 = torch.mul(input=t_4, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::mul19774
        t_4 = t_4.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::tanh19775
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/prim::Constant19776
        t_4 = torch.add(input=t_4, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::mul19766
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::add19778
        t_4 = torch.mul(input=t_12, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/aten::mul19779
        t_4 = self.l_46(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj]
        t_4 = self.l_47(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/aten::add19759
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout]
        t_4 = torch.add(input=t_5, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/aten::add19783
        t_5 = self.l_48(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1]
        t_5 = self.l_49(t_5)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19795
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19796
        t_5 = t_5.split(split_size=1600, dim=2)
        t_6 = t_5[0]
        t_2 = t_5[1]
        t_5 = t_5[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197980
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19801
        t_7 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197980
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19805
        t_1 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197980
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19809
        t_13 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19810
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19812
        t_13 = torch.div(input=t_13, other=25)
        t_13 = [t_7, t_1, 25, t_13]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197980
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19816
        t_13 = t_6.view(size=t_13)
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::view19817
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19822
        t_6 = t_13.permute(dims=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197981
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19824
        t_13 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197981
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19828
        t_1 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197981
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19832
        t_7 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19833
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19835
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_13, t_1, 25, t_7]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197981
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19839
        t_7 = t_2.view(size=t_7)
        t_2 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::view19840
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19845
        t_2 = t_7.permute(dims=t_2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197982
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19847
        t_7 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197982
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19851
        t_1 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197982
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19855
        t_13 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19856
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19858
        t_13 = torch.div(input=t_13, other=25)
        t_13 = [t_7, t_1, 25, t_13]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListUnpack197982
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19862
        t_13 = t_5.view(size=t_13)
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::view19863
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19868
        t_5 = t_13.permute(dims=t_5)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::permute19823
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::permute19846
        t_2 = t_6.matmul(other=t_2)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::matmul19870
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19871
        t_2 = torch.div(input=t_2, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::div19872
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19873
        t_6 = t_2.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::div19872
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19876
        t_13 = t_2.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19877
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19874
        t_6 = torch.sub(input=t_13, other=t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19884
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19885
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19886
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19887
        t_1 = self.b_5[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::slice19888
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19889
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19890
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19891
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19892
        t_1 = t_1[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::slice19893
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19894
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::sub19882
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19877
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19895
        t_6 = t_1[:, :, t_6:t_13:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::slice19896
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19897
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19898
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19877
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19899
        t_13 = t_6[:, :, :, 0:t_13:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::div19872
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::slice19900
        t_2 = torch.mul(input=t_2, other=t_13)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::slice19900
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19902
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19903
        t_13 = torch.rsub(t_13, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::rsub19904
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19905
        t_13 = torch.mul(input=t_13, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::mul19901
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::mul19906
        t_13 = torch.sub(input=t_2, other=t_13)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::sub19908
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19909
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19910
        t_13 = t_13.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::softmax19911
        t_13 = self.l_50(t_13)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::permute19869
        t_5 = t_13.matmul(other=t_5)
        t_13 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::matmul19913
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19918
        t_13 = t_5.permute(dims=t_13)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::permute19919
        t_13 = t_13.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::contiguous19921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19922
        t_5 = t_13.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::contiguous19921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19926
        t_2 = t_13.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::contiguous19921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19930
        t_6 = t_13.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::contiguous19921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::Constant19933
        t_1 = t_13.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19931
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::size19934
        t_1 = torch.mul(input=t_6, other=t_1)
        t_1 = [t_5, t_2, t_1]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::contiguous19921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/prim::ListConstruct19938
        t_1 = t_13.view(size=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/aten::view19939
        t_1 = self.l_51(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Conv1D[c_proj]
        t_1 = self.l_52(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/aten::add19783
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]/Dropout[resid_dropout]
        t_1 = torch.add(input=t_4, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/aten::add19943
        t_4 = self.l_53(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2]
        t_4 = self.l_54(t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/prim::Constant19949
        t_13 = torch.mul(input=t_4, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/prim::Constant19951
        t_2 = t_4.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::pow19952
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/prim::Constant19953
        t_2 = torch.mul(input=t_2, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::mul19954
        t_2 = torch.add(input=t_4, other=t_2)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::add19956
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/prim::Constant19957
        t_2 = torch.mul(input=t_2, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::mul19958
        t_2 = t_2.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::tanh19959
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/prim::Constant19960
        t_2 = torch.add(input=t_2, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::mul19950
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::add19962
        t_2 = torch.mul(input=t_13, other=t_2)
        # returing:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::mul19963
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/aten::add19943
        return (t_2, t_1)

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition1(nn.Module):
    def __init__(self, layers, tensors):
        super(Partition1, self).__init__()
        # initializing partition layers
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_0, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_0)}'
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_1, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_1)}'
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_2, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_2)}'
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_3, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_3)}'
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_4, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_4)}'
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_5, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_5)}'
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_6, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_6)}'
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_7, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_7)}'
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_8, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_8)}'
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_9, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_9)}'
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_10, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_10)}'
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_11, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_11)}'
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_12, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_12)}'
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_13, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_13)}'
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_14, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_14)}'
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_15, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_15)}'
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_16, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_16)}'
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_17, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_17)}'
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_18, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_18)}'
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_19, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_19)}'
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_20, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_20)}'
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_21, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_21)}'
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_22, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_22)}'
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_23, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_23)}'
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_24, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_24)}'
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_25, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_25)}'
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_26, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_26)}'
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_27, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_27)}'
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_28, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_28)}'
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_29, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_29)}'
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_30, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_30)}'
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_31, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_31)}'
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_32, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_32)}'
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_33, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_33)}'
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_34, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_34)}'
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_35, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_35)}'
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_36, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_36)}'
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_37, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_37)}'
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_38, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_38)}'
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_39, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_39)}'
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_40, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_40)}'
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_41, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_41)}'
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_42, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_42)}'
        self.l_43 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_43, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_43)}'
        self.l_44 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_44, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_44)}'
        self.l_45 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_45, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_45)}'
        self.l_46 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_46, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_46)}'
        self.l_47 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_47, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_47)}'
        self.l_48 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_48, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_48)}'
        self.l_49 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_49, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_49)}'
        self.l_50 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_50, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_50)}'
        self.l_51 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_51, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_51)}'
        self.l_52 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_52, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_52)}'
        self.l_53 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_53, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_53)}'
        self.l_54 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_54, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_54)}'

        # initializing partition buffers
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_4', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_5', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Tensor[bias]']
        )

        # initializing partition parameters

        self.device = torch.device('cuda:1')
        self.lookup = {
            'l_0': 'transformer.blocks.5.mlp.c_proj',
            'l_1': 'transformer.blocks.5.mlp.dropout',
            'l_2': 'transformer.blocks.6.ln_1',
            'l_3': 'transformer.blocks.6.attn.c_attn',
            'l_4': 'transformer.blocks.6.attn.attn_dropout',
            'l_5': 'transformer.blocks.6.attn.c_proj',
            'l_6': 'transformer.blocks.6.attn.resid_dropout',
            'l_7': 'transformer.blocks.6.ln_2',
            'l_8': 'transformer.blocks.6.mlp.c_fc',
            'l_9': 'transformer.blocks.6.mlp.c_proj',
            'l_10': 'transformer.blocks.6.mlp.dropout',
            'l_11': 'transformer.blocks.7.ln_1',
            'l_12': 'transformer.blocks.7.attn.c_attn',
            'l_13': 'transformer.blocks.7.attn.attn_dropout',
            'l_14': 'transformer.blocks.7.attn.c_proj',
            'l_15': 'transformer.blocks.7.attn.resid_dropout',
            'l_16': 'transformer.blocks.7.ln_2',
            'l_17': 'transformer.blocks.7.mlp.c_fc',
            'l_18': 'transformer.blocks.7.mlp.c_proj',
            'l_19': 'transformer.blocks.7.mlp.dropout',
            'l_20': 'transformer.blocks.8.ln_1',
            'l_21': 'transformer.blocks.8.attn.c_attn',
            'l_22': 'transformer.blocks.8.attn.attn_dropout',
            'l_23': 'transformer.blocks.8.attn.c_proj',
            'l_24': 'transformer.blocks.8.attn.resid_dropout',
            'l_25': 'transformer.blocks.8.ln_2',
            'l_26': 'transformer.blocks.8.mlp.c_fc',
            'l_27': 'transformer.blocks.8.mlp.c_proj',
            'l_28': 'transformer.blocks.8.mlp.dropout',
            'l_29': 'transformer.blocks.9.ln_1',
            'l_30': 'transformer.blocks.9.attn.c_attn',
            'l_31': 'transformer.blocks.9.attn.attn_dropout',
            'l_32': 'transformer.blocks.9.attn.c_proj',
            'l_33': 'transformer.blocks.9.attn.resid_dropout',
            'l_34': 'transformer.blocks.9.ln_2',
            'l_35': 'transformer.blocks.9.mlp.c_fc',
            'l_36': 'transformer.blocks.9.mlp.c_proj',
            'l_37': 'transformer.blocks.9.mlp.dropout',
            'l_38': 'transformer.blocks.10.ln_1',
            'l_39': 'transformer.blocks.10.attn.c_attn',
            'l_40': 'transformer.blocks.10.attn.attn_dropout',
            'l_41': 'transformer.blocks.10.attn.c_proj',
            'l_42': 'transformer.blocks.10.attn.resid_dropout',
            'l_43': 'transformer.blocks.10.ln_2',
            'l_44': 'transformer.blocks.10.mlp.c_fc',
            'l_45': 'transformer.blocks.10.mlp.c_proj',
            'l_46': 'transformer.blocks.10.mlp.dropout',
            'l_47': 'transformer.blocks.11.ln_1',
            'l_48': 'transformer.blocks.11.attn.c_attn',
            'l_49': 'transformer.blocks.11.attn.attn_dropout',
            'l_50': 'transformer.blocks.11.attn.c_proj',
            'l_51': 'transformer.blocks.11.attn.resid_dropout',
            'l_52': 'transformer.blocks.11.ln_2',
            'l_53': 'transformer.blocks.11.mlp.c_fc',
            'l_54': 'transformer.blocks.11.mlp.c_proj',
            'b_0': 'transformer.blocks.6.attn.bias',
            'b_1': 'transformer.blocks.7.attn.bias',
            'b_2': 'transformer.blocks.8.attn.bias',
            'b_3': 'transformer.blocks.9.attn.bias',
            'b_4': 'transformer.blocks.10.attn.bias',
            'b_5': 'transformer.blocks.11.attn.bias'
        }

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_attn] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[attn_dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_proj] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[resid_dropout] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_attn] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[attn_dropout] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_proj] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[resid_dropout] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_attn] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[attn_dropout] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_proj] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[resid_dropout] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_attn] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[attn_dropout] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_proj] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[resid_dropout] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_attn] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[attn_dropout] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_proj] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[resid_dropout] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_attn] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[attn_dropout] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_proj] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[resid_dropout] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::mul19963 <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/aten::add19943 <=> x1

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)

        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/aten::mul19963
        t_0 = self.l_0(x0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj]
        t_0 = self.l_1(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/aten::add19943
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout]
        t_0 = torch.add(input=x1, other=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/aten::add19967
        t_1 = self.l_2(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1]
        t_1 = self.l_3(t_1)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant19979
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant19980
        t_1 = t_1.split(split_size=1600, dim=2)
        t_3 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199820
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant19985
        t_5 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199820
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant19989
        t_6 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199820
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant19993
        t_7 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size19994
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant19996
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199820
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct20000
        t_7 = t_3.view(size=t_7)
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::view20001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct20006
        t_3 = t_7.permute(dims=t_3)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199821
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20008
        t_7 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199821
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20012
        t_6 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199821
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20016
        t_5 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20017
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20019
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_7, t_6, 25, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199821
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct20023
        t_5 = t_4.view(size=t_5)
        t_4 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::view20024
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct20029
        t_4 = t_5.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199822
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20031
        t_5 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199822
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20035
        t_6 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199822
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20039
        t_7 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20040
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20042
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListUnpack199822
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct20046
        t_7 = t_1.view(size=t_7)
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::view20047
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct20052
        t_1 = t_7.permute(dims=t_1)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::permute20007
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::permute20030
        t_4 = t_3.matmul(other=t_4)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::matmul20054
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20055
        t_4 = torch.div(input=t_4, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::div20056
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20057
        t_3 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::div20056
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20060
        t_7 = t_4.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20061
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20058
        t_3 = torch.sub(input=t_7, other=t_3)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20068
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20069
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20070
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20071
        t_6 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::slice20072
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20073
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20074
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20075
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20076
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::slice20077
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20078
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::sub20066
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20061
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20079
        t_3 = t_6[:, :, t_3:t_7:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::slice20080
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20081
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20082
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20061
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20083
        t_7 = t_3[:, :, :, 0:t_7:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::div20056
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::slice20084
        t_4 = torch.mul(input=t_4, other=t_7)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::slice20084
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20086
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20087
        t_7 = torch.rsub(t_7, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::rsub20088
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20089
        t_7 = torch.mul(input=t_7, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::mul20085
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::mul20090
        t_7 = torch.sub(input=t_4, other=t_7)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::sub20092
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20093
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20094
        t_7 = t_7.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::softmax20095
        t_7 = self.l_4(t_7)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::permute20053
        t_1 = t_7.matmul(other=t_1)
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::matmul20097
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct20102
        t_7 = t_1.permute(dims=t_7)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::permute20103
        t_7 = t_7.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::contiguous20105
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20106
        t_1 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::contiguous20105
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20110
        t_4 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::contiguous20105
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20114
        t_3 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::contiguous20105
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::Constant20117
        t_6 = t_7.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20115
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::size20118
        t_6 = torch.mul(input=t_3, other=t_6)
        t_6 = [t_1, t_4, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::contiguous20105
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/prim::ListConstruct20122
        t_6 = t_7.view(size=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/aten::view20123
        t_6 = self.l_5(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_6(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/aten::add19967
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_0, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/aten::add20127
        t_0 = self.l_7(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2]
        t_0 = self.l_8(t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/prim::Constant20133
        t_7 = torch.mul(input=t_0, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/prim::Constant20135
        t_4 = t_0.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::pow20136
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/prim::Constant20137
        t_4 = torch.mul(input=t_4, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::mul20138
        t_4 = torch.add(input=t_0, other=t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::add20140
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/prim::Constant20141
        t_4 = torch.mul(input=t_4, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::mul20142
        t_4 = t_4.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::tanh20143
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/prim::Constant20144
        t_4 = torch.add(input=t_4, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::mul20134
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::add20146
        t_4 = torch.mul(input=t_7, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/aten::mul20147
        t_4 = self.l_9(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj]
        t_4 = self.l_10(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/aten::add20127
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout]
        t_4 = torch.add(input=t_6, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/aten::add20151
        t_6 = self.l_11(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1]
        t_6 = self.l_12(t_6)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20163
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20164
        t_6 = t_6.split(split_size=1600, dim=2)
        t_0 = t_6[0]
        t_1 = t_6[1]
        t_6 = t_6[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201660
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20169
        t_3 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201660
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20173
        t_5 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201660
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20177
        t_8 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20178
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20180
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201660
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20184
        t_8 = t_0.view(size=t_8)
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::view20185
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20190
        t_0 = t_8.permute(dims=t_0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201661
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20192
        t_8 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201661
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20196
        t_5 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201661
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20200
        t_3 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20201
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20203
        t_3 = torch.div(input=t_3, other=25)
        t_3 = [t_8, t_5, 25, t_3]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201661
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20207
        t_3 = t_1.view(size=t_3)
        t_1 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::view20208
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20213
        t_1 = t_3.permute(dims=t_1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201662
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20215
        t_3 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201662
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20219
        t_5 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201662
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20223
        t_8 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20224
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20226
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListUnpack201662
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20230
        t_8 = t_6.view(size=t_8)
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::view20231
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20236
        t_6 = t_8.permute(dims=t_6)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::permute20191
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::permute20214
        t_1 = t_0.matmul(other=t_1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::matmul20238
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20239
        t_1 = torch.div(input=t_1, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::div20240
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20241
        t_0 = t_1.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::div20240
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20244
        t_8 = t_1.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20245
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20242
        t_0 = torch.sub(input=t_8, other=t_0)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20252
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20253
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20254
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20255
        t_5 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::slice20256
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20257
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20258
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20259
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20260
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::slice20261
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20262
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::sub20250
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20245
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20263
        t_0 = t_5[:, :, t_0:t_8:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::slice20264
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20265
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20266
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20245
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20267
        t_8 = t_0[:, :, :, 0:t_8:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::div20240
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::slice20268
        t_1 = torch.mul(input=t_1, other=t_8)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::slice20268
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20270
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20271
        t_8 = torch.rsub(t_8, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::rsub20272
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20273
        t_8 = torch.mul(input=t_8, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::mul20269
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::mul20274
        t_8 = torch.sub(input=t_1, other=t_8)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::sub20276
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20277
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20278
        t_8 = t_8.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::softmax20279
        t_8 = self.l_13(t_8)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::permute20237
        t_6 = t_8.matmul(other=t_6)
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::matmul20281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20286
        t_8 = t_6.permute(dims=t_8)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::permute20287
        t_8 = t_8.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::contiguous20289
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20290
        t_6 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::contiguous20289
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20294
        t_1 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::contiguous20289
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20298
        t_0 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::contiguous20289
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::Constant20301
        t_5 = t_8.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20299
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::size20302
        t_5 = torch.mul(input=t_0, other=t_5)
        t_5 = [t_6, t_1, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::contiguous20289
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/prim::ListConstruct20306
        t_5 = t_8.view(size=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/aten::view20307
        t_5 = self.l_14(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_15(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/aten::add20151
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=t_4, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/aten::add20311
        t_4 = self.l_16(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2]
        t_4 = self.l_17(t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/prim::Constant20317
        t_8 = torch.mul(input=t_4, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/prim::Constant20319
        t_1 = t_4.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::pow20320
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/prim::Constant20321
        t_1 = torch.mul(input=t_1, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::mul20322
        t_1 = torch.add(input=t_4, other=t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::add20324
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/prim::Constant20325
        t_1 = torch.mul(input=t_1, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::mul20326
        t_1 = t_1.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::tanh20327
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/prim::Constant20328
        t_1 = torch.add(input=t_1, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::mul20318
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::add20330
        t_1 = torch.mul(input=t_8, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/aten::mul20331
        t_1 = self.l_18(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj]
        t_1 = self.l_19(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/aten::add20311
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout]
        t_1 = torch.add(input=t_5, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/aten::add20335
        t_5 = self.l_20(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1]
        t_5 = self.l_21(t_5)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20347
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20348
        t_5 = t_5.split(split_size=1600, dim=2)
        t_4 = t_5[0]
        t_6 = t_5[1]
        t_5 = t_5[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203500
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20353
        t_0 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203500
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20357
        t_3 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203500
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20361
        t_9 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20362
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20364
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_0, t_3, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203500
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20368
        t_9 = t_4.view(size=t_9)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::view20369
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20374
        t_4 = t_9.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203501
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20376
        t_9 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203501
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20380
        t_3 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203501
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20384
        t_0 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20385
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20387
        t_0 = torch.div(input=t_0, other=25)
        t_0 = [t_9, t_3, 25, t_0]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203501
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20391
        t_0 = t_6.view(size=t_0)
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::view20392
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20397
        t_6 = t_0.permute(dims=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203502
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20399
        t_0 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203502
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20403
        t_3 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203502
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20407
        t_9 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20408
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20410
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_0, t_3, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListUnpack203502
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20414
        t_9 = t_5.view(size=t_9)
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::view20415
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20420
        t_5 = t_9.permute(dims=t_5)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::permute20375
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::permute20398
        t_6 = t_4.matmul(other=t_6)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::matmul20422
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20423
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::div20424
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20425
        t_4 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::div20424
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20428
        t_9 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20429
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20426
        t_4 = torch.sub(input=t_9, other=t_4)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20436
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20437
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20438
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20439
        t_3 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::slice20440
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20443
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20444
        t_3 = t_3[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::slice20445
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20446
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::sub20434
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20429
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20447
        t_4 = t_3[:, :, t_4:t_9:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::slice20448
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20449
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20450
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20429
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20451
        t_9 = t_4[:, :, :, 0:t_9:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::div20424
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::slice20452
        t_6 = torch.mul(input=t_6, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::slice20452
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20454
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20455
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::rsub20456
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20457
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::mul20453
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::mul20458
        t_9 = torch.sub(input=t_6, other=t_9)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::sub20460
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20461
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20462
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::softmax20463
        t_9 = self.l_22(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::permute20421
        t_5 = t_9.matmul(other=t_5)
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::matmul20465
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20470
        t_9 = t_5.permute(dims=t_9)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::permute20471
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::contiguous20473
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20474
        t_5 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::contiguous20473
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20478
        t_6 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::contiguous20473
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20482
        t_4 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::contiguous20473
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::Constant20485
        t_3 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20483
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::size20486
        t_3 = torch.mul(input=t_4, other=t_3)
        t_3 = [t_5, t_6, t_3]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::contiguous20473
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/prim::ListConstruct20490
        t_3 = t_9.view(size=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/aten::view20491
        t_3 = self.l_23(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Conv1D[c_proj]
        t_3 = self.l_24(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/aten::add20335
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]/Dropout[resid_dropout]
        t_3 = torch.add(input=t_1, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/aten::add20495
        t_1 = self.l_25(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2]
        t_1 = self.l_26(t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/prim::Constant20501
        t_9 = torch.mul(input=t_1, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/prim::Constant20503
        t_6 = t_1.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::pow20504
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/prim::Constant20505
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::mul20506
        t_6 = torch.add(input=t_1, other=t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::add20508
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/prim::Constant20509
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::mul20510
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::tanh20511
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/prim::Constant20512
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::mul20502
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::add20514
        t_6 = torch.mul(input=t_9, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/aten::mul20515
        t_6 = self.l_27(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj]
        t_6 = self.l_28(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/aten::add20495
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout]
        t_6 = torch.add(input=t_3, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/aten::add20519
        t_3 = self.l_29(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1]
        t_3 = self.l_30(t_3)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20531
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20532
        t_3 = t_3.split(split_size=1600, dim=2)
        t_1 = t_3[0]
        t_5 = t_3[1]
        t_3 = t_3[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205340
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20537
        t_4 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205340
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20541
        t_0 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205340
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20545
        t_10 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20546
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20548
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_0, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205340
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20552
        t_10 = t_1.view(size=t_10)
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::view20553
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20558
        t_1 = t_10.permute(dims=t_1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205341
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20560
        t_10 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205341
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20564
        t_0 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205341
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20568
        t_4 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20569
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20571
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_10, t_0, 25, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205341
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20575
        t_4 = t_5.view(size=t_4)
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::view20576
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20581
        t_5 = t_4.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205342
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20583
        t_4 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205342
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20587
        t_0 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205342
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20591
        t_10 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20592
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20594
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_0, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListUnpack205342
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20598
        t_10 = t_3.view(size=t_10)
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::view20599
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20604
        t_3 = t_10.permute(dims=t_3)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::permute20559
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::permute20582
        t_5 = t_1.matmul(other=t_5)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::matmul20606
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20607
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::div20608
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20609
        t_1 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::div20608
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20612
        t_10 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20613
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20610
        t_1 = torch.sub(input=t_10, other=t_1)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20620
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20621
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20622
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20623
        t_0 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::slice20624
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20625
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20626
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20627
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20628
        t_0 = t_0[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::slice20629
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20630
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::sub20618
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20613
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20631
        t_1 = t_0[:, :, t_1:t_10:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::slice20632
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20633
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20634
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20613
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20635
        t_10 = t_1[:, :, :, 0:t_10:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::div20608
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::slice20636
        t_5 = torch.mul(input=t_5, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::slice20636
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20638
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20639
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::rsub20640
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20641
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::mul20637
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::mul20642
        t_10 = torch.sub(input=t_5, other=t_10)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::sub20644
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20645
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20646
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::softmax20647
        t_10 = self.l_31(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::permute20605
        t_3 = t_10.matmul(other=t_3)
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::matmul20649
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20654
        t_10 = t_3.permute(dims=t_10)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::permute20655
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::contiguous20657
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20658
        t_3 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::contiguous20657
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20662
        t_5 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::contiguous20657
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20666
        t_1 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::contiguous20657
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::Constant20669
        t_0 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20667
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::size20670
        t_0 = torch.mul(input=t_1, other=t_0)
        t_0 = [t_3, t_5, t_0]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::contiguous20657
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/prim::ListConstruct20674
        t_0 = t_10.view(size=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/aten::view20675
        t_0 = self.l_32(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Conv1D[c_proj]
        t_0 = self.l_33(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/aten::add20519
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]/Dropout[resid_dropout]
        t_0 = torch.add(input=t_6, other=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/aten::add20679
        t_6 = self.l_34(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2]
        t_6 = self.l_35(t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/prim::Constant20685
        t_10 = torch.mul(input=t_6, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/prim::Constant20687
        t_5 = t_6.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::pow20688
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/prim::Constant20689
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::mul20690
        t_5 = torch.add(input=t_6, other=t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::add20692
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/prim::Constant20693
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::mul20694
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::tanh20695
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/prim::Constant20696
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::mul20686
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::add20698
        t_5 = torch.mul(input=t_10, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/aten::mul20699
        t_5 = self.l_36(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj]
        t_5 = self.l_37(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/aten::add20679
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout]
        t_5 = torch.add(input=t_0, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/aten::add20703
        t_0 = self.l_38(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1]
        t_0 = self.l_39(t_0)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20715
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20716
        t_0 = t_0.split(split_size=1600, dim=2)
        t_6 = t_0[0]
        t_3 = t_0[1]
        t_0 = t_0[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207180
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20721
        t_1 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207180
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20725
        t_4 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207180
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20729
        t_11 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20730
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20732
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_1, t_4, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207180
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20736
        t_11 = t_6.view(size=t_11)
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::view20737
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20742
        t_6 = t_11.permute(dims=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207181
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20744
        t_11 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207181
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20748
        t_4 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207181
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20752
        t_1 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20753
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20755
        t_1 = torch.div(input=t_1, other=25)
        t_1 = [t_11, t_4, 25, t_1]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207181
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20759
        t_1 = t_3.view(size=t_1)
        t_3 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::view20760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20765
        t_3 = t_1.permute(dims=t_3)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207182
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20767
        t_1 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207182
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20771
        t_4 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207182
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20775
        t_11 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20776
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20778
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_1, t_4, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListUnpack207182
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20782
        t_11 = t_0.view(size=t_11)
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::view20783
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20788
        t_0 = t_11.permute(dims=t_0)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::permute20743
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::permute20766
        t_3 = t_6.matmul(other=t_3)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::matmul20790
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20791
        t_3 = torch.div(input=t_3, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::div20792
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20793
        t_6 = t_3.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::div20792
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20796
        t_11 = t_3.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20797
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20794
        t_6 = torch.sub(input=t_11, other=t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20804
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20805
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20806
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20807
        t_4 = self.b_4[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::slice20808
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20809
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20810
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20811
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20812
        t_4 = t_4[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::slice20813
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20814
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::sub20802
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20797
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20815
        t_6 = t_4[:, :, t_6:t_11:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::slice20816
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20817
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20818
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20797
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20819
        t_11 = t_6[:, :, :, 0:t_11:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::div20792
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::slice20820
        t_3 = torch.mul(input=t_3, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::slice20820
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20822
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20823
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::rsub20824
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20825
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::mul20821
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::mul20826
        t_11 = torch.sub(input=t_3, other=t_11)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::sub20828
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20829
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20830
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::softmax20831
        t_11 = self.l_40(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::permute20789
        t_0 = t_11.matmul(other=t_0)
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::matmul20833
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20838
        t_11 = t_0.permute(dims=t_11)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::permute20839
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::contiguous20841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20842
        t_0 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::contiguous20841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20846
        t_3 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::contiguous20841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20850
        t_6 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::contiguous20841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::Constant20853
        t_4 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20851
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::size20854
        t_4 = torch.mul(input=t_6, other=t_4)
        t_4 = [t_0, t_3, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::contiguous20841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/prim::ListConstruct20858
        t_4 = t_11.view(size=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/aten::view20859
        t_4 = self.l_41(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_42(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/aten::add20703
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=t_5, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/aten::add20863
        t_5 = self.l_43(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2]
        t_5 = self.l_44(t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/prim::Constant20869
        t_11 = torch.mul(input=t_5, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/prim::Constant20871
        t_3 = t_5.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::pow20872
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/prim::Constant20873
        t_3 = torch.mul(input=t_3, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::mul20874
        t_3 = torch.add(input=t_5, other=t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::add20876
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/prim::Constant20877
        t_3 = torch.mul(input=t_3, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::mul20878
        t_3 = t_3.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::tanh20879
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/prim::Constant20880
        t_3 = torch.add(input=t_3, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::mul20870
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::add20882
        t_3 = torch.mul(input=t_11, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/aten::mul20883
        t_3 = self.l_45(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj]
        t_3 = self.l_46(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/aten::add20863
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout]
        t_3 = torch.add(input=t_4, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/aten::add20887
        t_4 = self.l_47(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1]
        t_4 = self.l_48(t_4)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20899
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20900
        t_4 = t_4.split(split_size=1600, dim=2)
        t_5 = t_4[0]
        t_0 = t_4[1]
        t_4 = t_4[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack209020
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20905
        t_6 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack209020
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20909
        t_1 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack209020
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20913
        t_12 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size20914
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20916
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_1, 25, t_12]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack209020
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct20920
        t_12 = t_5.view(size=t_12)
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::view20921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct20926
        t_5 = t_12.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack209021
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20928
        t_12 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack209021
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20932
        t_1 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack209021
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20936
        t_6 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size20937
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20939
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_12, t_1, 25, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack209021
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct20943
        t_6 = t_0.view(size=t_6)
        t_0 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::view20944
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct20949
        t_0 = t_6.permute(dims=t_0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack209022
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20951
        t_6 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack209022
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20955
        t_1 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack209022
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20959
        t_12 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size20960
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20962
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_1, 25, t_12]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListUnpack209022
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct20966
        t_12 = t_4.view(size=t_12)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::view20967
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct20972
        t_4 = t_12.permute(dims=t_4)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::permute20927
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::permute20950
        t_0 = t_5.matmul(other=t_0)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::matmul20974
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20975
        t_0 = torch.div(input=t_0, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::div20976
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20977
        t_5 = t_0.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::div20976
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20980
        t_12 = t_0.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size20981
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size20978
        t_5 = torch.sub(input=t_12, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20988
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20989
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20990
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20991
        t_1 = self.b_5[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::slice20992
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20993
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20994
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20995
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20996
        t_1 = t_1[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::slice20997
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20998
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::sub20986
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size20981
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant20999
        t_5 = t_1[:, :, t_5:t_12:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::slice21000
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21001
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21002
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size20981
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21003
        t_12 = t_5[:, :, :, 0:t_12:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::div20976
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::slice21004
        t_0 = torch.mul(input=t_0, other=t_12)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::slice21004
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21006
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21007
        t_12 = torch.rsub(t_12, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::rsub21008
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21009
        t_12 = torch.mul(input=t_12, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::mul21005
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::mul21010
        t_12 = torch.sub(input=t_0, other=t_12)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::sub21012
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21013
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21014
        t_12 = t_12.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::softmax21015
        t_12 = self.l_49(t_12)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::permute20973
        t_4 = t_12.matmul(other=t_4)
        t_12 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::matmul21017
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct21022
        t_12 = t_4.permute(dims=t_12)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::permute21023
        t_12 = t_12.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::contiguous21025
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21026
        t_4 = t_12.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::contiguous21025
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21030
        t_0 = t_12.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::contiguous21025
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21034
        t_5 = t_12.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::contiguous21025
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::Constant21037
        t_1 = t_12.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size21035
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::size21038
        t_1 = torch.mul(input=t_5, other=t_1)
        t_1 = [t_4, t_0, t_1]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::contiguous21025
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/prim::ListConstruct21042
        t_1 = t_12.view(size=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/aten::view21043
        t_1 = self.l_50(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Conv1D[c_proj]
        t_1 = self.l_51(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/aten::add20887
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]/Dropout[resid_dropout]
        t_1 = torch.add(input=t_3, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/aten::add21047
        t_3 = self.l_52(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2]
        t_3 = self.l_53(t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/prim::Constant21053
        t_12 = torch.mul(input=t_3, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/prim::Constant21055
        t_0 = t_3.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::pow21056
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/prim::Constant21057
        t_0 = torch.mul(input=t_0, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::mul21058
        t_0 = torch.add(input=t_3, other=t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::add21060
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/prim::Constant21061
        t_0 = torch.mul(input=t_0, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::mul21062
        t_0 = t_0.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::tanh21063
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/prim::Constant21064
        t_0 = torch.add(input=t_0, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::mul21054
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::add21066
        t_0 = torch.mul(input=t_12, other=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/aten::mul21067
        t_0 = self.l_54(t_0)
        # returing:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/aten::add21047
        return (t_0, t_1)

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition2(nn.Module):
    def __init__(self, layers, tensors):
        super(Partition2, self).__init__()
        # initializing partition layers
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_0, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_0)}'
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_1, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_1)}'
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_2, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_2)}'
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_3, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_3)}'
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_4, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_4)}'
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_5, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_5)}'
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_6, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_6)}'
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_7, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_7)}'
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_8, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_8)}'
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_9, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_9)}'
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_10, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_10)}'
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_11, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_11)}'
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_12, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_12)}'
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_13, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_13)}'
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_14, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_14)}'
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_15, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_15)}'
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_16, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_16)}'
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_17, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_17)}'
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_18, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_18)}'
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_19, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_19)}'
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_20, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_20)}'
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_21, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_21)}'
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_22, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_22)}'
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_23, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_23)}'
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_24, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_24)}'
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_25, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_25)}'
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_26, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_26)}'
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_27, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_27)}'
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_28, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_28)}'
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_29, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_29)}'
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_30, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_30)}'
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_31, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_31)}'
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_32, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_32)}'
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_33, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_33)}'
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_34, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_34)}'
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_35, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_35)}'
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_36, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_36)}'
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_37, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_37)}'
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_38, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_38)}'
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_39, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_39)}'
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_40, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_40)}'
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_41, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_41)}'
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_42, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_42)}'
        self.l_43 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_43, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_43)}'
        self.l_44 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_44, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_44)}'
        self.l_45 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_45, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_45)}'
        self.l_46 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_46, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_46)}'
        self.l_47 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_47, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_47)}'
        self.l_48 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_48, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_48)}'
        self.l_49 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_49, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_49)}'
        self.l_50 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_50, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_50)}'
        self.l_51 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_51, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_51)}'
        self.l_52 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_52, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_52)}'
        self.l_53 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_53, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_53)}'
        self.l_54 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_54, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_54)}'
        self.l_55 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_55, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_55)}'
        self.l_56 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_56, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_56)}'

        # initializing partition buffers
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_4', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_5', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Tensor[bias]']
        )

        # initializing partition parameters

        self.device = torch.device('cuda:2')
        self.lookup = {
            'l_0': 'transformer.blocks.11.mlp.dropout',
            'l_1': 'transformer.blocks.12.ln_1',
            'l_2': 'transformer.blocks.12.attn.c_attn',
            'l_3': 'transformer.blocks.12.attn.attn_dropout',
            'l_4': 'transformer.blocks.12.attn.c_proj',
            'l_5': 'transformer.blocks.12.attn.resid_dropout',
            'l_6': 'transformer.blocks.12.ln_2',
            'l_7': 'transformer.blocks.12.mlp.c_fc',
            'l_8': 'transformer.blocks.12.mlp.c_proj',
            'l_9': 'transformer.blocks.12.mlp.dropout',
            'l_10': 'transformer.blocks.13.ln_1',
            'l_11': 'transformer.blocks.13.attn.c_attn',
            'l_12': 'transformer.blocks.13.attn.attn_dropout',
            'l_13': 'transformer.blocks.13.attn.c_proj',
            'l_14': 'transformer.blocks.13.attn.resid_dropout',
            'l_15': 'transformer.blocks.13.ln_2',
            'l_16': 'transformer.blocks.13.mlp.c_fc',
            'l_17': 'transformer.blocks.13.mlp.c_proj',
            'l_18': 'transformer.blocks.13.mlp.dropout',
            'l_19': 'transformer.blocks.14.ln_1',
            'l_20': 'transformer.blocks.14.attn.c_attn',
            'l_21': 'transformer.blocks.14.attn.attn_dropout',
            'l_22': 'transformer.blocks.14.attn.c_proj',
            'l_23': 'transformer.blocks.14.attn.resid_dropout',
            'l_24': 'transformer.blocks.14.ln_2',
            'l_25': 'transformer.blocks.14.mlp.c_fc',
            'l_26': 'transformer.blocks.14.mlp.c_proj',
            'l_27': 'transformer.blocks.14.mlp.dropout',
            'l_28': 'transformer.blocks.15.ln_1',
            'l_29': 'transformer.blocks.15.attn.c_attn',
            'l_30': 'transformer.blocks.15.attn.attn_dropout',
            'l_31': 'transformer.blocks.15.attn.c_proj',
            'l_32': 'transformer.blocks.15.attn.resid_dropout',
            'l_33': 'transformer.blocks.15.ln_2',
            'l_34': 'transformer.blocks.15.mlp.c_fc',
            'l_35': 'transformer.blocks.15.mlp.c_proj',
            'l_36': 'transformer.blocks.15.mlp.dropout',
            'l_37': 'transformer.blocks.16.ln_1',
            'l_38': 'transformer.blocks.16.attn.c_attn',
            'l_39': 'transformer.blocks.16.attn.attn_dropout',
            'l_40': 'transformer.blocks.16.attn.c_proj',
            'l_41': 'transformer.blocks.16.attn.resid_dropout',
            'l_42': 'transformer.blocks.16.ln_2',
            'l_43': 'transformer.blocks.16.mlp.c_fc',
            'l_44': 'transformer.blocks.16.mlp.c_proj',
            'l_45': 'transformer.blocks.16.mlp.dropout',
            'l_46': 'transformer.blocks.17.ln_1',
            'l_47': 'transformer.blocks.17.attn.c_attn',
            'l_48': 'transformer.blocks.17.attn.attn_dropout',
            'l_49': 'transformer.blocks.17.attn.c_proj',
            'l_50': 'transformer.blocks.17.attn.resid_dropout',
            'l_51': 'transformer.blocks.17.ln_2',
            'l_52': 'transformer.blocks.17.mlp.c_fc',
            'l_53': 'transformer.blocks.17.mlp.c_proj',
            'l_54': 'transformer.blocks.17.mlp.dropout',
            'l_55': 'transformer.blocks.18.ln_1',
            'l_56': 'transformer.blocks.18.attn.c_attn',
            'b_0': 'transformer.blocks.12.attn.bias',
            'b_1': 'transformer.blocks.13.attn.bias',
            'b_2': 'transformer.blocks.14.attn.bias',
            'b_3': 'transformer.blocks.15.attn.bias',
            'b_4': 'transformer.blocks.16.attn.bias',
            'b_5': 'transformer.blocks.17.attn.bias'
        }

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_attn] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[attn_dropout] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_proj] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[resid_dropout] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_attn] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[attn_dropout] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_proj] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[resid_dropout] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_attn] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[attn_dropout] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_proj] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[resid_dropout] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_attn] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[attn_dropout] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_proj] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[resid_dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_attn] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[attn_dropout] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_proj] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[resid_dropout] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_attn] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[attn_dropout] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_proj] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[resid_dropout] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1] <=> self.l_55
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_attn] <=> self.l_56
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj] <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/aten::add21047 <=> x1

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)

        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj]
        t_0 = self.l_0(x0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/aten::add21047
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout]
        t_0 = torch.add(input=x1, other=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/aten::add21071
        t_1 = self.l_1(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1]
        t_1 = self.l_2(t_1)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21083
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21084
        t_1 = t_1.split(split_size=1600, dim=2)
        t_3 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210860
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21089
        t_5 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210860
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21093
        t_6 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210860
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21097
        t_7 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21098
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21100
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210860
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21104
        t_7 = t_3.view(size=t_7)
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::view21105
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21110
        t_3 = t_7.permute(dims=t_3)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210861
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21112
        t_7 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210861
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21116
        t_6 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210861
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21120
        t_5 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21121
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21123
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_7, t_6, 25, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210861
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21127
        t_5 = t_4.view(size=t_5)
        t_4 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::view21128
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21133
        t_4 = t_5.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210862
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21135
        t_5 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210862
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21139
        t_6 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210862
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21143
        t_7 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21144
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21146
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListUnpack210862
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21150
        t_7 = t_1.view(size=t_7)
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::view21151
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21156
        t_1 = t_7.permute(dims=t_1)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::permute21111
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::permute21134
        t_4 = t_3.matmul(other=t_4)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::matmul21158
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21159
        t_4 = torch.div(input=t_4, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::div21160
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21161
        t_3 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::div21160
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21164
        t_7 = t_4.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21165
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21162
        t_3 = torch.sub(input=t_7, other=t_3)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21172
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21173
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21174
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21175
        t_6 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::slice21176
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21177
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21178
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21179
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21180
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::slice21181
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21182
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::sub21170
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21165
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21183
        t_3 = t_6[:, :, t_3:t_7:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::slice21184
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21185
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21186
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21165
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21187
        t_7 = t_3[:, :, :, 0:t_7:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::div21160
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::slice21188
        t_4 = torch.mul(input=t_4, other=t_7)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::slice21188
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21190
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21191
        t_7 = torch.rsub(t_7, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::rsub21192
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21193
        t_7 = torch.mul(input=t_7, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::mul21189
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::mul21194
        t_7 = torch.sub(input=t_4, other=t_7)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::sub21196
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21197
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21198
        t_7 = t_7.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::softmax21199
        t_7 = self.l_3(t_7)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::permute21157
        t_1 = t_7.matmul(other=t_1)
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::matmul21201
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21206
        t_7 = t_1.permute(dims=t_7)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::permute21207
        t_7 = t_7.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::contiguous21209
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21210
        t_1 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::contiguous21209
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21214
        t_4 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::contiguous21209
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21218
        t_3 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::contiguous21209
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::Constant21221
        t_6 = t_7.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21219
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::size21222
        t_6 = torch.mul(input=t_3, other=t_6)
        t_6 = [t_1, t_4, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::contiguous21209
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/prim::ListConstruct21226
        t_6 = t_7.view(size=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/aten::view21227
        t_6 = self.l_4(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_5(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/aten::add21071
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_0, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/aten::add21231
        t_0 = self.l_6(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2]
        t_0 = self.l_7(t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/prim::Constant21237
        t_7 = torch.mul(input=t_0, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/prim::Constant21239
        t_4 = t_0.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::pow21240
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/prim::Constant21241
        t_4 = torch.mul(input=t_4, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::mul21242
        t_4 = torch.add(input=t_0, other=t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::add21244
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/prim::Constant21245
        t_4 = torch.mul(input=t_4, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::mul21246
        t_4 = t_4.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::tanh21247
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/prim::Constant21248
        t_4 = torch.add(input=t_4, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::mul21238
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::add21250
        t_4 = torch.mul(input=t_7, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/aten::mul21251
        t_4 = self.l_8(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj]
        t_4 = self.l_9(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/aten::add21231
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout]
        t_4 = torch.add(input=t_6, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/aten::add21255
        t_6 = self.l_10(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1]
        t_6 = self.l_11(t_6)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21267
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21268
        t_6 = t_6.split(split_size=1600, dim=2)
        t_0 = t_6[0]
        t_1 = t_6[1]
        t_6 = t_6[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212700
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21273
        t_3 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212700
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21277
        t_5 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212700
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21281
        t_8 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21284
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212700
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21288
        t_8 = t_0.view(size=t_8)
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::view21289
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21294
        t_0 = t_8.permute(dims=t_0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212701
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21296
        t_8 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212701
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21300
        t_5 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212701
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21304
        t_3 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21305
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21307
        t_3 = torch.div(input=t_3, other=25)
        t_3 = [t_8, t_5, 25, t_3]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212701
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21311
        t_3 = t_1.view(size=t_3)
        t_1 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::view21312
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21317
        t_1 = t_3.permute(dims=t_1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212702
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21319
        t_3 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212702
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21323
        t_5 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212702
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21327
        t_8 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21328
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21330
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListUnpack212702
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21334
        t_8 = t_6.view(size=t_8)
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::view21335
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21340
        t_6 = t_8.permute(dims=t_6)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::permute21295
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::permute21318
        t_1 = t_0.matmul(other=t_1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::matmul21342
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21343
        t_1 = torch.div(input=t_1, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::div21344
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21345
        t_0 = t_1.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::div21344
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21348
        t_8 = t_1.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21349
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21346
        t_0 = torch.sub(input=t_8, other=t_0)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21356
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21357
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21358
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21359
        t_5 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::slice21360
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21362
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21363
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21364
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::slice21365
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21366
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::sub21354
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21349
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21367
        t_0 = t_5[:, :, t_0:t_8:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::slice21368
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21369
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21370
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21349
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21371
        t_8 = t_0[:, :, :, 0:t_8:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::div21344
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::slice21372
        t_1 = torch.mul(input=t_1, other=t_8)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::slice21372
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21374
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21375
        t_8 = torch.rsub(t_8, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::rsub21376
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21377
        t_8 = torch.mul(input=t_8, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::mul21373
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::mul21378
        t_8 = torch.sub(input=t_1, other=t_8)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::sub21380
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21381
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21382
        t_8 = t_8.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::softmax21383
        t_8 = self.l_12(t_8)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::permute21341
        t_6 = t_8.matmul(other=t_6)
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::matmul21385
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21390
        t_8 = t_6.permute(dims=t_8)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::permute21391
        t_8 = t_8.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::contiguous21393
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21394
        t_6 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::contiguous21393
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21398
        t_1 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::contiguous21393
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21402
        t_0 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::contiguous21393
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::Constant21405
        t_5 = t_8.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21403
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::size21406
        t_5 = torch.mul(input=t_0, other=t_5)
        t_5 = [t_6, t_1, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::contiguous21393
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/prim::ListConstruct21410
        t_5 = t_8.view(size=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/aten::view21411
        t_5 = self.l_13(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_14(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/aten::add21255
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=t_4, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/aten::add21415
        t_4 = self.l_15(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2]
        t_4 = self.l_16(t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/prim::Constant21421
        t_8 = torch.mul(input=t_4, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/prim::Constant21423
        t_1 = t_4.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::pow21424
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/prim::Constant21425
        t_1 = torch.mul(input=t_1, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::mul21426
        t_1 = torch.add(input=t_4, other=t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::add21428
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/prim::Constant21429
        t_1 = torch.mul(input=t_1, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::mul21430
        t_1 = t_1.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::tanh21431
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/prim::Constant21432
        t_1 = torch.add(input=t_1, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::mul21422
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::add21434
        t_1 = torch.mul(input=t_8, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/aten::mul21435
        t_1 = self.l_17(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj]
        t_1 = self.l_18(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/aten::add21415
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout]
        t_1 = torch.add(input=t_5, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/aten::add21439
        t_5 = self.l_19(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1]
        t_5 = self.l_20(t_5)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21451
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21452
        t_5 = t_5.split(split_size=1600, dim=2)
        t_4 = t_5[0]
        t_6 = t_5[1]
        t_5 = t_5[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214540
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21457
        t_0 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214540
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21461
        t_3 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214540
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21465
        t_9 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21466
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21468
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_0, t_3, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214540
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21472
        t_9 = t_4.view(size=t_9)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::view21473
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21478
        t_4 = t_9.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214541
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21480
        t_9 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214541
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21484
        t_3 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214541
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21488
        t_0 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21489
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21491
        t_0 = torch.div(input=t_0, other=25)
        t_0 = [t_9, t_3, 25, t_0]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214541
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21495
        t_0 = t_6.view(size=t_0)
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::view21496
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21501
        t_6 = t_0.permute(dims=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214542
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21503
        t_0 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214542
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21507
        t_3 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214542
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21511
        t_9 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21512
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21514
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_0, t_3, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListUnpack214542
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21518
        t_9 = t_5.view(size=t_9)
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::view21519
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21524
        t_5 = t_9.permute(dims=t_5)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::permute21479
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::permute21502
        t_6 = t_4.matmul(other=t_6)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::matmul21526
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21527
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::div21528
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21529
        t_4 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::div21528
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21532
        t_9 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21533
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21530
        t_4 = torch.sub(input=t_9, other=t_4)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21540
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21541
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21542
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21543
        t_3 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::slice21544
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21545
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21546
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21547
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21548
        t_3 = t_3[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::slice21549
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21550
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::sub21538
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21533
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21551
        t_4 = t_3[:, :, t_4:t_9:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::slice21552
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21553
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21554
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21533
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21555
        t_9 = t_4[:, :, :, 0:t_9:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::div21528
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::slice21556
        t_6 = torch.mul(input=t_6, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::slice21556
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21558
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21559
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::rsub21560
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21561
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::mul21557
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::mul21562
        t_9 = torch.sub(input=t_6, other=t_9)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::sub21564
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21565
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21566
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::softmax21567
        t_9 = self.l_21(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::permute21525
        t_5 = t_9.matmul(other=t_5)
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::matmul21569
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21574
        t_9 = t_5.permute(dims=t_9)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::permute21575
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::contiguous21577
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21578
        t_5 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::contiguous21577
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21582
        t_6 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::contiguous21577
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21586
        t_4 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::contiguous21577
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::Constant21589
        t_3 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21587
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::size21590
        t_3 = torch.mul(input=t_4, other=t_3)
        t_3 = [t_5, t_6, t_3]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::contiguous21577
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/prim::ListConstruct21594
        t_3 = t_9.view(size=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/aten::view21595
        t_3 = self.l_22(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Conv1D[c_proj]
        t_3 = self.l_23(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/aten::add21439
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]/Dropout[resid_dropout]
        t_3 = torch.add(input=t_1, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/aten::add21599
        t_1 = self.l_24(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2]
        t_1 = self.l_25(t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/prim::Constant21605
        t_9 = torch.mul(input=t_1, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/prim::Constant21607
        t_6 = t_1.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::pow21608
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/prim::Constant21609
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::mul21610
        t_6 = torch.add(input=t_1, other=t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::add21612
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/prim::Constant21613
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::mul21614
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::tanh21615
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/prim::Constant21616
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::mul21606
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::add21618
        t_6 = torch.mul(input=t_9, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/aten::mul21619
        t_6 = self.l_26(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj]
        t_6 = self.l_27(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/aten::add21599
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout]
        t_6 = torch.add(input=t_3, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/aten::add21623
        t_3 = self.l_28(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1]
        t_3 = self.l_29(t_3)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21635
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21636
        t_3 = t_3.split(split_size=1600, dim=2)
        t_1 = t_3[0]
        t_5 = t_3[1]
        t_3 = t_3[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216380
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21641
        t_4 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216380
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21645
        t_0 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216380
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21649
        t_10 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21650
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21652
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_0, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216380
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21656
        t_10 = t_1.view(size=t_10)
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::view21657
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21662
        t_1 = t_10.permute(dims=t_1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216381
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21664
        t_10 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216381
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21668
        t_0 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216381
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21672
        t_4 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21673
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21675
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_10, t_0, 25, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216381
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21679
        t_4 = t_5.view(size=t_4)
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::view21680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21685
        t_5 = t_4.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216382
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21687
        t_4 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216382
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21691
        t_0 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216382
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21695
        t_10 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21696
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21698
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_0, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListUnpack216382
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21702
        t_10 = t_3.view(size=t_10)
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::view21703
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21708
        t_3 = t_10.permute(dims=t_3)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::permute21663
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::permute21686
        t_5 = t_1.matmul(other=t_5)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::matmul21710
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21711
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::div21712
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21713
        t_1 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::div21712
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21716
        t_10 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21717
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21714
        t_1 = torch.sub(input=t_10, other=t_1)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21724
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21725
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21726
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21727
        t_0 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::slice21728
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21729
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21730
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21731
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21732
        t_0 = t_0[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::slice21733
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21734
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::sub21722
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21717
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21735
        t_1 = t_0[:, :, t_1:t_10:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::slice21736
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21737
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21738
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21717
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21739
        t_10 = t_1[:, :, :, 0:t_10:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::div21712
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::slice21740
        t_5 = torch.mul(input=t_5, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::slice21740
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21742
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21743
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::rsub21744
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21745
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::mul21741
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::mul21746
        t_10 = torch.sub(input=t_5, other=t_10)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::sub21748
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21749
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21750
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::softmax21751
        t_10 = self.l_30(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::permute21709
        t_3 = t_10.matmul(other=t_3)
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::matmul21753
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21758
        t_10 = t_3.permute(dims=t_10)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::permute21759
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::contiguous21761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21762
        t_3 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::contiguous21761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21766
        t_5 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::contiguous21761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21770
        t_1 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::contiguous21761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::Constant21773
        t_0 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21771
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::size21774
        t_0 = torch.mul(input=t_1, other=t_0)
        t_0 = [t_3, t_5, t_0]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::contiguous21761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/prim::ListConstruct21778
        t_0 = t_10.view(size=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/aten::view21779
        t_0 = self.l_31(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Conv1D[c_proj]
        t_0 = self.l_32(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/aten::add21623
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]/Dropout[resid_dropout]
        t_0 = torch.add(input=t_6, other=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/aten::add21783
        t_6 = self.l_33(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2]
        t_6 = self.l_34(t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/prim::Constant21789
        t_10 = torch.mul(input=t_6, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/prim::Constant21791
        t_5 = t_6.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::pow21792
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/prim::Constant21793
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::mul21794
        t_5 = torch.add(input=t_6, other=t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::add21796
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/prim::Constant21797
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::mul21798
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::tanh21799
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/prim::Constant21800
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::mul21790
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::add21802
        t_5 = torch.mul(input=t_10, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/aten::mul21803
        t_5 = self.l_35(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj]
        t_5 = self.l_36(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/aten::add21783
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout]
        t_5 = torch.add(input=t_0, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/aten::add21807
        t_0 = self.l_37(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1]
        t_0 = self.l_38(t_0)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21819
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21820
        t_0 = t_0.split(split_size=1600, dim=2)
        t_6 = t_0[0]
        t_3 = t_0[1]
        t_0 = t_0[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218220
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21825
        t_1 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218220
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21829
        t_4 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218220
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21833
        t_11 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21834
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21836
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_1, t_4, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218220
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21840
        t_11 = t_6.view(size=t_11)
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::view21841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21846
        t_6 = t_11.permute(dims=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218221
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21848
        t_11 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218221
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21852
        t_4 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218221
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21856
        t_1 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21857
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21859
        t_1 = torch.div(input=t_1, other=25)
        t_1 = [t_11, t_4, 25, t_1]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218221
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21863
        t_1 = t_3.view(size=t_1)
        t_3 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::view21864
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21869
        t_3 = t_1.permute(dims=t_3)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218222
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21871
        t_1 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218222
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21875
        t_4 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218222
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21879
        t_11 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21880
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21882
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_1, t_4, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListUnpack218222
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21886
        t_11 = t_0.view(size=t_11)
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::view21887
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21892
        t_0 = t_11.permute(dims=t_0)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::permute21847
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::permute21870
        t_3 = t_6.matmul(other=t_3)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::matmul21894
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21895
        t_3 = torch.div(input=t_3, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::div21896
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21897
        t_6 = t_3.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::div21896
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21900
        t_11 = t_3.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21901
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21898
        t_6 = torch.sub(input=t_11, other=t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21908
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21909
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21910
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21911
        t_4 = self.b_4[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::slice21912
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21913
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21914
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21915
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21916
        t_4 = t_4[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::slice21917
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21918
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::sub21906
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21901
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21919
        t_6 = t_4[:, :, t_6:t_11:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::slice21920
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21921
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21922
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21901
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21923
        t_11 = t_6[:, :, :, 0:t_11:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::div21896
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::slice21924
        t_3 = torch.mul(input=t_3, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::slice21924
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21926
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21927
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::rsub21928
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21929
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::mul21925
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::mul21930
        t_11 = torch.sub(input=t_3, other=t_11)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::sub21932
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21933
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21934
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::softmax21935
        t_11 = self.l_39(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::permute21893
        t_0 = t_11.matmul(other=t_0)
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::matmul21937
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21942
        t_11 = t_0.permute(dims=t_11)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::permute21943
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::contiguous21945
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21946
        t_0 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::contiguous21945
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21950
        t_3 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::contiguous21945
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21954
        t_6 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::contiguous21945
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::Constant21957
        t_4 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21955
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::size21958
        t_4 = torch.mul(input=t_6, other=t_4)
        t_4 = [t_0, t_3, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::contiguous21945
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/prim::ListConstruct21962
        t_4 = t_11.view(size=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/aten::view21963
        t_4 = self.l_40(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_41(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/aten::add21807
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=t_5, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/aten::add21967
        t_5 = self.l_42(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2]
        t_5 = self.l_43(t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/prim::Constant21973
        t_11 = torch.mul(input=t_5, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/prim::Constant21975
        t_3 = t_5.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::pow21976
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/prim::Constant21977
        t_3 = torch.mul(input=t_3, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::mul21978
        t_3 = torch.add(input=t_5, other=t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::add21980
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/prim::Constant21981
        t_3 = torch.mul(input=t_3, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::mul21982
        t_3 = t_3.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::tanh21983
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/prim::Constant21984
        t_3 = torch.add(input=t_3, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::mul21974
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::add21986
        t_3 = torch.mul(input=t_11, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/aten::mul21987
        t_3 = self.l_44(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj]
        t_3 = self.l_45(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/aten::add21967
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout]
        t_3 = torch.add(input=t_4, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/aten::add21991
        t_4 = self.l_46(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1]
        t_4 = self.l_47(t_4)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22003
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22004
        t_4 = t_4.split(split_size=1600, dim=2)
        t_5 = t_4[0]
        t_0 = t_4[1]
        t_4 = t_4[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220060
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22009
        t_6 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220060
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22013
        t_1 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220060
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22017
        t_12 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22018
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22020
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_1, 25, t_12]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220060
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22024
        t_12 = t_5.view(size=t_12)
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::view22025
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22030
        t_5 = t_12.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220061
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22032
        t_12 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220061
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22036
        t_1 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220061
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22040
        t_6 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22041
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22043
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_12, t_1, 25, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220061
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22047
        t_6 = t_0.view(size=t_6)
        t_0 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::view22048
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22053
        t_0 = t_6.permute(dims=t_0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220062
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22055
        t_6 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220062
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22059
        t_1 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220062
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22063
        t_12 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22064
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22066
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_1, 25, t_12]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListUnpack220062
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22070
        t_12 = t_4.view(size=t_12)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::view22071
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22076
        t_4 = t_12.permute(dims=t_4)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::permute22031
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::permute22054
        t_0 = t_5.matmul(other=t_0)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::matmul22078
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22079
        t_0 = torch.div(input=t_0, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::div22080
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22081
        t_5 = t_0.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::div22080
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22084
        t_12 = t_0.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22085
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22082
        t_5 = torch.sub(input=t_12, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22092
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22093
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22094
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22095
        t_1 = self.b_5[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::slice22096
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22097
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22098
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22099
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22100
        t_1 = t_1[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::slice22101
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22102
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::sub22090
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22085
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22103
        t_5 = t_1[:, :, t_5:t_12:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::slice22104
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22105
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22106
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22085
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22107
        t_12 = t_5[:, :, :, 0:t_12:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::div22080
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::slice22108
        t_0 = torch.mul(input=t_0, other=t_12)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::slice22108
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22110
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22111
        t_12 = torch.rsub(t_12, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::rsub22112
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22113
        t_12 = torch.mul(input=t_12, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::mul22109
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::mul22114
        t_12 = torch.sub(input=t_0, other=t_12)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::sub22116
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22117
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22118
        t_12 = t_12.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::softmax22119
        t_12 = self.l_48(t_12)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::permute22077
        t_4 = t_12.matmul(other=t_4)
        t_12 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::matmul22121
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22126
        t_12 = t_4.permute(dims=t_12)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::permute22127
        t_12 = t_12.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::contiguous22129
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22130
        t_4 = t_12.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::contiguous22129
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22134
        t_0 = t_12.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::contiguous22129
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22138
        t_5 = t_12.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::contiguous22129
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::Constant22141
        t_1 = t_12.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22139
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::size22142
        t_1 = torch.mul(input=t_5, other=t_1)
        t_1 = [t_4, t_0, t_1]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::contiguous22129
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/prim::ListConstruct22146
        t_1 = t_12.view(size=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/aten::view22147
        t_1 = self.l_49(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Conv1D[c_proj]
        t_1 = self.l_50(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/aten::add21991
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]/Dropout[resid_dropout]
        t_1 = torch.add(input=t_3, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/aten::add22151
        t_3 = self.l_51(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2]
        t_3 = self.l_52(t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/prim::Constant22157
        t_12 = torch.mul(input=t_3, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/prim::Constant22159
        t_0 = t_3.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::pow22160
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/prim::Constant22161
        t_0 = torch.mul(input=t_0, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::mul22162
        t_0 = torch.add(input=t_3, other=t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::add22164
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/prim::Constant22165
        t_0 = torch.mul(input=t_0, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::mul22166
        t_0 = t_0.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::tanh22167
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/prim::Constant22168
        t_0 = torch.add(input=t_0, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::mul22158
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::add22170
        t_0 = torch.mul(input=t_12, other=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/aten::mul22171
        t_0 = self.l_53(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj]
        t_0 = self.l_54(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/aten::add22151
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout]
        t_0 = torch.add(input=t_1, other=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/aten::add22175
        t_1 = self.l_55(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1]
        t_1 = self.l_56(t_1)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22187
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22188
        t_1 = t_1.split(split_size=1600, dim=2)
        t_12 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221900
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22193
        t_4 = t_12.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221900
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22197
        t_5 = t_12.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221900
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22201
        t_6 = t_12.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22204
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_4, t_5, 25, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221900
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22208
        t_6 = t_12.view(size=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221901
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22216
        t_12 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221901
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22220
        t_5 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221901
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22224
        t_4 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22225
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22227
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_12, t_5, 25, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221901
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22231
        t_4 = t_3.view(size=t_4)
        t_3 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22232
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22237
        t_3 = t_4.permute(dims=t_3)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221902
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22239
        t_4 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221902
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22243
        t_5 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221902
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22247
        t_12 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22248
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22250
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_4, t_5, 25, t_12]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListUnpack221902
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22254
        t_12 = t_1.view(size=t_12)
        # returing:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/aten::add22175
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::permute22238
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22209
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22255
        return (t_0, t_3, t_6, t_12)

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition3(nn.Module):
    def __init__(self, layers, tensors):
        super(Partition3, self).__init__()
        # initializing partition layers
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_0, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_0)}'
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_1, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_1)}'
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_2, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_2)}'
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_3, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_3)}'
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_4, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_4)}'
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_5, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_5)}'
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_6, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_6)}'
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_7, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_7)}'
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_8, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_8)}'
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_9, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_9)}'
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_10, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_10)}'
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_11, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_11)}'
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_12, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_12)}'
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_13, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_13)}'
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_14, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_14)}'
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_15, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_15)}'
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_16, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_16)}'
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_17, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_17)}'
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_18, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_18)}'
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_19, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_19)}'
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_20, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_20)}'
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_21, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_21)}'
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_22, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_22)}'
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_23, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_23)}'
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_24, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_24)}'
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_25, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_25)}'
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_26, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_26)}'
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_27, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_27)}'
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_28, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_28)}'
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_29, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_29)}'
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_30, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_30)}'
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_31, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_31)}'
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_32, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_32)}'
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_33, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_33)}'
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_34, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_34)}'
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_35, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_35)}'
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_36, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_36)}'
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_37, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_37)}'
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_38, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_38)}'
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_39, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_39)}'
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_40, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_40)}'
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_41, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_41)}'
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_42, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_42)}'
        self.l_43 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_43, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_43)}'
        self.l_44 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_44, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_44)}'
        self.l_45 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_45, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_45)}'
        self.l_46 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_46, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_46)}'
        self.l_47 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_47, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_47)}'
        self.l_48 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_48, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_48)}'
        self.l_49 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_49, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_49)}'
        self.l_50 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_50, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_50)}'
        self.l_51 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_51, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_51)}'
        self.l_52 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_52, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_52)}'
        self.l_53 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_53, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_53)}'
        self.l_54 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_54, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_54)}'
        self.l_55 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_55, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_55)}'
        self.l_56 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_56, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_56)}'
        self.l_57 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_57, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_57)}'

        # initializing partition buffers
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_4', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_5', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_6', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Tensor[bias]']
        )

        # initializing partition parameters

        self.device = torch.device('cuda:3')
        self.lookup = {
            'l_0': 'transformer.blocks.18.attn.attn_dropout',
            'l_1': 'transformer.blocks.18.attn.c_proj',
            'l_2': 'transformer.blocks.18.attn.resid_dropout',
            'l_3': 'transformer.blocks.18.ln_2',
            'l_4': 'transformer.blocks.18.mlp.c_fc',
            'l_5': 'transformer.blocks.18.mlp.c_proj',
            'l_6': 'transformer.blocks.18.mlp.dropout',
            'l_7': 'transformer.blocks.19.ln_1',
            'l_8': 'transformer.blocks.19.attn.c_attn',
            'l_9': 'transformer.blocks.19.attn.attn_dropout',
            'l_10': 'transformer.blocks.19.attn.c_proj',
            'l_11': 'transformer.blocks.19.attn.resid_dropout',
            'l_12': 'transformer.blocks.19.ln_2',
            'l_13': 'transformer.blocks.19.mlp.c_fc',
            'l_14': 'transformer.blocks.19.mlp.c_proj',
            'l_15': 'transformer.blocks.19.mlp.dropout',
            'l_16': 'transformer.blocks.20.ln_1',
            'l_17': 'transformer.blocks.20.attn.c_attn',
            'l_18': 'transformer.blocks.20.attn.attn_dropout',
            'l_19': 'transformer.blocks.20.attn.c_proj',
            'l_20': 'transformer.blocks.20.attn.resid_dropout',
            'l_21': 'transformer.blocks.20.ln_2',
            'l_22': 'transformer.blocks.20.mlp.c_fc',
            'l_23': 'transformer.blocks.20.mlp.c_proj',
            'l_24': 'transformer.blocks.20.mlp.dropout',
            'l_25': 'transformer.blocks.21.ln_1',
            'l_26': 'transformer.blocks.21.attn.c_attn',
            'l_27': 'transformer.blocks.21.attn.attn_dropout',
            'l_28': 'transformer.blocks.21.attn.c_proj',
            'l_29': 'transformer.blocks.21.attn.resid_dropout',
            'l_30': 'transformer.blocks.21.ln_2',
            'l_31': 'transformer.blocks.21.mlp.c_fc',
            'l_32': 'transformer.blocks.21.mlp.c_proj',
            'l_33': 'transformer.blocks.21.mlp.dropout',
            'l_34': 'transformer.blocks.22.ln_1',
            'l_35': 'transformer.blocks.22.attn.c_attn',
            'l_36': 'transformer.blocks.22.attn.attn_dropout',
            'l_37': 'transformer.blocks.22.attn.c_proj',
            'l_38': 'transformer.blocks.22.attn.resid_dropout',
            'l_39': 'transformer.blocks.22.ln_2',
            'l_40': 'transformer.blocks.22.mlp.c_fc',
            'l_41': 'transformer.blocks.22.mlp.c_proj',
            'l_42': 'transformer.blocks.22.mlp.dropout',
            'l_43': 'transformer.blocks.23.ln_1',
            'l_44': 'transformer.blocks.23.attn.c_attn',
            'l_45': 'transformer.blocks.23.attn.attn_dropout',
            'l_46': 'transformer.blocks.23.attn.c_proj',
            'l_47': 'transformer.blocks.23.attn.resid_dropout',
            'l_48': 'transformer.blocks.23.ln_2',
            'l_49': 'transformer.blocks.23.mlp.c_fc',
            'l_50': 'transformer.blocks.23.mlp.c_proj',
            'l_51': 'transformer.blocks.23.mlp.dropout',
            'l_52': 'transformer.blocks.24.ln_1',
            'l_53': 'transformer.blocks.24.attn.c_attn',
            'l_54': 'transformer.blocks.24.attn.attn_dropout',
            'l_55': 'transformer.blocks.24.attn.c_proj',
            'l_56': 'transformer.blocks.24.attn.resid_dropout',
            'l_57': 'transformer.blocks.24.ln_2',
            'b_0': 'transformer.blocks.18.attn.bias',
            'b_1': 'transformer.blocks.19.attn.bias',
            'b_2': 'transformer.blocks.20.attn.bias',
            'b_3': 'transformer.blocks.21.attn.bias',
            'b_4': 'transformer.blocks.22.attn.bias',
            'b_5': 'transformer.blocks.23.attn.bias',
            'b_6': 'transformer.blocks.24.attn.bias'
        }

    def forward(self, x0, x1, x2, x3):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[attn_dropout] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_proj] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[resid_dropout] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_attn] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[attn_dropout] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_proj] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[resid_dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_attn] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[attn_dropout] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_proj] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[resid_dropout] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_attn] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[attn_dropout] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_proj] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[resid_dropout] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_attn] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[attn_dropout] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_proj] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[resid_dropout] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_attn] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[attn_dropout] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_proj] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[resid_dropout] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_attn] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[attn_dropout] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_proj] <=> self.l_55
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[resid_dropout] <=> self.l_56
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2] <=> self.l_57
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Tensor[bias] <=> self.b_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/aten::add22175 <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::permute22238 <=> x1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22209 <=> x2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22255 <=> x3

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)
        x2 = x2.to(self.device)
        x3 = x3.to(self.device)

        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22209
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22214
        t_1 = x2.permute(dims=t_1)
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22255
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22260
        t_2 = x3.permute(dims=t_2)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::permute22215
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::permute22238
        t_1 = t_1.matmul(other=x1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::matmul22262
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22263
        t_1 = torch.div(input=t_1, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::div22264
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22265
        t_3 = t_1.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::div22264
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22268
        t_4 = t_1.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22269
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22266
        t_3 = torch.sub(input=t_4, other=t_3)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22276
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22277
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22278
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22279
        t_5 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::slice22280
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22282
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22283
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22284
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::slice22285
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22286
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::sub22274
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22269
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22287
        t_3 = t_5[:, :, t_3:t_4:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::slice22288
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22289
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22290
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22269
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22291
        t_4 = t_3[:, :, :, 0:t_4:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::div22264
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::slice22292
        t_1 = torch.mul(input=t_1, other=t_4)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::slice22292
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22294
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22295
        t_4 = torch.rsub(t_4, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::rsub22296
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22297
        t_4 = torch.mul(input=t_4, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::mul22293
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::mul22298
        t_4 = torch.sub(input=t_1, other=t_4)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::sub22300
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22301
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22302
        t_4 = t_4.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::softmax22303
        t_4 = self.l_0(t_4)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::permute22261
        t_2 = t_4.matmul(other=t_2)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::matmul22305
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22310
        t_4 = t_2.permute(dims=t_4)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::permute22311
        t_4 = t_4.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::contiguous22313
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22314
        t_2 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::contiguous22313
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22318
        t_1 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::contiguous22313
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22322
        t_3 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::contiguous22313
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::Constant22325
        t_5 = t_4.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22323
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::size22326
        t_5 = torch.mul(input=t_3, other=t_5)
        t_5 = [t_2, t_1, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::contiguous22313
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/prim::ListConstruct22330
        t_5 = t_4.view(size=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/aten::view22331
        t_5 = self.l_1(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_2(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/aten::add22175
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=x0, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/aten::add22335
        t_4 = self.l_3(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2]
        t_4 = self.l_4(t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/prim::Constant22341
        t_1 = torch.mul(input=t_4, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/prim::Constant22343
        t_2 = t_4.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::pow22344
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/prim::Constant22345
        t_2 = torch.mul(input=t_2, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::mul22346
        t_2 = torch.add(input=t_4, other=t_2)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::add22348
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/prim::Constant22349
        t_2 = torch.mul(input=t_2, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::mul22350
        t_2 = t_2.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::tanh22351
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/prim::Constant22352
        t_2 = torch.add(input=t_2, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::mul22342
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::add22354
        t_2 = torch.mul(input=t_1, other=t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/aten::mul22355
        t_2 = self.l_5(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj]
        t_2 = self.l_6(t_2)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/aten::add22335
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout]
        t_2 = torch.add(input=t_5, other=t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/aten::add22359
        t_5 = self.l_7(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1]
        t_5 = self.l_8(t_5)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22371
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22372
        t_5 = t_5.split(split_size=1600, dim=2)
        t_4 = t_5[0]
        t_3 = t_5[1]
        t_5 = t_5[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223740
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22377
        t_6 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223740
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22381
        t_7 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223740
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22385
        t_8 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22386
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22388
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_6, t_7, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223740
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22392
        t_8 = t_4.view(size=t_8)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::view22393
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22398
        t_4 = t_8.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223741
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22400
        t_8 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223741
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22404
        t_7 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223741
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22408
        t_6 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22409
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22411
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_8, t_7, 25, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223741
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22415
        t_6 = t_3.view(size=t_6)
        t_3 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::view22416
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22421
        t_3 = t_6.permute(dims=t_3)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223742
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22423
        t_6 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223742
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22427
        t_7 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223742
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22431
        t_8 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22432
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22434
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_6, t_7, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListUnpack223742
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22438
        t_8 = t_5.view(size=t_8)
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::view22439
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22444
        t_5 = t_8.permute(dims=t_5)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::permute22399
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::permute22422
        t_3 = t_4.matmul(other=t_3)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::matmul22446
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22447
        t_3 = torch.div(input=t_3, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::div22448
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22449
        t_4 = t_3.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::div22448
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22452
        t_8 = t_3.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22453
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22450
        t_4 = torch.sub(input=t_8, other=t_4)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22460
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22461
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22462
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22463
        t_7 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::slice22464
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22465
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22466
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22467
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22468
        t_7 = t_7[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::slice22469
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22470
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::sub22458
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22453
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22471
        t_4 = t_7[:, :, t_4:t_8:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::slice22472
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22473
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22474
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22453
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22475
        t_8 = t_4[:, :, :, 0:t_8:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::div22448
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::slice22476
        t_3 = torch.mul(input=t_3, other=t_8)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::slice22476
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22478
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22479
        t_8 = torch.rsub(t_8, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::rsub22480
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22481
        t_8 = torch.mul(input=t_8, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::mul22477
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::mul22482
        t_8 = torch.sub(input=t_3, other=t_8)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::sub22484
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22485
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22486
        t_8 = t_8.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::softmax22487
        t_8 = self.l_9(t_8)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::permute22445
        t_5 = t_8.matmul(other=t_5)
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::matmul22489
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22494
        t_8 = t_5.permute(dims=t_8)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::permute22495
        t_8 = t_8.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::contiguous22497
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22498
        t_5 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::contiguous22497
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22502
        t_3 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::contiguous22497
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22506
        t_4 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::contiguous22497
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::Constant22509
        t_7 = t_8.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22507
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::size22510
        t_7 = torch.mul(input=t_4, other=t_7)
        t_7 = [t_5, t_3, t_7]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::contiguous22497
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/prim::ListConstruct22514
        t_7 = t_8.view(size=t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/aten::view22515
        t_7 = self.l_10(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Conv1D[c_proj]
        t_7 = self.l_11(t_7)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/aten::add22359
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]/Dropout[resid_dropout]
        t_7 = torch.add(input=t_2, other=t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/aten::add22519
        t_2 = self.l_12(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2]
        t_2 = self.l_13(t_2)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/prim::Constant22525
        t_8 = torch.mul(input=t_2, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/prim::Constant22527
        t_3 = t_2.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::pow22528
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/prim::Constant22529
        t_3 = torch.mul(input=t_3, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::mul22530
        t_3 = torch.add(input=t_2, other=t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::add22532
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/prim::Constant22533
        t_3 = torch.mul(input=t_3, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::mul22534
        t_3 = t_3.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::tanh22535
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/prim::Constant22536
        t_3 = torch.add(input=t_3, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::mul22526
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::add22538
        t_3 = torch.mul(input=t_8, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/aten::mul22539
        t_3 = self.l_14(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj]
        t_3 = self.l_15(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/aten::add22519
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout]
        t_3 = torch.add(input=t_7, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/aten::add22543
        t_7 = self.l_16(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1]
        t_7 = self.l_17(t_7)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22555
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22556
        t_7 = t_7.split(split_size=1600, dim=2)
        t_2 = t_7[0]
        t_5 = t_7[1]
        t_7 = t_7[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225580
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22561
        t_4 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225580
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22565
        t_6 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225580
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22569
        t_9 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22570
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22572
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_4, t_6, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225580
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22576
        t_9 = t_2.view(size=t_9)
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::view22577
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22582
        t_2 = t_9.permute(dims=t_2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225581
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22584
        t_9 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225581
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22588
        t_6 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225581
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22592
        t_4 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22593
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22595
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_9, t_6, 25, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225581
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22599
        t_4 = t_5.view(size=t_4)
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::view22600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22605
        t_5 = t_4.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225582
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22607
        t_4 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225582
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22611
        t_6 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225582
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22615
        t_9 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22616
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22618
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_4, t_6, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListUnpack225582
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22622
        t_9 = t_7.view(size=t_9)
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::view22623
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22628
        t_7 = t_9.permute(dims=t_7)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::permute22583
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::permute22606
        t_5 = t_2.matmul(other=t_5)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::matmul22630
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22631
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::div22632
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22633
        t_2 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::div22632
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22636
        t_9 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22637
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22634
        t_2 = torch.sub(input=t_9, other=t_2)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22644
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22645
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22646
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22647
        t_6 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::slice22648
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22649
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22650
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22651
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22652
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::slice22653
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22654
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::sub22642
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22637
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22655
        t_2 = t_6[:, :, t_2:t_9:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::slice22656
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22657
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22658
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22637
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22659
        t_9 = t_2[:, :, :, 0:t_9:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::div22632
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::slice22660
        t_5 = torch.mul(input=t_5, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::slice22660
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22662
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22663
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::rsub22664
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22665
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::mul22661
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::mul22666
        t_9 = torch.sub(input=t_5, other=t_9)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::sub22668
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22669
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22670
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::softmax22671
        t_9 = self.l_18(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::permute22629
        t_7 = t_9.matmul(other=t_7)
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::matmul22673
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22678
        t_9 = t_7.permute(dims=t_9)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::permute22679
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::contiguous22681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22682
        t_7 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::contiguous22681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22686
        t_5 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::contiguous22681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22690
        t_2 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::contiguous22681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::Constant22693
        t_6 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22691
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::size22694
        t_6 = torch.mul(input=t_2, other=t_6)
        t_6 = [t_7, t_5, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::contiguous22681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/prim::ListConstruct22698
        t_6 = t_9.view(size=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/aten::view22699
        t_6 = self.l_19(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_20(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/aten::add22543
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_3, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/aten::add22703
        t_3 = self.l_21(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2]
        t_3 = self.l_22(t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/prim::Constant22709
        t_9 = torch.mul(input=t_3, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/prim::Constant22711
        t_5 = t_3.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::pow22712
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/prim::Constant22713
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::mul22714
        t_5 = torch.add(input=t_3, other=t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::add22716
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/prim::Constant22717
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::mul22718
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::tanh22719
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/prim::Constant22720
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::mul22710
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::add22722
        t_5 = torch.mul(input=t_9, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/aten::mul22723
        t_5 = self.l_23(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj]
        t_5 = self.l_24(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/aten::add22703
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout]
        t_5 = torch.add(input=t_6, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/aten::add22727
        t_6 = self.l_25(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1]
        t_6 = self.l_26(t_6)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22739
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22740
        t_6 = t_6.split(split_size=1600, dim=2)
        t_3 = t_6[0]
        t_7 = t_6[1]
        t_6 = t_6[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227420
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22745
        t_2 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227420
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22749
        t_4 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227420
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22753
        t_10 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22754
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22756
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_2, t_4, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227420
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22760
        t_10 = t_3.view(size=t_10)
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::view22761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22766
        t_3 = t_10.permute(dims=t_3)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227421
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22768
        t_10 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227421
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22772
        t_4 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227421
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22776
        t_2 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22777
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22779
        t_2 = torch.div(input=t_2, other=25)
        t_2 = [t_10, t_4, 25, t_2]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227421
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22783
        t_2 = t_7.view(size=t_2)
        t_7 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::view22784
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22789
        t_7 = t_2.permute(dims=t_7)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227422
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22791
        t_2 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227422
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22795
        t_4 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227422
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22799
        t_10 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22800
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22802
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_2, t_4, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListUnpack227422
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22806
        t_10 = t_6.view(size=t_10)
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::view22807
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22812
        t_6 = t_10.permute(dims=t_6)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::permute22767
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::permute22790
        t_7 = t_3.matmul(other=t_7)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::matmul22814
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22815
        t_7 = torch.div(input=t_7, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::div22816
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22817
        t_3 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::div22816
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22820
        t_10 = t_7.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22821
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22818
        t_3 = torch.sub(input=t_10, other=t_3)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22828
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22829
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22830
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22831
        t_4 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::slice22832
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22833
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22834
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22835
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22836
        t_4 = t_4[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::slice22837
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22838
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::sub22826
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22821
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22839
        t_3 = t_4[:, :, t_3:t_10:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::slice22840
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22841
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22842
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22821
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22843
        t_10 = t_3[:, :, :, 0:t_10:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::div22816
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::slice22844
        t_7 = torch.mul(input=t_7, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::slice22844
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22846
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22847
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::rsub22848
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22849
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::mul22845
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::mul22850
        t_10 = torch.sub(input=t_7, other=t_10)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::sub22852
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22853
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22854
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::softmax22855
        t_10 = self.l_27(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::permute22813
        t_6 = t_10.matmul(other=t_6)
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::matmul22857
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22862
        t_10 = t_6.permute(dims=t_10)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::permute22863
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::contiguous22865
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22866
        t_6 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::contiguous22865
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22870
        t_7 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::contiguous22865
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22874
        t_3 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::contiguous22865
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::Constant22877
        t_4 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22875
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::size22878
        t_4 = torch.mul(input=t_3, other=t_4)
        t_4 = [t_6, t_7, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::contiguous22865
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/prim::ListConstruct22882
        t_4 = t_10.view(size=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/aten::view22883
        t_4 = self.l_28(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_29(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/aten::add22727
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=t_5, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/aten::add22887
        t_5 = self.l_30(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2]
        t_5 = self.l_31(t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/prim::Constant22893
        t_10 = torch.mul(input=t_5, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/prim::Constant22895
        t_7 = t_5.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::pow22896
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/prim::Constant22897
        t_7 = torch.mul(input=t_7, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::mul22898
        t_7 = torch.add(input=t_5, other=t_7)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::add22900
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/prim::Constant22901
        t_7 = torch.mul(input=t_7, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::mul22902
        t_7 = t_7.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::tanh22903
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/prim::Constant22904
        t_7 = torch.add(input=t_7, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::mul22894
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::add22906
        t_7 = torch.mul(input=t_10, other=t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/aten::mul22907
        t_7 = self.l_32(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj]
        t_7 = self.l_33(t_7)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/aten::add22887
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout]
        t_7 = torch.add(input=t_4, other=t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/aten::add22911
        t_4 = self.l_34(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1]
        t_4 = self.l_35(t_4)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22923
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22924
        t_4 = t_4.split(split_size=1600, dim=2)
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_4 = t_4[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229260
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22929
        t_3 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229260
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22933
        t_2 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229260
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22937
        t_11 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size22938
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22940
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_3, t_2, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229260
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct22944
        t_11 = t_5.view(size=t_11)
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::view22945
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct22950
        t_5 = t_11.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229261
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22952
        t_11 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229261
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22956
        t_2 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229261
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22960
        t_3 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size22961
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22963
        t_3 = torch.div(input=t_3, other=25)
        t_3 = [t_11, t_2, 25, t_3]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229261
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct22967
        t_3 = t_6.view(size=t_3)
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::view22968
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct22973
        t_6 = t_3.permute(dims=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229262
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22975
        t_3 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229262
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22979
        t_2 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229262
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22983
        t_11 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size22984
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22986
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_3, t_2, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListUnpack229262
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct22990
        t_11 = t_4.view(size=t_11)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::view22991
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct22996
        t_4 = t_11.permute(dims=t_4)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::permute22951
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::permute22974
        t_6 = t_5.matmul(other=t_6)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::matmul22998
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant22999
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::div23000
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23001
        t_5 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::div23000
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23004
        t_11 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size23005
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size23002
        t_5 = torch.sub(input=t_11, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23012
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23013
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23014
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23015
        t_2 = self.b_4[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::slice23016
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23017
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23018
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23019
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23020
        t_2 = t_2[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::slice23021
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23022
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::sub23010
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size23005
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23023
        t_5 = t_2[:, :, t_5:t_11:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::slice23024
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23025
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23026
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size23005
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23027
        t_11 = t_5[:, :, :, 0:t_11:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::div23000
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::slice23028
        t_6 = torch.mul(input=t_6, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::slice23028
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23030
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23031
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::rsub23032
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23033
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::mul23029
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::mul23034
        t_11 = torch.sub(input=t_6, other=t_11)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::sub23036
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23037
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23038
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::softmax23039
        t_11 = self.l_36(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::permute22997
        t_4 = t_11.matmul(other=t_4)
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::matmul23041
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct23046
        t_11 = t_4.permute(dims=t_11)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::permute23047
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::contiguous23049
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23050
        t_4 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::contiguous23049
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23054
        t_6 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::contiguous23049
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23058
        t_5 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::contiguous23049
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::Constant23061
        t_2 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size23059
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::size23062
        t_2 = torch.mul(input=t_5, other=t_2)
        t_2 = [t_4, t_6, t_2]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::contiguous23049
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/prim::ListConstruct23066
        t_2 = t_11.view(size=t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/aten::view23067
        t_2 = self.l_37(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Conv1D[c_proj]
        t_2 = self.l_38(t_2)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/aten::add22911
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]/Dropout[resid_dropout]
        t_2 = torch.add(input=t_7, other=t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/aten::add23071
        t_7 = self.l_39(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2]
        t_7 = self.l_40(t_7)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/prim::Constant23077
        t_11 = torch.mul(input=t_7, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/prim::Constant23079
        t_6 = t_7.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::pow23080
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/prim::Constant23081
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::mul23082
        t_6 = torch.add(input=t_7, other=t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::add23084
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/prim::Constant23085
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::mul23086
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::tanh23087
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/prim::Constant23088
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::mul23078
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::add23090
        t_6 = torch.mul(input=t_11, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/aten::mul23091
        t_6 = self.l_41(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj]
        t_6 = self.l_42(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/aten::add23071
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout]
        t_6 = torch.add(input=t_2, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/aten::add23095
        t_2 = self.l_43(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1]
        t_2 = self.l_44(t_2)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23107
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23108
        t_2 = t_2.split(split_size=1600, dim=2)
        t_7 = t_2[0]
        t_4 = t_2[1]
        t_2 = t_2[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231100
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23113
        t_5 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231100
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23117
        t_3 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231100
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23121
        t_12 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23124
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_5, t_3, 25, t_12]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231100
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23128
        t_12 = t_7.view(size=t_12)
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::view23129
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23134
        t_7 = t_12.permute(dims=t_7)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231101
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23136
        t_12 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231101
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23140
        t_3 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231101
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23144
        t_5 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23145
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23147
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_12, t_3, 25, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231101
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23151
        t_5 = t_4.view(size=t_5)
        t_4 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::view23152
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23157
        t_4 = t_5.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231102
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23159
        t_5 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231102
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23163
        t_3 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231102
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23167
        t_12 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23168
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23170
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_5, t_3, 25, t_12]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListUnpack231102
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23174
        t_12 = t_2.view(size=t_12)
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::view23175
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23180
        t_2 = t_12.permute(dims=t_2)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::permute23135
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::permute23158
        t_4 = t_7.matmul(other=t_4)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::matmul23182
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23183
        t_4 = torch.div(input=t_4, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::div23184
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23185
        t_7 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::div23184
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23188
        t_12 = t_4.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23189
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23186
        t_7 = torch.sub(input=t_12, other=t_7)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23196
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23197
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23198
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23199
        t_3 = self.b_5[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::slice23200
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23201
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23202
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23203
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23204
        t_3 = t_3[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::slice23205
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23206
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::sub23194
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23189
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23207
        t_7 = t_3[:, :, t_7:t_12:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::slice23208
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23209
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23210
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23189
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23211
        t_12 = t_7[:, :, :, 0:t_12:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::div23184
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::slice23212
        t_4 = torch.mul(input=t_4, other=t_12)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::slice23212
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23214
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23215
        t_12 = torch.rsub(t_12, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::rsub23216
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23217
        t_12 = torch.mul(input=t_12, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::mul23213
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::mul23218
        t_12 = torch.sub(input=t_4, other=t_12)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::sub23220
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23221
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23222
        t_12 = t_12.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::softmax23223
        t_12 = self.l_45(t_12)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::permute23181
        t_2 = t_12.matmul(other=t_2)
        t_12 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::matmul23225
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23230
        t_12 = t_2.permute(dims=t_12)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::permute23231
        t_12 = t_12.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::contiguous23233
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23234
        t_2 = t_12.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::contiguous23233
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23238
        t_4 = t_12.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::contiguous23233
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23242
        t_7 = t_12.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::contiguous23233
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::Constant23245
        t_3 = t_12.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23243
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::size23246
        t_3 = torch.mul(input=t_7, other=t_3)
        t_3 = [t_2, t_4, t_3]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::contiguous23233
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/prim::ListConstruct23250
        t_3 = t_12.view(size=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/aten::view23251
        t_3 = self.l_46(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Conv1D[c_proj]
        t_3 = self.l_47(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/aten::add23095
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]/Dropout[resid_dropout]
        t_3 = torch.add(input=t_6, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/aten::add23255
        t_6 = self.l_48(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2]
        t_6 = self.l_49(t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/prim::Constant23261
        t_12 = torch.mul(input=t_6, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/prim::Constant23263
        t_4 = t_6.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::pow23264
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/prim::Constant23265
        t_4 = torch.mul(input=t_4, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::mul23266
        t_4 = torch.add(input=t_6, other=t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::add23268
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/prim::Constant23269
        t_4 = torch.mul(input=t_4, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::mul23270
        t_4 = t_4.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::tanh23271
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/prim::Constant23272
        t_4 = torch.add(input=t_4, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::mul23262
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::add23274
        t_4 = torch.mul(input=t_12, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/aten::mul23275
        t_4 = self.l_50(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj]
        t_4 = self.l_51(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/aten::add23255
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout]
        t_4 = torch.add(input=t_3, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/aten::add23279
        t_3 = self.l_52(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1]
        t_3 = self.l_53(t_3)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23291
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23292
        t_3 = t_3.split(split_size=1600, dim=2)
        t_6 = t_3[0]
        t_2 = t_3[1]
        t_3 = t_3[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232940
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23297
        t_7 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232940
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23301
        t_5 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232940
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23305
        t_13 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23306
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23308
        t_13 = torch.div(input=t_13, other=25)
        t_13 = [t_7, t_5, 25, t_13]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232940
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23312
        t_13 = t_6.view(size=t_13)
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::view23313
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23318
        t_6 = t_13.permute(dims=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232941
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23320
        t_13 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232941
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23324
        t_5 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232941
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23328
        t_7 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23329
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23331
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_13, t_5, 25, t_7]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232941
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23335
        t_7 = t_2.view(size=t_7)
        t_2 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::view23336
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23341
        t_2 = t_7.permute(dims=t_2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232942
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23343
        t_7 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232942
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23347
        t_5 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232942
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23351
        t_13 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23352
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23354
        t_13 = torch.div(input=t_13, other=25)
        t_13 = [t_7, t_5, 25, t_13]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListUnpack232942
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23358
        t_13 = t_3.view(size=t_13)
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::view23359
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23364
        t_3 = t_13.permute(dims=t_3)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::permute23319
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::permute23342
        t_2 = t_6.matmul(other=t_2)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::matmul23366
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23367
        t_2 = torch.div(input=t_2, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::div23368
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23369
        t_6 = t_2.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::div23368
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23372
        t_13 = t_2.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23373
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23370
        t_6 = torch.sub(input=t_13, other=t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23380
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23381
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23382
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23383
        t_5 = self.b_6[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::slice23384
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23385
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23386
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23387
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23388
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::slice23389
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23390
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::sub23378
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23373
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23391
        t_6 = t_5[:, :, t_6:t_13:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::slice23392
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23393
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23394
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23373
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23395
        t_13 = t_6[:, :, :, 0:t_13:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::div23368
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::slice23396
        t_2 = torch.mul(input=t_2, other=t_13)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::slice23396
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23398
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23399
        t_13 = torch.rsub(t_13, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::rsub23400
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23401
        t_13 = torch.mul(input=t_13, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::mul23397
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::mul23402
        t_13 = torch.sub(input=t_2, other=t_13)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::sub23404
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23405
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23406
        t_13 = t_13.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::softmax23407
        t_13 = self.l_54(t_13)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::permute23365
        t_3 = t_13.matmul(other=t_3)
        t_13 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::matmul23409
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23414
        t_13 = t_3.permute(dims=t_13)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::permute23415
        t_13 = t_13.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::contiguous23417
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23418
        t_3 = t_13.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::contiguous23417
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23422
        t_2 = t_13.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::contiguous23417
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23426
        t_6 = t_13.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::contiguous23417
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::Constant23429
        t_5 = t_13.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23427
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::size23430
        t_5 = torch.mul(input=t_6, other=t_5)
        t_5 = [t_3, t_2, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::contiguous23417
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/prim::ListConstruct23434
        t_5 = t_13.view(size=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/aten::view23435
        t_5 = self.l_55(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_56(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/aten::add23279
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=t_4, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/aten::add23439
        t_4 = self.l_57(t_5)
        # returing:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/aten::add23439
        return (t_4, t_5)

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition4(nn.Module):
    def __init__(self, layers, tensors):
        super(Partition4, self).__init__()
        # initializing partition layers
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_0, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_0)}'
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_1, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_1)}'
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_2, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_2)}'
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_3, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_3)}'
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_4, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_4)}'
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_5, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_5)}'
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_6, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_6)}'
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_7, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_7)}'
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_8, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_8)}'
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_9, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_9)}'
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_10, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_10)}'
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_11, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_11)}'
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_12, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_12)}'
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_13, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_13)}'
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_14, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_14)}'
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_15, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_15)}'
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_16, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_16)}'
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_17, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_17)}'
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_18, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_18)}'
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_19, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_19)}'
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_20, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_20)}'
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_21, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_21)}'
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_22, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_22)}'
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_23, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_23)}'
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_24, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_24)}'
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_25, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_25)}'
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_26, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_26)}'
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_27, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_27)}'
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_28, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_28)}'
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_29, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_29)}'
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_30, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_30)}'
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_31, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_31)}'
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_32, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_32)}'
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_33, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_33)}'
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_34, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_34)}'
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_35, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_35)}'
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_36, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_36)}'
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_37, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_37)}'
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_38, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_38)}'
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_39, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_39)}'
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_40, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_40)}'
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_41, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_41)}'
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_42, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_42)}'
        self.l_43 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_43, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_43)}'
        self.l_44 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_44, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_44)}'
        self.l_45 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_45, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_45)}'
        self.l_46 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_46, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_46)}'
        self.l_47 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_47, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_47)}'
        self.l_48 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_48, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_48)}'
        self.l_49 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_49, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_49)}'
        self.l_50 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_50, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_50)}'
        self.l_51 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_51, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_51)}'
        self.l_52 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_52, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_52)}'
        self.l_53 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_53, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_53)}'
        self.l_54 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_54, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_54)}'

        # initializing partition buffers
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_4', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_5', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Tensor[bias]']
        )

        # initializing partition parameters

        self.device = torch.device('cuda:4')
        self.lookup = {
            'l_0': 'transformer.blocks.24.mlp.c_fc',
            'l_1': 'transformer.blocks.24.mlp.c_proj',
            'l_2': 'transformer.blocks.24.mlp.dropout',
            'l_3': 'transformer.blocks.25.ln_1',
            'l_4': 'transformer.blocks.25.attn.c_attn',
            'l_5': 'transformer.blocks.25.attn.attn_dropout',
            'l_6': 'transformer.blocks.25.attn.c_proj',
            'l_7': 'transformer.blocks.25.attn.resid_dropout',
            'l_8': 'transformer.blocks.25.ln_2',
            'l_9': 'transformer.blocks.25.mlp.c_fc',
            'l_10': 'transformer.blocks.25.mlp.c_proj',
            'l_11': 'transformer.blocks.25.mlp.dropout',
            'l_12': 'transformer.blocks.26.ln_1',
            'l_13': 'transformer.blocks.26.attn.c_attn',
            'l_14': 'transformer.blocks.26.attn.attn_dropout',
            'l_15': 'transformer.blocks.26.attn.c_proj',
            'l_16': 'transformer.blocks.26.attn.resid_dropout',
            'l_17': 'transformer.blocks.26.ln_2',
            'l_18': 'transformer.blocks.26.mlp.c_fc',
            'l_19': 'transformer.blocks.26.mlp.c_proj',
            'l_20': 'transformer.blocks.26.mlp.dropout',
            'l_21': 'transformer.blocks.27.ln_1',
            'l_22': 'transformer.blocks.27.attn.c_attn',
            'l_23': 'transformer.blocks.27.attn.attn_dropout',
            'l_24': 'transformer.blocks.27.attn.c_proj',
            'l_25': 'transformer.blocks.27.attn.resid_dropout',
            'l_26': 'transformer.blocks.27.ln_2',
            'l_27': 'transformer.blocks.27.mlp.c_fc',
            'l_28': 'transformer.blocks.27.mlp.c_proj',
            'l_29': 'transformer.blocks.27.mlp.dropout',
            'l_30': 'transformer.blocks.28.ln_1',
            'l_31': 'transformer.blocks.28.attn.c_attn',
            'l_32': 'transformer.blocks.28.attn.attn_dropout',
            'l_33': 'transformer.blocks.28.attn.c_proj',
            'l_34': 'transformer.blocks.28.attn.resid_dropout',
            'l_35': 'transformer.blocks.28.ln_2',
            'l_36': 'transformer.blocks.28.mlp.c_fc',
            'l_37': 'transformer.blocks.28.mlp.c_proj',
            'l_38': 'transformer.blocks.28.mlp.dropout',
            'l_39': 'transformer.blocks.29.ln_1',
            'l_40': 'transformer.blocks.29.attn.c_attn',
            'l_41': 'transformer.blocks.29.attn.attn_dropout',
            'l_42': 'transformer.blocks.29.attn.c_proj',
            'l_43': 'transformer.blocks.29.attn.resid_dropout',
            'l_44': 'transformer.blocks.29.ln_2',
            'l_45': 'transformer.blocks.29.mlp.c_fc',
            'l_46': 'transformer.blocks.29.mlp.c_proj',
            'l_47': 'transformer.blocks.29.mlp.dropout',
            'l_48': 'transformer.blocks.30.ln_1',
            'l_49': 'transformer.blocks.30.attn.c_attn',
            'l_50': 'transformer.blocks.30.attn.attn_dropout',
            'l_51': 'transformer.blocks.30.attn.c_proj',
            'l_52': 'transformer.blocks.30.attn.resid_dropout',
            'l_53': 'transformer.blocks.30.ln_2',
            'l_54': 'transformer.blocks.30.mlp.c_fc',
            'b_0': 'transformer.blocks.25.attn.bias',
            'b_1': 'transformer.blocks.26.attn.bias',
            'b_2': 'transformer.blocks.27.attn.bias',
            'b_3': 'transformer.blocks.28.attn.bias',
            'b_4': 'transformer.blocks.29.attn.bias',
            'b_5': 'transformer.blocks.30.attn.bias'
        }

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_attn] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[attn_dropout] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_proj] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[resid_dropout] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_attn] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[attn_dropout] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_proj] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[resid_dropout] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_attn] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[attn_dropout] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_proj] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[resid_dropout] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_attn] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[attn_dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_proj] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[resid_dropout] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_attn] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[attn_dropout] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_proj] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[resid_dropout] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_attn] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[attn_dropout] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_proj] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[resid_dropout] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2] <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/aten::add23439 <=> x1

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)

        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2]
        t_0 = self.l_0(x0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/prim::Constant23445
        t_1 = torch.mul(input=t_0, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/prim::Constant23447
        t_2 = t_0.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::pow23448
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/prim::Constant23449
        t_2 = torch.mul(input=t_2, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::mul23450
        t_2 = torch.add(input=t_0, other=t_2)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::add23452
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/prim::Constant23453
        t_2 = torch.mul(input=t_2, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::mul23454
        t_2 = t_2.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::tanh23455
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/prim::Constant23456
        t_2 = torch.add(input=t_2, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::mul23446
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::add23458
        t_2 = torch.mul(input=t_1, other=t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/aten::mul23459
        t_2 = self.l_1(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj]
        t_2 = self.l_2(t_2)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/aten::add23439
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout]
        t_2 = torch.add(input=x1, other=t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/aten::add23463
        t_1 = self.l_3(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1]
        t_1 = self.l_4(t_1)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23475
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23476
        t_1 = t_1.split(split_size=1600, dim=2)
        t_3 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234780
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23481
        t_5 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234780
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23485
        t_6 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234780
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23489
        t_7 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23490
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23492
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234780
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23496
        t_7 = t_3.view(size=t_7)
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::view23497
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23502
        t_3 = t_7.permute(dims=t_3)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234781
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23504
        t_7 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234781
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23508
        t_6 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234781
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23512
        t_5 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23513
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23515
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_7, t_6, 25, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234781
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23519
        t_5 = t_4.view(size=t_5)
        t_4 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::view23520
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23525
        t_4 = t_5.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234782
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23527
        t_5 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234782
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23531
        t_6 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234782
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23535
        t_7 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23536
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23538
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListUnpack234782
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23542
        t_7 = t_1.view(size=t_7)
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::view23543
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23548
        t_1 = t_7.permute(dims=t_1)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::permute23503
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::permute23526
        t_4 = t_3.matmul(other=t_4)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::matmul23550
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23551
        t_4 = torch.div(input=t_4, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::div23552
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23553
        t_3 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::div23552
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23556
        t_7 = t_4.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23557
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23554
        t_3 = torch.sub(input=t_7, other=t_3)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23564
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23565
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23566
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23567
        t_6 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::slice23568
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23569
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23570
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23571
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23572
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::slice23573
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23574
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::sub23562
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23557
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23575
        t_3 = t_6[:, :, t_3:t_7:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::slice23576
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23577
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23578
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23557
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23579
        t_7 = t_3[:, :, :, 0:t_7:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::div23552
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::slice23580
        t_4 = torch.mul(input=t_4, other=t_7)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::slice23580
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23582
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23583
        t_7 = torch.rsub(t_7, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::rsub23584
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23585
        t_7 = torch.mul(input=t_7, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::mul23581
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::mul23586
        t_7 = torch.sub(input=t_4, other=t_7)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::sub23588
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23589
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23590
        t_7 = t_7.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::softmax23591
        t_7 = self.l_5(t_7)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::permute23549
        t_1 = t_7.matmul(other=t_1)
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::matmul23593
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23598
        t_7 = t_1.permute(dims=t_7)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::permute23599
        t_7 = t_7.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::contiguous23601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23602
        t_1 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::contiguous23601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23606
        t_4 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::contiguous23601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23610
        t_3 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::contiguous23601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::Constant23613
        t_6 = t_7.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23611
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::size23614
        t_6 = torch.mul(input=t_3, other=t_6)
        t_6 = [t_1, t_4, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::contiguous23601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/prim::ListConstruct23618
        t_6 = t_7.view(size=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/aten::view23619
        t_6 = self.l_6(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_7(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/aten::add23463
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_2, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/aten::add23623
        t_2 = self.l_8(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2]
        t_2 = self.l_9(t_2)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/prim::Constant23629
        t_7 = torch.mul(input=t_2, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/prim::Constant23631
        t_4 = t_2.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::pow23632
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/prim::Constant23633
        t_4 = torch.mul(input=t_4, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::mul23634
        t_4 = torch.add(input=t_2, other=t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::add23636
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/prim::Constant23637
        t_4 = torch.mul(input=t_4, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::mul23638
        t_4 = t_4.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::tanh23639
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/prim::Constant23640
        t_4 = torch.add(input=t_4, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::mul23630
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::add23642
        t_4 = torch.mul(input=t_7, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/aten::mul23643
        t_4 = self.l_10(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj]
        t_4 = self.l_11(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/aten::add23623
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout]
        t_4 = torch.add(input=t_6, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/aten::add23647
        t_6 = self.l_12(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1]
        t_6 = self.l_13(t_6)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23659
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23660
        t_6 = t_6.split(split_size=1600, dim=2)
        t_2 = t_6[0]
        t_1 = t_6[1]
        t_6 = t_6[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236620
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23665
        t_3 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236620
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23669
        t_5 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236620
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23673
        t_8 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23674
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23676
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236620
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23680
        t_8 = t_2.view(size=t_8)
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::view23681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23686
        t_2 = t_8.permute(dims=t_2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236621
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23688
        t_8 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236621
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23692
        t_5 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236621
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23696
        t_3 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23697
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23699
        t_3 = torch.div(input=t_3, other=25)
        t_3 = [t_8, t_5, 25, t_3]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236621
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23703
        t_3 = t_1.view(size=t_3)
        t_1 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::view23704
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23709
        t_1 = t_3.permute(dims=t_1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236622
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23711
        t_3 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236622
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23715
        t_5 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236622
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23719
        t_8 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23720
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23722
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListUnpack236622
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23726
        t_8 = t_6.view(size=t_8)
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::view23727
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23732
        t_6 = t_8.permute(dims=t_6)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::permute23687
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::permute23710
        t_1 = t_2.matmul(other=t_1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::matmul23734
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23735
        t_1 = torch.div(input=t_1, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::div23736
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23737
        t_2 = t_1.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::div23736
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23740
        t_8 = t_1.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23741
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23738
        t_2 = torch.sub(input=t_8, other=t_2)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23748
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23749
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23750
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23751
        t_5 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::slice23752
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23753
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23754
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23755
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23756
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::slice23757
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23758
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::sub23746
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23741
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23759
        t_2 = t_5[:, :, t_2:t_8:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::slice23760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23761
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23762
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23741
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23763
        t_8 = t_2[:, :, :, 0:t_8:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::div23736
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::slice23764
        t_1 = torch.mul(input=t_1, other=t_8)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::slice23764
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23766
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23767
        t_8 = torch.rsub(t_8, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::rsub23768
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23769
        t_8 = torch.mul(input=t_8, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::mul23765
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::mul23770
        t_8 = torch.sub(input=t_1, other=t_8)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::sub23772
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23773
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23774
        t_8 = t_8.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::softmax23775
        t_8 = self.l_14(t_8)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::permute23733
        t_6 = t_8.matmul(other=t_6)
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::matmul23777
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23782
        t_8 = t_6.permute(dims=t_8)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::permute23783
        t_8 = t_8.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::contiguous23785
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23786
        t_6 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::contiguous23785
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23790
        t_1 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::contiguous23785
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23794
        t_2 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::contiguous23785
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::Constant23797
        t_5 = t_8.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23795
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::size23798
        t_5 = torch.mul(input=t_2, other=t_5)
        t_5 = [t_6, t_1, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::contiguous23785
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/prim::ListConstruct23802
        t_5 = t_8.view(size=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/aten::view23803
        t_5 = self.l_15(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_16(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/aten::add23647
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=t_4, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/aten::add23807
        t_4 = self.l_17(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2]
        t_4 = self.l_18(t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/prim::Constant23813
        t_8 = torch.mul(input=t_4, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/prim::Constant23815
        t_1 = t_4.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::pow23816
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/prim::Constant23817
        t_1 = torch.mul(input=t_1, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::mul23818
        t_1 = torch.add(input=t_4, other=t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::add23820
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/prim::Constant23821
        t_1 = torch.mul(input=t_1, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::mul23822
        t_1 = t_1.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::tanh23823
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/prim::Constant23824
        t_1 = torch.add(input=t_1, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::mul23814
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::add23826
        t_1 = torch.mul(input=t_8, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/aten::mul23827
        t_1 = self.l_19(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj]
        t_1 = self.l_20(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/aten::add23807
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout]
        t_1 = torch.add(input=t_5, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/aten::add23831
        t_5 = self.l_21(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1]
        t_5 = self.l_22(t_5)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23843
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23844
        t_5 = t_5.split(split_size=1600, dim=2)
        t_4 = t_5[0]
        t_6 = t_5[1]
        t_5 = t_5[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238460
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23849
        t_2 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238460
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23853
        t_3 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238460
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23857
        t_9 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23858
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23860
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_2, t_3, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238460
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23864
        t_9 = t_4.view(size=t_9)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::view23865
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23870
        t_4 = t_9.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238461
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23872
        t_9 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238461
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23876
        t_3 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238461
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23880
        t_2 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23881
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23883
        t_2 = torch.div(input=t_2, other=25)
        t_2 = [t_9, t_3, 25, t_2]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238461
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23887
        t_2 = t_6.view(size=t_2)
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::view23888
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23893
        t_6 = t_2.permute(dims=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238462
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23895
        t_2 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238462
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23899
        t_3 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238462
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23903
        t_9 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23904
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23906
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_2, t_3, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListUnpack238462
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23910
        t_9 = t_5.view(size=t_9)
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::view23911
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23916
        t_5 = t_9.permute(dims=t_5)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::permute23871
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::permute23894
        t_6 = t_4.matmul(other=t_6)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::matmul23918
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23919
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::div23920
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23921
        t_4 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::div23920
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23924
        t_9 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23925
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23922
        t_4 = torch.sub(input=t_9, other=t_4)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23932
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23933
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23934
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23935
        t_3 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::slice23936
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23937
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23938
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23939
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23940
        t_3 = t_3[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::slice23941
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23942
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::sub23930
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23925
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23943
        t_4 = t_3[:, :, t_4:t_9:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::slice23944
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23945
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23946
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23925
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23947
        t_9 = t_4[:, :, :, 0:t_9:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::div23920
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::slice23948
        t_6 = torch.mul(input=t_6, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::slice23948
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23950
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23951
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::rsub23952
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23953
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::mul23949
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::mul23954
        t_9 = torch.sub(input=t_6, other=t_9)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::sub23956
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23957
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23958
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::softmax23959
        t_9 = self.l_23(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::permute23917
        t_5 = t_9.matmul(other=t_5)
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::matmul23961
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23966
        t_9 = t_5.permute(dims=t_9)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::permute23967
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::contiguous23969
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23970
        t_5 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::contiguous23969
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23974
        t_6 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::contiguous23969
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23978
        t_4 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::contiguous23969
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::Constant23981
        t_3 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23979
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::size23982
        t_3 = torch.mul(input=t_4, other=t_3)
        t_3 = [t_5, t_6, t_3]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::contiguous23969
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/prim::ListConstruct23986
        t_3 = t_9.view(size=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/aten::view23987
        t_3 = self.l_24(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Conv1D[c_proj]
        t_3 = self.l_25(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/aten::add23831
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]/Dropout[resid_dropout]
        t_3 = torch.add(input=t_1, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/aten::add23991
        t_1 = self.l_26(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2]
        t_1 = self.l_27(t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/prim::Constant23997
        t_9 = torch.mul(input=t_1, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/prim::Constant23999
        t_6 = t_1.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::pow24000
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/prim::Constant24001
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::mul24002
        t_6 = torch.add(input=t_1, other=t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::add24004
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/prim::Constant24005
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::mul24006
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::tanh24007
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/prim::Constant24008
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::mul23998
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::add24010
        t_6 = torch.mul(input=t_9, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/aten::mul24011
        t_6 = self.l_28(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj]
        t_6 = self.l_29(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/aten::add23991
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout]
        t_6 = torch.add(input=t_3, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/aten::add24015
        t_3 = self.l_30(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1]
        t_3 = self.l_31(t_3)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24027
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24028
        t_3 = t_3.split(split_size=1600, dim=2)
        t_1 = t_3[0]
        t_5 = t_3[1]
        t_3 = t_3[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240300
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24033
        t_4 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240300
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24037
        t_2 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240300
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24041
        t_10 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24042
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24044
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_2, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240300
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24048
        t_10 = t_1.view(size=t_10)
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::view24049
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24054
        t_1 = t_10.permute(dims=t_1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240301
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24056
        t_10 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240301
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24060
        t_2 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240301
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24064
        t_4 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24065
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24067
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_10, t_2, 25, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240301
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24071
        t_4 = t_5.view(size=t_4)
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::view24072
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24077
        t_5 = t_4.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240302
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24079
        t_4 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240302
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24083
        t_2 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240302
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24087
        t_10 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24088
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24090
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_2, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListUnpack240302
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24094
        t_10 = t_3.view(size=t_10)
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::view24095
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24100
        t_3 = t_10.permute(dims=t_3)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::permute24055
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::permute24078
        t_5 = t_1.matmul(other=t_5)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::matmul24102
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24103
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::div24104
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24105
        t_1 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::div24104
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24108
        t_10 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24109
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24106
        t_1 = torch.sub(input=t_10, other=t_1)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24116
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24117
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24118
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24119
        t_2 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::slice24120
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24121
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24122
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24123
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24124
        t_2 = t_2[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::slice24125
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24126
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::sub24114
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24109
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24127
        t_1 = t_2[:, :, t_1:t_10:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::slice24128
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24129
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24130
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24109
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24131
        t_10 = t_1[:, :, :, 0:t_10:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::div24104
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::slice24132
        t_5 = torch.mul(input=t_5, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::slice24132
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24134
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24135
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::rsub24136
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24137
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::mul24133
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::mul24138
        t_10 = torch.sub(input=t_5, other=t_10)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::sub24140
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24141
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24142
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::softmax24143
        t_10 = self.l_32(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::permute24101
        t_3 = t_10.matmul(other=t_3)
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::matmul24145
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24150
        t_10 = t_3.permute(dims=t_10)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::permute24151
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::contiguous24153
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24154
        t_3 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::contiguous24153
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24158
        t_5 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::contiguous24153
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24162
        t_1 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::contiguous24153
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::Constant24165
        t_2 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24163
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::size24166
        t_2 = torch.mul(input=t_1, other=t_2)
        t_2 = [t_3, t_5, t_2]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::contiguous24153
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/prim::ListConstruct24170
        t_2 = t_10.view(size=t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/aten::view24171
        t_2 = self.l_33(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Conv1D[c_proj]
        t_2 = self.l_34(t_2)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/aten::add24015
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]/Dropout[resid_dropout]
        t_2 = torch.add(input=t_6, other=t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/aten::add24175
        t_6 = self.l_35(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2]
        t_6 = self.l_36(t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/prim::Constant24181
        t_10 = torch.mul(input=t_6, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/prim::Constant24183
        t_5 = t_6.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::pow24184
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/prim::Constant24185
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::mul24186
        t_5 = torch.add(input=t_6, other=t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::add24188
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/prim::Constant24189
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::mul24190
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::tanh24191
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/prim::Constant24192
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::mul24182
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::add24194
        t_5 = torch.mul(input=t_10, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/aten::mul24195
        t_5 = self.l_37(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj]
        t_5 = self.l_38(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/aten::add24175
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout]
        t_5 = torch.add(input=t_2, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/aten::add24199
        t_2 = self.l_39(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1]
        t_2 = self.l_40(t_2)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24211
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24212
        t_2 = t_2.split(split_size=1600, dim=2)
        t_6 = t_2[0]
        t_3 = t_2[1]
        t_2 = t_2[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242140
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24217
        t_1 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242140
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24221
        t_4 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242140
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24225
        t_11 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24226
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24228
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_1, t_4, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242140
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24232
        t_11 = t_6.view(size=t_11)
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::view24233
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24238
        t_6 = t_11.permute(dims=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242141
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24240
        t_11 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242141
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24244
        t_4 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242141
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24248
        t_1 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24249
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24251
        t_1 = torch.div(input=t_1, other=25)
        t_1 = [t_11, t_4, 25, t_1]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242141
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24255
        t_1 = t_3.view(size=t_1)
        t_3 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::view24256
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24261
        t_3 = t_1.permute(dims=t_3)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242142
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24263
        t_1 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242142
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24267
        t_4 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242142
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24271
        t_11 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24272
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24274
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_1, t_4, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListUnpack242142
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24278
        t_11 = t_2.view(size=t_11)
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::view24279
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24284
        t_2 = t_11.permute(dims=t_2)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::permute24239
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::permute24262
        t_3 = t_6.matmul(other=t_3)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::matmul24286
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24287
        t_3 = torch.div(input=t_3, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::div24288
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24289
        t_6 = t_3.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::div24288
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24292
        t_11 = t_3.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24293
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24290
        t_6 = torch.sub(input=t_11, other=t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24300
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24301
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24302
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24303
        t_4 = self.b_4[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::slice24304
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24305
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24306
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24307
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24308
        t_4 = t_4[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::slice24309
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24310
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::sub24298
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24293
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24311
        t_6 = t_4[:, :, t_6:t_11:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::slice24312
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24313
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24314
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24293
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24315
        t_11 = t_6[:, :, :, 0:t_11:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::div24288
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::slice24316
        t_3 = torch.mul(input=t_3, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::slice24316
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24318
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24319
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::rsub24320
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24321
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::mul24317
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::mul24322
        t_11 = torch.sub(input=t_3, other=t_11)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::sub24324
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24325
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24326
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::softmax24327
        t_11 = self.l_41(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::permute24285
        t_2 = t_11.matmul(other=t_2)
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::matmul24329
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24334
        t_11 = t_2.permute(dims=t_11)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::permute24335
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::contiguous24337
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24338
        t_2 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::contiguous24337
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24342
        t_3 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::contiguous24337
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24346
        t_6 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::contiguous24337
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::Constant24349
        t_4 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24347
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::size24350
        t_4 = torch.mul(input=t_6, other=t_4)
        t_4 = [t_2, t_3, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::contiguous24337
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/prim::ListConstruct24354
        t_4 = t_11.view(size=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/aten::view24355
        t_4 = self.l_42(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_43(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/aten::add24199
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=t_5, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/aten::add24359
        t_5 = self.l_44(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2]
        t_5 = self.l_45(t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/prim::Constant24365
        t_11 = torch.mul(input=t_5, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/prim::Constant24367
        t_3 = t_5.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::pow24368
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/prim::Constant24369
        t_3 = torch.mul(input=t_3, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::mul24370
        t_3 = torch.add(input=t_5, other=t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::add24372
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/prim::Constant24373
        t_3 = torch.mul(input=t_3, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::mul24374
        t_3 = t_3.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::tanh24375
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/prim::Constant24376
        t_3 = torch.add(input=t_3, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::mul24366
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::add24378
        t_3 = torch.mul(input=t_11, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/aten::mul24379
        t_3 = self.l_46(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj]
        t_3 = self.l_47(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/aten::add24359
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout]
        t_3 = torch.add(input=t_4, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/aten::add24383
        t_4 = self.l_48(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1]
        t_4 = self.l_49(t_4)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24395
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24396
        t_4 = t_4.split(split_size=1600, dim=2)
        t_5 = t_4[0]
        t_2 = t_4[1]
        t_4 = t_4[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243980
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24401
        t_6 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243980
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24405
        t_1 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243980
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24409
        t_12 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24410
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24412
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_1, 25, t_12]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243980
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24416
        t_12 = t_5.view(size=t_12)
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::view24417
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24422
        t_5 = t_12.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243981
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24424
        t_12 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243981
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24428
        t_1 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243981
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24432
        t_6 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24433
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24435
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_12, t_1, 25, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243981
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24439
        t_6 = t_2.view(size=t_6)
        t_2 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::view24440
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24445
        t_2 = t_6.permute(dims=t_2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243982
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24447
        t_6 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243982
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24451
        t_1 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243982
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24455
        t_12 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24456
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24458
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_1, 25, t_12]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListUnpack243982
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24462
        t_12 = t_4.view(size=t_12)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::view24463
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24468
        t_4 = t_12.permute(dims=t_4)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::permute24423
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::permute24446
        t_2 = t_5.matmul(other=t_2)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::matmul24470
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24471
        t_2 = torch.div(input=t_2, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::div24472
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24473
        t_5 = t_2.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::div24472
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24476
        t_12 = t_2.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24477
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24474
        t_5 = torch.sub(input=t_12, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24484
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24485
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24486
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24487
        t_1 = self.b_5[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::slice24488
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24489
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24490
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24491
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24492
        t_1 = t_1[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::slice24493
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24494
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::sub24482
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24477
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24495
        t_5 = t_1[:, :, t_5:t_12:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::slice24496
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24497
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24498
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24477
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24499
        t_12 = t_5[:, :, :, 0:t_12:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::div24472
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::slice24500
        t_2 = torch.mul(input=t_2, other=t_12)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::slice24500
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24502
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24503
        t_12 = torch.rsub(t_12, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::rsub24504
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24505
        t_12 = torch.mul(input=t_12, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::mul24501
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::mul24506
        t_12 = torch.sub(input=t_2, other=t_12)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::sub24508
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24509
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24510
        t_12 = t_12.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::softmax24511
        t_12 = self.l_50(t_12)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::permute24469
        t_4 = t_12.matmul(other=t_4)
        t_12 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::matmul24513
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24518
        t_12 = t_4.permute(dims=t_12)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::permute24519
        t_12 = t_12.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::contiguous24521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24522
        t_4 = t_12.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::contiguous24521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24526
        t_2 = t_12.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::contiguous24521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24530
        t_5 = t_12.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::contiguous24521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::Constant24533
        t_1 = t_12.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24531
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::size24534
        t_1 = torch.mul(input=t_5, other=t_1)
        t_1 = [t_4, t_2, t_1]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::contiguous24521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/prim::ListConstruct24538
        t_1 = t_12.view(size=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/aten::view24539
        t_1 = self.l_51(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Conv1D[c_proj]
        t_1 = self.l_52(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/aten::add24383
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]/Dropout[resid_dropout]
        t_1 = torch.add(input=t_3, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/aten::add24543
        t_3 = self.l_53(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2]
        t_3 = self.l_54(t_3)
        # returing:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/aten::add24543
        return (t_3, t_1)

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition5(nn.Module):
    def __init__(self, layers, tensors):
        super(Partition5, self).__init__()
        # initializing partition layers
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_0, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_0)}'
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_1, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_1)}'
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_2, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_2)}'
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_3, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_3)}'
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_4, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_4)}'
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_5, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_5)}'
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_6, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_6)}'
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_7, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_7)}'
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_8, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_8)}'
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_9, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_9)}'
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_10, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_10)}'
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_11, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_11)}'
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_12, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_12)}'
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_13, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_13)}'
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_14, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_14)}'
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_15, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_15)}'
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_16, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_16)}'
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_17, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_17)}'
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_18, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_18)}'
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_19, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_19)}'
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_20, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_20)}'
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_21, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_21)}'
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_22, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_22)}'
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_23, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_23)}'
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_24, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_24)}'
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_25, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_25)}'
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_26, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_26)}'
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_27, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_27)}'
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_28, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_28)}'
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_29, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_29)}'
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_30, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_30)}'
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_31, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_31)}'
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_32, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_32)}'
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_33, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_33)}'
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_34, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_34)}'
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_35, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_35)}'
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_36, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_36)}'
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_37, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_37)}'
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_38, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_38)}'
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_39, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_39)}'
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_40, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_40)}'
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_41, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_41)}'
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_42, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_42)}'
        self.l_43 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_43, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_43)}'
        self.l_44 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_44, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_44)}'
        self.l_45 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_45, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_45)}'
        self.l_46 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_46, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_46)}'
        self.l_47 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_47, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_47)}'
        self.l_48 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_48, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_48)}'
        self.l_49 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_49, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_49)}'
        self.l_50 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_50, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_50)}'
        self.l_51 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_51, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_51)}'
        self.l_52 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_52, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_52)}'
        self.l_53 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_53, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_53)}'
        self.l_54 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_54, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_54)}'
        self.l_55 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_55, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_55)}'

        # initializing partition buffers
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_4', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_5', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Tensor[bias]']
        )

        # initializing partition parameters

        self.device = torch.device('cuda:5')
        self.lookup = {
            'l_0': 'transformer.blocks.30.mlp.c_proj',
            'l_1': 'transformer.blocks.30.mlp.dropout',
            'l_2': 'transformer.blocks.31.ln_1',
            'l_3': 'transformer.blocks.31.attn.c_attn',
            'l_4': 'transformer.blocks.31.attn.attn_dropout',
            'l_5': 'transformer.blocks.31.attn.c_proj',
            'l_6': 'transformer.blocks.31.attn.resid_dropout',
            'l_7': 'transformer.blocks.31.ln_2',
            'l_8': 'transformer.blocks.31.mlp.c_fc',
            'l_9': 'transformer.blocks.31.mlp.c_proj',
            'l_10': 'transformer.blocks.31.mlp.dropout',
            'l_11': 'transformer.blocks.32.ln_1',
            'l_12': 'transformer.blocks.32.attn.c_attn',
            'l_13': 'transformer.blocks.32.attn.attn_dropout',
            'l_14': 'transformer.blocks.32.attn.c_proj',
            'l_15': 'transformer.blocks.32.attn.resid_dropout',
            'l_16': 'transformer.blocks.32.ln_2',
            'l_17': 'transformer.blocks.32.mlp.c_fc',
            'l_18': 'transformer.blocks.32.mlp.c_proj',
            'l_19': 'transformer.blocks.32.mlp.dropout',
            'l_20': 'transformer.blocks.33.ln_1',
            'l_21': 'transformer.blocks.33.attn.c_attn',
            'l_22': 'transformer.blocks.33.attn.attn_dropout',
            'l_23': 'transformer.blocks.33.attn.c_proj',
            'l_24': 'transformer.blocks.33.attn.resid_dropout',
            'l_25': 'transformer.blocks.33.ln_2',
            'l_26': 'transformer.blocks.33.mlp.c_fc',
            'l_27': 'transformer.blocks.33.mlp.c_proj',
            'l_28': 'transformer.blocks.33.mlp.dropout',
            'l_29': 'transformer.blocks.34.ln_1',
            'l_30': 'transformer.blocks.34.attn.c_attn',
            'l_31': 'transformer.blocks.34.attn.attn_dropout',
            'l_32': 'transformer.blocks.34.attn.c_proj',
            'l_33': 'transformer.blocks.34.attn.resid_dropout',
            'l_34': 'transformer.blocks.34.ln_2',
            'l_35': 'transformer.blocks.34.mlp.c_fc',
            'l_36': 'transformer.blocks.34.mlp.c_proj',
            'l_37': 'transformer.blocks.34.mlp.dropout',
            'l_38': 'transformer.blocks.35.ln_1',
            'l_39': 'transformer.blocks.35.attn.c_attn',
            'l_40': 'transformer.blocks.35.attn.attn_dropout',
            'l_41': 'transformer.blocks.35.attn.c_proj',
            'l_42': 'transformer.blocks.35.attn.resid_dropout',
            'l_43': 'transformer.blocks.35.ln_2',
            'l_44': 'transformer.blocks.35.mlp.c_fc',
            'l_45': 'transformer.blocks.35.mlp.c_proj',
            'l_46': 'transformer.blocks.35.mlp.dropout',
            'l_47': 'transformer.blocks.36.ln_1',
            'l_48': 'transformer.blocks.36.attn.c_attn',
            'l_49': 'transformer.blocks.36.attn.attn_dropout',
            'l_50': 'transformer.blocks.36.attn.c_proj',
            'l_51': 'transformer.blocks.36.attn.resid_dropout',
            'l_52': 'transformer.blocks.36.ln_2',
            'l_53': 'transformer.blocks.36.mlp.c_fc',
            'l_54': 'transformer.blocks.36.mlp.c_proj',
            'l_55': 'transformer.blocks.36.mlp.dropout',
            'b_0': 'transformer.blocks.31.attn.bias',
            'b_1': 'transformer.blocks.32.attn.bias',
            'b_2': 'transformer.blocks.33.attn.bias',
            'b_3': 'transformer.blocks.34.attn.bias',
            'b_4': 'transformer.blocks.35.attn.bias',
            'b_5': 'transformer.blocks.36.attn.bias'
        }

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_attn] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[attn_dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_proj] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[resid_dropout] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_attn] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[attn_dropout] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_proj] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[resid_dropout] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_attn] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[attn_dropout] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_proj] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[resid_dropout] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_attn] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[attn_dropout] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_proj] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[resid_dropout] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_attn] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[attn_dropout] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_proj] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[resid_dropout] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_attn] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[attn_dropout] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_proj] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[resid_dropout] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout] <=> self.l_55
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc] <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/aten::add24543 <=> x1

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)

        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/prim::Constant24549
        t_0 = torch.mul(input=x0, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/prim::Constant24551
        t_1 = x0.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::pow24552
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/prim::Constant24553
        t_1 = torch.mul(input=t_1, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::mul24554
        t_1 = torch.add(input=x0, other=t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::add24556
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/prim::Constant24557
        t_1 = torch.mul(input=t_1, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::mul24558
        t_1 = t_1.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::tanh24559
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/prim::Constant24560
        t_1 = torch.add(input=t_1, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::mul24550
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::add24562
        t_1 = torch.mul(input=t_0, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/aten::mul24563
        t_1 = self.l_0(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj]
        t_1 = self.l_1(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/aten::add24543
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout]
        t_1 = torch.add(input=x1, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/aten::add24567
        t_0 = self.l_2(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1]
        t_0 = self.l_3(t_0)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24579
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24580
        t_0 = t_0.split(split_size=1600, dim=2)
        t_3 = t_0[0]
        t_4 = t_0[1]
        t_0 = t_0[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245820
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24585
        t_5 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245820
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24589
        t_6 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245820
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24593
        t_7 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24594
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24596
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245820
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24600
        t_7 = t_3.view(size=t_7)
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::view24601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24606
        t_3 = t_7.permute(dims=t_3)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245821
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24608
        t_7 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245821
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24612
        t_6 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245821
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24616
        t_5 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24617
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24619
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_7, t_6, 25, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245821
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24623
        t_5 = t_4.view(size=t_5)
        t_4 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::view24624
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24629
        t_4 = t_5.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245822
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24631
        t_5 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245822
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24635
        t_6 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245822
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24639
        t_7 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24640
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24642
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListUnpack245822
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24646
        t_7 = t_0.view(size=t_7)
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::view24647
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24652
        t_0 = t_7.permute(dims=t_0)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::permute24607
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::permute24630
        t_4 = t_3.matmul(other=t_4)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::matmul24654
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24655
        t_4 = torch.div(input=t_4, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::div24656
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24657
        t_3 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::div24656
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24660
        t_7 = t_4.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24661
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24658
        t_3 = torch.sub(input=t_7, other=t_3)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24668
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24669
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24670
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24671
        t_6 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::slice24672
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24673
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24674
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24675
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24676
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::slice24677
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24678
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::sub24666
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24661
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24679
        t_3 = t_6[:, :, t_3:t_7:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::slice24680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24681
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24682
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24661
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24683
        t_7 = t_3[:, :, :, 0:t_7:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::div24656
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::slice24684
        t_4 = torch.mul(input=t_4, other=t_7)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::slice24684
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24686
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24687
        t_7 = torch.rsub(t_7, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::rsub24688
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24689
        t_7 = torch.mul(input=t_7, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::mul24685
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::mul24690
        t_7 = torch.sub(input=t_4, other=t_7)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::sub24692
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24693
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24694
        t_7 = t_7.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::softmax24695
        t_7 = self.l_4(t_7)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::permute24653
        t_0 = t_7.matmul(other=t_0)
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::matmul24697
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24702
        t_7 = t_0.permute(dims=t_7)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::permute24703
        t_7 = t_7.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::contiguous24705
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24706
        t_0 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::contiguous24705
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24710
        t_4 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::contiguous24705
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24714
        t_3 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::contiguous24705
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::Constant24717
        t_6 = t_7.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24715
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::size24718
        t_6 = torch.mul(input=t_3, other=t_6)
        t_6 = [t_0, t_4, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::contiguous24705
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/prim::ListConstruct24722
        t_6 = t_7.view(size=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/aten::view24723
        t_6 = self.l_5(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_6(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/aten::add24567
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_1, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/aten::add24727
        t_1 = self.l_7(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2]
        t_1 = self.l_8(t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/prim::Constant24733
        t_7 = torch.mul(input=t_1, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/prim::Constant24735
        t_4 = t_1.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::pow24736
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/prim::Constant24737
        t_4 = torch.mul(input=t_4, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::mul24738
        t_4 = torch.add(input=t_1, other=t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::add24740
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/prim::Constant24741
        t_4 = torch.mul(input=t_4, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::mul24742
        t_4 = t_4.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::tanh24743
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/prim::Constant24744
        t_4 = torch.add(input=t_4, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::mul24734
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::add24746
        t_4 = torch.mul(input=t_7, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/aten::mul24747
        t_4 = self.l_9(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj]
        t_4 = self.l_10(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/aten::add24727
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout]
        t_4 = torch.add(input=t_6, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/aten::add24751
        t_6 = self.l_11(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1]
        t_6 = self.l_12(t_6)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24763
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24764
        t_6 = t_6.split(split_size=1600, dim=2)
        t_1 = t_6[0]
        t_0 = t_6[1]
        t_6 = t_6[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247660
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24769
        t_3 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247660
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24773
        t_5 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247660
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24777
        t_8 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24778
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24780
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247660
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24784
        t_8 = t_1.view(size=t_8)
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::view24785
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24790
        t_1 = t_8.permute(dims=t_1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247661
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24792
        t_8 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247661
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24796
        t_5 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247661
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24800
        t_3 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24801
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24803
        t_3 = torch.div(input=t_3, other=25)
        t_3 = [t_8, t_5, 25, t_3]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247661
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24807
        t_3 = t_0.view(size=t_3)
        t_0 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::view24808
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24813
        t_0 = t_3.permute(dims=t_0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247662
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24815
        t_3 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247662
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24819
        t_5 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247662
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24823
        t_8 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24824
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24826
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListUnpack247662
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24830
        t_8 = t_6.view(size=t_8)
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::view24831
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24836
        t_6 = t_8.permute(dims=t_6)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::permute24791
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::permute24814
        t_0 = t_1.matmul(other=t_0)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::matmul24838
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24839
        t_0 = torch.div(input=t_0, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::div24840
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24841
        t_1 = t_0.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::div24840
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24844
        t_8 = t_0.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24845
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24842
        t_1 = torch.sub(input=t_8, other=t_1)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24852
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24853
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24854
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24855
        t_5 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::slice24856
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24857
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24858
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24859
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24860
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::slice24861
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24862
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::sub24850
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24845
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24863
        t_1 = t_5[:, :, t_1:t_8:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::slice24864
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24865
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24866
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24845
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24867
        t_8 = t_1[:, :, :, 0:t_8:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::div24840
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::slice24868
        t_0 = torch.mul(input=t_0, other=t_8)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::slice24868
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24870
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24871
        t_8 = torch.rsub(t_8, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::rsub24872
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24873
        t_8 = torch.mul(input=t_8, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::mul24869
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::mul24874
        t_8 = torch.sub(input=t_0, other=t_8)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::sub24876
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24877
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24878
        t_8 = t_8.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::softmax24879
        t_8 = self.l_13(t_8)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::permute24837
        t_6 = t_8.matmul(other=t_6)
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::matmul24881
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24886
        t_8 = t_6.permute(dims=t_8)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::permute24887
        t_8 = t_8.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::contiguous24889
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24890
        t_6 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::contiguous24889
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24894
        t_0 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::contiguous24889
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24898
        t_1 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::contiguous24889
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::Constant24901
        t_5 = t_8.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24899
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::size24902
        t_5 = torch.mul(input=t_1, other=t_5)
        t_5 = [t_6, t_0, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::contiguous24889
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/prim::ListConstruct24906
        t_5 = t_8.view(size=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/aten::view24907
        t_5 = self.l_14(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_15(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/aten::add24751
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=t_4, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/aten::add24911
        t_4 = self.l_16(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2]
        t_4 = self.l_17(t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/prim::Constant24917
        t_8 = torch.mul(input=t_4, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/prim::Constant24919
        t_0 = t_4.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::pow24920
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/prim::Constant24921
        t_0 = torch.mul(input=t_0, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::mul24922
        t_0 = torch.add(input=t_4, other=t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::add24924
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/prim::Constant24925
        t_0 = torch.mul(input=t_0, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::mul24926
        t_0 = t_0.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::tanh24927
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/prim::Constant24928
        t_0 = torch.add(input=t_0, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::mul24918
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::add24930
        t_0 = torch.mul(input=t_8, other=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/aten::mul24931
        t_0 = self.l_18(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj]
        t_0 = self.l_19(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/aten::add24911
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout]
        t_0 = torch.add(input=t_5, other=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/aten::add24935
        t_5 = self.l_20(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1]
        t_5 = self.l_21(t_5)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24947
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24948
        t_5 = t_5.split(split_size=1600, dim=2)
        t_4 = t_5[0]
        t_6 = t_5[1]
        t_5 = t_5[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249500
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24953
        t_1 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249500
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24957
        t_3 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249500
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24961
        t_9 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size24962
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24964
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_1, t_3, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249500
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct24968
        t_9 = t_4.view(size=t_9)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::view24969
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct24974
        t_4 = t_9.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249501
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24976
        t_9 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249501
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24980
        t_3 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249501
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24984
        t_1 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size24985
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24987
        t_1 = torch.div(input=t_1, other=25)
        t_1 = [t_9, t_3, 25, t_1]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249501
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct24991
        t_1 = t_6.view(size=t_1)
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::view24992
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct24997
        t_6 = t_1.permute(dims=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249502
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant24999
        t_1 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249502
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25003
        t_3 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249502
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25007
        t_9 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size25008
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25010
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_1, t_3, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListUnpack249502
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct25014
        t_9 = t_5.view(size=t_9)
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::view25015
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct25020
        t_5 = t_9.permute(dims=t_5)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::permute24975
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::permute24998
        t_6 = t_4.matmul(other=t_6)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::matmul25022
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25023
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::div25024
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25025
        t_4 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::div25024
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25028
        t_9 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size25029
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size25026
        t_4 = torch.sub(input=t_9, other=t_4)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25036
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25037
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25038
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25039
        t_3 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::slice25040
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25041
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25042
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25043
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25044
        t_3 = t_3[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::slice25045
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25046
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::sub25034
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size25029
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25047
        t_4 = t_3[:, :, t_4:t_9:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::slice25048
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25049
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25050
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size25029
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25051
        t_9 = t_4[:, :, :, 0:t_9:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::div25024
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::slice25052
        t_6 = torch.mul(input=t_6, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::slice25052
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25054
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25055
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::rsub25056
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25057
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::mul25053
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::mul25058
        t_9 = torch.sub(input=t_6, other=t_9)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::sub25060
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25061
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25062
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::softmax25063
        t_9 = self.l_22(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::permute25021
        t_5 = t_9.matmul(other=t_5)
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::matmul25065
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct25070
        t_9 = t_5.permute(dims=t_9)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::permute25071
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::contiguous25073
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25074
        t_5 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::contiguous25073
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25078
        t_6 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::contiguous25073
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25082
        t_4 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::contiguous25073
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::Constant25085
        t_3 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size25083
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::size25086
        t_3 = torch.mul(input=t_4, other=t_3)
        t_3 = [t_5, t_6, t_3]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::contiguous25073
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/prim::ListConstruct25090
        t_3 = t_9.view(size=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/aten::view25091
        t_3 = self.l_23(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Conv1D[c_proj]
        t_3 = self.l_24(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/aten::add24935
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]/Dropout[resid_dropout]
        t_3 = torch.add(input=t_0, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/aten::add25095
        t_0 = self.l_25(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2]
        t_0 = self.l_26(t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/prim::Constant25101
        t_9 = torch.mul(input=t_0, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/prim::Constant25103
        t_6 = t_0.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::pow25104
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/prim::Constant25105
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::mul25106
        t_6 = torch.add(input=t_0, other=t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::add25108
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/prim::Constant25109
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::mul25110
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::tanh25111
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/prim::Constant25112
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::mul25102
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::add25114
        t_6 = torch.mul(input=t_9, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/aten::mul25115
        t_6 = self.l_27(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj]
        t_6 = self.l_28(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/aten::add25095
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout]
        t_6 = torch.add(input=t_3, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/aten::add25119
        t_3 = self.l_29(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1]
        t_3 = self.l_30(t_3)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25131
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25132
        t_3 = t_3.split(split_size=1600, dim=2)
        t_0 = t_3[0]
        t_5 = t_3[1]
        t_3 = t_3[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251340
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25137
        t_4 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251340
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25141
        t_1 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251340
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25145
        t_10 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25146
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25148
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_1, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251340
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25152
        t_10 = t_0.view(size=t_10)
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::view25153
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25158
        t_0 = t_10.permute(dims=t_0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251341
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25160
        t_10 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251341
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25164
        t_1 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251341
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25168
        t_4 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25169
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25171
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_10, t_1, 25, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251341
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25175
        t_4 = t_5.view(size=t_4)
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::view25176
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25181
        t_5 = t_4.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251342
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25183
        t_4 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251342
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25187
        t_1 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251342
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25191
        t_10 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25192
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25194
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_1, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListUnpack251342
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25198
        t_10 = t_3.view(size=t_10)
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::view25199
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25204
        t_3 = t_10.permute(dims=t_3)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::permute25159
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::permute25182
        t_5 = t_0.matmul(other=t_5)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::matmul25206
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25207
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::div25208
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25209
        t_0 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::div25208
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25212
        t_10 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25213
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25210
        t_0 = torch.sub(input=t_10, other=t_0)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25220
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25221
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25222
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25223
        t_1 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::slice25224
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25225
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25226
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25227
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25228
        t_1 = t_1[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::slice25229
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25230
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::sub25218
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25213
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25231
        t_0 = t_1[:, :, t_0:t_10:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::slice25232
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25233
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25234
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25213
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25235
        t_10 = t_0[:, :, :, 0:t_10:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::div25208
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::slice25236
        t_5 = torch.mul(input=t_5, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::slice25236
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25238
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25239
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::rsub25240
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25241
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::mul25237
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::mul25242
        t_10 = torch.sub(input=t_5, other=t_10)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::sub25244
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25245
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25246
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::softmax25247
        t_10 = self.l_31(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::permute25205
        t_3 = t_10.matmul(other=t_3)
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::matmul25249
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25254
        t_10 = t_3.permute(dims=t_10)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::permute25255
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::contiguous25257
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25258
        t_3 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::contiguous25257
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25262
        t_5 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::contiguous25257
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25266
        t_0 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::contiguous25257
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::Constant25269
        t_1 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25267
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::size25270
        t_1 = torch.mul(input=t_0, other=t_1)
        t_1 = [t_3, t_5, t_1]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::contiguous25257
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/prim::ListConstruct25274
        t_1 = t_10.view(size=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/aten::view25275
        t_1 = self.l_32(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Conv1D[c_proj]
        t_1 = self.l_33(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/aten::add25119
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]/Dropout[resid_dropout]
        t_1 = torch.add(input=t_6, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/aten::add25279
        t_6 = self.l_34(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2]
        t_6 = self.l_35(t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/prim::Constant25285
        t_10 = torch.mul(input=t_6, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/prim::Constant25287
        t_5 = t_6.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::pow25288
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/prim::Constant25289
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::mul25290
        t_5 = torch.add(input=t_6, other=t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::add25292
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/prim::Constant25293
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::mul25294
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::tanh25295
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/prim::Constant25296
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::mul25286
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::add25298
        t_5 = torch.mul(input=t_10, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/aten::mul25299
        t_5 = self.l_36(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj]
        t_5 = self.l_37(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/aten::add25279
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout]
        t_5 = torch.add(input=t_1, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/aten::add25303
        t_1 = self.l_38(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1]
        t_1 = self.l_39(t_1)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25315
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25316
        t_1 = t_1.split(split_size=1600, dim=2)
        t_6 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253180
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25321
        t_0 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253180
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25325
        t_4 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253180
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25329
        t_11 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25330
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25332
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_0, t_4, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253180
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25336
        t_11 = t_6.view(size=t_11)
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::view25337
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25342
        t_6 = t_11.permute(dims=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253181
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25344
        t_11 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253181
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25348
        t_4 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253181
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25352
        t_0 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25353
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25355
        t_0 = torch.div(input=t_0, other=25)
        t_0 = [t_11, t_4, 25, t_0]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253181
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25359
        t_0 = t_3.view(size=t_0)
        t_3 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::view25360
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25365
        t_3 = t_0.permute(dims=t_3)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253182
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25367
        t_0 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253182
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25371
        t_4 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253182
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25375
        t_11 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25376
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25378
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_0, t_4, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListUnpack253182
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25382
        t_11 = t_1.view(size=t_11)
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::view25383
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25388
        t_1 = t_11.permute(dims=t_1)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::permute25343
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::permute25366
        t_3 = t_6.matmul(other=t_3)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::matmul25390
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25391
        t_3 = torch.div(input=t_3, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::div25392
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25393
        t_6 = t_3.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::div25392
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25396
        t_11 = t_3.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25397
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25394
        t_6 = torch.sub(input=t_11, other=t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25404
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25405
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25406
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25407
        t_4 = self.b_4[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::slice25408
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25409
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25410
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25411
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25412
        t_4 = t_4[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::slice25413
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25414
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::sub25402
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25397
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25415
        t_6 = t_4[:, :, t_6:t_11:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::slice25416
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25417
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25418
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25397
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25419
        t_11 = t_6[:, :, :, 0:t_11:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::div25392
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::slice25420
        t_3 = torch.mul(input=t_3, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::slice25420
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25422
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25423
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::rsub25424
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25425
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::mul25421
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::mul25426
        t_11 = torch.sub(input=t_3, other=t_11)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::sub25428
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25429
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25430
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::softmax25431
        t_11 = self.l_40(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::permute25389
        t_1 = t_11.matmul(other=t_1)
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::matmul25433
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25438
        t_11 = t_1.permute(dims=t_11)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::permute25439
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::contiguous25441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25442
        t_1 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::contiguous25441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25446
        t_3 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::contiguous25441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25450
        t_6 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::contiguous25441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::Constant25453
        t_4 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25451
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::size25454
        t_4 = torch.mul(input=t_6, other=t_4)
        t_4 = [t_1, t_3, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::contiguous25441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/prim::ListConstruct25458
        t_4 = t_11.view(size=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/aten::view25459
        t_4 = self.l_41(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_42(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/aten::add25303
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=t_5, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/aten::add25463
        t_5 = self.l_43(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2]
        t_5 = self.l_44(t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/prim::Constant25469
        t_11 = torch.mul(input=t_5, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/prim::Constant25471
        t_3 = t_5.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::pow25472
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/prim::Constant25473
        t_3 = torch.mul(input=t_3, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::mul25474
        t_3 = torch.add(input=t_5, other=t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::add25476
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/prim::Constant25477
        t_3 = torch.mul(input=t_3, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::mul25478
        t_3 = t_3.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::tanh25479
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/prim::Constant25480
        t_3 = torch.add(input=t_3, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::mul25470
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::add25482
        t_3 = torch.mul(input=t_11, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/aten::mul25483
        t_3 = self.l_45(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj]
        t_3 = self.l_46(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/aten::add25463
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout]
        t_3 = torch.add(input=t_4, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/aten::add25487
        t_4 = self.l_47(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1]
        t_4 = self.l_48(t_4)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25499
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25500
        t_4 = t_4.split(split_size=1600, dim=2)
        t_5 = t_4[0]
        t_1 = t_4[1]
        t_4 = t_4[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack255020
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25505
        t_6 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack255020
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25509
        t_0 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack255020
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25513
        t_12 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25514
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25516
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_0, 25, t_12]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack255020
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25520
        t_12 = t_5.view(size=t_12)
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::view25521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25526
        t_5 = t_12.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack255021
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25528
        t_12 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack255021
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25532
        t_0 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack255021
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25536
        t_6 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25537
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25539
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_12, t_0, 25, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack255021
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25543
        t_6 = t_1.view(size=t_6)
        t_1 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::view25544
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25549
        t_1 = t_6.permute(dims=t_1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack255022
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25551
        t_6 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack255022
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25555
        t_0 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack255022
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25559
        t_12 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25560
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25562
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_0, 25, t_12]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListUnpack255022
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25566
        t_12 = t_4.view(size=t_12)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::view25567
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25572
        t_4 = t_12.permute(dims=t_4)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::permute25527
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::permute25550
        t_1 = t_5.matmul(other=t_1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::matmul25574
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25575
        t_1 = torch.div(input=t_1, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::div25576
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25577
        t_5 = t_1.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::div25576
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25580
        t_12 = t_1.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25581
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25578
        t_5 = torch.sub(input=t_12, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25588
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25589
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25590
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25591
        t_0 = self.b_5[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::slice25592
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25593
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25594
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25595
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25596
        t_0 = t_0[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::slice25597
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25598
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::sub25586
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25581
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25599
        t_5 = t_0[:, :, t_5:t_12:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::slice25600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25601
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25602
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25581
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25603
        t_12 = t_5[:, :, :, 0:t_12:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::div25576
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::slice25604
        t_1 = torch.mul(input=t_1, other=t_12)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::slice25604
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25606
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25607
        t_12 = torch.rsub(t_12, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::rsub25608
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25609
        t_12 = torch.mul(input=t_12, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::mul25605
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::mul25610
        t_12 = torch.sub(input=t_1, other=t_12)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::sub25612
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25613
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25614
        t_12 = t_12.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::softmax25615
        t_12 = self.l_49(t_12)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::permute25573
        t_4 = t_12.matmul(other=t_4)
        t_12 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::matmul25617
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25622
        t_12 = t_4.permute(dims=t_12)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::permute25623
        t_12 = t_12.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::contiguous25625
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25626
        t_4 = t_12.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::contiguous25625
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25630
        t_1 = t_12.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::contiguous25625
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25634
        t_5 = t_12.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::contiguous25625
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::Constant25637
        t_0 = t_12.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25635
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::size25638
        t_0 = torch.mul(input=t_5, other=t_0)
        t_0 = [t_4, t_1, t_0]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::contiguous25625
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/prim::ListConstruct25642
        t_0 = t_12.view(size=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/aten::view25643
        t_0 = self.l_50(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Conv1D[c_proj]
        t_0 = self.l_51(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/aten::add25487
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]/Dropout[resid_dropout]
        t_0 = torch.add(input=t_3, other=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/aten::add25647
        t_3 = self.l_52(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2]
        t_3 = self.l_53(t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/prim::Constant25653
        t_12 = torch.mul(input=t_3, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/prim::Constant25655
        t_1 = t_3.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::pow25656
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/prim::Constant25657
        t_1 = torch.mul(input=t_1, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::mul25658
        t_1 = torch.add(input=t_3, other=t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::add25660
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/prim::Constant25661
        t_1 = torch.mul(input=t_1, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::mul25662
        t_1 = t_1.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::tanh25663
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/prim::Constant25664
        t_1 = torch.add(input=t_1, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::mul25654
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::add25666
        t_1 = torch.mul(input=t_12, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/aten::mul25667
        t_1 = self.l_54(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj]
        t_1 = self.l_55(t_1)
        # returing:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/aten::add25647
        return (t_1, t_0)

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition6(nn.Module):
    def __init__(self, layers, tensors):
        super(Partition6, self).__init__()
        # initializing partition layers
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_0, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_0)}'
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_1, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_1)}'
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_2, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_2)}'
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_3, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_3)}'
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_4, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_4)}'
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_5, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_5)}'
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_6, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_6)}'
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_7, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_7)}'
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_8, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_8)}'
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_9, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_9)}'
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_10, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_10)}'
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_11, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_11)}'
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_12, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_12)}'
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_13, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_13)}'
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_14, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_14)}'
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_15, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_15)}'
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_16, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_16)}'
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_17, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_17)}'
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_18, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_18)}'
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_19, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_19)}'
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_20, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_20)}'
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_21, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_21)}'
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_22, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_22)}'
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_23, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_23)}'
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_24, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_24)}'
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_25, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_25)}'
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_26, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_26)}'
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_27, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_27)}'
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_28, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_28)}'
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_29, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_29)}'
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_30, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_30)}'
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_31, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_31)}'
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_32, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_32)}'
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_33, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_33)}'
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_34, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_34)}'
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_35, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_35)}'
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_36, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_36)}'
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_37, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_37)}'
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_38, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_38)}'
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_39, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_39)}'
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_40, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_40)}'
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_41, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_41)}'
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_42, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_42)}'
        self.l_43 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_43, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_43)}'
        self.l_44 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_44, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_44)}'
        self.l_45 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_45, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_45)}'
        self.l_46 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_46, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_46)}'
        self.l_47 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_47, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_47)}'
        self.l_48 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_48, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_48)}'
        self.l_49 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_49, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_49)}'
        self.l_50 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_50, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_50)}'
        self.l_51 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_51, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_51)}'
        self.l_52 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_52, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_52)}'
        self.l_53 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_53, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_53)}'
        self.l_54 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_54, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_54)}'
        self.l_55 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_55, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_55)}'
        self.l_56 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_56, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_56)}'

        # initializing partition buffers
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_4', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_5', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_6', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Tensor[bias]']
        )

        # initializing partition parameters

        self.device = torch.device('cuda:6')
        self.lookup = {
            'l_0': 'transformer.blocks.37.ln_1',
            'l_1': 'transformer.blocks.37.attn.c_attn',
            'l_2': 'transformer.blocks.37.attn.attn_dropout',
            'l_3': 'transformer.blocks.37.attn.c_proj',
            'l_4': 'transformer.blocks.37.attn.resid_dropout',
            'l_5': 'transformer.blocks.37.ln_2',
            'l_6': 'transformer.blocks.37.mlp.c_fc',
            'l_7': 'transformer.blocks.37.mlp.c_proj',
            'l_8': 'transformer.blocks.37.mlp.dropout',
            'l_9': 'transformer.blocks.38.ln_1',
            'l_10': 'transformer.blocks.38.attn.c_attn',
            'l_11': 'transformer.blocks.38.attn.attn_dropout',
            'l_12': 'transformer.blocks.38.attn.c_proj',
            'l_13': 'transformer.blocks.38.attn.resid_dropout',
            'l_14': 'transformer.blocks.38.ln_2',
            'l_15': 'transformer.blocks.38.mlp.c_fc',
            'l_16': 'transformer.blocks.38.mlp.c_proj',
            'l_17': 'transformer.blocks.38.mlp.dropout',
            'l_18': 'transformer.blocks.39.ln_1',
            'l_19': 'transformer.blocks.39.attn.c_attn',
            'l_20': 'transformer.blocks.39.attn.attn_dropout',
            'l_21': 'transformer.blocks.39.attn.c_proj',
            'l_22': 'transformer.blocks.39.attn.resid_dropout',
            'l_23': 'transformer.blocks.39.ln_2',
            'l_24': 'transformer.blocks.39.mlp.c_fc',
            'l_25': 'transformer.blocks.39.mlp.c_proj',
            'l_26': 'transformer.blocks.39.mlp.dropout',
            'l_27': 'transformer.blocks.40.ln_1',
            'l_28': 'transformer.blocks.40.attn.c_attn',
            'l_29': 'transformer.blocks.40.attn.attn_dropout',
            'l_30': 'transformer.blocks.40.attn.c_proj',
            'l_31': 'transformer.blocks.40.attn.resid_dropout',
            'l_32': 'transformer.blocks.40.ln_2',
            'l_33': 'transformer.blocks.40.mlp.c_fc',
            'l_34': 'transformer.blocks.40.mlp.c_proj',
            'l_35': 'transformer.blocks.40.mlp.dropout',
            'l_36': 'transformer.blocks.41.ln_1',
            'l_37': 'transformer.blocks.41.attn.c_attn',
            'l_38': 'transformer.blocks.41.attn.attn_dropout',
            'l_39': 'transformer.blocks.41.attn.c_proj',
            'l_40': 'transformer.blocks.41.attn.resid_dropout',
            'l_41': 'transformer.blocks.41.ln_2',
            'l_42': 'transformer.blocks.41.mlp.c_fc',
            'l_43': 'transformer.blocks.41.mlp.c_proj',
            'l_44': 'transformer.blocks.41.mlp.dropout',
            'l_45': 'transformer.blocks.42.ln_1',
            'l_46': 'transformer.blocks.42.attn.c_attn',
            'l_47': 'transformer.blocks.42.attn.attn_dropout',
            'l_48': 'transformer.blocks.42.attn.c_proj',
            'l_49': 'transformer.blocks.42.attn.resid_dropout',
            'l_50': 'transformer.blocks.42.ln_2',
            'l_51': 'transformer.blocks.42.mlp.c_fc',
            'l_52': 'transformer.blocks.42.mlp.c_proj',
            'l_53': 'transformer.blocks.42.mlp.dropout',
            'l_54': 'transformer.blocks.43.ln_1',
            'l_55': 'transformer.blocks.43.attn.c_attn',
            'l_56': 'transformer.blocks.43.attn.attn_dropout',
            'b_0': 'transformer.blocks.37.attn.bias',
            'b_1': 'transformer.blocks.38.attn.bias',
            'b_2': 'transformer.blocks.39.attn.bias',
            'b_3': 'transformer.blocks.40.attn.bias',
            'b_4': 'transformer.blocks.41.attn.bias',
            'b_5': 'transformer.blocks.42.attn.bias',
            'b_6': 'transformer.blocks.43.attn.bias'
        }

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_attn] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[attn_dropout] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_proj] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[resid_dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_attn] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[attn_dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_proj] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[resid_dropout] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_attn] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[attn_dropout] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_proj] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[resid_dropout] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_attn] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[attn_dropout] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_proj] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[resid_dropout] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_attn] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[attn_dropout] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_proj] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[resid_dropout] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_attn] <=> self.l_46
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[attn_dropout] <=> self.l_47
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_proj] <=> self.l_48
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[resid_dropout] <=> self.l_49
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2] <=> self.l_50
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc] <=> self.l_51
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj] <=> self.l_52
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout] <=> self.l_53
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1] <=> self.l_54
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_attn] <=> self.l_55
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[attn_dropout] <=> self.l_56
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Tensor[bias] <=> self.b_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Tensor[bias] <=> self.b_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Tensor[bias] <=> self.b_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout] <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/aten::add25647 <=> x1

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)

        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/aten::add25647
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout]
        t_0 = torch.add(input=x1, other=x0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/aten::add25671
        t_1 = self.l_0(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1]
        t_1 = self.l_1(t_1)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25683
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25684
        t_1 = t_1.split(split_size=1600, dim=2)
        t_3 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256860
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25689
        t_5 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256860
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25693
        t_6 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256860
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25697
        t_7 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25698
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25700
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256860
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25704
        t_7 = t_3.view(size=t_7)
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::view25705
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25710
        t_3 = t_7.permute(dims=t_3)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256861
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25712
        t_7 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256861
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25716
        t_6 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256861
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25720
        t_5 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25721
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25723
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_7, t_6, 25, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256861
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25727
        t_5 = t_4.view(size=t_5)
        t_4 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::view25728
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25733
        t_4 = t_5.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256862
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25735
        t_5 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256862
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25739
        t_6 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256862
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25743
        t_7 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25744
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25746
        t_7 = torch.div(input=t_7, other=25)
        t_7 = [t_5, t_6, 25, t_7]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListUnpack256862
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25750
        t_7 = t_1.view(size=t_7)
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::view25751
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25756
        t_1 = t_7.permute(dims=t_1)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::permute25711
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::permute25734
        t_4 = t_3.matmul(other=t_4)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::matmul25758
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25759
        t_4 = torch.div(input=t_4, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::div25760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25761
        t_3 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::div25760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25764
        t_7 = t_4.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25765
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25762
        t_3 = torch.sub(input=t_7, other=t_3)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25772
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25773
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25774
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25775
        t_6 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::slice25776
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25777
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25778
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25779
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25780
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::slice25781
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25782
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::sub25770
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25765
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25783
        t_3 = t_6[:, :, t_3:t_7:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::slice25784
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25785
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25786
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25765
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25787
        t_7 = t_3[:, :, :, 0:t_7:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::div25760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::slice25788
        t_4 = torch.mul(input=t_4, other=t_7)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::slice25788
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25790
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25791
        t_7 = torch.rsub(t_7, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::rsub25792
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25793
        t_7 = torch.mul(input=t_7, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::mul25789
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::mul25794
        t_7 = torch.sub(input=t_4, other=t_7)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::sub25796
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25797
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25798
        t_7 = t_7.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::softmax25799
        t_7 = self.l_2(t_7)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::permute25757
        t_1 = t_7.matmul(other=t_1)
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::matmul25801
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25806
        t_7 = t_1.permute(dims=t_7)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::permute25807
        t_7 = t_7.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::contiguous25809
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25810
        t_1 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::contiguous25809
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25814
        t_4 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::contiguous25809
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25818
        t_3 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::contiguous25809
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::Constant25821
        t_6 = t_7.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25819
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::size25822
        t_6 = torch.mul(input=t_3, other=t_6)
        t_6 = [t_1, t_4, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::contiguous25809
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/prim::ListConstruct25826
        t_6 = t_7.view(size=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/aten::view25827
        t_6 = self.l_3(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_4(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/aten::add25671
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_0, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/aten::add25831
        t_0 = self.l_5(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2]
        t_0 = self.l_6(t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/prim::Constant25837
        t_7 = torch.mul(input=t_0, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/prim::Constant25839
        t_4 = t_0.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::pow25840
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/prim::Constant25841
        t_4 = torch.mul(input=t_4, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::mul25842
        t_4 = torch.add(input=t_0, other=t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::add25844
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/prim::Constant25845
        t_4 = torch.mul(input=t_4, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::mul25846
        t_4 = t_4.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::tanh25847
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/prim::Constant25848
        t_4 = torch.add(input=t_4, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::mul25838
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::add25850
        t_4 = torch.mul(input=t_7, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/aten::mul25851
        t_4 = self.l_7(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj]
        t_4 = self.l_8(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/aten::add25831
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout]
        t_4 = torch.add(input=t_6, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/aten::add25855
        t_6 = self.l_9(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1]
        t_6 = self.l_10(t_6)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25867
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25868
        t_6 = t_6.split(split_size=1600, dim=2)
        t_0 = t_6[0]
        t_1 = t_6[1]
        t_6 = t_6[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258700
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25873
        t_3 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258700
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25877
        t_5 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258700
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25881
        t_8 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size25882
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25884
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258700
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct25888
        t_8 = t_0.view(size=t_8)
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::view25889
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct25894
        t_0 = t_8.permute(dims=t_0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258701
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25896
        t_8 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258701
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25900
        t_5 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258701
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25904
        t_3 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size25905
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25907
        t_3 = torch.div(input=t_3, other=25)
        t_3 = [t_8, t_5, 25, t_3]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258701
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct25911
        t_3 = t_1.view(size=t_3)
        t_1 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::view25912
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct25917
        t_1 = t_3.permute(dims=t_1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258702
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25919
        t_3 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258702
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25923
        t_5 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258702
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25927
        t_8 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size25928
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25930
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_3, t_5, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListUnpack258702
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct25934
        t_8 = t_6.view(size=t_8)
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::view25935
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct25940
        t_6 = t_8.permute(dims=t_6)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::permute25895
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::permute25918
        t_1 = t_0.matmul(other=t_1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::matmul25942
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25943
        t_1 = torch.div(input=t_1, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::div25944
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25945
        t_0 = t_1.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::div25944
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25948
        t_8 = t_1.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size25949
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size25946
        t_0 = torch.sub(input=t_8, other=t_0)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25956
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25957
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25958
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25959
        t_5 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::slice25960
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25961
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25962
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25963
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25964
        t_5 = t_5[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::slice25965
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25966
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::sub25954
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size25949
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25967
        t_0 = t_5[:, :, t_0:t_8:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::slice25968
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25969
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25970
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size25949
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25971
        t_8 = t_0[:, :, :, 0:t_8:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::div25944
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::slice25972
        t_1 = torch.mul(input=t_1, other=t_8)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::slice25972
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25974
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25975
        t_8 = torch.rsub(t_8, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::rsub25976
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25977
        t_8 = torch.mul(input=t_8, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::mul25973
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::mul25978
        t_8 = torch.sub(input=t_1, other=t_8)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::sub25980
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25981
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25982
        t_8 = t_8.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::softmax25983
        t_8 = self.l_11(t_8)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::permute25941
        t_6 = t_8.matmul(other=t_6)
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::matmul25985
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct25990
        t_8 = t_6.permute(dims=t_8)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::permute25991
        t_8 = t_8.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::contiguous25993
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25994
        t_6 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::contiguous25993
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant25998
        t_1 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::contiguous25993
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant26002
        t_0 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::contiguous25993
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::Constant26005
        t_5 = t_8.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size26003
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::size26006
        t_5 = torch.mul(input=t_0, other=t_5)
        t_5 = [t_6, t_1, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::contiguous25993
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/prim::ListConstruct26010
        t_5 = t_8.view(size=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/aten::view26011
        t_5 = self.l_12(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Conv1D[c_proj]
        t_5 = self.l_13(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/aten::add25855
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]/Dropout[resid_dropout]
        t_5 = torch.add(input=t_4, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/aten::add26015
        t_4 = self.l_14(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2]
        t_4 = self.l_15(t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/prim::Constant26021
        t_8 = torch.mul(input=t_4, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/prim::Constant26023
        t_1 = t_4.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::pow26024
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/prim::Constant26025
        t_1 = torch.mul(input=t_1, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::mul26026
        t_1 = torch.add(input=t_4, other=t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::add26028
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/prim::Constant26029
        t_1 = torch.mul(input=t_1, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::mul26030
        t_1 = t_1.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::tanh26031
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/prim::Constant26032
        t_1 = torch.add(input=t_1, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::mul26022
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::add26034
        t_1 = torch.mul(input=t_8, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/aten::mul26035
        t_1 = self.l_16(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj]
        t_1 = self.l_17(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/aten::add26015
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout]
        t_1 = torch.add(input=t_5, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/aten::add26039
        t_5 = self.l_18(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1]
        t_5 = self.l_19(t_5)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26051
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26052
        t_5 = t_5.split(split_size=1600, dim=2)
        t_4 = t_5[0]
        t_6 = t_5[1]
        t_5 = t_5[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260540
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26057
        t_0 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260540
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26061
        t_3 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260540
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26065
        t_9 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26066
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26068
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_0, t_3, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260540
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26072
        t_9 = t_4.view(size=t_9)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::view26073
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26078
        t_4 = t_9.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260541
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26080
        t_9 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260541
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26084
        t_3 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260541
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26088
        t_0 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26089
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26091
        t_0 = torch.div(input=t_0, other=25)
        t_0 = [t_9, t_3, 25, t_0]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260541
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26095
        t_0 = t_6.view(size=t_0)
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::view26096
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26101
        t_6 = t_0.permute(dims=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260542
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26103
        t_0 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260542
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26107
        t_3 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260542
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26111
        t_9 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26112
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26114
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_0, t_3, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListUnpack260542
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26118
        t_9 = t_5.view(size=t_9)
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::view26119
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26124
        t_5 = t_9.permute(dims=t_5)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::permute26079
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::permute26102
        t_6 = t_4.matmul(other=t_6)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::matmul26126
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26127
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::div26128
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26129
        t_4 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::div26128
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26132
        t_9 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26133
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26130
        t_4 = torch.sub(input=t_9, other=t_4)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26140
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26141
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26142
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26143
        t_3 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::slice26144
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26145
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26146
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26147
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26148
        t_3 = t_3[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::slice26149
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26150
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::sub26138
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26133
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26151
        t_4 = t_3[:, :, t_4:t_9:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::slice26152
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26153
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26154
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26133
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26155
        t_9 = t_4[:, :, :, 0:t_9:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::div26128
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::slice26156
        t_6 = torch.mul(input=t_6, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::slice26156
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26158
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26159
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::rsub26160
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26161
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::mul26157
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::mul26162
        t_9 = torch.sub(input=t_6, other=t_9)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::sub26164
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26165
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26166
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::softmax26167
        t_9 = self.l_20(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::permute26125
        t_5 = t_9.matmul(other=t_5)
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::matmul26169
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26174
        t_9 = t_5.permute(dims=t_9)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::permute26175
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::contiguous26177
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26178
        t_5 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::contiguous26177
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26182
        t_6 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::contiguous26177
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26186
        t_4 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::contiguous26177
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::Constant26189
        t_3 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26187
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::size26190
        t_3 = torch.mul(input=t_4, other=t_3)
        t_3 = [t_5, t_6, t_3]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::contiguous26177
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/prim::ListConstruct26194
        t_3 = t_9.view(size=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/aten::view26195
        t_3 = self.l_21(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Conv1D[c_proj]
        t_3 = self.l_22(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/aten::add26039
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]/Dropout[resid_dropout]
        t_3 = torch.add(input=t_1, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/aten::add26199
        t_1 = self.l_23(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2]
        t_1 = self.l_24(t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/prim::Constant26205
        t_9 = torch.mul(input=t_1, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/prim::Constant26207
        t_6 = t_1.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::pow26208
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/prim::Constant26209
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::mul26210
        t_6 = torch.add(input=t_1, other=t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::add26212
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/prim::Constant26213
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::mul26214
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::tanh26215
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/prim::Constant26216
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::mul26206
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::add26218
        t_6 = torch.mul(input=t_9, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/aten::mul26219
        t_6 = self.l_25(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj]
        t_6 = self.l_26(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/aten::add26199
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout]
        t_6 = torch.add(input=t_3, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/aten::add26223
        t_3 = self.l_27(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1]
        t_3 = self.l_28(t_3)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26235
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26236
        t_3 = t_3.split(split_size=1600, dim=2)
        t_1 = t_3[0]
        t_5 = t_3[1]
        t_3 = t_3[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262380
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26241
        t_4 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262380
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26245
        t_0 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262380
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26249
        t_10 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26250
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26252
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_0, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262380
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26256
        t_10 = t_1.view(size=t_10)
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::view26257
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26262
        t_1 = t_10.permute(dims=t_1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262381
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26264
        t_10 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262381
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26268
        t_0 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262381
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26272
        t_4 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26273
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26275
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_10, t_0, 25, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262381
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26279
        t_4 = t_5.view(size=t_4)
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::view26280
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26285
        t_5 = t_4.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262382
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26287
        t_4 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262382
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26291
        t_0 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262382
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26295
        t_10 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26296
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26298
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_0, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListUnpack262382
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26302
        t_10 = t_3.view(size=t_10)
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::view26303
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26308
        t_3 = t_10.permute(dims=t_3)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::permute26263
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::permute26286
        t_5 = t_1.matmul(other=t_5)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::matmul26310
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26311
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::div26312
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26313
        t_1 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::div26312
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26316
        t_10 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26317
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26314
        t_1 = torch.sub(input=t_10, other=t_1)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26324
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26325
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26326
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26327
        t_0 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::slice26328
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26329
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26330
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26331
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26332
        t_0 = t_0[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::slice26333
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26334
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::sub26322
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26317
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26335
        t_1 = t_0[:, :, t_1:t_10:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::slice26336
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26337
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26338
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26317
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26339
        t_10 = t_1[:, :, :, 0:t_10:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::div26312
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::slice26340
        t_5 = torch.mul(input=t_5, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::slice26340
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26342
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26343
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::rsub26344
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26345
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::mul26341
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::mul26346
        t_10 = torch.sub(input=t_5, other=t_10)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::sub26348
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26349
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26350
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::softmax26351
        t_10 = self.l_29(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::permute26309
        t_3 = t_10.matmul(other=t_3)
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::matmul26353
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26358
        t_10 = t_3.permute(dims=t_10)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::permute26359
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::contiguous26361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26362
        t_3 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::contiguous26361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26366
        t_5 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::contiguous26361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26370
        t_1 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::contiguous26361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::Constant26373
        t_0 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26371
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::size26374
        t_0 = torch.mul(input=t_1, other=t_0)
        t_0 = [t_3, t_5, t_0]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::contiguous26361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/prim::ListConstruct26378
        t_0 = t_10.view(size=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/aten::view26379
        t_0 = self.l_30(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Conv1D[c_proj]
        t_0 = self.l_31(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/aten::add26223
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]/Dropout[resid_dropout]
        t_0 = torch.add(input=t_6, other=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/aten::add26383
        t_6 = self.l_32(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2]
        t_6 = self.l_33(t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/prim::Constant26389
        t_10 = torch.mul(input=t_6, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/prim::Constant26391
        t_5 = t_6.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::pow26392
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/prim::Constant26393
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::mul26394
        t_5 = torch.add(input=t_6, other=t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::add26396
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/prim::Constant26397
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::mul26398
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::tanh26399
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/prim::Constant26400
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::mul26390
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::add26402
        t_5 = torch.mul(input=t_10, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/aten::mul26403
        t_5 = self.l_34(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj]
        t_5 = self.l_35(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/aten::add26383
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout]
        t_5 = torch.add(input=t_0, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/aten::add26407
        t_0 = self.l_36(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1]
        t_0 = self.l_37(t_0)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26419
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26420
        t_0 = t_0.split(split_size=1600, dim=2)
        t_6 = t_0[0]
        t_3 = t_0[1]
        t_0 = t_0[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264220
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26425
        t_1 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264220
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26429
        t_4 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264220
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26433
        t_11 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26434
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26436
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_1, t_4, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264220
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26440
        t_11 = t_6.view(size=t_11)
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::view26441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26446
        t_6 = t_11.permute(dims=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264221
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26448
        t_11 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264221
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26452
        t_4 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264221
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26456
        t_1 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26457
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26459
        t_1 = torch.div(input=t_1, other=25)
        t_1 = [t_11, t_4, 25, t_1]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264221
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26463
        t_1 = t_3.view(size=t_1)
        t_3 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::view26464
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26469
        t_3 = t_1.permute(dims=t_3)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264222
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26471
        t_1 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264222
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26475
        t_4 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264222
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26479
        t_11 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26480
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26482
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_1, t_4, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListUnpack264222
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26486
        t_11 = t_0.view(size=t_11)
        t_0 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::view26487
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26492
        t_0 = t_11.permute(dims=t_0)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::permute26447
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::permute26470
        t_3 = t_6.matmul(other=t_3)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::matmul26494
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26495
        t_3 = torch.div(input=t_3, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::div26496
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26497
        t_6 = t_3.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::div26496
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26500
        t_11 = t_3.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26501
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26498
        t_6 = torch.sub(input=t_11, other=t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26508
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26509
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26510
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26511
        t_4 = self.b_4[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::slice26512
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26513
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26514
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26515
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26516
        t_4 = t_4[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::slice26517
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26518
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::sub26506
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26501
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26519
        t_6 = t_4[:, :, t_6:t_11:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::slice26520
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26521
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26522
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26501
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26523
        t_11 = t_6[:, :, :, 0:t_11:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::div26496
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::slice26524
        t_3 = torch.mul(input=t_3, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::slice26524
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26526
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26527
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::rsub26528
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26529
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::mul26525
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::mul26530
        t_11 = torch.sub(input=t_3, other=t_11)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::sub26532
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26533
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26534
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::softmax26535
        t_11 = self.l_38(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::permute26493
        t_0 = t_11.matmul(other=t_0)
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::matmul26537
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26542
        t_11 = t_0.permute(dims=t_11)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::permute26543
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::contiguous26545
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26546
        t_0 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::contiguous26545
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26550
        t_3 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::contiguous26545
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26554
        t_6 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::contiguous26545
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::Constant26557
        t_4 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26555
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::size26558
        t_4 = torch.mul(input=t_6, other=t_4)
        t_4 = [t_0, t_3, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::contiguous26545
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/prim::ListConstruct26562
        t_4 = t_11.view(size=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/aten::view26563
        t_4 = self.l_39(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_40(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/aten::add26407
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=t_5, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/aten::add26567
        t_5 = self.l_41(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2]
        t_5 = self.l_42(t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/prim::Constant26573
        t_11 = torch.mul(input=t_5, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/prim::Constant26575
        t_3 = t_5.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::pow26576
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/prim::Constant26577
        t_3 = torch.mul(input=t_3, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::mul26578
        t_3 = torch.add(input=t_5, other=t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::add26580
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/prim::Constant26581
        t_3 = torch.mul(input=t_3, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::mul26582
        t_3 = t_3.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::tanh26583
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/prim::Constant26584
        t_3 = torch.add(input=t_3, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::mul26574
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::add26586
        t_3 = torch.mul(input=t_11, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/aten::mul26587
        t_3 = self.l_43(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]
        t_3 = self.l_44(t_3)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/aten::add26567
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout]
        t_3 = torch.add(input=t_4, other=t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/aten::add26591
        t_4 = self.l_45(t_3)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1]
        t_4 = self.l_46(t_4)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26603
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26604
        t_4 = t_4.split(split_size=1600, dim=2)
        t_5 = t_4[0]
        t_0 = t_4[1]
        t_4 = t_4[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266060
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26609
        t_6 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266060
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26613
        t_1 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266060
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26617
        t_12 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26618
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26620
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_1, 25, t_12]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266060
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26624
        t_12 = t_5.view(size=t_12)
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::view26625
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26630
        t_5 = t_12.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266061
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26632
        t_12 = t_0.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266061
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26636
        t_1 = t_0.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266061
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26640
        t_6 = t_0.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26641
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26643
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_12, t_1, 25, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266061
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26647
        t_6 = t_0.view(size=t_6)
        t_0 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::view26648
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26653
        t_0 = t_6.permute(dims=t_0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266062
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26655
        t_6 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266062
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26659
        t_1 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266062
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26663
        t_12 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26664
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26666
        t_12 = torch.div(input=t_12, other=25)
        t_12 = [t_6, t_1, 25, t_12]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListUnpack266062
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26670
        t_12 = t_4.view(size=t_12)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::view26671
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26676
        t_4 = t_12.permute(dims=t_4)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::permute26631
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::permute26654
        t_0 = t_5.matmul(other=t_0)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::matmul26678
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26679
        t_0 = torch.div(input=t_0, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::div26680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26681
        t_5 = t_0.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::div26680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26684
        t_12 = t_0.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26685
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26682
        t_5 = torch.sub(input=t_12, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26692
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26693
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26694
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26695
        t_1 = self.b_5[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::slice26696
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26697
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26698
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26699
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26700
        t_1 = t_1[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::slice26701
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26702
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::sub26690
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26685
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26703
        t_5 = t_1[:, :, t_5:t_12:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::slice26704
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26705
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26706
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26685
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26707
        t_12 = t_5[:, :, :, 0:t_12:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::div26680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::slice26708
        t_0 = torch.mul(input=t_0, other=t_12)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::slice26708
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26710
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26711
        t_12 = torch.rsub(t_12, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::rsub26712
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26713
        t_12 = torch.mul(input=t_12, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::mul26709
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::mul26714
        t_12 = torch.sub(input=t_0, other=t_12)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::sub26716
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26717
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26718
        t_12 = t_12.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::softmax26719
        t_12 = self.l_47(t_12)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::permute26677
        t_4 = t_12.matmul(other=t_4)
        t_12 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::matmul26721
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26726
        t_12 = t_4.permute(dims=t_12)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::permute26727
        t_12 = t_12.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::contiguous26729
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26730
        t_4 = t_12.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::contiguous26729
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26734
        t_0 = t_12.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::contiguous26729
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26738
        t_5 = t_12.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::contiguous26729
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::Constant26741
        t_1 = t_12.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26739
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::size26742
        t_1 = torch.mul(input=t_5, other=t_1)
        t_1 = [t_4, t_0, t_1]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::contiguous26729
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/prim::ListConstruct26746
        t_1 = t_12.view(size=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/aten::view26747
        t_1 = self.l_48(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Conv1D[c_proj]
        t_1 = self.l_49(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/aten::add26591
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]/Dropout[resid_dropout]
        t_1 = torch.add(input=t_3, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/aten::add26751
        t_3 = self.l_50(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2]
        t_3 = self.l_51(t_3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/prim::Constant26757
        t_12 = torch.mul(input=t_3, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/prim::Constant26759
        t_0 = t_3.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::pow26760
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/prim::Constant26761
        t_0 = torch.mul(input=t_0, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::mul26762
        t_0 = torch.add(input=t_3, other=t_0)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::add26764
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/prim::Constant26765
        t_0 = torch.mul(input=t_0, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::mul26766
        t_0 = t_0.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::tanh26767
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/prim::Constant26768
        t_0 = torch.add(input=t_0, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::mul26758
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::add26770
        t_0 = torch.mul(input=t_12, other=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/aten::mul26771
        t_0 = self.l_52(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj]
        t_0 = self.l_53(t_0)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/aten::add26751
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout]
        t_0 = torch.add(input=t_1, other=t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/aten::add26775
        t_1 = self.l_54(t_0)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1]
        t_1 = self.l_55(t_1)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26787
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26788
        t_1 = t_1.split(split_size=1600, dim=2)
        t_3 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267900
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26793
        t_5 = t_3.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267900
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26797
        t_6 = t_3.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267900
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26801
        t_13 = t_3.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26802
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26804
        t_13 = torch.div(input=t_13, other=25)
        t_13 = [t_5, t_6, 25, t_13]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267900
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26808
        t_13 = t_3.view(size=t_13)
        t_3 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::view26809
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26814
        t_3 = t_13.permute(dims=t_3)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267901
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26816
        t_13 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267901
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26820
        t_6 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267901
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26824
        t_5 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26825
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26827
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_13, t_6, 25, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267901
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26831
        t_5 = t_4.view(size=t_5)
        t_4 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::view26832
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26837
        t_4 = t_5.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267902
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26839
        t_5 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267902
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26843
        t_6 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267902
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26847
        t_13 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26848
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26850
        t_13 = torch.div(input=t_13, other=25)
        t_13 = [t_5, t_6, 25, t_13]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListUnpack267902
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26854
        t_13 = t_1.view(size=t_13)
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::view26855
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26860
        t_1 = t_13.permute(dims=t_1)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::permute26815
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::permute26838
        t_4 = t_3.matmul(other=t_4)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::matmul26862
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26863
        t_4 = torch.div(input=t_4, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::div26864
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26865
        t_3 = t_4.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::div26864
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26868
        t_13 = t_4.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26869
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26866
        t_3 = torch.sub(input=t_13, other=t_3)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26876
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26877
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26878
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26879
        t_6 = self.b_6[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::slice26880
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26881
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26882
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26883
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26884
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::slice26885
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26886
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::sub26874
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26869
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26887
        t_3 = t_6[:, :, t_3:t_13:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::slice26888
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26889
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26890
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26869
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26891
        t_13 = t_3[:, :, :, 0:t_13:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::div26864
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::slice26892
        t_4 = torch.mul(input=t_4, other=t_13)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::slice26892
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26894
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26895
        t_13 = torch.rsub(t_13, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::rsub26896
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26897
        t_13 = torch.mul(input=t_13, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::mul26893
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::mul26898
        t_13 = torch.sub(input=t_4, other=t_13)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::sub26900
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26901
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26902
        t_13 = t_13.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::softmax26903
        t_13 = self.l_56(t_13)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::permute26861
        t_1 = t_13.matmul(other=t_1)
        t_13 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::matmul26905
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26910
        t_13 = t_1.permute(dims=t_13)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::permute26911
        t_13 = t_13.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::contiguous26913
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26914
        t_1 = t_13.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::contiguous26913
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26918
        t_4 = t_13.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::contiguous26913
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26922
        t_3 = t_13.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::contiguous26913
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::Constant26925
        t_6 = t_13.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26923
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::size26926
        t_6 = torch.mul(input=t_3, other=t_6)
        t_6 = [t_1, t_4, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::contiguous26913
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/prim::ListConstruct26930
        t_6 = t_13.view(size=t_6)
        # returing:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/aten::add26775
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::view26931
        return (t_0, t_6)

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition7(nn.Module):
    def __init__(self, layers, tensors):
        super(Partition7, self).__init__()
        # initializing partition layers
        self.l_0 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_0, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_0)}'
        self.l_1 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_1, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_1)}'
        self.l_2 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_2, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_2)}'
        self.l_3 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_3, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_3)}'
        self.l_4 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_4, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_4)}'
        self.l_5 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_5, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_5)}'
        self.l_6 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_6, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_6)}'
        self.l_7 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_7, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_7)}'
        self.l_8 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_8, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_8)}'
        self.l_9 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_9, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_9)}'
        self.l_10 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_10, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_10)}'
        self.l_11 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_11, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_11)}'
        self.l_12 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_12, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_12)}'
        self.l_13 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_13, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_13)}'
        self.l_14 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_14, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_14)}'
        self.l_15 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_15, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_15)}'
        self.l_16 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_16, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_16)}'
        self.l_17 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_17, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_17)}'
        self.l_18 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_18, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_18)}'
        self.l_19 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_19, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_19)}'
        self.l_20 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_20, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_20)}'
        self.l_21 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_21, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_21)}'
        self.l_22 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_22, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_22)}'
        self.l_23 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_23, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_23)}'
        self.l_24 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_24, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_24)}'
        self.l_25 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_25, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_25)}'
        self.l_26 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_26, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_26)}'
        self.l_27 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_27, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_27)}'
        self.l_28 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_28, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_28)}'
        self.l_29 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_29, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_29)}'
        self.l_30 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_30, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_30)}'
        self.l_31 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_31, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_31)}'
        self.l_32 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_32, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_32)}'
        self.l_33 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1]']
        assert isinstance(
            self.l_33, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1]] is expected to be of type LayerNorm but was of type {type(self.l_33)}'
        self.l_34 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_attn]']
        assert isinstance(
            self.l_34, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_attn]] is expected to be of type Conv1D but was of type {type(self.l_34)}'
        self.l_35 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[attn_dropout]']
        assert isinstance(
            self.l_35, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[attn_dropout]] is expected to be of type Dropout but was of type {type(self.l_35)}'
        self.l_36 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_proj]']
        assert isinstance(
            self.l_36, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_36)}'
        self.l_37 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[resid_dropout]']
        assert isinstance(
            self.l_37, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[resid_dropout]] is expected to be of type Dropout but was of type {type(self.l_37)}'
        self.l_38 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2]']
        assert isinstance(
            self.l_38, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2]] is expected to be of type LayerNorm but was of type {type(self.l_38)}'
        self.l_39 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc]']
        assert isinstance(
            self.l_39, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc]] is expected to be of type Conv1D but was of type {type(self.l_39)}'
        self.l_40 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj]']
        assert isinstance(
            self.l_40, Conv1D
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj]] is expected to be of type Conv1D but was of type {type(self.l_40)}'
        self.l_41 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout]']
        assert isinstance(
            self.l_41, Dropout
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout]] is expected to be of type Dropout but was of type {type(self.l_41)}'
        self.l_42 = layers[
            'GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]']
        assert isinstance(
            self.l_42, LayerNorm
        ), f'layers[GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]] is expected to be of type LayerNorm but was of type {type(self.l_42)}'
        self.l_43 = layers['GPT2LMHeadModel/Linear[lm_head]']
        assert isinstance(
            self.l_43, Linear
        ), f'layers[GPT2LMHeadModel/Linear[lm_head]] is expected to be of type Linear but was of type {type(self.l_43)}'

        # initializing partition buffers
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_0', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_1', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_2', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Tensor[bias]']
        )
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Tensor[bias]
        self.register_buffer(
            'b_3', tensors[
                'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Tensor[bias]']
        )

        # initializing partition parameters

        self.device = torch.device('cuda:7')
        self.lookup = {
            'l_0': 'transformer.blocks.43.attn.c_proj',
            'l_1': 'transformer.blocks.43.attn.resid_dropout',
            'l_2': 'transformer.blocks.43.ln_2',
            'l_3': 'transformer.blocks.43.mlp.c_fc',
            'l_4': 'transformer.blocks.43.mlp.c_proj',
            'l_5': 'transformer.blocks.43.mlp.dropout',
            'l_6': 'transformer.blocks.44.ln_1',
            'l_7': 'transformer.blocks.44.attn.c_attn',
            'l_8': 'transformer.blocks.44.attn.attn_dropout',
            'l_9': 'transformer.blocks.44.attn.c_proj',
            'l_10': 'transformer.blocks.44.attn.resid_dropout',
            'l_11': 'transformer.blocks.44.ln_2',
            'l_12': 'transformer.blocks.44.mlp.c_fc',
            'l_13': 'transformer.blocks.44.mlp.c_proj',
            'l_14': 'transformer.blocks.44.mlp.dropout',
            'l_15': 'transformer.blocks.45.ln_1',
            'l_16': 'transformer.blocks.45.attn.c_attn',
            'l_17': 'transformer.blocks.45.attn.attn_dropout',
            'l_18': 'transformer.blocks.45.attn.c_proj',
            'l_19': 'transformer.blocks.45.attn.resid_dropout',
            'l_20': 'transformer.blocks.45.ln_2',
            'l_21': 'transformer.blocks.45.mlp.c_fc',
            'l_22': 'transformer.blocks.45.mlp.c_proj',
            'l_23': 'transformer.blocks.45.mlp.dropout',
            'l_24': 'transformer.blocks.46.ln_1',
            'l_25': 'transformer.blocks.46.attn.c_attn',
            'l_26': 'transformer.blocks.46.attn.attn_dropout',
            'l_27': 'transformer.blocks.46.attn.c_proj',
            'l_28': 'transformer.blocks.46.attn.resid_dropout',
            'l_29': 'transformer.blocks.46.ln_2',
            'l_30': 'transformer.blocks.46.mlp.c_fc',
            'l_31': 'transformer.blocks.46.mlp.c_proj',
            'l_32': 'transformer.blocks.46.mlp.dropout',
            'l_33': 'transformer.blocks.47.ln_1',
            'l_34': 'transformer.blocks.47.attn.c_attn',
            'l_35': 'transformer.blocks.47.attn.attn_dropout',
            'l_36': 'transformer.blocks.47.attn.c_proj',
            'l_37': 'transformer.blocks.47.attn.resid_dropout',
            'l_38': 'transformer.blocks.47.ln_2',
            'l_39': 'transformer.blocks.47.mlp.c_fc',
            'l_40': 'transformer.blocks.47.mlp.c_proj',
            'l_41': 'transformer.blocks.47.mlp.dropout',
            'l_42': 'transformer.ln_f',
            'l_43': 'lm_head',
            'b_0': 'transformer.blocks.44.attn.bias',
            'b_1': 'transformer.blocks.45.attn.bias',
            'b_2': 'transformer.blocks.46.attn.bias',
            'b_3': 'transformer.blocks.47.attn.bias'
        }

    def forward(self, x2, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_proj] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[resid_dropout] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_attn] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[attn_dropout] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_proj] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[resid_dropout] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_attn] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[attn_dropout] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_proj] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[resid_dropout] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_attn] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[attn_dropout] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_proj] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[resid_dropout] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_attn] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[attn_dropout] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_proj] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[resid_dropout] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f] <=> self.l_42
        # GPT2LMHeadModel/Linear[lm_head] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Tensor[bias] <=> self.b_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Tensor[bias] <=> self.b_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Tensor[bias] <=> self.b_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Tensor[bias] <=> self.b_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/aten::add26775 <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::view26931 <=> x1
        # input1 <=> x2

        # moving inputs to current device no op if already on the correct device
        x0 = x0.to(self.device)
        x1 = x1.to(self.device)
        x2 = x2.to(self.device)

        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/aten::view26931
        t_1 = self.l_0(x1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Conv1D[c_proj]
        t_1 = self.l_1(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/aten::add26775
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]/Dropout[resid_dropout]
        t_1 = torch.add(input=x0, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/aten::add26935
        t_2 = self.l_2(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2]
        t_2 = self.l_3(t_2)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/prim::Constant26941
        t_3 = torch.mul(input=t_2, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/prim::Constant26943
        t_4 = t_2.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::pow26944
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/prim::Constant26945
        t_4 = torch.mul(input=t_4, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::mul26946
        t_4 = torch.add(input=t_2, other=t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::add26948
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/prim::Constant26949
        t_4 = torch.mul(input=t_4, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::mul26950
        t_4 = t_4.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::tanh26951
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/prim::Constant26952
        t_4 = torch.add(input=t_4, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::mul26942
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::add26954
        t_4 = torch.mul(input=t_3, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/aten::mul26955
        t_4 = self.l_4(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj]
        t_4 = self.l_5(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/aten::add26935
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout]
        t_4 = torch.add(input=t_1, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/aten::add26959
        t_1 = self.l_6(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1]
        t_1 = self.l_7(t_1)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant26971
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant26972
        t_1 = t_1.split(split_size=1600, dim=2)
        t_2 = t_1[0]
        t_5 = t_1[1]
        t_1 = t_1[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269740
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant26977
        t_6 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269740
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant26981
        t_7 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269740
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant26985
        t_8 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size26986
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant26988
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_6, t_7, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269740
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct26992
        t_8 = t_2.view(size=t_8)
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::view26993
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct26998
        t_2 = t_8.permute(dims=t_2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269741
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27000
        t_8 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269741
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27004
        t_7 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269741
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27008
        t_6 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27009
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27011
        t_6 = torch.div(input=t_6, other=25)
        t_6 = [t_8, t_7, 25, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269741
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct27015
        t_6 = t_5.view(size=t_6)
        t_5 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::view27016
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct27021
        t_5 = t_6.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269742
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27023
        t_6 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269742
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27027
        t_7 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269742
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27031
        t_8 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27032
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27034
        t_8 = torch.div(input=t_8, other=25)
        t_8 = [t_6, t_7, 25, t_8]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListUnpack269742
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct27038
        t_8 = t_1.view(size=t_8)
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::view27039
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct27044
        t_1 = t_8.permute(dims=t_1)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::permute26999
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::permute27022
        t_5 = t_2.matmul(other=t_5)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::matmul27046
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27047
        t_5 = torch.div(input=t_5, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::div27048
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27049
        t_2 = t_5.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::div27048
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27052
        t_8 = t_5.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27053
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27050
        t_2 = torch.sub(input=t_8, other=t_2)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27060
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27061
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27062
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27063
        t_7 = self.b_0[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::slice27064
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27065
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27066
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27067
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27068
        t_7 = t_7[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::slice27069
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27070
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::sub27058
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27053
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27071
        t_2 = t_7[:, :, t_2:t_8:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::slice27072
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27073
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27074
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27053
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27075
        t_8 = t_2[:, :, :, 0:t_8:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::div27048
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::slice27076
        t_5 = torch.mul(input=t_5, other=t_8)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::slice27076
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27078
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27079
        t_8 = torch.rsub(t_8, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::rsub27080
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27081
        t_8 = torch.mul(input=t_8, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::mul27077
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::mul27082
        t_8 = torch.sub(input=t_5, other=t_8)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::sub27084
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27085
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27086
        t_8 = t_8.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::softmax27087
        t_8 = self.l_8(t_8)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::permute27045
        t_1 = t_8.matmul(other=t_1)
        t_8 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::matmul27089
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct27094
        t_8 = t_1.permute(dims=t_8)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::permute27095
        t_8 = t_8.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::contiguous27097
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27098
        t_1 = t_8.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::contiguous27097
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27102
        t_5 = t_8.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::contiguous27097
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27106
        t_2 = t_8.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::contiguous27097
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::Constant27109
        t_7 = t_8.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27107
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::size27110
        t_7 = torch.mul(input=t_2, other=t_7)
        t_7 = [t_1, t_5, t_7]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::contiguous27097
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/prim::ListConstruct27114
        t_7 = t_8.view(size=t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/aten::view27115
        t_7 = self.l_9(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Conv1D[c_proj]
        t_7 = self.l_10(t_7)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/aten::add26959
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]/Dropout[resid_dropout]
        t_7 = torch.add(input=t_4, other=t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/aten::add27119
        t_4 = self.l_11(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2]
        t_4 = self.l_12(t_4)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/prim::Constant27125
        t_8 = torch.mul(input=t_4, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/prim::Constant27127
        t_5 = t_4.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::pow27128
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/prim::Constant27129
        t_5 = torch.mul(input=t_5, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::mul27130
        t_5 = torch.add(input=t_4, other=t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::add27132
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/prim::Constant27133
        t_5 = torch.mul(input=t_5, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::mul27134
        t_5 = t_5.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::tanh27135
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/prim::Constant27136
        t_5 = torch.add(input=t_5, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::mul27126
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::add27138
        t_5 = torch.mul(input=t_8, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/aten::mul27139
        t_5 = self.l_13(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj]
        t_5 = self.l_14(t_5)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/aten::add27119
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout]
        t_5 = torch.add(input=t_7, other=t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/aten::add27143
        t_7 = self.l_15(t_5)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1]
        t_7 = self.l_16(t_7)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27155
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27156
        t_7 = t_7.split(split_size=1600, dim=2)
        t_4 = t_7[0]
        t_1 = t_7[1]
        t_7 = t_7[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271580
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27161
        t_2 = t_4.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271580
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27165
        t_6 = t_4.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271580
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27169
        t_9 = t_4.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27170
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27172
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_2, t_6, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271580
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27176
        t_9 = t_4.view(size=t_9)
        t_4 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::view27177
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27182
        t_4 = t_9.permute(dims=t_4)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271581
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27184
        t_9 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271581
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27188
        t_6 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271581
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27192
        t_2 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27193
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27195
        t_2 = torch.div(input=t_2, other=25)
        t_2 = [t_9, t_6, 25, t_2]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271581
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27199
        t_2 = t_1.view(size=t_2)
        t_1 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::view27200
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27205
        t_1 = t_2.permute(dims=t_1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271582
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27207
        t_2 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271582
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27211
        t_6 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271582
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27215
        t_9 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27216
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27218
        t_9 = torch.div(input=t_9, other=25)
        t_9 = [t_2, t_6, 25, t_9]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListUnpack271582
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27222
        t_9 = t_7.view(size=t_9)
        t_7 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::view27223
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27228
        t_7 = t_9.permute(dims=t_7)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::permute27183
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::permute27206
        t_1 = t_4.matmul(other=t_1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::matmul27230
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27231
        t_1 = torch.div(input=t_1, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::div27232
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27233
        t_4 = t_1.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::div27232
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27236
        t_9 = t_1.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27237
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27234
        t_4 = torch.sub(input=t_9, other=t_4)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27244
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27245
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27246
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27247
        t_6 = self.b_1[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::slice27248
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27249
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27250
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27251
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27252
        t_6 = t_6[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::slice27253
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27254
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::sub27242
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27237
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27255
        t_4 = t_6[:, :, t_4:t_9:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::slice27256
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27257
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27258
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27237
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27259
        t_9 = t_4[:, :, :, 0:t_9:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::div27232
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::slice27260
        t_1 = torch.mul(input=t_1, other=t_9)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::slice27260
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27262
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27263
        t_9 = torch.rsub(t_9, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::rsub27264
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27265
        t_9 = torch.mul(input=t_9, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::mul27261
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::mul27266
        t_9 = torch.sub(input=t_1, other=t_9)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::sub27268
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27269
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27270
        t_9 = t_9.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::softmax27271
        t_9 = self.l_17(t_9)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::permute27229
        t_7 = t_9.matmul(other=t_7)
        t_9 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::matmul27273
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27278
        t_9 = t_7.permute(dims=t_9)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::permute27279
        t_9 = t_9.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::contiguous27281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27282
        t_7 = t_9.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::contiguous27281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27286
        t_1 = t_9.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::contiguous27281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27290
        t_4 = t_9.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::contiguous27281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::Constant27293
        t_6 = t_9.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27291
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::size27294
        t_6 = torch.mul(input=t_4, other=t_6)
        t_6 = [t_7, t_1, t_6]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::contiguous27281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/prim::ListConstruct27298
        t_6 = t_9.view(size=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/aten::view27299
        t_6 = self.l_18(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Conv1D[c_proj]
        t_6 = self.l_19(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/aten::add27143
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]/Dropout[resid_dropout]
        t_6 = torch.add(input=t_5, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/aten::add27303
        t_5 = self.l_20(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2]
        t_5 = self.l_21(t_5)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/prim::Constant27309
        t_9 = torch.mul(input=t_5, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/prim::Constant27311
        t_1 = t_5.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::pow27312
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/prim::Constant27313
        t_1 = torch.mul(input=t_1, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::mul27314
        t_1 = torch.add(input=t_5, other=t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::add27316
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/prim::Constant27317
        t_1 = torch.mul(input=t_1, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::mul27318
        t_1 = t_1.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::tanh27319
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/prim::Constant27320
        t_1 = torch.add(input=t_1, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::mul27310
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::add27322
        t_1 = torch.mul(input=t_9, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/aten::mul27323
        t_1 = self.l_22(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj]
        t_1 = self.l_23(t_1)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/aten::add27303
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout]
        t_1 = torch.add(input=t_6, other=t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/aten::add27327
        t_6 = self.l_24(t_1)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1]
        t_6 = self.l_25(t_6)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27339
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27340
        t_6 = t_6.split(split_size=1600, dim=2)
        t_5 = t_6[0]
        t_7 = t_6[1]
        t_6 = t_6[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273420
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27345
        t_4 = t_5.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273420
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27349
        t_2 = t_5.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273420
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27353
        t_10 = t_5.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27354
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27356
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_2, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273420
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27360
        t_10 = t_5.view(size=t_10)
        t_5 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::view27361
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27366
        t_5 = t_10.permute(dims=t_5)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273421
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27368
        t_10 = t_7.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273421
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27372
        t_2 = t_7.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273421
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27376
        t_4 = t_7.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27377
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27379
        t_4 = torch.div(input=t_4, other=25)
        t_4 = [t_10, t_2, 25, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273421
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27383
        t_4 = t_7.view(size=t_4)
        t_7 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::view27384
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27389
        t_7 = t_4.permute(dims=t_7)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273422
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27391
        t_4 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273422
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27395
        t_2 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273422
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27399
        t_10 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27400
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27402
        t_10 = torch.div(input=t_10, other=25)
        t_10 = [t_4, t_2, 25, t_10]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListUnpack273422
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27406
        t_10 = t_6.view(size=t_10)
        t_6 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::view27407
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27412
        t_6 = t_10.permute(dims=t_6)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::permute27367
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::permute27390
        t_7 = t_5.matmul(other=t_7)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::matmul27414
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27415
        t_7 = torch.div(input=t_7, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::div27416
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27417
        t_5 = t_7.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::div27416
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27420
        t_10 = t_7.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27421
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27418
        t_5 = torch.sub(input=t_10, other=t_5)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27428
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27429
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27430
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27431
        t_2 = self.b_2[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::slice27432
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27433
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27434
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27435
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27436
        t_2 = t_2[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::slice27437
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27438
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::sub27426
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27421
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27439
        t_5 = t_2[:, :, t_5:t_10:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::slice27440
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27441
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27442
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27421
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27443
        t_10 = t_5[:, :, :, 0:t_10:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::div27416
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::slice27444
        t_7 = torch.mul(input=t_7, other=t_10)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::slice27444
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27446
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27447
        t_10 = torch.rsub(t_10, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::rsub27448
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27449
        t_10 = torch.mul(input=t_10, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::mul27445
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::mul27450
        t_10 = torch.sub(input=t_7, other=t_10)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::sub27452
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27453
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27454
        t_10 = t_10.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::softmax27455
        t_10 = self.l_26(t_10)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::permute27413
        t_6 = t_10.matmul(other=t_6)
        t_10 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::matmul27457
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27462
        t_10 = t_6.permute(dims=t_10)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::permute27463
        t_10 = t_10.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::contiguous27465
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27466
        t_6 = t_10.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::contiguous27465
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27470
        t_7 = t_10.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::contiguous27465
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27474
        t_5 = t_10.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::contiguous27465
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::Constant27477
        t_2 = t_10.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27475
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::size27478
        t_2 = torch.mul(input=t_5, other=t_2)
        t_2 = [t_6, t_7, t_2]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::contiguous27465
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/prim::ListConstruct27482
        t_2 = t_10.view(size=t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/aten::view27483
        t_2 = self.l_27(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Conv1D[c_proj]
        t_2 = self.l_28(t_2)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/aten::add27327
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]/Dropout[resid_dropout]
        t_2 = torch.add(input=t_1, other=t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/aten::add27487
        t_1 = self.l_29(t_2)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2]
        t_1 = self.l_30(t_1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/prim::Constant27493
        t_10 = torch.mul(input=t_1, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/prim::Constant27495
        t_7 = t_1.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::pow27496
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/prim::Constant27497
        t_7 = torch.mul(input=t_7, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::mul27498
        t_7 = torch.add(input=t_1, other=t_7)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::add27500
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/prim::Constant27501
        t_7 = torch.mul(input=t_7, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::mul27502
        t_7 = t_7.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::tanh27503
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/prim::Constant27504
        t_7 = torch.add(input=t_7, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::mul27494
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::add27506
        t_7 = torch.mul(input=t_10, other=t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/aten::mul27507
        t_7 = self.l_31(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj]
        t_7 = self.l_32(t_7)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/aten::add27487
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout]
        t_7 = torch.add(input=t_2, other=t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/aten::add27511
        t_2 = self.l_33(t_7)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_attn] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1]
        t_2 = self.l_34(t_2)
        # calling torch.split with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_attn]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27523
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27524
        t_2 = t_2.split(split_size=1600, dim=2)
        t_1 = t_2[0]
        t_6 = t_2[1]
        t_2 = t_2[2]
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275260
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27529
        t_5 = t_1.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275260
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27533
        t_4 = t_1.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275260
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27537
        t_11 = t_1.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27538
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27540
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_5, t_4, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275260
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27544
        t_11 = t_1.view(size=t_11)
        t_1 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::view27545
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27550
        t_1 = t_11.permute(dims=t_1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275261
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27552
        t_11 = t_6.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275261
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27556
        t_4 = t_6.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275261
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27560
        t_5 = t_6.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27561
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27563
        t_5 = torch.div(input=t_5, other=25)
        t_5 = [t_11, t_4, 25, t_5]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275261
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27567
        t_5 = t_6.view(size=t_5)
        t_6 = [0, 2, 3, 1]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::view27568
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27573
        t_6 = t_5.permute(dims=t_6)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275262
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27575
        t_5 = t_2.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275262
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27579
        t_4 = t_2.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275262
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27583
        t_11 = t_2.size(dim=-1)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27584
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27586
        t_11 = torch.div(input=t_11, other=25)
        t_11 = [t_5, t_4, 25, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListUnpack275262
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27590
        t_11 = t_2.view(size=t_11)
        t_2 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::view27591
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27596
        t_2 = t_11.permute(dims=t_2)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::permute27551
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::permute27574
        t_6 = t_1.matmul(other=t_6)
        # calling torch.div with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::matmul27598
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27599
        t_6 = torch.div(input=t_6, other=8.0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::div27600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27601
        t_1 = t_6.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::div27600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27604
        t_11 = t_6.size(dim=-1)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27605
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27602
        t_1 = torch.sub(input=t_11, other=t_1)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Tensor[bias]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27612
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27613
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27614
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27615
        t_4 = self.b_3[0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::slice27616
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27617
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27618
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27619
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27620
        t_4 = t_4[:, 0:9223372036854775807:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::slice27621
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27622
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::sub27610
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27605
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27623
        t_1 = t_4[:, :, t_1:t_11:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::slice27624
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27625
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27626
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27605
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27627
        t_11 = t_1[:, :, :, 0:t_11:1]
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::div27600
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::slice27628
        t_6 = torch.mul(input=t_6, other=t_11)
        # calling torch.rsub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::slice27628
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27630
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27631
        t_11 = torch.rsub(t_11, other=1, alpha=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::rsub27632
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27633
        t_11 = torch.mul(input=t_11, other=10000.0)
        # calling torch.sub with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::mul27629
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::mul27634
        t_11 = torch.sub(input=t_6, other=t_11)
        # calling torch.softmax with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::sub27636
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27637
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27638
        t_11 = t_11.softmax(dim=-1, dtype=None)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[attn_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::softmax27639
        t_11 = self.l_35(t_11)
        # calling torch.matmul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[attn_dropout]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::permute27597
        t_2 = t_11.matmul(other=t_2)
        t_11 = [0, 2, 1, 3]
        # calling Tensor.permute with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::matmul27641
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27646
        t_11 = t_2.permute(dims=t_11)
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::permute27647
        t_11 = t_11.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::contiguous27649
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27650
        t_2 = t_11.size(dim=0)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::contiguous27649
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27654
        t_6 = t_11.size(dim=1)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::contiguous27649
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27658
        t_1 = t_11.size(dim=-2)
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::contiguous27649
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::Constant27661
        t_4 = t_11.size(dim=-1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27659
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::size27662
        t_4 = torch.mul(input=t_1, other=t_4)
        t_4 = [t_2, t_6, t_4]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::contiguous27649
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/prim::ListConstruct27666
        t_4 = t_11.view(size=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/aten::view27667
        t_4 = self.l_36(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[resid_dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Conv1D[c_proj]
        t_4 = self.l_37(t_4)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/aten::add27511
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]/Dropout[resid_dropout]
        t_4 = torch.add(input=t_7, other=t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/aten::add27671
        t_7 = self.l_38(t_4)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2]
        t_7 = self.l_39(t_7)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/prim::Constant27677
        t_11 = torch.mul(input=t_7, other=0.5)
        # calling torch.pow with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/prim::Constant27679
        t_6 = t_7.pow(exponent=3)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::pow27680
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/prim::Constant27681
        t_6 = torch.mul(input=t_6, other=0.044715)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc]
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::mul27682
        t_6 = torch.add(input=t_7, other=t_6)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::add27684
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/prim::Constant27685
        t_6 = torch.mul(input=t_6, other=0.7978845608028654)
        # calling torch.tanh with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::mul27686
        t_6 = t_6.tanh()
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::tanh27687
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/prim::Constant27688
        t_6 = torch.add(input=t_6, other=1)
        # calling torch.mul with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::mul27678
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::add27690
        t_6 = torch.mul(input=t_11, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/aten::mul27691
        t_6 = self.l_40(t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj]
        t_6 = self.l_41(t_6)
        # calling torch.add with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/aten::add27671
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout]
        t_6 = torch.add(input=t_4, other=t_6)
        # calling GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/aten::add27695
        t_6 = self.l_42(t_6)
        # calling GPT2LMHeadModel/Linear[lm_head] with arguments:
        # GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]
        t_6 = self.l_43(t_6)
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/Linear[lm_head]
        # GPT2LMHeadModel/prim::Constant16356
        # GPT2LMHeadModel/prim::Constant16357
        # GPT2LMHeadModel/prim::Constant16358
        # GPT2LMHeadModel/prim::Constant16359
        t_6 = t_6[:, 0:-1:1]
        # calling Tensor.slice with arguments:
        # GPT2LMHeadModel/aten::slice16360
        # GPT2LMHeadModel/prim::Constant16361
        # GPT2LMHeadModel/prim::Constant16362
        # GPT2LMHeadModel/prim::Constant16363
        # GPT2LMHeadModel/prim::Constant16364
        t_6 = t_6[:, :, 0:9223372036854775807:1]
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/aten::slice16365
        t_6 = t_6.contiguous()
        # calling Tensor.slice with arguments:
        # input1
        # GPT2LMHeadModel/prim::Constant16368
        # GPT2LMHeadModel/prim::Constant16369
        # GPT2LMHeadModel/prim::Constant16370
        # GPT2LMHeadModel/prim::Constant16371
        t_4 = x2[:, 1:9223372036854775807:1]
        # calling Tensor.contiguous with arguments:
        # GPT2LMHeadModel/aten::slice16372
        t_4 = t_4.contiguous()
        # calling Tensor.size with arguments:
        # GPT2LMHeadModel/aten::contiguous16367
        # GPT2LMHeadModel/prim::Constant16375
        t_11 = t_6.size(dim=-1)
        t_11 = [-1, t_11]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/aten::contiguous16367
        # GPT2LMHeadModel/prim::ListConstruct16380
        t_11 = t_6.view(size=t_11)
        t_6 = [-1]
        # calling Tensor.view with arguments:
        # GPT2LMHeadModel/aten::contiguous16374
        # GPT2LMHeadModel/prim::ListConstruct16383
        t_6 = t_4.view(size=t_6)
        # calling torch.log_softmax with arguments:
        # GPT2LMHeadModel/aten::view16381
        # GPT2LMHeadModel/prim::Constant16385
        # GPT2LMHeadModel/prim::Constant16386
        t_11 = t_11.log_softmax(dim=1, dtype=None)
        # calling F.nll_loss with arguments:
        # GPT2LMHeadModel/aten::log_softmax16387
        # GPT2LMHeadModel/aten::view16384
        # GPT2LMHeadModel/prim::Constant16395
        # GPT2LMHeadModel/prim::Constant16396
        # GPT2LMHeadModel/prim::Constant16397
        t_6 = F.nll_loss(input=t_11,
                         target=t_6,
                         weight=None,
                         reduction='mean',
                         ignore_index=-100)
        # returing:
        # GPT2LMHeadModel/aten::nll_loss16398
        return (t_6, )

    def state_dict(self, device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, device=device)

    def load_state_dict(self, state):
        return load_state_dict(self, state)

    def named_parameters(self, recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, recurse=recurse)

    def named_buffers(self, recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


def traverse_model(
        module: nn.Module,
        depth: int,
        prefix: Optional[str] = None,
        basic_blocks: Optional[Iterable[nn.Module]] = None,
        full: bool = False) -> Iterator[Tuple[nn.Module, str, nn.Module]]:
    '''
    iterate over model layers yielding the layer,layer_scope,encasing_module
    Parameters:
    -----------
    model:
        the model to iterate over
    depth:
        how far down in the model tree to go
    basic_blocks:
        a list of modules that if encountered will not be broken down
    full:
        whether to yield only layers specified by the depth and basick_block options or to yield all layers
    '''
    if prefix is None:
        prefix = type(module).__name__

    for name, sub_module in module.named_children():
        scope = prefix + "/" + type(sub_module).__name__ + f"[{name}]"
        if len(list(sub_module.children())) == 0 or (
            (basic_blocks is not None)
                and isinstance(sub_module, tuple(basic_blocks))) or depth == 0:
            yield sub_module, scope, module
        else:
            if full:
                yield sub_module, scope, module
            yield from traverse_model(
                sub_module, depth - 1,
                prefix + "/" + type(sub_module).__name__ + f"[{name}]",
                basic_blocks, full)


def layerDict(model: nn.Module,
              depth=1000,
              basic_blocks=None) -> Dict[str, nn.Module]:
    return {
        s: l
        for l, s, _ in traverse_model(model, depth, basic_blocks=basic_blocks)
    }


def traverse_params_buffs(
        module: nn.Module,
        prefix: Optional[str] = None) -> Iterator[Tuple[torch.tensor, str]]:
    '''
    iterate over model's buffers and parameters yielding obj,obj_scope

    Parameters:
    -----------
    model:
        the model to iterate over
    '''
    if prefix is None:
        prefix = type(module).__name__

    # params
    for param_name, param in module.named_parameters(recurse=False):
        param_scope = f"{prefix}/{type(param).__name__}[{param_name}]"
        yield param, param_scope

    # buffs
    for buffer_name, buffer in module.named_buffers(recurse=False):
        buffer_scope = f"{prefix}/{type(buffer).__name__}[{buffer_name}]"
        yield buffer, buffer_scope

    # recurse
    for name, sub_module in module.named_children():
        yield from traverse_params_buffs(
            sub_module, prefix + "/" + type(sub_module).__name__ + f"[{name}]")


def tensorDict(model: nn.Module) -> OrderedDict[str, Tensor]:
    return collections.OrderedDict(
        (s, t) for t, s in traverse_params_buffs(model))


def state_dict(partition, device=None):
    # we return the state dict of this part as it should be in the original model
    state = nn.Module.state_dict(partition)
    lookup = partition.lookup
    result = dict()
    for k, v in state.items():
        if k in lookup:
            result[lookup[k]] = v if device is None else v.to(device)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            result[new_k] = v if device is None else v.to(device)
    return result


def load_state_dict(partition, state):
    reverse_lookup = {v: k for k, v in partition.lookup.items()}
    device = partition.device
    keys = list(partition.state_dict(None).keys())
    new_state = dict()
    for k in keys:
        if k in reverse_lookup:
            new_state[reverse_lookup[k]] = state[k].to(device)
            continue
        idx = k.rfind(".")
        to_replace = k[:idx]
        if to_replace in reverse_lookup:
            key = reverse_lookup[to_replace] + k[idx:]
            new_state[key] = state[k].to(device)
    nn.Module.load_state_dict(partition, new_state, strict=True)


def named_buffers(partition, recurse=True):
    # we return the named buffers of this part as it should be in the original model
    params = nn.Module.named_buffers(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def named_parameters(partition, recurse=True):
    # we return the named parameters of this part as it should be in the original model
    params = nn.Module.named_parameters(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def cpu(partition):
    partition.device = torch.device('cpu')
    return nn.Module.cpu(partition)


def cuda(partition, device=None):
    if device is None:
        device = torch.cuda.current_device()
    partition.device = torch.device(device)
    return nn.Module.cuda(partition, partition.device)


def to(partition, *args, **kwargs):
    device = None
    if 'device' in kwargs:
        device = kwargs['device']
    elif 'tensor' in kwargs:
        device = kwargs['tensor'].device
    if args:
        if isinstance(args[0], (torch.device, int, str)):
            device = args[0]
        if torch.is_tensor(args[0]):
            device = args[0].device
    if not (device is None):
        partition.device = torch.device(device)
    return nn.Module.to(partition, *args, **kwargs)


"""analysis summary
-I- Printing Report
cutting edges are edges between partitions
number of cutting edges: 18

backward times include recomputation
Analysis for async_pipeline=True: last partition will not do recomputation.

real times are based on real measurements of execution time of generated partitions ms
forward {0: 59.55, 1: 60.73, 2: 61.78, 3: 62.74, 4: 62.9, 5: 61.17, 6: 64.08, 7: 58.26}
backward {0: 153.66, 1: 165.31, 2: 162.92, 3: 168.64, 4: 163.98, 5: 167.15, 6: 170.18, 7: 114.59}

balance is ratio of computation time between fastest and slowest parts. (between 0 and 1 higher is better)

real balance:
forward 0.909
backward 0.673

Assuming bandwidth of 12 GBps between GPUs

communication volumes size of activations of each partition
0: input size:'0.01 MB', recieve_time:'0.00 ms', out:'32.77 MB', send time:'2.73 ms'
1: input size:'32.77 MB', recieve_time:'2.73 ms', out:'13.11 MB', send time:'1.09 ms'
2: input size:'13.11 MB', recieve_time:'1.09 ms', out:'26.21 MB', send time:'2.18 ms'
3: input size:'26.21 MB', recieve_time:'2.18 ms', out:'13.11 MB', send time:'1.09 ms'
4: input size:'13.11 MB', recieve_time:'1.09 ms', out:'32.77 MB', send time:'2.73 ms'
5: input size:'32.77 MB', recieve_time:'2.73 ms', out:'13.11 MB', send time:'1.09 ms'
6: input size:'13.11 MB', recieve_time:'1.09 ms', out:'13.11 MB', send time:'1.09 ms'
7: input size:'13.12 MB', recieve_time:'1.09 ms', out:'0.00 MB', send time:'0.00 ms'

Compuatation Communication ratio (comp/(comp+comm)):
forward {0: 0.95, 1: 0.98, 2: 0.96, 3: 0.98, 4: 0.96, 5: 0.98, 6: 0.98, 7: 1.0} 
backward {0: 1.0, 1: 0.98, 2: 0.99, 3: 0.99, 4: 0.99, 5: 0.98, 6: 0.99, 7: 0.99}

Pipeline Slowdown: (compared to sequential executation with no communication)
forward 1.070
backward 1.085

Expected utilization by partition
forward {0: 0.89, 1: 0.93, 2: 0.93, 3: 0.96, 4: 0.94, 5: 0.93, 6: 0.98, 7: 0.91}
backward {0: 0.9, 1: 0.95, 2: 0.95, 3: 0.98, 4: 0.95, 5: 0.96, 6: 0.99, 7: 0.66}

Expected speedup for 8 partitions is: 7.400
"""
