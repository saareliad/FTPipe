"""AutoGenerated with:
python partition_glue_models.py --model_type roberta --objective stage_time --model_name_or_path roberta-large --n_partitions 8 --hetrogenous_bw --hetrogenous_nodes --bwd_to_fwd_ratio -1 --auto_infer_node_bwd_to_fwd_ratio --bw 11 --partitioning_batch_size 32 --analysis_batch_size 32 --n_iter 50 --data_dir glue_data
"""
import torch.nn.functional
import torch.functional
import torch
import math
from torch import Tensor
import torch.nn as nn
from itertools import chain
from typing import Optional, Tuple, Iterator, Iterable, OrderedDict, Dict
import collections
import os
from torch.nn.modules.activation import Softmax
from torch.nn.modules.sparse import Embedding
from torch.nn.modules.normalization import LayerNorm
from torch.nn.modules.dropout import Dropout
from torch.nn.modules.activation import Tanh
from torch.nn.modules.linear import Linear
# this is an auto generated file do not edit unless you know what you are doing


# partition adjacency
# model inputs {0}
# partition 0 {'inputs': {'input1', 'input0'}, 'outputs': {1}}
# partition 1 {'inputs': {0}, 'outputs': {2}}
# partition 2 {'inputs': {1}, 'outputs': {3}}
# partition 3 {'inputs': {2}, 'outputs': {4}}
# partition 4 {'inputs': {3}, 'outputs': {5}}
# partition 5 {'inputs': {4}, 'outputs': {6}}
# partition 6 {'inputs': {5}, 'outputs': {7}}
# partition 7 {'inputs': {6}, 'outputs': {'output'}}
# model outputs {7}


def create_pipeline_configuration(DEBUG=False):
    basic_blocks = (Softmax,Embedding,LayerNorm,Dropout,Tanh,Linear)
    module_path = os.path.relpath(__file__).replace("/",".")[:-3]
    
    config = {
        'batch_dim': 0,
        'depth': 10000,
        'basic_blocks': ['torch.nn.modules.activation.Softmax', 'torch.nn.modules.sparse.Embedding', 'torch.nn.modules.normalization.LayerNorm', 'torch.nn.modules.dropout.Dropout', 'torch.nn.modules.activation.Tanh', 'torch.nn.modules.linear.Linear'],
        'model_inputs': {
            'input0': {
                'shape': torch.Size([32, 128]),
                'dtype': torch.int64,
                'is_batched': True},
            'input1': {
                'shape': torch.Size([32, 128]),
                'dtype': torch.int64,
                'is_batched': True}},
        'model_outputs': {
            'RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Linear[out_proj]': {
                'shape': torch.Size([32, 3]),
                'dtype': torch.float32,
                'is_batched': True}},
        'stages': {
            0: {
                'inputs': {
                    'input0': {
                        'shape': torch.Size([32, 128]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True},
                    'input1': {
                        'shape': torch.Size([32, 128]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True}},
                'outputs': {
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___277': {
                        'shape': torch.Size([32, 1, 1, 128]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/Linear[dense]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            1: {
                'inputs': {
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___277': {
                        'shape': torch.Size([32, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/Linear[dense]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True}},
                'outputs': {
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/prim::TupleConstruct_765_0': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/prim::TupleConstruct_765_1': {
                        'shape': torch.Size([32, 1, 1, 128]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___767': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___782': {
                        'shape': None,
                        'dtype': torch.Size,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___784': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___786': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___788': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False}}},
            2: {
                'inputs': {
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/prim::TupleConstruct_765_0': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/prim::TupleConstruct_765_1': {
                        'shape': torch.Size([32, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___767': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___782': {
                        'shape': None,
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___784': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___786': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___788': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False}},
                'outputs': {
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/prim::TupleConstruct_1134_0': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/prim::TupleConstruct_1134_1': {
                        'shape': torch.Size([32, 1, 1, 128]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1136': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1199': {
                        'shape': None,
                        'dtype': torch.Size,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1201': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False}}},
            3: {
                'inputs': {
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/prim::TupleConstruct_1134_0': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/prim::TupleConstruct_1134_1': {
                        'shape': torch.Size([32, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1136': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1199': {
                        'shape': None,
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1201': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False}},
                'outputs': {
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/prim::TupleConstruct_1503_0': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/prim::TupleConstruct_1503_1': {
                        'shape': torch.Size([32, 1, 1, 128]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1505': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1520': {
                        'shape': None,
                        'dtype': torch.Size,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1528': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1544': {
                        'shape': None,
                        'dtype': torch.Size,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1546': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1548': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Tensor::view_1577': {
                        'shape': torch.Size([32, 128, 16, 64]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            4: {
                'inputs': {
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/prim::TupleConstruct_1503_0': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/prim::TupleConstruct_1503_1': {
                        'shape': torch.Size([32, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1505': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1520': {
                        'shape': None,
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1528': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1544': {
                        'shape': None,
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1546': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1548': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Tensor::view_1577': {
                        'shape': torch.Size([32, 128, 16, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True}},
                'outputs': {
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1874': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1876': {
                        'shape': torch.Size([32, 1, 1, 128]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1885': {
                        'shape': None,
                        'dtype': torch.Size,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1913': {
                        'shape': None,
                        'dtype': torch.Size,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1915': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1917': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1921': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Tensor::permute_1951': {
                        'shape': torch.Size([32, 16, 128, 64]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            5: {
                'inputs': {
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1874': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1876': {
                        'shape': torch.Size([32, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1885': {
                        'shape': None,
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1913': {
                        'shape': None,
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1915': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1917': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1921': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Tensor::permute_1951': {
                        'shape': torch.Size([32, 16, 128, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True}},
                'outputs': {
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/prim::TupleConstruct_2241_0': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/prim::TupleConstruct_2241_1': {
                        'shape': torch.Size([32, 1, 1, 128]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___2243': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2258': {
                        'shape': None,
                        'dtype': torch.Size,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2260': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2264': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2266': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2282': {
                        'shape': None,
                        'dtype': torch.Size,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2288': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2290': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2306': {
                        'shape': None,
                        'dtype': torch.Size,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2308': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2312': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False}}},
            6: {
                'inputs': {
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/prim::TupleConstruct_2241_0': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/prim::TupleConstruct_2241_1': {
                        'shape': torch.Size([32, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___2243': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2258': {
                        'shape': None,
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2260': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2264': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2266': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2282': {
                        'shape': None,
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2288': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2290': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2306': {
                        'shape': None,
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2308': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2312': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False}},
                'outputs': {
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___2612': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___2614': {
                        'shape': torch.Size([32, 1, 1, 128]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2627': {
                        'shape': None,
                        'dtype': torch.Size,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2629': {
                        'shape': None,
                        'dtype': int,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Tensor::permute_2665': {
                        'shape': torch.Size([32, 16, 128, 64]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2675': {
                        'shape': None,
                        'dtype': torch.Size,
                        'is_batched': False}}},
            7: {
                'inputs': {
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___2612': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___2614': {
                        'shape': torch.Size([32, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]': {
                        'shape': torch.Size([32, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2627': {
                        'shape': None,
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2629': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Tensor::permute_2665': {
                        'shape': torch.Size([32, 16, 128, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2675': {
                        'shape': None,
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False}},
                'outputs': {
                    'RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Linear[out_proj]': {
                        'shape': torch.Size([32, 3]),
                        'dtype': torch.float32,
                        'is_batched': True}}}}}
    
    config['stages'][0]['stage_cls'] = module_path+'.Partition0'
    config['stages'][1]['stage_cls'] = module_path+'.Partition1'
    config['stages'][2]['stage_cls'] = module_path+'.Partition2'
    config['stages'][3]['stage_cls'] = module_path+'.Partition3'
    config['stages'][4]['stage_cls'] = module_path+'.Partition4'
    config['stages'][5]['stage_cls'] = module_path+'.Partition5'
    config['stages'][6]['stage_cls'] = module_path+'.Partition6'
    config['stages'][7]['stage_cls'] = module_path+'.Partition7'
    
    config['stages'][0]['devices'] = ['cpu' if DEBUG else 'cuda:0']
    config['stages'][1]['devices'] = ['cpu' if DEBUG else 'cuda:1']
    config['stages'][2]['devices'] = ['cpu' if DEBUG else 'cuda:2']
    config['stages'][3]['devices'] = ['cpu' if DEBUG else 'cuda:3']
    config['stages'][4]['devices'] = ['cpu' if DEBUG else 'cuda:4']
    config['stages'][5]['devices'] = ['cpu' if DEBUG else 'cuda:5']
    config['stages'][6]['devices'] = ['cpu' if DEBUG else 'cuda:6']
    config['stages'][7]['devices'] = ['cpu' if DEBUG else 'cuda:7']
    
    return config

class Partition0(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Softmax,
            Dropout,
            Linear,
            Embedding,
        )
    LAYER_SCOPES=[
            'RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/Embedding[word_embeddings]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/Embedding[position_embeddings]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/Embedding[token_type_embeddings]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/Linear[dense]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:0'):
        super(Partition0, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = { 'l_0': 'roberta.embeddings.word_embeddings',
                        'l_1': 'roberta.embeddings.position_embeddings',
                        'l_2': 'roberta.embeddings.token_type_embeddings',
                        'l_3': 'roberta.embeddings.LayerNorm',
                        'l_4': 'roberta.embeddings.dropout',
                        'l_5': 'roberta.encoder.0.attention.self.query',
                        'l_6': 'roberta.encoder.0.attention.self.key',
                        'l_7': 'roberta.encoder.0.attention.self.value',
                        'l_8': 'roberta.encoder.0.attention.self.softmax',
                        'l_9': 'roberta.encoder.0.attention.self.dropout',
                        'l_10': 'roberta.encoder.0.attention.output.dense',
                        'l_11': 'roberta.encoder.0.attention.output.dropout',
                        'l_12': 'roberta.encoder.0.attention.output.LayerNorm',
                        'l_13': 'roberta.encoder.0.intermediate.dense',
                        'l_14': 'roberta.encoder.0.output.dense',
                        'l_15': 'roberta.encoder.0.output.dropout',
                        'l_16': 'roberta.encoder.0.output.LayerNorm',
                        'l_17': 'roberta.encoder.1.attention.self.query',
                        'l_18': 'roberta.encoder.1.attention.self.key',
                        'l_19': 'roberta.encoder.1.attention.self.value',
                        'l_20': 'roberta.encoder.1.attention.self.softmax',
                        'l_21': 'roberta.encoder.1.attention.self.dropout',
                        'l_22': 'roberta.encoder.1.attention.output.dense',
                        'l_23': 'roberta.encoder.1.attention.output.dropout',
                        'l_24': 'roberta.encoder.1.attention.output.LayerNorm',
                        'l_25': 'roberta.encoder.1.intermediate.dense',
                        'l_26': 'roberta.encoder.1.output.dense',
                        'l_27': 'roberta.encoder.1.output.dropout',
                        'l_28': 'roberta.encoder.1.output.LayerNorm',
                        'l_29': 'roberta.encoder.2.attention.self.query',
                        'l_30': 'roberta.encoder.2.attention.self.key',
                        'l_31': 'roberta.encoder.2.attention.self.value',
                        'l_32': 'roberta.encoder.2.attention.self.softmax',
                        'l_33': 'roberta.encoder.2.attention.self.dropout',
                        'l_34': 'roberta.encoder.2.attention.output.dense',
                        'l_35': 'roberta.encoder.2.attention.output.dropout',
                        'l_36': 'roberta.encoder.2.attention.output.LayerNorm',
                        'l_37': 'roberta.encoder.2.intermediate.dense',
                        'l_38': 'roberta.encoder.2.output.dense'}
        self.to(self.device)

    def forward(self, *args):
        # RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/Embedding[word_embeddings] <=> self.l_0
        # RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/Embedding[position_embeddings] <=> self.l_1
        # RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/Embedding[token_type_embeddings] <=> self.l_2
        # RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/LayerNorm[LayerNorm] <=> self.l_3
        # RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/Dropout[dropout] <=> self.l_4
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_5
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_6
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_7
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_8
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_9
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_10
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_11
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_12
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_13
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertOutput[output]/Linear[dense] <=> self.l_14
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertOutput[output]/Dropout[dropout] <=> self.l_15
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[0]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_16
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_17
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_18
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_19
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_20
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_21
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_22
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_23
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_24
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_25
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertOutput[output]/Linear[dense] <=> self.l_26
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertOutput[output]/Dropout[dropout] <=> self.l_27
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[1]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_28
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_29
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_30
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_31
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_32
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_33
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_34
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_35
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_36
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_37
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/Linear[dense] <=> self.l_38
        # input0 <=> x0
        # input1 <=> x1

        # moving inputs to current device no op if already on the correct device
        x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = torch.zeros_like(x0)
        t_1 = x1.unsqueeze(1)
        t_1 = t_1.unsqueeze(2)
        t_1 = t_1.to(dtype=torch.float32)
        t_1 = 1.0 - t_1
        t_1 = t_1 * -10000.0
        t_2 = x0.ne(1)
        t_2 = t_2.int()
        t_3 = torch.cumsum(t_2, dim=1)
        t_3 = t_3.type_as(t_2)
        t_2 = t_3 * t_2
        t_2 = t_2.long()
        t_2 = t_2 + 1
        t_2 = t_2.to(self.device)
        t_3 = self.l_0(x0)
        t_2 = self.l_1(t_2)
        t_0 = self.l_2(t_0)
        t_2 = t_3 + t_2
        t_0 = t_2 + t_0
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_2 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_4 = self.l_7(t_0)
        t_5 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_5[t_6]
        t_5 = (16, 64)
        t_5 = t_6 + t_5
        t_6 = t_5[0]
        t_7 = t_5[1]
        t_8 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_2.view(t_6, t_7, t_8, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_8 = t_3.size()
        t_7 = slice(None, -1, None)
        t_7 = t_8[t_7]
        t_8 = (16, 64)
        t_8 = t_7 + t_8
        t_7 = t_8[0]
        t_6 = t_8[1]
        t_2 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_3.view(t_7, t_6, t_2, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_2 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_2[t_6]
        t_2 = (16, 64)
        t_2 = t_6 + t_2
        t_6 = t_2[0]
        t_7 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_4.view(t_6, t_7, t_3, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_8 = t_8.transpose(-1, -2)
        t_8 = torch.matmul(t_5, t_8)
        t_5 = math.sqrt(64)
        t_5 = t_8 / t_5
        t_5 = t_5 + t_1
        t_5 = self.l_8(t_5)
        t_5 = self.l_9(t_5)
        t_2 = torch.matmul(t_5, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_5 = t_2.size()
        t_8 = slice(None, -2, None)
        t_8 = t_5[t_8]
        t_5 = (1024,)
        t_5 = t_8 + t_5
        t_8 = t_5[0]
        t_3 = t_5[1]
        t_5 = t_5[2]
        t_5 = t_2.view(t_8, t_3, t_5)
        t_5 = self.l_10(t_5)
        t_5 = self.l_11(t_5)
        t_0 = t_5 + t_0
        t_0 = self.l_12(t_0)
        t_5 = self.l_13(t_0)
        t_5 = torch.nn.functional.gelu(t_5)
        t_5 = self.l_14(t_5)
        t_5 = self.l_15(t_5)
        t_0 = t_5 + t_0
        t_0 = self.l_16(t_0)
        t_1 = (t_0, t_1)
        t_0 = t_1[0]
        t_1 = t_1[1]
        t_5 = self.l_17(t_0)
        t_3 = self.l_18(t_0)
        t_8 = self.l_19(t_0)
        t_2 = t_5.size()
        t_7 = slice(None, -1, None)
        t_7 = t_2[t_7]
        t_2 = (16, 64)
        t_2 = t_7 + t_2
        t_7 = t_2[0]
        t_6 = t_2[1]
        t_4 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_5.view(t_7, t_6, t_4, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_4 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_4[t_6]
        t_4 = (16, 64)
        t_4 = t_6 + t_4
        t_6 = t_4[0]
        t_7 = t_4[1]
        t_5 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_3.view(t_6, t_7, t_5, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_5 = t_8.size()
        t_7 = slice(None, -1, None)
        t_7 = t_5[t_7]
        t_5 = (16, 64)
        t_5 = t_7 + t_5
        t_7 = t_5[0]
        t_6 = t_5[1]
        t_3 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_8.view(t_7, t_6, t_3, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_4 = t_4.transpose(-1, -2)
        t_4 = torch.matmul(t_2, t_4)
        t_2 = math.sqrt(64)
        t_2 = t_4 / t_2
        t_2 = t_2 + t_1
        t_2 = self.l_20(t_2)
        t_2 = self.l_21(t_2)
        t_5 = torch.matmul(t_2, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_5 = t_5.contiguous()
        t_2 = t_5.size()
        t_4 = slice(None, -2, None)
        t_4 = t_2[t_4]
        t_2 = (1024,)
        t_2 = t_4 + t_2
        t_4 = t_2[0]
        t_3 = t_2[1]
        t_2 = t_2[2]
        t_2 = t_5.view(t_4, t_3, t_2)
        t_2 = self.l_22(t_2)
        t_2 = self.l_23(t_2)
        t_0 = t_2 + t_0
        t_0 = self.l_24(t_0)
        t_2 = self.l_25(t_0)
        t_2 = torch.nn.functional.gelu(t_2)
        t_2 = self.l_26(t_2)
        t_2 = self.l_27(t_2)
        t_0 = t_2 + t_0
        t_0 = self.l_28(t_0)
        t_1 = (t_0, t_1)
        t_0 = t_1[0]
        t_1 = t_1[1]
        t_2 = self.l_29(t_0)
        t_3 = self.l_30(t_0)
        t_4 = self.l_31(t_0)
        t_5 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_5[t_6]
        t_5 = (16, 64)
        t_5 = t_6 + t_5
        t_6 = t_5[0]
        t_7 = t_5[1]
        t_8 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_2.view(t_6, t_7, t_8, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_8 = t_3.size()
        t_7 = slice(None, -1, None)
        t_7 = t_8[t_7]
        t_8 = (16, 64)
        t_8 = t_7 + t_8
        t_7 = t_8[0]
        t_6 = t_8[1]
        t_2 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_3.view(t_7, t_6, t_2, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_2 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_2[t_6]
        t_2 = (16, 64)
        t_2 = t_6 + t_2
        t_6 = t_2[0]
        t_7 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_4.view(t_6, t_7, t_3, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_8 = t_8.transpose(-1, -2)
        t_8 = torch.matmul(t_5, t_8)
        t_5 = math.sqrt(64)
        t_5 = t_8 / t_5
        t_5 = t_5 + t_1
        t_5 = self.l_32(t_5)
        t_5 = self.l_33(t_5)
        t_2 = torch.matmul(t_5, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_5 = t_2.size()
        t_8 = slice(None, -2, None)
        t_8 = t_5[t_8]
        t_5 = (1024,)
        t_5 = t_8 + t_5
        t_8 = t_5[0]
        t_3 = t_5[1]
        t_5 = t_5[2]
        t_5 = t_2.view(t_8, t_3, t_5)
        t_5 = self.l_34(t_5)
        t_5 = self.l_35(t_5)
        t_0 = t_5 + t_0
        t_0 = self.l_36(t_0)
        t_5 = self.l_37(t_0)
        t_5 = torch.nn.functional.gelu(t_5)
        t_5 = self.l_38(t_5)
        # returning:
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___277
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/Linear[dense]
        return list(flatten((t_1, t_0, t_5)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition1(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Softmax,
            Dropout,
            Linear,
        )
    LAYER_SCOPES=[
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:1'):
        super(Partition1, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'roberta.encoder.2.output.dropout',
                        'l_1': 'roberta.encoder.2.output.LayerNorm',
                        'l_2': 'roberta.encoder.3.attention.self.query',
                        'l_3': 'roberta.encoder.3.attention.self.key',
                        'l_4': 'roberta.encoder.3.attention.self.value',
                        'l_5': 'roberta.encoder.3.attention.self.softmax',
                        'l_6': 'roberta.encoder.3.attention.self.dropout',
                        'l_7': 'roberta.encoder.3.attention.output.dense',
                        'l_8': 'roberta.encoder.3.attention.output.dropout',
                        'l_9': 'roberta.encoder.3.attention.output.LayerNorm',
                        'l_10': 'roberta.encoder.3.intermediate.dense',
                        'l_11': 'roberta.encoder.3.output.dense',
                        'l_12': 'roberta.encoder.3.output.dropout',
                        'l_13': 'roberta.encoder.3.output.LayerNorm',
                        'l_14': 'roberta.encoder.4.attention.self.query',
                        'l_15': 'roberta.encoder.4.attention.self.key',
                        'l_16': 'roberta.encoder.4.attention.self.value',
                        'l_17': 'roberta.encoder.4.attention.self.softmax',
                        'l_18': 'roberta.encoder.4.attention.self.dropout',
                        'l_19': 'roberta.encoder.4.attention.output.dense',
                        'l_20': 'roberta.encoder.4.attention.output.dropout',
                        'l_21': 'roberta.encoder.4.attention.output.LayerNorm',
                        'l_22': 'roberta.encoder.4.intermediate.dense',
                        'l_23': 'roberta.encoder.4.output.dense',
                        'l_24': 'roberta.encoder.4.output.dropout',
                        'l_25': 'roberta.encoder.4.output.LayerNorm',
                        'l_26': 'roberta.encoder.5.attention.self.query',
                        'l_27': 'roberta.encoder.5.attention.self.key',
                        'l_28': 'roberta.encoder.5.attention.self.value',
                        'l_29': 'roberta.encoder.5.attention.self.softmax',
                        'l_30': 'roberta.encoder.5.attention.self.dropout',
                        'l_31': 'roberta.encoder.5.attention.output.dense',
                        'l_32': 'roberta.encoder.5.attention.output.dropout',
                        'l_33': 'roberta.encoder.5.attention.output.LayerNorm',
                        'l_34': 'roberta.encoder.5.intermediate.dense',
                        'l_35': 'roberta.encoder.5.output.dense',
                        'l_36': 'roberta.encoder.5.output.dropout',
                        'l_37': 'roberta.encoder.5.output.LayerNorm',
                        'l_38': 'roberta.encoder.6.attention.self.query'}
        self.to(self.device)

    def forward(self, *args):
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/Dropout[dropout] <=> self.l_0
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_1
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_2
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_3
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_4
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_5
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_6
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_7
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_8
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_9
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_10
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertOutput[output]/Linear[dense] <=> self.l_11
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertOutput[output]/Dropout[dropout] <=> self.l_12
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[3]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_13
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_14
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_15
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_16
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_17
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_18
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_19
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_20
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_21
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_22
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertOutput[output]/Linear[dense] <=> self.l_23
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertOutput[output]/Dropout[dropout] <=> self.l_24
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[4]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_25
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_26
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_27
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_28
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_29
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_30
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_31
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_32
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_33
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_34
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertOutput[output]/Linear[dense] <=> self.l_35
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertOutput[output]/Dropout[dropout] <=> self.l_36
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_37
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_38
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___277 <=> x0
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> x1
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/Linear[dense] <=> x2

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x2)
        t_0 = t_0 + x1
        t_0 = self.l_1(t_0)
        t_0 = (t_0, x0)
        t_1 = t_0[0]
        t_0 = t_0[1]
        t_2 = self.l_2(t_1)
        t_3 = self.l_3(t_1)
        t_4 = self.l_4(t_1)
        t_5 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_5[t_6]
        t_5 = (16, 64)
        t_5 = t_6 + t_5
        t_6 = t_5[0]
        t_7 = t_5[1]
        t_8 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_2.view(t_6, t_7, t_8, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_8 = t_3.size()
        t_7 = slice(None, -1, None)
        t_7 = t_8[t_7]
        t_8 = (16, 64)
        t_8 = t_7 + t_8
        t_7 = t_8[0]
        t_6 = t_8[1]
        t_2 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_3.view(t_7, t_6, t_2, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_2 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_2[t_6]
        t_2 = (16, 64)
        t_2 = t_6 + t_2
        t_6 = t_2[0]
        t_7 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_4.view(t_6, t_7, t_3, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_8 = t_8.transpose(-1, -2)
        t_8 = torch.matmul(t_5, t_8)
        t_5 = math.sqrt(64)
        t_5 = t_8 / t_5
        t_5 = t_5 + t_0
        t_5 = self.l_5(t_5)
        t_5 = self.l_6(t_5)
        t_2 = torch.matmul(t_5, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_5 = t_2.size()
        t_8 = slice(None, -2, None)
        t_8 = t_5[t_8]
        t_5 = (1024,)
        t_5 = t_8 + t_5
        t_8 = t_5[0]
        t_3 = t_5[1]
        t_5 = t_5[2]
        t_5 = t_2.view(t_8, t_3, t_5)
        t_5 = self.l_7(t_5)
        t_5 = self.l_8(t_5)
        t_1 = t_5 + t_1
        t_1 = self.l_9(t_1)
        t_5 = self.l_10(t_1)
        t_5 = torch.nn.functional.gelu(t_5)
        t_5 = self.l_11(t_5)
        t_5 = self.l_12(t_5)
        t_1 = t_5 + t_1
        t_1 = self.l_13(t_1)
        t_0 = (t_1, t_0)
        t_1 = t_0[0]
        t_0 = t_0[1]
        t_5 = self.l_14(t_1)
        t_3 = self.l_15(t_1)
        t_8 = self.l_16(t_1)
        t_2 = t_5.size()
        t_7 = slice(None, -1, None)
        t_7 = t_2[t_7]
        t_2 = (16, 64)
        t_2 = t_7 + t_2
        t_7 = t_2[0]
        t_6 = t_2[1]
        t_4 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_5.view(t_7, t_6, t_4, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_4 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_4[t_6]
        t_4 = (16, 64)
        t_4 = t_6 + t_4
        t_6 = t_4[0]
        t_7 = t_4[1]
        t_5 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_3.view(t_6, t_7, t_5, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_5 = t_8.size()
        t_7 = slice(None, -1, None)
        t_7 = t_5[t_7]
        t_5 = (16, 64)
        t_5 = t_7 + t_5
        t_7 = t_5[0]
        t_6 = t_5[1]
        t_3 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_8.view(t_7, t_6, t_3, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_4 = t_4.transpose(-1, -2)
        t_4 = torch.matmul(t_2, t_4)
        t_2 = math.sqrt(64)
        t_2 = t_4 / t_2
        t_2 = t_2 + t_0
        t_2 = self.l_17(t_2)
        t_2 = self.l_18(t_2)
        t_5 = torch.matmul(t_2, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_5 = t_5.contiguous()
        t_2 = t_5.size()
        t_4 = slice(None, -2, None)
        t_4 = t_2[t_4]
        t_2 = (1024,)
        t_2 = t_4 + t_2
        t_4 = t_2[0]
        t_3 = t_2[1]
        t_2 = t_2[2]
        t_2 = t_5.view(t_4, t_3, t_2)
        t_2 = self.l_19(t_2)
        t_2 = self.l_20(t_2)
        t_1 = t_2 + t_1
        t_1 = self.l_21(t_1)
        t_2 = self.l_22(t_1)
        t_2 = torch.nn.functional.gelu(t_2)
        t_2 = self.l_23(t_2)
        t_2 = self.l_24(t_2)
        t_1 = t_2 + t_1
        t_1 = self.l_25(t_1)
        t_0 = (t_1, t_0)
        t_1 = t_0[0]
        t_0 = t_0[1]
        t_2 = self.l_26(t_1)
        t_3 = self.l_27(t_1)
        t_4 = self.l_28(t_1)
        t_5 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_5[t_6]
        t_5 = (16, 64)
        t_5 = t_6 + t_5
        t_6 = t_5[0]
        t_7 = t_5[1]
        t_8 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_2.view(t_6, t_7, t_8, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_8 = t_3.size()
        t_7 = slice(None, -1, None)
        t_7 = t_8[t_7]
        t_8 = (16, 64)
        t_8 = t_7 + t_8
        t_7 = t_8[0]
        t_6 = t_8[1]
        t_2 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_3.view(t_7, t_6, t_2, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_2 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_2[t_6]
        t_2 = (16, 64)
        t_2 = t_6 + t_2
        t_6 = t_2[0]
        t_7 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_4.view(t_6, t_7, t_3, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_8 = t_8.transpose(-1, -2)
        t_8 = torch.matmul(t_5, t_8)
        t_5 = math.sqrt(64)
        t_5 = t_8 / t_5
        t_5 = t_5 + t_0
        t_5 = self.l_29(t_5)
        t_5 = self.l_30(t_5)
        t_2 = torch.matmul(t_5, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_5 = t_2.size()
        t_8 = slice(None, -2, None)
        t_8 = t_5[t_8]
        t_5 = (1024,)
        t_5 = t_8 + t_5
        t_8 = t_5[0]
        t_3 = t_5[1]
        t_5 = t_5[2]
        t_5 = t_2.view(t_8, t_3, t_5)
        t_5 = self.l_31(t_5)
        t_5 = self.l_32(t_5)
        t_1 = t_5 + t_1
        t_1 = self.l_33(t_1)
        t_5 = self.l_34(t_1)
        t_5 = torch.nn.functional.gelu(t_5)
        t_5 = self.l_35(t_5)
        t_5 = self.l_36(t_5)
        t_1 = t_5 + t_1
        t_1 = self.l_37(t_1)
        t_0 = (t_1, t_0)
        t_1 = t_0[0]
        t_5 = self.l_38(t_1)
        t_3 = t_5.size()
        t_8 = slice(None, -1, None)
        t_8 = t_3[t_8]
        t_3 = (16, 64)
        t_3 = t_8 + t_3
        t_8 = t_3[0]
        t_2 = t_3[1]
        t_7 = t_3[2]
        # returning:
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/prim::TupleConstruct_765
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___767
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___782
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___784
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___786
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___788
        return list(flatten((t_0, t_1, t_5, t_3, t_8, t_2, t_7)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition2(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Softmax,
            Dropout,
            Linear,
        )
    LAYER_SCOPES=[
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:2'):
        super(Partition2, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [(1, 1), 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'roberta.encoder.6.attention.self.key',
                        'l_1': 'roberta.encoder.6.attention.self.value',
                        'l_2': 'roberta.encoder.6.attention.self.softmax',
                        'l_3': 'roberta.encoder.6.attention.self.dropout',
                        'l_4': 'roberta.encoder.6.attention.output.dense',
                        'l_5': 'roberta.encoder.6.attention.output.dropout',
                        'l_6': 'roberta.encoder.6.attention.output.LayerNorm',
                        'l_7': 'roberta.encoder.6.intermediate.dense',
                        'l_8': 'roberta.encoder.6.output.dense',
                        'l_9': 'roberta.encoder.6.output.dropout',
                        'l_10': 'roberta.encoder.6.output.LayerNorm',
                        'l_11': 'roberta.encoder.7.attention.self.query',
                        'l_12': 'roberta.encoder.7.attention.self.key',
                        'l_13': 'roberta.encoder.7.attention.self.value',
                        'l_14': 'roberta.encoder.7.attention.self.softmax',
                        'l_15': 'roberta.encoder.7.attention.self.dropout',
                        'l_16': 'roberta.encoder.7.attention.output.dense',
                        'l_17': 'roberta.encoder.7.attention.output.dropout',
                        'l_18': 'roberta.encoder.7.attention.output.LayerNorm',
                        'l_19': 'roberta.encoder.7.intermediate.dense',
                        'l_20': 'roberta.encoder.7.output.dense',
                        'l_21': 'roberta.encoder.7.output.dropout',
                        'l_22': 'roberta.encoder.7.output.LayerNorm',
                        'l_23': 'roberta.encoder.8.attention.self.query',
                        'l_24': 'roberta.encoder.8.attention.self.key',
                        'l_25': 'roberta.encoder.8.attention.self.value',
                        'l_26': 'roberta.encoder.8.attention.self.softmax',
                        'l_27': 'roberta.encoder.8.attention.self.dropout',
                        'l_28': 'roberta.encoder.8.attention.output.dense',
                        'l_29': 'roberta.encoder.8.attention.output.dropout',
                        'l_30': 'roberta.encoder.8.attention.output.LayerNorm',
                        'l_31': 'roberta.encoder.8.intermediate.dense',
                        'l_32': 'roberta.encoder.8.output.dense',
                        'l_33': 'roberta.encoder.8.output.dropout',
                        'l_34': 'roberta.encoder.8.output.LayerNorm',
                        'l_35': 'roberta.encoder.9.attention.self.query',
                        'l_36': 'roberta.encoder.9.attention.self.value'}
        self.to(self.device)

    def forward(self, *args):
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_0
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_1
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_2
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_3
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_4
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_5
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_6
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_7
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertOutput[output]/Linear[dense] <=> self.l_8
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertOutput[output]/Dropout[dropout] <=> self.l_9
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_10
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_11
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_12
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_13
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_14
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_15
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_16
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_17
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_18
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_19
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertOutput[output]/Linear[dense] <=> self.l_20
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertOutput[output]/Dropout[dropout] <=> self.l_21
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[7]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_22
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_23
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_24
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_25
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_26
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_27
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_28
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_29
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_30
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_31
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertOutput[output]/Linear[dense] <=> self.l_32
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertOutput[output]/Dropout[dropout] <=> self.l_33
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_34
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_35
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_36
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[5]/prim::TupleConstruct_765 <=> x0
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___767 <=> x1
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> x2
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___782 <=> x3
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___784 <=> x4
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___786 <=> x5
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___788 <=> x6

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5, x6 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = x0[1]
        t_1 = self.l_0(x1)
        t_2 = self.l_1(x1)
        t_3 = x3[3]
        t_3 = x2.view(x4, x5, x6, t_3)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_4 = t_1.size()
        t_5 = slice(None, -1, None)
        t_5 = t_4[t_5]
        t_4 = (16, 64)
        t_4 = t_5 + t_4
        t_5 = t_4[0]
        t_6 = t_4[1]
        t_7 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_1.view(t_5, t_6, t_7, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_7 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        t_7 = (16, 64)
        t_7 = t_6 + t_7
        t_6 = t_7[0]
        t_5 = t_7[1]
        t_1 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_2.view(t_6, t_5, t_1, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_4 = t_4.transpose(-1, -2)
        t_4 = torch.matmul(t_3, t_4)
        t_3 = math.sqrt(64)
        t_3 = t_4 / t_3
        t_3 = t_3 + t_0
        t_3 = self.l_2(t_3)
        t_3 = self.l_3(t_3)
        t_7 = torch.matmul(t_3, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_7 = t_7.contiguous()
        t_3 = t_7.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        t_3 = (1024,)
        t_3 = t_4 + t_3
        t_4 = t_3[0]
        t_1 = t_3[1]
        t_3 = t_3[2]
        t_3 = t_7.view(t_4, t_1, t_3)
        t_3 = self.l_4(t_3)
        t_3 = self.l_5(t_3)
        t_3 = t_3 + x1
        t_3 = self.l_6(t_3)
        t_1 = self.l_7(t_3)
        t_1 = torch.nn.functional.gelu(t_1)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        t_3 = t_1 + t_3
        t_3 = self.l_10(t_3)
        t_0 = (t_3, t_0)
        t_3 = t_0[0]
        t_0 = t_0[1]
        t_1 = self.l_11(t_3)
        t_4 = self.l_12(t_3)
        t_7 = self.l_13(t_3)
        t_5 = t_1.size()
        t_6 = slice(None, -1, None)
        t_6 = t_5[t_6]
        t_5 = (16, 64)
        t_5 = t_6 + t_5
        t_6 = t_5[0]
        t_2 = t_5[1]
        t_8 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_1.view(t_6, t_2, t_8, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_8 = t_4.size()
        t_2 = slice(None, -1, None)
        t_2 = t_8[t_2]
        t_8 = (16, 64)
        t_8 = t_2 + t_8
        t_2 = t_8[0]
        t_6 = t_8[1]
        t_1 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_4.view(t_2, t_6, t_1, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_1 = t_7.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        t_1 = (16, 64)
        t_1 = t_6 + t_1
        t_6 = t_1[0]
        t_2 = t_1[1]
        t_4 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_7.view(t_6, t_2, t_4, t_1)
        t_1 = t_1.permute(0, 2, 1, 3)
        t_8 = t_8.transpose(-1, -2)
        t_8 = torch.matmul(t_5, t_8)
        t_5 = math.sqrt(64)
        t_5 = t_8 / t_5
        t_5 = t_5 + t_0
        t_5 = self.l_14(t_5)
        t_5 = self.l_15(t_5)
        t_1 = torch.matmul(t_5, t_1)
        t_1 = t_1.permute(0, 2, 1, 3)
        t_1 = t_1.contiguous()
        t_5 = t_1.size()
        t_8 = slice(None, -2, None)
        t_8 = t_5[t_8]
        t_5 = (1024,)
        t_5 = t_8 + t_5
        t_8 = t_5[0]
        t_4 = t_5[1]
        t_5 = t_5[2]
        t_5 = t_1.view(t_8, t_4, t_5)
        t_5 = self.l_16(t_5)
        t_5 = self.l_17(t_5)
        t_3 = t_5 + t_3
        t_3 = self.l_18(t_3)
        t_5 = self.l_19(t_3)
        t_5 = torch.nn.functional.gelu(t_5)
        t_5 = self.l_20(t_5)
        t_5 = self.l_21(t_5)
        t_3 = t_5 + t_3
        t_3 = self.l_22(t_3)
        t_0 = (t_3, t_0)
        t_3 = t_0[0]
        t_0 = t_0[1]
        t_5 = self.l_23(t_3)
        t_4 = self.l_24(t_3)
        t_8 = self.l_25(t_3)
        t_1 = t_5.size()
        t_2 = slice(None, -1, None)
        t_2 = t_1[t_2]
        t_1 = (16, 64)
        t_1 = t_2 + t_1
        t_2 = t_1[0]
        t_6 = t_1[1]
        t_7 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_5.view(t_2, t_6, t_7, t_1)
        t_1 = t_1.permute(0, 2, 1, 3)
        t_7 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_7[t_6]
        t_7 = (16, 64)
        t_7 = t_6 + t_7
        t_6 = t_7[0]
        t_2 = t_7[1]
        t_5 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_4.view(t_6, t_2, t_5, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_5 = t_8.size()
        t_2 = slice(None, -1, None)
        t_2 = t_5[t_2]
        t_5 = (16, 64)
        t_5 = t_2 + t_5
        t_2 = t_5[0]
        t_6 = t_5[1]
        t_4 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_8.view(t_2, t_6, t_4, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_7 = t_7.transpose(-1, -2)
        t_7 = torch.matmul(t_1, t_7)
        t_1 = math.sqrt(64)
        t_1 = t_7 / t_1
        t_1 = t_1 + t_0
        t_1 = self.l_26(t_1)
        t_1 = self.l_27(t_1)
        t_5 = torch.matmul(t_1, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_5 = t_5.contiguous()
        t_1 = t_5.size()
        t_7 = slice(None, -2, None)
        t_7 = t_1[t_7]
        t_1 = (1024,)
        t_1 = t_7 + t_1
        t_7 = t_1[0]
        t_4 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_5.view(t_7, t_4, t_1)
        t_1 = self.l_28(t_1)
        t_1 = self.l_29(t_1)
        t_3 = t_1 + t_3
        t_3 = self.l_30(t_3)
        t_1 = self.l_31(t_3)
        t_1 = torch.nn.functional.gelu(t_1)
        t_1 = self.l_32(t_1)
        t_1 = self.l_33(t_1)
        t_3 = t_1 + t_3
        t_3 = self.l_34(t_3)
        t_0 = (t_3, t_0)
        t_3 = t_0[0]
        t_1 = self.l_35(t_3)
        t_4 = self.l_36(t_3)
        t_7 = t_4.size()
        t_5 = slice(None, -1, None)
        t_5 = t_7[t_5]
        t_7 = (16, 64)
        t_7 = t_5 + t_7
        t_5 = t_7[0]
        # returning:
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/prim::TupleConstruct_1134
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1136
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1199
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1201
        return list(flatten((t_0, t_3, t_1, t_4, t_7, t_5)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition3(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Softmax,
            Dropout,
            Linear,
        )
    LAYER_SCOPES=[
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:3'):
        super(Partition3, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [(1, 1), 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'roberta.encoder.9.attention.self.key',
                        'l_1': 'roberta.encoder.9.attention.self.softmax',
                        'l_2': 'roberta.encoder.9.attention.self.dropout',
                        'l_3': 'roberta.encoder.9.attention.output.dense',
                        'l_4': 'roberta.encoder.9.attention.output.dropout',
                        'l_5': 'roberta.encoder.9.attention.output.LayerNorm',
                        'l_6': 'roberta.encoder.9.intermediate.dense',
                        'l_7': 'roberta.encoder.9.output.dense',
                        'l_8': 'roberta.encoder.9.output.dropout',
                        'l_9': 'roberta.encoder.9.output.LayerNorm',
                        'l_10': 'roberta.encoder.10.attention.self.query',
                        'l_11': 'roberta.encoder.10.attention.self.key',
                        'l_12': 'roberta.encoder.10.attention.self.value',
                        'l_13': 'roberta.encoder.10.attention.self.softmax',
                        'l_14': 'roberta.encoder.10.attention.self.dropout',
                        'l_15': 'roberta.encoder.10.attention.output.dense',
                        'l_16': 'roberta.encoder.10.attention.output.dropout',
                        'l_17': 'roberta.encoder.10.attention.output.LayerNorm',
                        'l_18': 'roberta.encoder.10.intermediate.dense',
                        'l_19': 'roberta.encoder.10.output.dense',
                        'l_20': 'roberta.encoder.10.output.dropout',
                        'l_21': 'roberta.encoder.10.output.LayerNorm',
                        'l_22': 'roberta.encoder.11.attention.self.query',
                        'l_23': 'roberta.encoder.11.attention.self.key',
                        'l_24': 'roberta.encoder.11.attention.self.value',
                        'l_25': 'roberta.encoder.11.attention.self.softmax',
                        'l_26': 'roberta.encoder.11.attention.self.dropout',
                        'l_27': 'roberta.encoder.11.attention.output.dense',
                        'l_28': 'roberta.encoder.11.attention.output.dropout',
                        'l_29': 'roberta.encoder.11.attention.output.LayerNorm',
                        'l_30': 'roberta.encoder.11.intermediate.dense',
                        'l_31': 'roberta.encoder.11.output.dense',
                        'l_32': 'roberta.encoder.11.output.dropout',
                        'l_33': 'roberta.encoder.11.output.LayerNorm',
                        'l_34': 'roberta.encoder.12.attention.self.query',
                        'l_35': 'roberta.encoder.12.attention.self.key',
                        'l_36': 'roberta.encoder.12.attention.self.value'}
        self.to(self.device)

    def forward(self, *args):
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_0
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_1
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_2
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_3
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_4
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_5
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_6
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/Linear[dense] <=> self.l_7
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/Dropout[dropout] <=> self.l_8
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_9
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_10
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_11
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_12
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_13
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_14
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_15
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_16
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_17
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_18
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertOutput[output]/Linear[dense] <=> self.l_19
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertOutput[output]/Dropout[dropout] <=> self.l_20
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[10]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_21
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_22
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_23
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_24
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_25
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_26
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_27
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_28
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_29
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_30
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertOutput[output]/Linear[dense] <=> self.l_31
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertOutput[output]/Dropout[dropout] <=> self.l_32
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_33
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_34
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_35
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_36
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[8]/prim::TupleConstruct_1134 <=> x0
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1136 <=> x1
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> x2
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> x3
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1199 <=> x4
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1201 <=> x5

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = x0[1]
        t_1 = self.l_0(x1)
        t_2 = x2.size()
        t_3 = slice(None, -1, None)
        t_3 = t_2[t_3]
        t_2 = (16, 64)
        t_2 = t_3 + t_2
        t_3 = t_2[0]
        t_4 = t_2[1]
        t_5 = t_2[2]
        t_2 = t_2[3]
        t_2 = x2.view(t_3, t_4, t_5, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_5 = t_1.size()
        t_4 = slice(None, -1, None)
        t_4 = t_5[t_4]
        t_5 = (16, 64)
        t_5 = t_4 + t_5
        t_4 = t_5[0]
        t_3 = t_5[1]
        t_6 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_1.view(t_4, t_3, t_6, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_6 = x4[1]
        t_3 = x4[2]
        t_4 = x4[3]
        t_4 = x3.view(x5, t_6, t_3, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_5 = t_5.transpose(-1, -2)
        t_5 = torch.matmul(t_2, t_5)
        t_2 = math.sqrt(64)
        t_2 = t_5 / t_2
        t_2 = t_2 + t_0
        t_2 = self.l_1(t_2)
        t_2 = self.l_2(t_2)
        t_4 = torch.matmul(t_2, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_5 = slice(None, -2, None)
        t_5 = t_2[t_5]
        t_2 = (1024,)
        t_2 = t_5 + t_2
        t_5 = t_2[0]
        t_3 = t_2[1]
        t_2 = t_2[2]
        t_2 = t_4.view(t_5, t_3, t_2)
        t_2 = self.l_3(t_2)
        t_2 = self.l_4(t_2)
        t_2 = t_2 + x1
        t_2 = self.l_5(t_2)
        t_3 = self.l_6(t_2)
        t_3 = torch.nn.functional.gelu(t_3)
        t_3 = self.l_7(t_3)
        t_3 = self.l_8(t_3)
        t_2 = t_3 + t_2
        t_2 = self.l_9(t_2)
        t_0 = (t_2, t_0)
        t_2 = t_0[0]
        t_0 = t_0[1]
        t_3 = self.l_10(t_2)
        t_5 = self.l_11(t_2)
        t_4 = self.l_12(t_2)
        t_6 = t_3.size()
        t_1 = slice(None, -1, None)
        t_1 = t_6[t_1]
        t_6 = (16, 64)
        t_6 = t_1 + t_6
        t_1 = t_6[0]
        t_7 = t_6[1]
        t_8 = t_6[2]
        t_6 = t_6[3]
        t_6 = t_3.view(t_1, t_7, t_8, t_6)
        t_6 = t_6.permute(0, 2, 1, 3)
        t_8 = t_5.size()
        t_7 = slice(None, -1, None)
        t_7 = t_8[t_7]
        t_8 = (16, 64)
        t_8 = t_7 + t_8
        t_7 = t_8[0]
        t_1 = t_8[1]
        t_3 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_5.view(t_7, t_1, t_3, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_3 = t_4.size()
        t_1 = slice(None, -1, None)
        t_1 = t_3[t_1]
        t_3 = (16, 64)
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_7 = t_3[1]
        t_5 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_4.view(t_1, t_7, t_5, t_3)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_8 = t_8.transpose(-1, -2)
        t_8 = torch.matmul(t_6, t_8)
        t_6 = math.sqrt(64)
        t_6 = t_8 / t_6
        t_6 = t_6 + t_0
        t_6 = self.l_13(t_6)
        t_6 = self.l_14(t_6)
        t_3 = torch.matmul(t_6, t_3)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_6 = t_3.size()
        t_8 = slice(None, -2, None)
        t_8 = t_6[t_8]
        t_6 = (1024,)
        t_6 = t_8 + t_6
        t_8 = t_6[0]
        t_5 = t_6[1]
        t_6 = t_6[2]
        t_6 = t_3.view(t_8, t_5, t_6)
        t_6 = self.l_15(t_6)
        t_6 = self.l_16(t_6)
        t_2 = t_6 + t_2
        t_2 = self.l_17(t_2)
        t_6 = self.l_18(t_2)
        t_6 = torch.nn.functional.gelu(t_6)
        t_6 = self.l_19(t_6)
        t_6 = self.l_20(t_6)
        t_2 = t_6 + t_2
        t_2 = self.l_21(t_2)
        t_0 = (t_2, t_0)
        t_2 = t_0[0]
        t_0 = t_0[1]
        t_6 = self.l_22(t_2)
        t_5 = self.l_23(t_2)
        t_8 = self.l_24(t_2)
        t_3 = t_6.size()
        t_7 = slice(None, -1, None)
        t_7 = t_3[t_7]
        t_3 = (16, 64)
        t_3 = t_7 + t_3
        t_7 = t_3[0]
        t_1 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_6.view(t_7, t_1, t_4, t_3)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_4 = t_5.size()
        t_1 = slice(None, -1, None)
        t_1 = t_4[t_1]
        t_4 = (16, 64)
        t_4 = t_1 + t_4
        t_1 = t_4[0]
        t_7 = t_4[1]
        t_6 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_5.view(t_1, t_7, t_6, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_6 = t_8.size()
        t_7 = slice(None, -1, None)
        t_7 = t_6[t_7]
        t_6 = (16, 64)
        t_6 = t_7 + t_6
        t_7 = t_6[0]
        t_1 = t_6[1]
        t_5 = t_6[2]
        t_6 = t_6[3]
        t_6 = t_8.view(t_7, t_1, t_5, t_6)
        t_6 = t_6.permute(0, 2, 1, 3)
        t_4 = t_4.transpose(-1, -2)
        t_4 = torch.matmul(t_3, t_4)
        t_3 = math.sqrt(64)
        t_3 = t_4 / t_3
        t_3 = t_3 + t_0
        t_3 = self.l_25(t_3)
        t_3 = self.l_26(t_3)
        t_6 = torch.matmul(t_3, t_6)
        t_6 = t_6.permute(0, 2, 1, 3)
        t_6 = t_6.contiguous()
        t_3 = t_6.size()
        t_4 = slice(None, -2, None)
        t_4 = t_3[t_4]
        t_3 = (1024,)
        t_3 = t_4 + t_3
        t_4 = t_3[0]
        t_5 = t_3[1]
        t_3 = t_3[2]
        t_3 = t_6.view(t_4, t_5, t_3)
        t_3 = self.l_27(t_3)
        t_3 = self.l_28(t_3)
        t_2 = t_3 + t_2
        t_2 = self.l_29(t_2)
        t_3 = self.l_30(t_2)
        t_3 = torch.nn.functional.gelu(t_3)
        t_3 = self.l_31(t_3)
        t_3 = self.l_32(t_3)
        t_2 = t_3 + t_2
        t_2 = self.l_33(t_2)
        t_0 = (t_2, t_0)
        t_2 = t_0[0]
        t_3 = self.l_34(t_2)
        t_5 = self.l_35(t_2)
        t_4 = self.l_36(t_2)
        t_6 = t_3.size()
        t_1 = slice(None, -1, None)
        t_1 = t_6[t_1]
        t_6 = (16, 64)
        t_6 = t_1 + t_6
        t_1 = t_6[3]
        t_7 = t_5.size()
        t_8 = slice(None, -1, None)
        t_8 = t_7[t_8]
        t_7 = (16, 64)
        t_7 = t_8 + t_7
        t_8 = t_7[0]
        t_9 = t_7[1]
        t_10 = t_4.size()
        t_11 = slice(None, -1, None)
        t_11 = t_10[t_11]
        t_10 = (16, 64)
        t_10 = t_11 + t_10
        t_11 = t_10[0]
        t_12 = t_10[1]
        t_13 = t_10[2]
        t_10 = t_10[3]
        t_10 = t_4.view(t_11, t_12, t_13, t_10)
        # returning:
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/prim::TupleConstruct_1503
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1505
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1520
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1528
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1544
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1546
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1548
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Tensor::view_1577
        return list(flatten((t_0, t_2, t_3, t_5, t_6, t_1, t_7, t_8, t_9, t_10)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition4(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Softmax,
            Dropout,
            Linear,
        )
    LAYER_SCOPES=[
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:4'):
        super(Partition4, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [(1, 1), 1, 1, 1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'roberta.encoder.12.attention.self.softmax',
                        'l_1': 'roberta.encoder.12.attention.self.dropout',
                        'l_2': 'roberta.encoder.12.attention.output.dense',
                        'l_3': 'roberta.encoder.12.attention.output.dropout',
                        'l_4': 'roberta.encoder.12.attention.output.LayerNorm',
                        'l_5': 'roberta.encoder.12.intermediate.dense',
                        'l_6': 'roberta.encoder.12.output.dense',
                        'l_7': 'roberta.encoder.12.output.dropout',
                        'l_8': 'roberta.encoder.12.output.LayerNorm',
                        'l_9': 'roberta.encoder.13.attention.self.query',
                        'l_10': 'roberta.encoder.13.attention.self.key',
                        'l_11': 'roberta.encoder.13.attention.self.value',
                        'l_12': 'roberta.encoder.13.attention.self.softmax',
                        'l_13': 'roberta.encoder.13.attention.self.dropout',
                        'l_14': 'roberta.encoder.13.attention.output.dense',
                        'l_15': 'roberta.encoder.13.attention.output.dropout',
                        'l_16': 'roberta.encoder.13.attention.output.LayerNorm',
                        'l_17': 'roberta.encoder.13.intermediate.dense',
                        'l_18': 'roberta.encoder.13.output.dense',
                        'l_19': 'roberta.encoder.13.output.dropout',
                        'l_20': 'roberta.encoder.13.output.LayerNorm',
                        'l_21': 'roberta.encoder.14.attention.self.query',
                        'l_22': 'roberta.encoder.14.attention.self.key',
                        'l_23': 'roberta.encoder.14.attention.self.value',
                        'l_24': 'roberta.encoder.14.attention.self.softmax',
                        'l_25': 'roberta.encoder.14.attention.self.dropout',
                        'l_26': 'roberta.encoder.14.attention.output.dense',
                        'l_27': 'roberta.encoder.14.attention.output.dropout',
                        'l_28': 'roberta.encoder.14.attention.output.LayerNorm',
                        'l_29': 'roberta.encoder.14.intermediate.dense',
                        'l_30': 'roberta.encoder.14.output.dense',
                        'l_31': 'roberta.encoder.14.output.dropout',
                        'l_32': 'roberta.encoder.14.output.LayerNorm',
                        'l_33': 'roberta.encoder.15.attention.self.query',
                        'l_34': 'roberta.encoder.15.attention.self.key',
                        'l_35': 'roberta.encoder.15.attention.self.value'}
        self.to(self.device)

    def forward(self, *args):
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_0
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_1
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_2
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_3
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_4
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_5
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertOutput[output]/Linear[dense] <=> self.l_6
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertOutput[output]/Dropout[dropout] <=> self.l_7
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_8
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_9
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_10
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_11
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_12
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_13
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_14
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_15
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_16
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_17
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertOutput[output]/Linear[dense] <=> self.l_18
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertOutput[output]/Dropout[dropout] <=> self.l_19
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[13]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_20
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_21
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_22
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_23
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_24
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_25
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_26
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_27
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_28
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_29
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertOutput[output]/Linear[dense] <=> self.l_30
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertOutput[output]/Dropout[dropout] <=> self.l_31
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[14]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_32
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_33
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_34
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_35
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[11]/prim::TupleConstruct_1503 <=> x0
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1505 <=> x1
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> x2
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> x3
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1520 <=> x4
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1528 <=> x5
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1544 <=> x6
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1546 <=> x7
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1548 <=> x8
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Tensor::view_1577 <=> x9

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5, x6, x7, x8, x9 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = x0[1]
        t_1 = x4[0]
        t_2 = x4[1]
        t_3 = x4[2]
        t_3 = x2.view(t_1, t_2, t_3, x5)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_2 = x6[2]
        t_1 = x6[3]
        t_1 = x3.view(x7, x8, t_2, t_1)
        t_1 = t_1.permute(0, 2, 1, 3)
        t_2 = x9.permute(0, 2, 1, 3)
        t_1 = t_1.transpose(-1, -2)
        t_1 = torch.matmul(t_3, t_1)
        t_3 = math.sqrt(64)
        t_3 = t_1 / t_3
        t_3 = t_3 + t_0
        t_3 = self.l_0(t_3)
        t_3 = self.l_1(t_3)
        t_2 = torch.matmul(t_3, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_3 = t_2.size()
        t_1 = slice(None, -2, None)
        t_1 = t_3[t_1]
        t_3 = (1024,)
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = t_3[1]
        t_3 = t_3[2]
        t_3 = t_2.view(t_1, t_4, t_3)
        t_3 = self.l_2(t_3)
        t_3 = self.l_3(t_3)
        t_3 = t_3 + x1
        t_3 = self.l_4(t_3)
        t_4 = self.l_5(t_3)
        t_4 = torch.nn.functional.gelu(t_4)
        t_4 = self.l_6(t_4)
        t_4 = self.l_7(t_4)
        t_3 = t_4 + t_3
        t_3 = self.l_8(t_3)
        t_0 = (t_3, t_0)
        t_3 = t_0[0]
        t_0 = t_0[1]
        t_4 = self.l_9(t_3)
        t_1 = self.l_10(t_3)
        t_2 = self.l_11(t_3)
        t_5 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_5[t_6]
        t_5 = (16, 64)
        t_5 = t_6 + t_5
        t_6 = t_5[0]
        t_7 = t_5[1]
        t_8 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_4.view(t_6, t_7, t_8, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_8 = t_1.size()
        t_7 = slice(None, -1, None)
        t_7 = t_8[t_7]
        t_8 = (16, 64)
        t_8 = t_7 + t_8
        t_7 = t_8[0]
        t_6 = t_8[1]
        t_4 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_1.view(t_7, t_6, t_4, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_4 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_4[t_6]
        t_4 = (16, 64)
        t_4 = t_6 + t_4
        t_6 = t_4[0]
        t_7 = t_4[1]
        t_1 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_2.view(t_6, t_7, t_1, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_8 = t_8.transpose(-1, -2)
        t_8 = torch.matmul(t_5, t_8)
        t_5 = math.sqrt(64)
        t_5 = t_8 / t_5
        t_5 = t_5 + t_0
        t_5 = self.l_12(t_5)
        t_5 = self.l_13(t_5)
        t_4 = torch.matmul(t_5, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_5 = t_4.size()
        t_8 = slice(None, -2, None)
        t_8 = t_5[t_8]
        t_5 = (1024,)
        t_5 = t_8 + t_5
        t_8 = t_5[0]
        t_1 = t_5[1]
        t_5 = t_5[2]
        t_5 = t_4.view(t_8, t_1, t_5)
        t_5 = self.l_14(t_5)
        t_5 = self.l_15(t_5)
        t_3 = t_5 + t_3
        t_3 = self.l_16(t_3)
        t_5 = self.l_17(t_3)
        t_5 = torch.nn.functional.gelu(t_5)
        t_5 = self.l_18(t_5)
        t_5 = self.l_19(t_5)
        t_3 = t_5 + t_3
        t_3 = self.l_20(t_3)
        t_0 = (t_3, t_0)
        t_3 = t_0[0]
        t_0 = t_0[1]
        t_5 = self.l_21(t_3)
        t_1 = self.l_22(t_3)
        t_8 = self.l_23(t_3)
        t_4 = t_5.size()
        t_7 = slice(None, -1, None)
        t_7 = t_4[t_7]
        t_4 = (16, 64)
        t_4 = t_7 + t_4
        t_7 = t_4[0]
        t_6 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_5.view(t_7, t_6, t_2, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_2 = t_1.size()
        t_6 = slice(None, -1, None)
        t_6 = t_2[t_6]
        t_2 = (16, 64)
        t_2 = t_6 + t_2
        t_6 = t_2[0]
        t_7 = t_2[1]
        t_5 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_1.view(t_6, t_7, t_5, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_5 = t_8.size()
        t_7 = slice(None, -1, None)
        t_7 = t_5[t_7]
        t_5 = (16, 64)
        t_5 = t_7 + t_5
        t_7 = t_5[0]
        t_6 = t_5[1]
        t_1 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_8.view(t_7, t_6, t_1, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_2 = t_2.transpose(-1, -2)
        t_2 = torch.matmul(t_4, t_2)
        t_4 = math.sqrt(64)
        t_4 = t_2 / t_4
        t_4 = t_4 + t_0
        t_4 = self.l_24(t_4)
        t_4 = self.l_25(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_5 = t_5.contiguous()
        t_4 = t_5.size()
        t_2 = slice(None, -2, None)
        t_2 = t_4[t_2]
        t_4 = (1024,)
        t_4 = t_2 + t_4
        t_2 = t_4[0]
        t_1 = t_4[1]
        t_4 = t_4[2]
        t_4 = t_5.view(t_2, t_1, t_4)
        t_4 = self.l_26(t_4)
        t_4 = self.l_27(t_4)
        t_3 = t_4 + t_3
        t_3 = self.l_28(t_3)
        t_4 = self.l_29(t_3)
        t_4 = torch.nn.functional.gelu(t_4)
        t_4 = self.l_30(t_4)
        t_4 = self.l_31(t_4)
        t_3 = t_4 + t_3
        t_3 = self.l_32(t_3)
        t_0 = (t_3, t_0)
        t_3 = t_0[0]
        t_0 = t_0[1]
        t_4 = self.l_33(t_3)
        t_1 = self.l_34(t_3)
        t_2 = self.l_35(t_3)
        t_5 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_5[t_6]
        t_5 = t_1.size()
        t_7 = slice(None, -1, None)
        t_7 = t_5[t_7]
        t_5 = (16, 64)
        t_5 = t_7 + t_5
        t_7 = t_5[0]
        t_8 = t_5[1]
        t_9 = t_5[3]
        t_10 = t_2.size()
        t_11 = slice(None, -1, None)
        t_11 = t_10[t_11]
        t_10 = (16, 64)
        t_10 = t_11 + t_10
        t_11 = t_10[0]
        t_12 = t_10[1]
        t_13 = t_10[2]
        t_10 = t_10[3]
        t_10 = t_2.view(t_11, t_12, t_13, t_10)
        t_10 = t_10.permute(0, 2, 1, 3)
        # returning:
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1874
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1876
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1885
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1913
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1915
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1917
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1921
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Tensor::permute_1951
        return list(flatten((t_3, t_0, t_4, t_1, t_6, t_5, t_7, t_8, t_9, t_10)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition5(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Softmax,
            Dropout,
            Linear,
        )
    LAYER_SCOPES=[
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:5'):
        super(Partition5, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'roberta.encoder.15.attention.self.softmax',
                        'l_1': 'roberta.encoder.15.attention.self.dropout',
                        'l_2': 'roberta.encoder.15.attention.output.dense',
                        'l_3': 'roberta.encoder.15.attention.output.dropout',
                        'l_4': 'roberta.encoder.15.attention.output.LayerNorm',
                        'l_5': 'roberta.encoder.15.intermediate.dense',
                        'l_6': 'roberta.encoder.15.output.dense',
                        'l_7': 'roberta.encoder.15.output.dropout',
                        'l_8': 'roberta.encoder.15.output.LayerNorm',
                        'l_9': 'roberta.encoder.16.attention.self.query',
                        'l_10': 'roberta.encoder.16.attention.self.key',
                        'l_11': 'roberta.encoder.16.attention.self.value',
                        'l_12': 'roberta.encoder.16.attention.self.softmax',
                        'l_13': 'roberta.encoder.16.attention.self.dropout',
                        'l_14': 'roberta.encoder.16.attention.output.dense',
                        'l_15': 'roberta.encoder.16.attention.output.dropout',
                        'l_16': 'roberta.encoder.16.attention.output.LayerNorm',
                        'l_17': 'roberta.encoder.16.intermediate.dense',
                        'l_18': 'roberta.encoder.16.output.dense',
                        'l_19': 'roberta.encoder.16.output.dropout',
                        'l_20': 'roberta.encoder.16.output.LayerNorm',
                        'l_21': 'roberta.encoder.17.attention.self.query',
                        'l_22': 'roberta.encoder.17.attention.self.key',
                        'l_23': 'roberta.encoder.17.attention.self.value',
                        'l_24': 'roberta.encoder.17.attention.self.softmax',
                        'l_25': 'roberta.encoder.17.attention.self.dropout',
                        'l_26': 'roberta.encoder.17.attention.output.dense',
                        'l_27': 'roberta.encoder.17.attention.output.dropout',
                        'l_28': 'roberta.encoder.17.attention.output.LayerNorm',
                        'l_29': 'roberta.encoder.17.intermediate.dense',
                        'l_30': 'roberta.encoder.17.output.dense',
                        'l_31': 'roberta.encoder.17.output.dropout',
                        'l_32': 'roberta.encoder.17.output.LayerNorm',
                        'l_33': 'roberta.encoder.18.attention.self.query',
                        'l_34': 'roberta.encoder.18.attention.self.key',
                        'l_35': 'roberta.encoder.18.attention.self.value'}
        self.to(self.device)

    def forward(self, *args):
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_0
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_1
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_2
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_3
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_4
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_5
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertOutput[output]/Linear[dense] <=> self.l_6
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertOutput[output]/Dropout[dropout] <=> self.l_7
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_8
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_9
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_10
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_11
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_12
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_13
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_14
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_15
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_16
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_17
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertOutput[output]/Linear[dense] <=> self.l_18
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertOutput[output]/Dropout[dropout] <=> self.l_19
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[16]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_20
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_21
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_22
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_23
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_24
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_25
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_26
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_27
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_28
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_29
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertOutput[output]/Linear[dense] <=> self.l_30
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertOutput[output]/Dropout[dropout] <=> self.l_31
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_32
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_33
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_34
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_35
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1874 <=> x0
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___1876 <=> x1
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> x2
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> x3
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1885 <=> x4
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___1913 <=> x5
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1915 <=> x6
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1917 <=> x7
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___1921 <=> x8
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Tensor::permute_1951 <=> x9

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5, x6, x7, x8, x9 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = (16, 64)
        t_0 = x4 + t_0
        t_1 = t_0[0]
        t_2 = t_0[1]
        t_3 = t_0[2]
        t_0 = t_0[3]
        t_0 = x2.view(t_1, t_2, t_3, t_0)
        t_0 = t_0.permute(0, 2, 1, 3)
        t_3 = x5[2]
        t_3 = x3.view(x6, x7, t_3, x8)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.transpose(-1, -2)
        t_3 = torch.matmul(t_0, t_3)
        t_0 = math.sqrt(64)
        t_0 = t_3 / t_0
        t_0 = t_0 + x1
        t_0 = self.l_0(t_0)
        t_0 = self.l_1(t_0)
        t_0 = torch.matmul(t_0, x9)
        t_0 = t_0.permute(0, 2, 1, 3)
        t_0 = t_0.contiguous()
        t_3 = t_0.size()
        t_2 = slice(None, -2, None)
        t_2 = t_3[t_2]
        t_3 = (1024,)
        t_3 = t_2 + t_3
        t_2 = t_3[0]
        t_1 = t_3[1]
        t_3 = t_3[2]
        t_3 = t_0.view(t_2, t_1, t_3)
        t_3 = self.l_2(t_3)
        t_3 = self.l_3(t_3)
        t_3 = t_3 + x0
        t_3 = self.l_4(t_3)
        t_1 = self.l_5(t_3)
        t_1 = torch.nn.functional.gelu(t_1)
        t_1 = self.l_6(t_1)
        t_1 = self.l_7(t_1)
        t_3 = t_1 + t_3
        t_3 = self.l_8(t_3)
        t_3 = (t_3, x1)
        t_1 = t_3[0]
        t_3 = t_3[1]
        t_2 = self.l_9(t_1)
        t_0 = self.l_10(t_1)
        t_4 = self.l_11(t_1)
        t_5 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_5[t_6]
        t_5 = (16, 64)
        t_5 = t_6 + t_5
        t_6 = t_5[0]
        t_7 = t_5[1]
        t_8 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_2.view(t_6, t_7, t_8, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_8 = t_0.size()
        t_7 = slice(None, -1, None)
        t_7 = t_8[t_7]
        t_8 = (16, 64)
        t_8 = t_7 + t_8
        t_7 = t_8[0]
        t_6 = t_8[1]
        t_2 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_0.view(t_7, t_6, t_2, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_2 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_2[t_6]
        t_2 = (16, 64)
        t_2 = t_6 + t_2
        t_6 = t_2[0]
        t_7 = t_2[1]
        t_0 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_4.view(t_6, t_7, t_0, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_8 = t_8.transpose(-1, -2)
        t_8 = torch.matmul(t_5, t_8)
        t_5 = math.sqrt(64)
        t_5 = t_8 / t_5
        t_5 = t_5 + t_3
        t_5 = self.l_12(t_5)
        t_5 = self.l_13(t_5)
        t_2 = torch.matmul(t_5, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_5 = t_2.size()
        t_8 = slice(None, -2, None)
        t_8 = t_5[t_8]
        t_5 = (1024,)
        t_5 = t_8 + t_5
        t_8 = t_5[0]
        t_0 = t_5[1]
        t_5 = t_5[2]
        t_5 = t_2.view(t_8, t_0, t_5)
        t_5 = self.l_14(t_5)
        t_5 = self.l_15(t_5)
        t_1 = t_5 + t_1
        t_1 = self.l_16(t_1)
        t_5 = self.l_17(t_1)
        t_5 = torch.nn.functional.gelu(t_5)
        t_5 = self.l_18(t_5)
        t_5 = self.l_19(t_5)
        t_1 = t_5 + t_1
        t_1 = self.l_20(t_1)
        t_3 = (t_1, t_3)
        t_1 = t_3[0]
        t_3 = t_3[1]
        t_5 = self.l_21(t_1)
        t_0 = self.l_22(t_1)
        t_8 = self.l_23(t_1)
        t_2 = t_5.size()
        t_7 = slice(None, -1, None)
        t_7 = t_2[t_7]
        t_2 = (16, 64)
        t_2 = t_7 + t_2
        t_7 = t_2[0]
        t_6 = t_2[1]
        t_4 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_5.view(t_7, t_6, t_4, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_4 = t_0.size()
        t_6 = slice(None, -1, None)
        t_6 = t_4[t_6]
        t_4 = (16, 64)
        t_4 = t_6 + t_4
        t_6 = t_4[0]
        t_7 = t_4[1]
        t_5 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_0.view(t_6, t_7, t_5, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_5 = t_8.size()
        t_7 = slice(None, -1, None)
        t_7 = t_5[t_7]
        t_5 = (16, 64)
        t_5 = t_7 + t_5
        t_7 = t_5[0]
        t_6 = t_5[1]
        t_0 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_8.view(t_7, t_6, t_0, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_4 = t_4.transpose(-1, -2)
        t_4 = torch.matmul(t_2, t_4)
        t_2 = math.sqrt(64)
        t_2 = t_4 / t_2
        t_2 = t_2 + t_3
        t_2 = self.l_24(t_2)
        t_2 = self.l_25(t_2)
        t_5 = torch.matmul(t_2, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_5 = t_5.contiguous()
        t_2 = t_5.size()
        t_4 = slice(None, -2, None)
        t_4 = t_2[t_4]
        t_2 = (1024,)
        t_2 = t_4 + t_2
        t_4 = t_2[0]
        t_0 = t_2[1]
        t_2 = t_2[2]
        t_2 = t_5.view(t_4, t_0, t_2)
        t_2 = self.l_26(t_2)
        t_2 = self.l_27(t_2)
        t_1 = t_2 + t_1
        t_1 = self.l_28(t_1)
        t_2 = self.l_29(t_1)
        t_2 = torch.nn.functional.gelu(t_2)
        t_2 = self.l_30(t_2)
        t_2 = self.l_31(t_2)
        t_1 = t_2 + t_1
        t_1 = self.l_32(t_1)
        t_3 = (t_1, t_3)
        t_1 = t_3[0]
        t_2 = self.l_33(t_1)
        t_0 = self.l_34(t_1)
        t_4 = self.l_35(t_1)
        t_5 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_5[t_6]
        t_5 = (16, 64)
        t_5 = t_6 + t_5
        t_6 = t_5[0]
        t_7 = t_5[2]
        t_8 = t_5[3]
        t_9 = t_0.size()
        t_10 = slice(None, -1, None)
        t_10 = t_9[t_10]
        t_9 = (16, 64)
        t_9 = t_10 + t_9
        t_10 = t_9[2]
        t_11 = t_9[3]
        t_12 = t_4.size()
        t_13 = slice(None, -1, None)
        t_13 = t_12[t_13]
        t_12 = (16, 64)
        t_12 = t_13 + t_12
        t_13 = t_12[0]
        t_14 = t_12[2]
        # returning:
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/prim::TupleConstruct_2241
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___2243
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2258
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2260
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2264
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2266
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2282
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2288
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2290
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2306
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2308
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2312
        return list(flatten((t_3, t_1, t_2, t_0, t_4, t_5, t_6, t_7, t_8, t_9, t_10, t_11, t_12, t_13, t_14)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition6(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Softmax,
            Dropout,
            Linear,
        )
    LAYER_SCOPES=[
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:6'):
        super(Partition6, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [(1, 1), 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'roberta.encoder.18.attention.self.softmax',
                        'l_1': 'roberta.encoder.18.attention.self.dropout',
                        'l_2': 'roberta.encoder.18.attention.output.dense',
                        'l_3': 'roberta.encoder.18.attention.output.dropout',
                        'l_4': 'roberta.encoder.18.attention.output.LayerNorm',
                        'l_5': 'roberta.encoder.18.intermediate.dense',
                        'l_6': 'roberta.encoder.18.output.dense',
                        'l_7': 'roberta.encoder.18.output.dropout',
                        'l_8': 'roberta.encoder.18.output.LayerNorm',
                        'l_9': 'roberta.encoder.19.attention.self.query',
                        'l_10': 'roberta.encoder.19.attention.self.key',
                        'l_11': 'roberta.encoder.19.attention.self.value',
                        'l_12': 'roberta.encoder.19.attention.self.softmax',
                        'l_13': 'roberta.encoder.19.attention.self.dropout',
                        'l_14': 'roberta.encoder.19.attention.output.dense',
                        'l_15': 'roberta.encoder.19.attention.output.dropout',
                        'l_16': 'roberta.encoder.19.attention.output.LayerNorm',
                        'l_17': 'roberta.encoder.19.intermediate.dense',
                        'l_18': 'roberta.encoder.19.output.dense',
                        'l_19': 'roberta.encoder.19.output.dropout',
                        'l_20': 'roberta.encoder.19.output.LayerNorm',
                        'l_21': 'roberta.encoder.20.attention.self.query',
                        'l_22': 'roberta.encoder.20.attention.self.key',
                        'l_23': 'roberta.encoder.20.attention.self.value',
                        'l_24': 'roberta.encoder.20.attention.self.softmax',
                        'l_25': 'roberta.encoder.20.attention.self.dropout',
                        'l_26': 'roberta.encoder.20.attention.output.dense',
                        'l_27': 'roberta.encoder.20.attention.output.dropout',
                        'l_28': 'roberta.encoder.20.attention.output.LayerNorm',
                        'l_29': 'roberta.encoder.20.intermediate.dense',
                        'l_30': 'roberta.encoder.20.output.dense',
                        'l_31': 'roberta.encoder.20.output.dropout',
                        'l_32': 'roberta.encoder.20.output.LayerNorm',
                        'l_33': 'roberta.encoder.21.attention.self.query',
                        'l_34': 'roberta.encoder.21.attention.self.key',
                        'l_35': 'roberta.encoder.21.attention.self.value'}
        self.to(self.device)

    def forward(self, *args):
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_0
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_1
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_2
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_3
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_4
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_5
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertOutput[output]/Linear[dense] <=> self.l_6
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertOutput[output]/Dropout[dropout] <=> self.l_7
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_8
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_9
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_10
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_11
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_12
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_13
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_14
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_15
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_16
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_17
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertOutput[output]/Linear[dense] <=> self.l_18
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertOutput[output]/Dropout[dropout] <=> self.l_19
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[19]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_20
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_21
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_22
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_23
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_24
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_25
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_26
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_27
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_28
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_29
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertOutput[output]/Linear[dense] <=> self.l_30
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertOutput[output]/Dropout[dropout] <=> self.l_31
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[20]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_32
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_33
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_34
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_35
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[17]/prim::TupleConstruct_2241 <=> x0
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___2243 <=> x1
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> x2
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> x3
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> x4
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2258 <=> x5
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2260 <=> x6
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2264 <=> x7
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2266 <=> x8
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2282 <=> x9
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2288 <=> x10
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2290 <=> x11
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2306 <=> x12
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2308 <=> x13
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2312 <=> x14

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = x0[1]
        t_1 = x5[1]
        t_1 = x2.view(x6, t_1, x7, x8)
        t_1 = t_1.permute(0, 2, 1, 3)
        t_2 = x9[0]
        t_3 = x9[1]
        t_3 = x3.view(t_2, t_3, x10, x11)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_2 = x12[1]
        t_4 = x12[3]
        t_4 = x4.view(x13, t_2, x14, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_3 = t_3.transpose(-1, -2)
        t_3 = torch.matmul(t_1, t_3)
        t_1 = math.sqrt(64)
        t_1 = t_3 / t_1
        t_1 = t_1 + t_0
        t_1 = self.l_0(t_1)
        t_1 = self.l_1(t_1)
        t_4 = torch.matmul(t_1, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_1 = t_4.size()
        t_3 = slice(None, -2, None)
        t_3 = t_1[t_3]
        t_1 = (1024,)
        t_1 = t_3 + t_1
        t_3 = t_1[0]
        t_2 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_4.view(t_3, t_2, t_1)
        t_1 = self.l_2(t_1)
        t_1 = self.l_3(t_1)
        t_1 = t_1 + x1
        t_1 = self.l_4(t_1)
        t_2 = self.l_5(t_1)
        t_2 = torch.nn.functional.gelu(t_2)
        t_2 = self.l_6(t_2)
        t_2 = self.l_7(t_2)
        t_1 = t_2 + t_1
        t_1 = self.l_8(t_1)
        t_0 = (t_1, t_0)
        t_1 = t_0[0]
        t_0 = t_0[1]
        t_2 = self.l_9(t_1)
        t_3 = self.l_10(t_1)
        t_4 = self.l_11(t_1)
        t_5 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_5[t_6]
        t_5 = (16, 64)
        t_5 = t_6 + t_5
        t_6 = t_5[0]
        t_7 = t_5[1]
        t_8 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_2.view(t_6, t_7, t_8, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_8 = t_3.size()
        t_7 = slice(None, -1, None)
        t_7 = t_8[t_7]
        t_8 = (16, 64)
        t_8 = t_7 + t_8
        t_7 = t_8[0]
        t_6 = t_8[1]
        t_2 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_3.view(t_7, t_6, t_2, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_2 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_2[t_6]
        t_2 = (16, 64)
        t_2 = t_6 + t_2
        t_6 = t_2[0]
        t_7 = t_2[1]
        t_3 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_4.view(t_6, t_7, t_3, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_8 = t_8.transpose(-1, -2)
        t_8 = torch.matmul(t_5, t_8)
        t_5 = math.sqrt(64)
        t_5 = t_8 / t_5
        t_5 = t_5 + t_0
        t_5 = self.l_12(t_5)
        t_5 = self.l_13(t_5)
        t_2 = torch.matmul(t_5, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_5 = t_2.size()
        t_8 = slice(None, -2, None)
        t_8 = t_5[t_8]
        t_5 = (1024,)
        t_5 = t_8 + t_5
        t_8 = t_5[0]
        t_3 = t_5[1]
        t_5 = t_5[2]
        t_5 = t_2.view(t_8, t_3, t_5)
        t_5 = self.l_14(t_5)
        t_5 = self.l_15(t_5)
        t_1 = t_5 + t_1
        t_1 = self.l_16(t_1)
        t_5 = self.l_17(t_1)
        t_5 = torch.nn.functional.gelu(t_5)
        t_5 = self.l_18(t_5)
        t_5 = self.l_19(t_5)
        t_1 = t_5 + t_1
        t_1 = self.l_20(t_1)
        t_0 = (t_1, t_0)
        t_1 = t_0[0]
        t_0 = t_0[1]
        t_5 = self.l_21(t_1)
        t_3 = self.l_22(t_1)
        t_8 = self.l_23(t_1)
        t_2 = t_5.size()
        t_7 = slice(None, -1, None)
        t_7 = t_2[t_7]
        t_2 = (16, 64)
        t_2 = t_7 + t_2
        t_7 = t_2[0]
        t_6 = t_2[1]
        t_4 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_5.view(t_7, t_6, t_4, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_4 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_4[t_6]
        t_4 = (16, 64)
        t_4 = t_6 + t_4
        t_6 = t_4[0]
        t_7 = t_4[1]
        t_5 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_3.view(t_6, t_7, t_5, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_5 = t_8.size()
        t_7 = slice(None, -1, None)
        t_7 = t_5[t_7]
        t_5 = (16, 64)
        t_5 = t_7 + t_5
        t_7 = t_5[0]
        t_6 = t_5[1]
        t_3 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_8.view(t_7, t_6, t_3, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_4 = t_4.transpose(-1, -2)
        t_4 = torch.matmul(t_2, t_4)
        t_2 = math.sqrt(64)
        t_2 = t_4 / t_2
        t_2 = t_2 + t_0
        t_2 = self.l_24(t_2)
        t_2 = self.l_25(t_2)
        t_5 = torch.matmul(t_2, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_5 = t_5.contiguous()
        t_2 = t_5.size()
        t_4 = slice(None, -2, None)
        t_4 = t_2[t_4]
        t_2 = (1024,)
        t_2 = t_4 + t_2
        t_4 = t_2[0]
        t_3 = t_2[1]
        t_2 = t_2[2]
        t_2 = t_5.view(t_4, t_3, t_2)
        t_2 = self.l_26(t_2)
        t_2 = self.l_27(t_2)
        t_1 = t_2 + t_1
        t_1 = self.l_28(t_1)
        t_2 = self.l_29(t_1)
        t_2 = torch.nn.functional.gelu(t_2)
        t_2 = self.l_30(t_2)
        t_2 = self.l_31(t_2)
        t_1 = t_2 + t_1
        t_1 = self.l_32(t_1)
        t_0 = (t_1, t_0)
        t_1 = t_0[0]
        t_0 = t_0[1]
        t_2 = self.l_33(t_1)
        t_3 = self.l_34(t_1)
        t_4 = self.l_35(t_1)
        t_5 = t_2.size()
        t_6 = slice(None, -1, None)
        t_6 = t_5[t_6]
        t_5 = (16, 64)
        t_5 = t_6 + t_5
        t_6 = t_5[0]
        t_7 = t_3.size()
        t_8 = slice(None, -1, None)
        t_8 = t_7[t_8]
        t_7 = (16, 64)
        t_7 = t_8 + t_7
        t_8 = t_7[0]
        t_9 = t_7[1]
        t_10 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_3.view(t_8, t_9, t_10, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_10 = t_4.size()
        t_9 = slice(None, -1, None)
        t_9 = t_10[t_9]
        t_10 = (16, 64)
        t_10 = t_9 + t_10
        # returning:
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___2612
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___2614
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2627
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2629
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Tensor::permute_2665
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2675
        return list(flatten((t_1, t_0, t_2, t_4, t_5, t_6, t_7, t_10)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition7(nn.Module):
    BASIC_BLOCKS=(
            LayerNorm,
            Softmax,
            Dropout,
            Tanh,
            Linear,
        )
    LAYER_SCOPES=[
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertIntermediate[intermediate]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertOutput[output]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertOutput[output]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertOutput[output]/LayerNorm[LayerNorm]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertPooler[pooler]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaModel[roberta]/BertPooler[pooler]/Tanh[activation]',
            'RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Linear[dense]',
            'RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Dropout[dropout]',
            'RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Linear[out_proj]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:7'):
        super(Partition7, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'roberta.encoder.21.attention.self.softmax',
                        'l_1': 'roberta.encoder.21.attention.self.dropout',
                        'l_2': 'roberta.encoder.21.attention.output.dense',
                        'l_3': 'roberta.encoder.21.attention.output.dropout',
                        'l_4': 'roberta.encoder.21.attention.output.LayerNorm',
                        'l_5': 'roberta.encoder.21.intermediate.dense',
                        'l_6': 'roberta.encoder.21.output.dense',
                        'l_7': 'roberta.encoder.21.output.dropout',
                        'l_8': 'roberta.encoder.21.output.LayerNorm',
                        'l_9': 'roberta.encoder.22.attention.self.query',
                        'l_10': 'roberta.encoder.22.attention.self.key',
                        'l_11': 'roberta.encoder.22.attention.self.value',
                        'l_12': 'roberta.encoder.22.attention.self.softmax',
                        'l_13': 'roberta.encoder.22.attention.self.dropout',
                        'l_14': 'roberta.encoder.22.attention.output.dense',
                        'l_15': 'roberta.encoder.22.attention.output.dropout',
                        'l_16': 'roberta.encoder.22.attention.output.LayerNorm',
                        'l_17': 'roberta.encoder.22.intermediate.dense',
                        'l_18': 'roberta.encoder.22.output.dense',
                        'l_19': 'roberta.encoder.22.output.dropout',
                        'l_20': 'roberta.encoder.22.output.LayerNorm',
                        'l_21': 'roberta.encoder.23.attention.self.query',
                        'l_22': 'roberta.encoder.23.attention.self.key',
                        'l_23': 'roberta.encoder.23.attention.self.value',
                        'l_24': 'roberta.encoder.23.attention.self.softmax',
                        'l_25': 'roberta.encoder.23.attention.self.dropout',
                        'l_26': 'roberta.encoder.23.attention.output.dense',
                        'l_27': 'roberta.encoder.23.attention.output.dropout',
                        'l_28': 'roberta.encoder.23.attention.output.LayerNorm',
                        'l_29': 'roberta.encoder.23.intermediate.dense',
                        'l_30': 'roberta.encoder.23.output.dense',
                        'l_31': 'roberta.encoder.23.output.dropout',
                        'l_32': 'roberta.encoder.23.output.LayerNorm',
                        'l_33': 'roberta.pooler.dense',
                        'l_34': 'roberta.pooler.activation',
                        'l_35': 'classifier.dropout',
                        'l_36': 'classifier.dense',
                        'l_37': 'classifier.dropout',
                        'l_38': 'classifier.out_proj'}
        self.to(self.device)

    def forward(self, *args):
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_0
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_1
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_2
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_3
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_4
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_5
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertOutput[output]/Linear[dense] <=> self.l_6
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertOutput[output]/Dropout[dropout] <=> self.l_7
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_8
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_9
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_10
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_11
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_12
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_13
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_14
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_15
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_16
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_17
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertOutput[output]/Linear[dense] <=> self.l_18
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertOutput[output]/Dropout[dropout] <=> self.l_19
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[22]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_20
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_21
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_22
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_23
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_24
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_25
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_26
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_27
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_28
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_29
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertOutput[output]/Linear[dense] <=> self.l_30
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertOutput[output]/Dropout[dropout] <=> self.l_31
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[23]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_32
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertPooler[pooler]/Linear[dense] <=> self.l_33
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertPooler[pooler]/Tanh[activation] <=> self.l_34
        # RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Dropout[dropout] <=> self.l_35
        # RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Linear[dense] <=> self.l_36
        # RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Dropout[dropout] <=> self.l_37
        # RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Linear[out_proj] <=> self.l_38
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___2612 <=> x0
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/tuple::__getitem___2614 <=> x1
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> x2
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> x3
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2627 <=> x4
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Size::__getitem___2629 <=> x5
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Tensor::permute_2665 <=> x6
        # RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Size::__add___2675 <=> x7

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5, x6, x7 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = x4[1]
        t_1 = x4[2]
        t_2 = x4[3]
        t_2 = x2.view(x5, t_0, t_1, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_1 = x7[0]
        t_0 = x7[1]
        t_3 = x7[2]
        t_4 = x7[3]
        t_4 = x3.view(t_1, t_0, t_3, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_3 = x6.transpose(-1, -2)
        t_3 = torch.matmul(t_2, t_3)
        t_2 = math.sqrt(64)
        t_2 = t_3 / t_2
        t_2 = t_2 + x1
        t_2 = self.l_0(t_2)
        t_2 = self.l_1(t_2)
        t_4 = torch.matmul(t_2, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_2 = t_4.size()
        t_3 = slice(None, -2, None)
        t_3 = t_2[t_3]
        t_2 = (1024,)
        t_2 = t_3 + t_2
        t_3 = t_2[0]
        t_0 = t_2[1]
        t_2 = t_2[2]
        t_2 = t_4.view(t_3, t_0, t_2)
        t_2 = self.l_2(t_2)
        t_2 = self.l_3(t_2)
        t_2 = t_2 + x0
        t_2 = self.l_4(t_2)
        t_0 = self.l_5(t_2)
        t_0 = torch.nn.functional.gelu(t_0)
        t_0 = self.l_6(t_0)
        t_0 = self.l_7(t_0)
        t_2 = t_0 + t_2
        t_2 = self.l_8(t_2)
        t_2 = (t_2, x1)
        t_0 = t_2[0]
        t_2 = t_2[1]
        t_3 = self.l_9(t_0)
        t_4 = self.l_10(t_0)
        t_1 = self.l_11(t_0)
        t_5 = t_3.size()
        t_6 = slice(None, -1, None)
        t_6 = t_5[t_6]
        t_5 = (16, 64)
        t_5 = t_6 + t_5
        t_6 = t_5[0]
        t_7 = t_5[1]
        t_8 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_3.view(t_6, t_7, t_8, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_8 = t_4.size()
        t_7 = slice(None, -1, None)
        t_7 = t_8[t_7]
        t_8 = (16, 64)
        t_8 = t_7 + t_8
        t_7 = t_8[0]
        t_6 = t_8[1]
        t_3 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_4.view(t_7, t_6, t_3, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_3 = t_1.size()
        t_6 = slice(None, -1, None)
        t_6 = t_3[t_6]
        t_3 = (16, 64)
        t_3 = t_6 + t_3
        t_6 = t_3[0]
        t_7 = t_3[1]
        t_4 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_1.view(t_6, t_7, t_4, t_3)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_8 = t_8.transpose(-1, -2)
        t_8 = torch.matmul(t_5, t_8)
        t_5 = math.sqrt(64)
        t_5 = t_8 / t_5
        t_5 = t_5 + t_2
        t_5 = self.l_12(t_5)
        t_5 = self.l_13(t_5)
        t_3 = torch.matmul(t_5, t_3)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_5 = t_3.size()
        t_8 = slice(None, -2, None)
        t_8 = t_5[t_8]
        t_5 = (1024,)
        t_5 = t_8 + t_5
        t_8 = t_5[0]
        t_4 = t_5[1]
        t_5 = t_5[2]
        t_5 = t_3.view(t_8, t_4, t_5)
        t_5 = self.l_14(t_5)
        t_5 = self.l_15(t_5)
        t_0 = t_5 + t_0
        t_0 = self.l_16(t_0)
        t_5 = self.l_17(t_0)
        t_5 = torch.nn.functional.gelu(t_5)
        t_5 = self.l_18(t_5)
        t_5 = self.l_19(t_5)
        t_0 = t_5 + t_0
        t_0 = self.l_20(t_0)
        t_2 = (t_0, t_2)
        t_0 = t_2[0]
        t_2 = t_2[1]
        t_5 = self.l_21(t_0)
        t_4 = self.l_22(t_0)
        t_8 = self.l_23(t_0)
        t_3 = t_5.size()
        t_7 = slice(None, -1, None)
        t_7 = t_3[t_7]
        t_3 = (16, 64)
        t_3 = t_7 + t_3
        t_7 = t_3[0]
        t_6 = t_3[1]
        t_1 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_5.view(t_7, t_6, t_1, t_3)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_1 = t_4.size()
        t_6 = slice(None, -1, None)
        t_6 = t_1[t_6]
        t_1 = (16, 64)
        t_1 = t_6 + t_1
        t_6 = t_1[0]
        t_7 = t_1[1]
        t_5 = t_1[2]
        t_1 = t_1[3]
        t_1 = t_4.view(t_6, t_7, t_5, t_1)
        t_1 = t_1.permute(0, 2, 1, 3)
        t_5 = t_8.size()
        t_7 = slice(None, -1, None)
        t_7 = t_5[t_7]
        t_5 = (16, 64)
        t_5 = t_7 + t_5
        t_7 = t_5[0]
        t_6 = t_5[1]
        t_4 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_8.view(t_7, t_6, t_4, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_1 = t_1.transpose(-1, -2)
        t_1 = torch.matmul(t_3, t_1)
        t_3 = math.sqrt(64)
        t_3 = t_1 / t_3
        t_3 = t_3 + t_2
        t_3 = self.l_24(t_3)
        t_3 = self.l_25(t_3)
        t_5 = torch.matmul(t_3, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_5 = t_5.contiguous()
        t_3 = t_5.size()
        t_1 = slice(None, -2, None)
        t_1 = t_3[t_1]
        t_3 = (1024,)
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = t_3[1]
        t_3 = t_3[2]
        t_3 = t_5.view(t_1, t_4, t_3)
        t_3 = self.l_26(t_3)
        t_3 = self.l_27(t_3)
        t_0 = t_3 + t_0
        t_0 = self.l_28(t_0)
        t_3 = self.l_29(t_0)
        t_3 = torch.nn.functional.gelu(t_3)
        t_3 = self.l_30(t_3)
        t_3 = self.l_31(t_3)
        t_0 = t_3 + t_0
        t_0 = self.l_32(t_0)
        t_2 = (t_0, t_2)
        t_0 = t_2[0]
        t_2 = t_2[1]
        t_3 = slice(None, None, None)
        t_3 = (t_3, 0)
        t_3 = t_0[t_3]
        t_3 = self.l_33(t_3)
        t_3 = self.l_34(t_3)
        t_3 = (t_0, t_3)
        t_3 = t_3[0]
        t_0 = slice(None, None, None)
        t_4 = slice(None, None, None)
        t_4 = (t_0, 0, t_4)
        t_4 = t_3[t_4]
        t_4 = self.l_35(t_4)
        t_4 = self.l_36(t_4)
        t_4 = torch.tanh(t_4)
        t_4 = self.l_37(t_4)
        t_4 = self.l_38(t_4)
        # returning:
        # RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Linear[out_proj]
        return (t_4,)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


def traverse_model(module: nn.Module, depth: int, prefix: Optional[str] = None,
                   basic_blocks: Tuple[nn.Module] = (), full: bool = False) -> Iterator[Tuple[nn.Module, str, nn.Module]]:
    '''
    iterate over model layers yielding the layer,layer_scope,encasing_module
    Parameters:
    -----------
    model:
        the model to iterate over
    depth:
        how far down in the model tree to go
    basic_blocks:
        a list of modules that if encountered will not be broken down
    full:
        whether to yield only layers specified by the depth and basick_block options or to yield all layers
    '''
    if prefix is None:
        prefix = type(module).__name__

    for name, sub_module in module.named_children():
        scope = prefix + "/" + type(sub_module).__name__ + f"[{name}]"
        if len(list(sub_module.children())) == 0 or isinstance(sub_module, tuple(basic_blocks)) or depth == 0:
            if full:
                yield sub_module, scope, module, True
            else:
                yield sub_module, scope, module
        else:
            if full:
                yield sub_module, scope, module, False
            yield from traverse_model(sub_module, depth - 1, scope, basic_blocks, full)


def layerDict(model: nn.Module, depth=1000, basic_blocks=()) -> Dict[str, nn.Module]:
    return {s: l for l, s, _ in traverse_model(model, depth, basic_blocks=basic_blocks)}


def traverse_params_buffs(module: nn.Module, prefix: Optional[str] = None) -> Iterator[Tuple[torch.tensor, str]]:
    '''
    iterate over model's buffers and parameters yielding obj,obj_scope

    Parameters:
    -----------
    model:
        the model to iterate over
    '''
    if prefix is None:
        prefix = type(module).__name__

    # params
    for param_name, param in module.named_parameters(recurse=False):
        param_scope = f"{prefix}/{type(param).__name__}[{param_name}]"
        yield param, param_scope

    # buffs
    for buffer_name, buffer in module.named_buffers(recurse=False):
        buffer_scope = f"{prefix}/{type(buffer).__name__}[{buffer_name}]"
        yield buffer, buffer_scope

    # recurse
    for name, sub_module in module.named_children():
        yield from traverse_params_buffs(sub_module, prefix + "/" + type(sub_module).__name__ + f"[{name}]")


def tensorDict(model: nn.Module) -> OrderedDict[str, Tensor]:
    return collections.OrderedDict((s, t)for t, s in traverse_params_buffs(model))


def move_tensors(ts, device):
    def move(t):
        if isinstance(t, (nn.Module, Tensor)):
            return t.to(device)
        return t

    return nested_map(move, ts)


def nested_map(func, ts,full=False):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        return func(ts)
    elif isinstance(ts, (list, tuple, set)):
        return type(ts)(nested_map(func, t,full=full) for t in ts)
    elif isinstance(ts, dict):
        return {k: nested_map(func, v,full=full) for k, v in ts.items()}
    elif isinstance(ts, slice) and full:
        start = nested_map(func, ts.start,full=full)
        stop = nested_map(func, ts.stop,full=full)
        step = nested_map(func, ts.step,full=full)
        return slice(start, stop, step)
    return func(ts)


def flatten(ts):
    if isinstance(ts,torch.Size):
        # size is inheriting from tuple which is stupid
        yield ts
    elif isinstance(ts, (list, tuple, set)):
        yield from chain(*[flatten(t) for t in ts])
    elif isinstance(ts, dict):
        yield from chain(*[flatten(t) for k,t in sorted(ts.items(),key=lambda t:t[0])])
    else:
        yield ts


def unflatten(xs,structure):
    return _unflatten(xs,structure)[0]


def _unflatten(xs,structure):
    if isinstance(structure,torch.Size):
        #torch.Size is subclass of tuple which is stupid
        return xs[0],1

    if not isinstance(structure,(list,tuple,set,dict)):
        return xs[0],1
    
    if isinstance(structure,(list,tuple,set)):
        offset=0
        elements = []
        for s in structure:
            e,n = _unflatten(xs[offset:],s)
            elements.append(e)
            offset += n
        
        return type(structure)(elements),offset
    
    assert isinstance(structure,dict)
    offset = 0
    elements = dict()
    for k,v in sorted(structure.items(),key=lambda t: t[0]):
        e,n = _unflatten(xs[offset:],v)
        elements[k] = e
        offset += n
    
    return elements,offset


def state_dict(partition, device=None):
    # we return the state dict of this part as it should be in the original model
    state = nn.Module.state_dict(partition)
    lookup = partition.lookup
    result = dict()
    for k, v in state.items():
        if k in lookup:
            result[lookup[k]] = v if device is None else v.to(device)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            result[new_k] = v if device is None else v.to(device)
    return result


def load_state_dict(partition, state):
    reverse_lookup = {v: k for k, v in partition.lookup.items()}
    device = partition.device
    keys = list(partition.state_dict(None).keys())
    new_state = dict()
    for k in keys:
        if k in reverse_lookup:
            new_state[reverse_lookup[k]] = state[k].to(device)
            continue
        idx = k.rfind(".")
        to_replace = k[:idx]
        if to_replace in reverse_lookup:
            key = reverse_lookup[to_replace] + k[idx:]
            new_state[key] = state[k].to(device)
    nn.Module.load_state_dict(partition, new_state, strict=True)


def named_buffers(partition, recurse=True):
    # we return the named buffers of this part as it should be in the original model
    params = nn.Module.named_buffers(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def named_parameters(partition, recurse=True):
    # we return the named parameters of this part as it should be in the original model
    params = nn.Module.named_parameters(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def cpu(partition):
    partition.device = torch.device('cpu')
    return nn.Module.cpu(partition)


def cuda(partition, device=None):
    if device is None:
        device = torch.cuda.current_device()
    partition.device = torch.device(device)
    return nn.Module.cuda(partition, partition.device)


def to(partition, *args, **kwargs):
    device = None
    if 'device' in kwargs:
        device = kwargs['device']
    elif 'tensor' in kwargs:
        device = kwargs['tensor'].device
    if args:
        if isinstance(args[0], (torch.device, int, str)):
            device = args[0]
        if torch.is_tensor(args[0]):
            device = args[0].device
    if not (device is None):
        partition.device = torch.device(device)
    return nn.Module.to(partition, *args, **kwargs)

"""analysis summary
-I- Printing Report
warnings:
Partition4 output:RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Tensor::permute_1951 is not contiguous!
Partition6 output:RobertaForSequenceClassification/RobertaModel[roberta]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Tensor::permute_2665 is not contiguous!
Number of stages: 8
backward times include recomputation

real times are based on real measurements of execution time of generated partitions ms
forward {0: 44.76, 1: 46.56, 2: 47.61, 3: 49.3, 4: 46.89, 5: 48.44, 6: 46.9, 7: 37.5}
backward {0: 115.96, 1: 120.86, 2: 121.74, 3: 123.32, 4: 122.09, 5: 120.61, 6: 122.22, 7: 111.88}

balance is ratio of computation time between fastest and slowest parts. (between 0 and 1 higher is better)

real balance:
forward 0.761
backward 0.907

Assuming bandwidth of 11.0 GBps between GPUs

communication volumes size of activations of each partition
0: input size:'0.07 MB', recieve_time:'0.01 ms', out:'33.57 MB', send time:'3.05 ms'
1: input size:'33.57 MB', recieve_time:'3.05 ms', out:'50.35 MB', send time:'4.58 ms'
2: input size:'50.35 MB', recieve_time:'4.58 ms', out:'67.13 MB', send time:'6.10 ms'
3: input size:'67.13 MB', recieve_time:'6.10 ms', out:'83.90 MB', send time:'7.63 ms'
4: input size:'83.90 MB', recieve_time:'7.63 ms', out:'67.13 MB', send time:'6.10 ms'
5: input size:'67.13 MB', recieve_time:'6.10 ms', out:'83.90 MB', send time:'7.63 ms'
6: input size:'83.90 MB', recieve_time:'7.63 ms', out:'67.13 MB', send time:'6.10 ms'
7: input size:'67.13 MB', recieve_time:'6.10 ms', out:'0.00 MB', send time:'0.00 ms'

Compuatation Communication ratio (comp/(comp+comm)):
forward {0: 0.93, 1: 0.9, 2: 0.87, 3: 0.85, 4: 0.87, 5: 0.84, 6: 0.87, 7: 1.0} 
backward {0: 1.0, 1: 0.97, 2: 0.96, 3: 0.95, 4: 0.94, 5: 0.95, 6: 0.94, 7: 0.95}

Pipeline Slowdown: (compared to sequential executation with no communication, and same recompute policy)
forward 1.207
backward 1.075

Expected utilization by partition
forward {0: 0.85, 1: 0.85, 2: 0.85, 3: 0.85, 4: 0.83, 5: 0.83, 6: 0.83, 7: 0.76}
backward {0: 0.94, 1: 0.96, 2: 0.95, 3: 0.95, 4: 0.93, 5: 0.93, 6: 0.93, 7: 0.86}

worstcase: bwd: 123.324 fwd: 49.302
expected_speedup_compared_to_seq_no_recomp_no_comm: 5.422
Expected speedup for 8 partitions is: 7.188
"""