"""AutoGenerated with:
python -m autopipe.partitionbert_squad --n_partitions 2 --partitioning_batch_size 8 --analysis_batch_size 8 --n_iter 10 --model_name_or_path bert-large-uncased-whole-word-masking --do_lower_case --train_file /home_local/saareliad/data/squad1/train-v1.1.json --max_seq_length 384 --doc_stride 128 --partitioning_method pipedream --preset ftpipe
"""
import math
import torch
import torch.functional
import torch.nn.functional
from torch import Tensor
import torch.nn as nn
from itertools import chain
from typing import Optional, Tuple, Iterator, Iterable, OrderedDict, Dict
import collections

from typing import Type
from torch.nn.modules.normalization import LayerNorm
from torch.nn.modules.dropout import Dropout
from torch.nn.modules.activation import Softmax
from torch.nn.modules.activation import Tanh
from torch.nn.modules.linear import Linear
from torch.nn.modules.sparse import Embedding
# this is an auto generated file do not edit unless you know what you are doing


# partition adjacency
# model inputs {0}
# partition 0 {'inputs': {'attention_mask', 'input_ids', 'token_type_ids'}, 'outputs': {1}}
# partition 1 {'inputs': {0}, 'outputs': {'output'}}
# model outputs {1}


def create_pipeline_configuration(DEBUG=False, batch_size=8):
    config = {
        'batch_dim': 0,
        'depth': 10000,
        'basic_blocks': (LayerNorm,Dropout,Softmax,Tanh,Linear,Embedding),
        'model_inputs': {
            'attention_mask': {
                'shape': torch.Size([8, 384]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0]},
            'input_ids': {
                'shape': torch.Size([8, 384]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0]},
            'token_type_ids': {
                'shape': torch.Size([8, 384]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0]}},
        'model_outputs': {
            'BertForQuestionAnswering/Linear[qa_outputs]': {
                'shape': torch.Size([8, 384, 2]),
                'dtype': torch.float32,
                'is_batched': True,
                'created_by': 1}},
        'stages': {
            0: {
                'stage_cls': Partition0,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([8, 384]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'input_ids': {
                        'shape': torch.Size([8, 384]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'token_type_ids': {
                        'shape': torch.Size([8, 384]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1}},
                'outputs': {
                    'BertForQuestionAnswering/BertModel[bert]/Tensor::__mul___12': {
                        'shape': torch.Size([8, 1, 1, 384]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [1]},
                    'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/LayerNorm[LayerNorm]': {
                        'shape': torch.Size([8, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]},
                    'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Tensor::permute_1093': {
                        'shape': torch.Size([8, 384, 16, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]}},
                'devices': ['cpu' if DEBUG else 'cuda:0'],
                'stage_depth': 1},
            1: {
                'stage_cls': Partition1,
                'inputs': {
                    'BertForQuestionAnswering/BertModel[bert]/Tensor::__mul___12': {
                        'shape': torch.Size([8, 1, 1, 384]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 0},
                    'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/LayerNorm[LayerNorm]': {
                        'shape': torch.Size([8, 384, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Tensor::permute_1093': {
                        'shape': torch.Size([8, 384, 16, 64]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0}},
                'outputs': {
                    'BertForQuestionAnswering/Linear[qa_outputs]': {
                        'shape': torch.Size([8, 384, 2]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [-1]}},
                'devices': ['cpu' if DEBUG else 'cuda:1'],
                'stage_depth': 0}}}
    
    
    # switching batch size
    batch_dim = config['batch_dim']
    for d in chain(config['model_inputs'].values(),config['model_outputs'].values()):
        if d['is_batched']:
            shape = d['shape']
            d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    for s in config['stages'].values():
        for d in chain(s['inputs'].values(),s['outputs'].values()):
            if d['is_batched']:
                shape = d['shape']
                d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    return config

class Partition0(nn.Module):
    LAYER_SCOPES = [
            'BertForQuestionAnswering/BertModel[bert]/BertEmbeddings[embeddings]/Embedding[word_embeddings]',
            'BertForQuestionAnswering/BertModel[bert]/BertEmbeddings[embeddings]/Embedding[position_embeddings]',
            'BertForQuestionAnswering/BertModel[bert]/BertEmbeddings[embeddings]/Embedding[token_type_embeddings]',
            'BertForQuestionAnswering/BertModel[bert]/BertEmbeddings[embeddings]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEmbeddings[embeddings]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:0'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = {'l_0': 'bert.embeddings.word_embeddings',
                        'l_1': 'bert.embeddings.position_embeddings',
                        'l_2': 'bert.embeddings.token_type_embeddings',
                        'l_3': 'bert.embeddings.LayerNorm',
                        'l_4': 'bert.embeddings.dropout',
                        'l_5': 'bert.encoder.0.attention.self.query',
                        'l_6': 'bert.encoder.0.attention.self.key',
                        'l_7': 'bert.encoder.0.attention.self.value',
                        'l_8': 'bert.encoder.0.attention.self.softmax',
                        'l_9': 'bert.encoder.0.attention.self.dropout',
                        'l_10': 'bert.encoder.0.attention.output.dense',
                        'l_11': 'bert.encoder.0.attention.output.dropout',
                        'l_12': 'bert.encoder.0.attention.output.LayerNorm',
                        'l_13': 'bert.encoder.0.intermediate.dense',
                        'l_14': 'bert.encoder.0.output.dense',
                        'l_15': 'bert.encoder.0.output.dropout',
                        'l_16': 'bert.encoder.0.output.LayerNorm',
                        'l_17': 'bert.encoder.1.attention.self.query',
                        'l_18': 'bert.encoder.1.attention.self.key',
                        'l_19': 'bert.encoder.1.attention.self.value',
                        'l_20': 'bert.encoder.1.attention.self.softmax',
                        'l_21': 'bert.encoder.1.attention.self.dropout',
                        'l_22': 'bert.encoder.1.attention.output.dense',
                        'l_23': 'bert.encoder.1.attention.output.dropout',
                        'l_24': 'bert.encoder.1.attention.output.LayerNorm',
                        'l_25': 'bert.encoder.1.intermediate.dense',
                        'l_26': 'bert.encoder.1.output.dense',
                        'l_27': 'bert.encoder.1.output.dropout',
                        'l_28': 'bert.encoder.1.output.LayerNorm',
                        'l_29': 'bert.encoder.2.attention.self.query',
                        'l_30': 'bert.encoder.2.attention.self.key',
                        'l_31': 'bert.encoder.2.attention.self.value',
                        'l_32': 'bert.encoder.2.attention.self.softmax',
                        'l_33': 'bert.encoder.2.attention.self.dropout',
                        'l_34': 'bert.encoder.2.attention.output.dense',
                        'l_35': 'bert.encoder.2.attention.output.dropout',
                        'l_36': 'bert.encoder.2.attention.output.LayerNorm',
                        'l_37': 'bert.encoder.2.intermediate.dense',
                        'l_38': 'bert.encoder.2.output.dense',
                        'l_39': 'bert.encoder.2.output.dropout',
                        'l_40': 'bert.encoder.2.output.LayerNorm',
                        'l_41': 'bert.encoder.3.attention.self.query',
                        'l_42': 'bert.encoder.3.attention.self.key',
                        'l_43': 'bert.encoder.3.attention.self.value',
                        'l_44': 'bert.encoder.3.attention.self.softmax',
                        'l_45': 'bert.encoder.3.attention.self.dropout',
                        'l_46': 'bert.encoder.3.attention.output.dense',
                        'l_47': 'bert.encoder.3.attention.output.dropout',
                        'l_48': 'bert.encoder.3.attention.output.LayerNorm',
                        'l_49': 'bert.encoder.3.intermediate.dense',
                        'l_50': 'bert.encoder.3.output.dense',
                        'l_51': 'bert.encoder.3.output.dropout',
                        'l_52': 'bert.encoder.3.output.LayerNorm',
                        'l_53': 'bert.encoder.4.attention.self.query',
                        'l_54': 'bert.encoder.4.attention.self.key',
                        'l_55': 'bert.encoder.4.attention.self.value',
                        'l_56': 'bert.encoder.4.attention.self.softmax',
                        'l_57': 'bert.encoder.4.attention.self.dropout',
                        'l_58': 'bert.encoder.4.attention.output.dense',
                        'l_59': 'bert.encoder.4.attention.output.dropout',
                        'l_60': 'bert.encoder.4.attention.output.LayerNorm',
                        'l_61': 'bert.encoder.4.intermediate.dense',
                        'l_62': 'bert.encoder.4.output.dense',
                        'l_63': 'bert.encoder.4.output.dropout',
                        'l_64': 'bert.encoder.4.output.LayerNorm',
                        'l_65': 'bert.encoder.5.attention.self.query',
                        'l_66': 'bert.encoder.5.attention.self.key',
                        'l_67': 'bert.encoder.5.attention.self.value',
                        'l_68': 'bert.encoder.5.attention.self.softmax',
                        'l_69': 'bert.encoder.5.attention.self.dropout',
                        'l_70': 'bert.encoder.5.attention.output.dense',
                        'l_71': 'bert.encoder.5.attention.output.dropout',
                        'l_72': 'bert.encoder.5.attention.output.LayerNorm',
                        'l_73': 'bert.encoder.5.intermediate.dense',
                        'l_74': 'bert.encoder.5.output.dense',
                        'l_75': 'bert.encoder.5.output.dropout',
                        'l_76': 'bert.encoder.5.output.LayerNorm',
                        'l_77': 'bert.encoder.6.attention.self.query',
                        'l_78': 'bert.encoder.6.attention.self.key',
                        'l_79': 'bert.encoder.6.attention.self.value',
                        'l_80': 'bert.encoder.6.attention.self.softmax',
                        'l_81': 'bert.encoder.6.attention.self.dropout',
                        'l_82': 'bert.encoder.6.attention.output.dense',
                        'l_83': 'bert.encoder.6.attention.output.dropout',
                        'l_84': 'bert.encoder.6.attention.output.LayerNorm',
                        'l_85': 'bert.encoder.6.intermediate.dense',
                        'l_86': 'bert.encoder.6.output.dense',
                        'l_87': 'bert.encoder.6.output.dropout',
                        'l_88': 'bert.encoder.6.output.LayerNorm',
                        'l_89': 'bert.encoder.7.attention.self.query',
                        'l_90': 'bert.encoder.7.attention.self.key',
                        'l_91': 'bert.encoder.7.attention.self.value',
                        'l_92': 'bert.encoder.7.attention.self.softmax',
                        'l_93': 'bert.encoder.7.attention.self.dropout',
                        'l_94': 'bert.encoder.7.attention.output.dense',
                        'l_95': 'bert.encoder.7.attention.output.dropout',
                        'l_96': 'bert.encoder.7.attention.output.LayerNorm',
                        'l_97': 'bert.encoder.7.intermediate.dense',
                        'l_98': 'bert.encoder.7.output.dense',
                        'l_99': 'bert.encoder.7.output.dropout',
                        'l_100': 'bert.encoder.7.output.LayerNorm',
                        'l_101': 'bert.encoder.8.attention.self.query',
                        'l_102': 'bert.encoder.8.attention.self.key',
                        'l_103': 'bert.encoder.8.attention.self.value',
                        'l_104': 'bert.encoder.8.attention.self.softmax',
                        'l_105': 'bert.encoder.8.attention.self.dropout',
                        'l_106': 'bert.encoder.8.attention.output.dense',
                        'l_107': 'bert.encoder.8.attention.output.dropout',
                        'l_108': 'bert.encoder.8.attention.output.LayerNorm',
                        'l_109': 'bert.encoder.8.intermediate.dense',
                        'l_110': 'bert.encoder.8.output.dense',
                        'l_111': 'bert.encoder.8.output.dropout',
                        'l_112': 'bert.encoder.8.output.LayerNorm',
                        'l_113': 'bert.encoder.9.attention.self.query',
                        'l_114': 'bert.encoder.9.attention.self.key',
                        'l_115': 'bert.encoder.9.attention.self.value',
                        'l_116': 'bert.encoder.9.attention.self.softmax',
                        'l_117': 'bert.encoder.9.attention.self.dropout',
                        'l_118': 'bert.encoder.9.attention.output.dense',
                        'l_119': 'bert.encoder.9.attention.output.dropout',
                        'l_120': 'bert.encoder.9.attention.output.LayerNorm',
                        'l_121': 'bert.encoder.9.intermediate.dense',
                        'l_122': 'bert.encoder.9.output.dense',
                        'l_123': 'bert.encoder.9.output.dropout',
                        'l_124': 'bert.encoder.9.output.LayerNorm',
                        'l_125': 'bert.encoder.10.attention.self.query',
                        'l_126': 'bert.encoder.10.attention.self.key',
                        'l_127': 'bert.encoder.10.attention.self.value',
                        'l_128': 'bert.encoder.10.attention.self.softmax',
                        'l_129': 'bert.encoder.10.attention.self.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # BertForQuestionAnswering/BertModel[bert]/BertEmbeddings[embeddings]/Embedding[word_embeddings] <=> self.l_0
        # BertForQuestionAnswering/BertModel[bert]/BertEmbeddings[embeddings]/Embedding[position_embeddings] <=> self.l_1
        # BertForQuestionAnswering/BertModel[bert]/BertEmbeddings[embeddings]/Embedding[token_type_embeddings] <=> self.l_2
        # BertForQuestionAnswering/BertModel[bert]/BertEmbeddings[embeddings]/LayerNorm[LayerNorm] <=> self.l_3
        # BertForQuestionAnswering/BertModel[bert]/BertEmbeddings[embeddings]/Dropout[dropout] <=> self.l_4
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_5
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_6
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_7
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_8
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_9
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_10
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_11
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_12
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_13
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertOutput[output]/Linear[dense] <=> self.l_14
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertOutput[output]/Dropout[dropout] <=> self.l_15
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[0]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_16
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_17
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_18
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_19
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_20
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_21
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_22
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_23
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_24
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_25
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertOutput[output]/Linear[dense] <=> self.l_26
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertOutput[output]/Dropout[dropout] <=> self.l_27
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[1]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_28
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_29
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_30
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_31
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_32
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_33
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_34
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_35
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_36
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_37
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/Linear[dense] <=> self.l_38
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/Dropout[dropout] <=> self.l_39
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[2]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_40
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_41
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_42
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_43
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_44
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_45
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_46
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_47
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_48
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_49
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertOutput[output]/Linear[dense] <=> self.l_50
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertOutput[output]/Dropout[dropout] <=> self.l_51
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[3]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_52
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_53
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_54
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_55
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_56
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_57
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_58
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_59
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_60
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_61
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertOutput[output]/Linear[dense] <=> self.l_62
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertOutput[output]/Dropout[dropout] <=> self.l_63
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[4]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_64
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_65
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_66
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_67
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_68
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_69
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_70
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_71
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_72
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_73
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertOutput[output]/Linear[dense] <=> self.l_74
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertOutput[output]/Dropout[dropout] <=> self.l_75
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[5]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_76
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_77
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_78
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_79
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_80
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_81
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_82
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_83
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_84
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_85
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertOutput[output]/Linear[dense] <=> self.l_86
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertOutput[output]/Dropout[dropout] <=> self.l_87
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[6]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_88
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_89
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_90
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_91
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_92
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_93
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_94
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_95
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_96
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_97
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertOutput[output]/Linear[dense] <=> self.l_98
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertOutput[output]/Dropout[dropout] <=> self.l_99
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[7]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_100
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_101
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_102
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_103
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_104
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_105
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_106
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_107
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_108
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_109
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertOutput[output]/Linear[dense] <=> self.l_110
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertOutput[output]/Dropout[dropout] <=> self.l_111
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[8]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_112
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_113
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_114
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_115
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_116
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_117
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_118
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_119
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_120
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_121
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/Linear[dense] <=> self.l_122
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/Dropout[dropout] <=> self.l_123
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_124
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_125
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_126
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_127
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_128
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_129
        # input0 <=> attention_mask
        # input1 <=> input_ids
        # input2 <=> token_type_ids
        attention_mask, input_ids, token_type_ids = unflatten(args,self.input_structure)
        t_0 = self.l_0(input_ids)
        t_1 = self.l_2(token_type_ids)
        t_2 = attention_mask.unsqueeze(1)
        t_2 = t_2.unsqueeze(2)
        t_2 = t_2.to(dtype=torch.float32)
        t_2 = 1.0 - t_2
        t_2 = t_2 * -10000.0
        t_3 = input_ids.size(1)
        t_3 = torch.arange(t_3, dtype=torch.int64, device=self.device)
        t_3 = t_3.unsqueeze(0)
        t_3 = t_3.expand_as(input_ids)
        t_3 = self.l_1(t_3)
        t_3 = t_0 + t_3
        t_1 = t_3 + t_1
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_3 = self.l_5(t_1)
        t_0 = self.l_6(t_1)
        t_4 = self.l_7(t_1)
        t_5 = t_3.size()
        t_6 = t_0.size()
        t_7 = t_4.size()
        t_5 = t_5[slice(None, -1, None)]
        t_5 = t_5 + (16, 64)
        t_8 = t_5[0]
        t_9 = t_5[1]
        t_10 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_3.view(t_8, t_9, t_10, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_6 = t_6[slice(None, -1, None)]
        t_6 = t_6 + (16, 64)
        t_10 = t_6[0]
        t_9 = t_6[1]
        t_8 = t_6[2]
        t_6 = t_6[3]
        t_6 = t_0.view(t_10, t_9, t_8, t_6)
        t_6 = t_6.permute(0, 2, 1, 3)
        t_7 = t_7[slice(None, -1, None)]
        t_7 = t_7 + (16, 64)
        t_8 = t_7[0]
        t_9 = t_7[1]
        t_10 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_4.view(t_8, t_9, t_10, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_6 = t_6.transpose(-1, -2)
        t_6 = torch.matmul(t_5, t_6)
        t_5 = math.sqrt(64)
        t_5 = t_6 / t_5
        t_5 = t_5 + t_2
        t_5 = self.l_8(t_5)
        t_5 = self.l_9(t_5)
        t_7 = torch.matmul(t_5, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_7 = t_7.contiguous()
        t_5 = t_7.size()
        t_5 = t_5[slice(None, -2, None)]
        t_5 = t_5 + (1024,)
        t_6 = t_5[0]
        t_10 = t_5[1]
        t_5 = t_5[2]
        t_5 = t_7.view(t_6, t_10, t_5)
        t_5 = self.l_10(t_5)
        t_5 = self.l_11(t_5)
        t_1 = t_5 + t_1
        t_1 = self.l_12(t_1)
        t_5 = self.l_13(t_1)
        t_5 = torch.nn.functional.gelu(t_5)
        t_5 = self.l_14(t_5)
        t_5 = self.l_15(t_5)
        t_1 = t_5 + t_1
        t_1 = self.l_16(t_1)
        t_5 = self.l_17(t_1)
        t_10 = self.l_18(t_1)
        t_6 = self.l_19(t_1)
        t_7 = t_5.size()
        t_9 = t_10.size()
        t_8 = t_6.size()
        t_7 = t_7[slice(None, -1, None)]
        t_7 = t_7 + (16, 64)
        t_4 = t_7[0]
        t_0 = t_7[1]
        t_3 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_5.view(t_4, t_0, t_3, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_9 = t_9[slice(None, -1, None)]
        t_9 = t_9 + (16, 64)
        t_3 = t_9[0]
        t_0 = t_9[1]
        t_4 = t_9[2]
        t_9 = t_9[3]
        t_9 = t_10.view(t_3, t_0, t_4, t_9)
        t_9 = t_9.permute(0, 2, 1, 3)
        t_8 = t_8[slice(None, -1, None)]
        t_8 = t_8 + (16, 64)
        t_4 = t_8[0]
        t_0 = t_8[1]
        t_3 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_6.view(t_4, t_0, t_3, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_9 = t_9.transpose(-1, -2)
        t_9 = torch.matmul(t_7, t_9)
        t_7 = math.sqrt(64)
        t_7 = t_9 / t_7
        t_7 = t_7 + t_2
        t_7 = self.l_20(t_7)
        t_7 = self.l_21(t_7)
        t_8 = torch.matmul(t_7, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_8 = t_8.contiguous()
        t_7 = t_8.size()
        t_7 = t_7[slice(None, -2, None)]
        t_7 = t_7 + (1024,)
        t_9 = t_7[0]
        t_3 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_8.view(t_9, t_3, t_7)
        t_7 = self.l_22(t_7)
        t_7 = self.l_23(t_7)
        t_1 = t_7 + t_1
        t_1 = self.l_24(t_1)
        t_7 = self.l_25(t_1)
        t_7 = torch.nn.functional.gelu(t_7)
        t_7 = self.l_26(t_7)
        t_7 = self.l_27(t_7)
        t_1 = t_7 + t_1
        t_1 = self.l_28(t_1)
        t_7 = self.l_29(t_1)
        t_3 = self.l_30(t_1)
        t_9 = self.l_31(t_1)
        t_8 = t_7.size()
        t_0 = t_3.size()
        t_4 = t_9.size()
        t_8 = t_8[slice(None, -1, None)]
        t_8 = t_8 + (16, 64)
        t_6 = t_8[0]
        t_10 = t_8[1]
        t_5 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_7.view(t_6, t_10, t_5, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_0 = t_0[slice(None, -1, None)]
        t_0 = t_0 + (16, 64)
        t_5 = t_0[0]
        t_10 = t_0[1]
        t_6 = t_0[2]
        t_0 = t_0[3]
        t_0 = t_3.view(t_5, t_10, t_6, t_0)
        t_0 = t_0.permute(0, 2, 1, 3)
        t_4 = t_4[slice(None, -1, None)]
        t_4 = t_4 + (16, 64)
        t_6 = t_4[0]
        t_10 = t_4[1]
        t_5 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_9.view(t_6, t_10, t_5, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_0 = t_0.transpose(-1, -2)
        t_0 = torch.matmul(t_8, t_0)
        t_8 = math.sqrt(64)
        t_8 = t_0 / t_8
        t_8 = t_8 + t_2
        t_8 = self.l_32(t_8)
        t_8 = self.l_33(t_8)
        t_4 = torch.matmul(t_8, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_8 = t_4.size()
        t_8 = t_8[slice(None, -2, None)]
        t_8 = t_8 + (1024,)
        t_0 = t_8[0]
        t_5 = t_8[1]
        t_8 = t_8[2]
        t_8 = t_4.view(t_0, t_5, t_8)
        t_8 = self.l_34(t_8)
        t_8 = self.l_35(t_8)
        t_1 = t_8 + t_1
        t_1 = self.l_36(t_1)
        t_8 = self.l_37(t_1)
        t_8 = torch.nn.functional.gelu(t_8)
        t_8 = self.l_38(t_8)
        t_8 = self.l_39(t_8)
        t_1 = t_8 + t_1
        t_1 = self.l_40(t_1)
        t_8 = self.l_41(t_1)
        t_5 = self.l_42(t_1)
        t_0 = self.l_43(t_1)
        t_4 = t_8.size()
        t_10 = t_5.size()
        t_6 = t_0.size()
        t_4 = t_4[slice(None, -1, None)]
        t_4 = t_4 + (16, 64)
        t_9 = t_4[0]
        t_3 = t_4[1]
        t_7 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_8.view(t_9, t_3, t_7, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_10 = t_10[slice(None, -1, None)]
        t_10 = t_10 + (16, 64)
        t_7 = t_10[0]
        t_3 = t_10[1]
        t_9 = t_10[2]
        t_10 = t_10[3]
        t_10 = t_5.view(t_7, t_3, t_9, t_10)
        t_10 = t_10.permute(0, 2, 1, 3)
        t_6 = t_6[slice(None, -1, None)]
        t_6 = t_6 + (16, 64)
        t_9 = t_6[0]
        t_3 = t_6[1]
        t_7 = t_6[2]
        t_6 = t_6[3]
        t_6 = t_0.view(t_9, t_3, t_7, t_6)
        t_6 = t_6.permute(0, 2, 1, 3)
        t_10 = t_10.transpose(-1, -2)
        t_10 = torch.matmul(t_4, t_10)
        t_4 = math.sqrt(64)
        t_4 = t_10 / t_4
        t_4 = t_4 + t_2
        t_4 = self.l_44(t_4)
        t_4 = self.l_45(t_4)
        t_6 = torch.matmul(t_4, t_6)
        t_6 = t_6.permute(0, 2, 1, 3)
        t_6 = t_6.contiguous()
        t_4 = t_6.size()
        t_4 = t_4[slice(None, -2, None)]
        t_4 = t_4 + (1024,)
        t_10 = t_4[0]
        t_7 = t_4[1]
        t_4 = t_4[2]
        t_4 = t_6.view(t_10, t_7, t_4)
        t_4 = self.l_46(t_4)
        t_4 = self.l_47(t_4)
        t_1 = t_4 + t_1
        t_1 = self.l_48(t_1)
        t_4 = self.l_49(t_1)
        t_4 = torch.nn.functional.gelu(t_4)
        t_4 = self.l_50(t_4)
        t_4 = self.l_51(t_4)
        t_1 = t_4 + t_1
        t_1 = self.l_52(t_1)
        t_4 = self.l_53(t_1)
        t_7 = self.l_54(t_1)
        t_10 = self.l_55(t_1)
        t_6 = t_4.size()
        t_3 = t_7.size()
        t_9 = t_10.size()
        t_6 = t_6[slice(None, -1, None)]
        t_6 = t_6 + (16, 64)
        t_0 = t_6[0]
        t_5 = t_6[1]
        t_8 = t_6[2]
        t_6 = t_6[3]
        t_6 = t_4.view(t_0, t_5, t_8, t_6)
        t_6 = t_6.permute(0, 2, 1, 3)
        t_3 = t_3[slice(None, -1, None)]
        t_3 = t_3 + (16, 64)
        t_8 = t_3[0]
        t_5 = t_3[1]
        t_0 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_7.view(t_8, t_5, t_0, t_3)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_9 = t_9[slice(None, -1, None)]
        t_9 = t_9 + (16, 64)
        t_0 = t_9[0]
        t_5 = t_9[1]
        t_8 = t_9[2]
        t_9 = t_9[3]
        t_9 = t_10.view(t_0, t_5, t_8, t_9)
        t_9 = t_9.permute(0, 2, 1, 3)
        t_3 = t_3.transpose(-1, -2)
        t_3 = torch.matmul(t_6, t_3)
        t_6 = math.sqrt(64)
        t_6 = t_3 / t_6
        t_6 = t_6 + t_2
        t_6 = self.l_56(t_6)
        t_6 = self.l_57(t_6)
        t_9 = torch.matmul(t_6, t_9)
        t_9 = t_9.permute(0, 2, 1, 3)
        t_9 = t_9.contiguous()
        t_6 = t_9.size()
        t_6 = t_6[slice(None, -2, None)]
        t_6 = t_6 + (1024,)
        t_3 = t_6[0]
        t_8 = t_6[1]
        t_6 = t_6[2]
        t_6 = t_9.view(t_3, t_8, t_6)
        t_6 = self.l_58(t_6)
        t_6 = self.l_59(t_6)
        t_1 = t_6 + t_1
        t_1 = self.l_60(t_1)
        t_6 = self.l_61(t_1)
        t_6 = torch.nn.functional.gelu(t_6)
        t_6 = self.l_62(t_6)
        t_6 = self.l_63(t_6)
        t_1 = t_6 + t_1
        t_1 = self.l_64(t_1)
        t_6 = self.l_65(t_1)
        t_8 = self.l_66(t_1)
        t_3 = self.l_67(t_1)
        t_9 = t_6.size()
        t_5 = t_8.size()
        t_0 = t_3.size()
        t_9 = t_9[slice(None, -1, None)]
        t_9 = t_9 + (16, 64)
        t_10 = t_9[0]
        t_7 = t_9[1]
        t_4 = t_9[2]
        t_9 = t_9[3]
        t_9 = t_6.view(t_10, t_7, t_4, t_9)
        t_9 = t_9.permute(0, 2, 1, 3)
        t_5 = t_5[slice(None, -1, None)]
        t_5 = t_5 + (16, 64)
        t_4 = t_5[0]
        t_7 = t_5[1]
        t_10 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_8.view(t_4, t_7, t_10, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_0 = t_0[slice(None, -1, None)]
        t_0 = t_0 + (16, 64)
        t_10 = t_0[0]
        t_7 = t_0[1]
        t_4 = t_0[2]
        t_0 = t_0[3]
        t_0 = t_3.view(t_10, t_7, t_4, t_0)
        t_0 = t_0.permute(0, 2, 1, 3)
        t_5 = t_5.transpose(-1, -2)
        t_5 = torch.matmul(t_9, t_5)
        t_9 = math.sqrt(64)
        t_9 = t_5 / t_9
        t_9 = t_9 + t_2
        t_9 = self.l_68(t_9)
        t_9 = self.l_69(t_9)
        t_0 = torch.matmul(t_9, t_0)
        t_0 = t_0.permute(0, 2, 1, 3)
        t_0 = t_0.contiguous()
        t_9 = t_0.size()
        t_9 = t_9[slice(None, -2, None)]
        t_9 = t_9 + (1024,)
        t_5 = t_9[0]
        t_4 = t_9[1]
        t_9 = t_9[2]
        t_9 = t_0.view(t_5, t_4, t_9)
        t_9 = self.l_70(t_9)
        t_9 = self.l_71(t_9)
        t_1 = t_9 + t_1
        t_1 = self.l_72(t_1)
        t_9 = self.l_73(t_1)
        t_9 = torch.nn.functional.gelu(t_9)
        t_9 = self.l_74(t_9)
        t_9 = self.l_75(t_9)
        t_1 = t_9 + t_1
        t_1 = self.l_76(t_1)
        t_9 = self.l_77(t_1)
        t_4 = self.l_78(t_1)
        t_5 = self.l_79(t_1)
        t_0 = t_9.size()
        t_7 = t_4.size()
        t_10 = t_5.size()
        t_0 = t_0[slice(None, -1, None)]
        t_0 = t_0 + (16, 64)
        t_3 = t_0[0]
        t_8 = t_0[1]
        t_6 = t_0[2]
        t_0 = t_0[3]
        t_0 = t_9.view(t_3, t_8, t_6, t_0)
        t_0 = t_0.permute(0, 2, 1, 3)
        t_7 = t_7[slice(None, -1, None)]
        t_7 = t_7 + (16, 64)
        t_6 = t_7[0]
        t_8 = t_7[1]
        t_3 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_4.view(t_6, t_8, t_3, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_10 = t_10[slice(None, -1, None)]
        t_10 = t_10 + (16, 64)
        t_3 = t_10[0]
        t_8 = t_10[1]
        t_6 = t_10[2]
        t_10 = t_10[3]
        t_10 = t_5.view(t_3, t_8, t_6, t_10)
        t_10 = t_10.permute(0, 2, 1, 3)
        t_7 = t_7.transpose(-1, -2)
        t_7 = torch.matmul(t_0, t_7)
        t_0 = math.sqrt(64)
        t_0 = t_7 / t_0
        t_0 = t_0 + t_2
        t_0 = self.l_80(t_0)
        t_0 = self.l_81(t_0)
        t_10 = torch.matmul(t_0, t_10)
        t_10 = t_10.permute(0, 2, 1, 3)
        t_10 = t_10.contiguous()
        t_0 = t_10.size()
        t_0 = t_0[slice(None, -2, None)]
        t_0 = t_0 + (1024,)
        t_7 = t_0[0]
        t_6 = t_0[1]
        t_0 = t_0[2]
        t_0 = t_10.view(t_7, t_6, t_0)
        t_0 = self.l_82(t_0)
        t_0 = self.l_83(t_0)
        t_1 = t_0 + t_1
        t_1 = self.l_84(t_1)
        t_0 = self.l_85(t_1)
        t_0 = torch.nn.functional.gelu(t_0)
        t_0 = self.l_86(t_0)
        t_0 = self.l_87(t_0)
        t_1 = t_0 + t_1
        t_1 = self.l_88(t_1)
        t_0 = self.l_89(t_1)
        t_6 = self.l_90(t_1)
        t_7 = self.l_91(t_1)
        t_10 = t_0.size()
        t_8 = t_6.size()
        t_3 = t_7.size()
        t_10 = t_10[slice(None, -1, None)]
        t_10 = t_10 + (16, 64)
        t_5 = t_10[0]
        t_4 = t_10[1]
        t_9 = t_10[2]
        t_10 = t_10[3]
        t_10 = t_0.view(t_5, t_4, t_9, t_10)
        t_10 = t_10.permute(0, 2, 1, 3)
        t_8 = t_8[slice(None, -1, None)]
        t_8 = t_8 + (16, 64)
        t_9 = t_8[0]
        t_4 = t_8[1]
        t_5 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_6.view(t_9, t_4, t_5, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_3 = t_3[slice(None, -1, None)]
        t_3 = t_3 + (16, 64)
        t_5 = t_3[0]
        t_4 = t_3[1]
        t_9 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_7.view(t_5, t_4, t_9, t_3)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_8 = t_8.transpose(-1, -2)
        t_8 = torch.matmul(t_10, t_8)
        t_10 = math.sqrt(64)
        t_10 = t_8 / t_10
        t_10 = t_10 + t_2
        t_10 = self.l_92(t_10)
        t_10 = self.l_93(t_10)
        t_3 = torch.matmul(t_10, t_3)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_10 = t_3.size()
        t_10 = t_10[slice(None, -2, None)]
        t_10 = t_10 + (1024,)
        t_8 = t_10[0]
        t_9 = t_10[1]
        t_10 = t_10[2]
        t_10 = t_3.view(t_8, t_9, t_10)
        t_10 = self.l_94(t_10)
        t_10 = self.l_95(t_10)
        t_1 = t_10 + t_1
        t_1 = self.l_96(t_1)
        t_10 = self.l_97(t_1)
        t_10 = torch.nn.functional.gelu(t_10)
        t_10 = self.l_98(t_10)
        t_10 = self.l_99(t_10)
        t_1 = t_10 + t_1
        t_1 = self.l_100(t_1)
        t_10 = self.l_101(t_1)
        t_9 = self.l_102(t_1)
        t_8 = self.l_103(t_1)
        t_3 = t_10.size()
        t_4 = t_9.size()
        t_5 = t_8.size()
        t_3 = t_3[slice(None, -1, None)]
        t_3 = t_3 + (16, 64)
        t_7 = t_3[0]
        t_6 = t_3[1]
        t_0 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_10.view(t_7, t_6, t_0, t_3)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_4 = t_4[slice(None, -1, None)]
        t_4 = t_4 + (16, 64)
        t_0 = t_4[0]
        t_6 = t_4[1]
        t_7 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_9.view(t_0, t_6, t_7, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_5 = t_5[slice(None, -1, None)]
        t_5 = t_5 + (16, 64)
        t_7 = t_5[0]
        t_6 = t_5[1]
        t_0 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_8.view(t_7, t_6, t_0, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_4 = t_4.transpose(-1, -2)
        t_4 = torch.matmul(t_3, t_4)
        t_3 = math.sqrt(64)
        t_3 = t_4 / t_3
        t_3 = t_3 + t_2
        t_3 = self.l_104(t_3)
        t_3 = self.l_105(t_3)
        t_5 = torch.matmul(t_3, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_5 = t_5.contiguous()
        t_3 = t_5.size()
        t_3 = t_3[slice(None, -2, None)]
        t_3 = t_3 + (1024,)
        t_4 = t_3[0]
        t_0 = t_3[1]
        t_3 = t_3[2]
        t_3 = t_5.view(t_4, t_0, t_3)
        t_3 = self.l_106(t_3)
        t_3 = self.l_107(t_3)
        t_1 = t_3 + t_1
        t_1 = self.l_108(t_1)
        t_3 = self.l_109(t_1)
        t_3 = torch.nn.functional.gelu(t_3)
        t_3 = self.l_110(t_3)
        t_3 = self.l_111(t_3)
        t_1 = t_3 + t_1
        t_1 = self.l_112(t_1)
        t_3 = self.l_113(t_1)
        t_0 = self.l_114(t_1)
        t_4 = self.l_115(t_1)
        t_5 = t_3.size()
        t_6 = t_0.size()
        t_7 = t_4.size()
        t_5 = t_5[slice(None, -1, None)]
        t_5 = t_5 + (16, 64)
        t_8 = t_5[0]
        t_9 = t_5[1]
        t_10 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_3.view(t_8, t_9, t_10, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_6 = t_6[slice(None, -1, None)]
        t_6 = t_6 + (16, 64)
        t_10 = t_6[0]
        t_9 = t_6[1]
        t_8 = t_6[2]
        t_6 = t_6[3]
        t_6 = t_0.view(t_10, t_9, t_8, t_6)
        t_6 = t_6.permute(0, 2, 1, 3)
        t_7 = t_7[slice(None, -1, None)]
        t_7 = t_7 + (16, 64)
        t_8 = t_7[0]
        t_9 = t_7[1]
        t_10 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_4.view(t_8, t_9, t_10, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_6 = t_6.transpose(-1, -2)
        t_6 = torch.matmul(t_5, t_6)
        t_5 = math.sqrt(64)
        t_5 = t_6 / t_5
        t_5 = t_5 + t_2
        t_5 = self.l_116(t_5)
        t_5 = self.l_117(t_5)
        t_7 = torch.matmul(t_5, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_7 = t_7.contiguous()
        t_5 = t_7.size()
        t_5 = t_5[slice(None, -2, None)]
        t_5 = t_5 + (1024,)
        t_6 = t_5[0]
        t_10 = t_5[1]
        t_5 = t_5[2]
        t_5 = t_7.view(t_6, t_10, t_5)
        t_5 = self.l_118(t_5)
        t_5 = self.l_119(t_5)
        t_1 = t_5 + t_1
        t_1 = self.l_120(t_1)
        t_5 = self.l_121(t_1)
        t_5 = torch.nn.functional.gelu(t_5)
        t_5 = self.l_122(t_5)
        t_5 = self.l_123(t_5)
        t_1 = t_5 + t_1
        t_1 = self.l_124(t_1)
        t_5 = self.l_125(t_1)
        t_10 = self.l_126(t_1)
        t_6 = self.l_127(t_1)
        t_7 = t_5.size()
        t_9 = t_10.size()
        t_8 = t_6.size()
        t_7 = t_7[slice(None, -1, None)]
        t_7 = t_7 + (16, 64)
        t_4 = t_7[0]
        t_0 = t_7[1]
        t_3 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_5.view(t_4, t_0, t_3, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_9 = t_9[slice(None, -1, None)]
        t_9 = t_9 + (16, 64)
        t_3 = t_9[0]
        t_0 = t_9[1]
        t_4 = t_9[2]
        t_9 = t_9[3]
        t_9 = t_10.view(t_3, t_0, t_4, t_9)
        t_9 = t_9.permute(0, 2, 1, 3)
        t_8 = t_8[slice(None, -1, None)]
        t_8 = t_8 + (16, 64)
        t_4 = t_8[0]
        t_0 = t_8[1]
        t_3 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_6.view(t_4, t_0, t_3, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_9 = t_9.transpose(-1, -2)
        t_9 = torch.matmul(t_7, t_9)
        t_7 = math.sqrt(64)
        t_7 = t_9 / t_7
        t_7 = t_7 + t_2
        t_7 = self.l_128(t_7)
        t_7 = self.l_129(t_7)
        t_8 = torch.matmul(t_7, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        # Returning:
        # BertForQuestionAnswering/BertModel[bert]/Tensor::__mul___12
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/LayerNorm[LayerNorm]
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Tensor::permute_1093
        return list(flatten((t_2, t_1, t_8)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition1(nn.Module):
    LAYER_SCOPES = [
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Linear[query]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Linear[key]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Linear[value]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertIntermediate[intermediate]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertOutput[output]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertOutput[output]/Dropout[dropout]',
            'BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertOutput[output]/LayerNorm[LayerNorm]',
            'BertForQuestionAnswering/BertModel[bert]/BertPooler[pooler]/Linear[dense]',
            'BertForQuestionAnswering/BertModel[bert]/BertPooler[pooler]/Tanh[activation]',
            'BertForQuestionAnswering/Linear[qa_outputs]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:1'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = {'l_0': 'bert.encoder.10.attention.output.dense',
                        'l_1': 'bert.encoder.10.attention.output.dropout',
                        'l_2': 'bert.encoder.10.attention.output.LayerNorm',
                        'l_3': 'bert.encoder.10.intermediate.dense',
                        'l_4': 'bert.encoder.10.output.dense',
                        'l_5': 'bert.encoder.10.output.dropout',
                        'l_6': 'bert.encoder.10.output.LayerNorm',
                        'l_7': 'bert.encoder.11.attention.self.query',
                        'l_8': 'bert.encoder.11.attention.self.key',
                        'l_9': 'bert.encoder.11.attention.self.value',
                        'l_10': 'bert.encoder.11.attention.self.softmax',
                        'l_11': 'bert.encoder.11.attention.self.dropout',
                        'l_12': 'bert.encoder.11.attention.output.dense',
                        'l_13': 'bert.encoder.11.attention.output.dropout',
                        'l_14': 'bert.encoder.11.attention.output.LayerNorm',
                        'l_15': 'bert.encoder.11.intermediate.dense',
                        'l_16': 'bert.encoder.11.output.dense',
                        'l_17': 'bert.encoder.11.output.dropout',
                        'l_18': 'bert.encoder.11.output.LayerNorm',
                        'l_19': 'bert.encoder.12.attention.self.query',
                        'l_20': 'bert.encoder.12.attention.self.key',
                        'l_21': 'bert.encoder.12.attention.self.value',
                        'l_22': 'bert.encoder.12.attention.self.softmax',
                        'l_23': 'bert.encoder.12.attention.self.dropout',
                        'l_24': 'bert.encoder.12.attention.output.dense',
                        'l_25': 'bert.encoder.12.attention.output.dropout',
                        'l_26': 'bert.encoder.12.attention.output.LayerNorm',
                        'l_27': 'bert.encoder.12.intermediate.dense',
                        'l_28': 'bert.encoder.12.output.dense',
                        'l_29': 'bert.encoder.12.output.dropout',
                        'l_30': 'bert.encoder.12.output.LayerNorm',
                        'l_31': 'bert.encoder.13.attention.self.query',
                        'l_32': 'bert.encoder.13.attention.self.key',
                        'l_33': 'bert.encoder.13.attention.self.value',
                        'l_34': 'bert.encoder.13.attention.self.softmax',
                        'l_35': 'bert.encoder.13.attention.self.dropout',
                        'l_36': 'bert.encoder.13.attention.output.dense',
                        'l_37': 'bert.encoder.13.attention.output.dropout',
                        'l_38': 'bert.encoder.13.attention.output.LayerNorm',
                        'l_39': 'bert.encoder.13.intermediate.dense',
                        'l_40': 'bert.encoder.13.output.dense',
                        'l_41': 'bert.encoder.13.output.dropout',
                        'l_42': 'bert.encoder.13.output.LayerNorm',
                        'l_43': 'bert.encoder.14.attention.self.query',
                        'l_44': 'bert.encoder.14.attention.self.key',
                        'l_45': 'bert.encoder.14.attention.self.value',
                        'l_46': 'bert.encoder.14.attention.self.softmax',
                        'l_47': 'bert.encoder.14.attention.self.dropout',
                        'l_48': 'bert.encoder.14.attention.output.dense',
                        'l_49': 'bert.encoder.14.attention.output.dropout',
                        'l_50': 'bert.encoder.14.attention.output.LayerNorm',
                        'l_51': 'bert.encoder.14.intermediate.dense',
                        'l_52': 'bert.encoder.14.output.dense',
                        'l_53': 'bert.encoder.14.output.dropout',
                        'l_54': 'bert.encoder.14.output.LayerNorm',
                        'l_55': 'bert.encoder.15.attention.self.query',
                        'l_56': 'bert.encoder.15.attention.self.key',
                        'l_57': 'bert.encoder.15.attention.self.value',
                        'l_58': 'bert.encoder.15.attention.self.softmax',
                        'l_59': 'bert.encoder.15.attention.self.dropout',
                        'l_60': 'bert.encoder.15.attention.output.dense',
                        'l_61': 'bert.encoder.15.attention.output.dropout',
                        'l_62': 'bert.encoder.15.attention.output.LayerNorm',
                        'l_63': 'bert.encoder.15.intermediate.dense',
                        'l_64': 'bert.encoder.15.output.dense',
                        'l_65': 'bert.encoder.15.output.dropout',
                        'l_66': 'bert.encoder.15.output.LayerNorm',
                        'l_67': 'bert.encoder.16.attention.self.query',
                        'l_68': 'bert.encoder.16.attention.self.key',
                        'l_69': 'bert.encoder.16.attention.self.value',
                        'l_70': 'bert.encoder.16.attention.self.softmax',
                        'l_71': 'bert.encoder.16.attention.self.dropout',
                        'l_72': 'bert.encoder.16.attention.output.dense',
                        'l_73': 'bert.encoder.16.attention.output.dropout',
                        'l_74': 'bert.encoder.16.attention.output.LayerNorm',
                        'l_75': 'bert.encoder.16.intermediate.dense',
                        'l_76': 'bert.encoder.16.output.dense',
                        'l_77': 'bert.encoder.16.output.dropout',
                        'l_78': 'bert.encoder.16.output.LayerNorm',
                        'l_79': 'bert.encoder.17.attention.self.query',
                        'l_80': 'bert.encoder.17.attention.self.key',
                        'l_81': 'bert.encoder.17.attention.self.value',
                        'l_82': 'bert.encoder.17.attention.self.softmax',
                        'l_83': 'bert.encoder.17.attention.self.dropout',
                        'l_84': 'bert.encoder.17.attention.output.dense',
                        'l_85': 'bert.encoder.17.attention.output.dropout',
                        'l_86': 'bert.encoder.17.attention.output.LayerNorm',
                        'l_87': 'bert.encoder.17.intermediate.dense',
                        'l_88': 'bert.encoder.17.output.dense',
                        'l_89': 'bert.encoder.17.output.dropout',
                        'l_90': 'bert.encoder.17.output.LayerNorm',
                        'l_91': 'bert.encoder.18.attention.self.query',
                        'l_92': 'bert.encoder.18.attention.self.key',
                        'l_93': 'bert.encoder.18.attention.self.value',
                        'l_94': 'bert.encoder.18.attention.self.softmax',
                        'l_95': 'bert.encoder.18.attention.self.dropout',
                        'l_96': 'bert.encoder.18.attention.output.dense',
                        'l_97': 'bert.encoder.18.attention.output.dropout',
                        'l_98': 'bert.encoder.18.attention.output.LayerNorm',
                        'l_99': 'bert.encoder.18.intermediate.dense',
                        'l_100': 'bert.encoder.18.output.dense',
                        'l_101': 'bert.encoder.18.output.dropout',
                        'l_102': 'bert.encoder.18.output.LayerNorm',
                        'l_103': 'bert.encoder.19.attention.self.query',
                        'l_104': 'bert.encoder.19.attention.self.key',
                        'l_105': 'bert.encoder.19.attention.self.value',
                        'l_106': 'bert.encoder.19.attention.self.softmax',
                        'l_107': 'bert.encoder.19.attention.self.dropout',
                        'l_108': 'bert.encoder.19.attention.output.dense',
                        'l_109': 'bert.encoder.19.attention.output.dropout',
                        'l_110': 'bert.encoder.19.attention.output.LayerNorm',
                        'l_111': 'bert.encoder.19.intermediate.dense',
                        'l_112': 'bert.encoder.19.output.dense',
                        'l_113': 'bert.encoder.19.output.dropout',
                        'l_114': 'bert.encoder.19.output.LayerNorm',
                        'l_115': 'bert.encoder.20.attention.self.query',
                        'l_116': 'bert.encoder.20.attention.self.key',
                        'l_117': 'bert.encoder.20.attention.self.value',
                        'l_118': 'bert.encoder.20.attention.self.softmax',
                        'l_119': 'bert.encoder.20.attention.self.dropout',
                        'l_120': 'bert.encoder.20.attention.output.dense',
                        'l_121': 'bert.encoder.20.attention.output.dropout',
                        'l_122': 'bert.encoder.20.attention.output.LayerNorm',
                        'l_123': 'bert.encoder.20.intermediate.dense',
                        'l_124': 'bert.encoder.20.output.dense',
                        'l_125': 'bert.encoder.20.output.dropout',
                        'l_126': 'bert.encoder.20.output.LayerNorm',
                        'l_127': 'bert.encoder.21.attention.self.query',
                        'l_128': 'bert.encoder.21.attention.self.key',
                        'l_129': 'bert.encoder.21.attention.self.value',
                        'l_130': 'bert.encoder.21.attention.self.softmax',
                        'l_131': 'bert.encoder.21.attention.self.dropout',
                        'l_132': 'bert.encoder.21.attention.output.dense',
                        'l_133': 'bert.encoder.21.attention.output.dropout',
                        'l_134': 'bert.encoder.21.attention.output.LayerNorm',
                        'l_135': 'bert.encoder.21.intermediate.dense',
                        'l_136': 'bert.encoder.21.output.dense',
                        'l_137': 'bert.encoder.21.output.dropout',
                        'l_138': 'bert.encoder.21.output.LayerNorm',
                        'l_139': 'bert.encoder.22.attention.self.query',
                        'l_140': 'bert.encoder.22.attention.self.key',
                        'l_141': 'bert.encoder.22.attention.self.value',
                        'l_142': 'bert.encoder.22.attention.self.softmax',
                        'l_143': 'bert.encoder.22.attention.self.dropout',
                        'l_144': 'bert.encoder.22.attention.output.dense',
                        'l_145': 'bert.encoder.22.attention.output.dropout',
                        'l_146': 'bert.encoder.22.attention.output.LayerNorm',
                        'l_147': 'bert.encoder.22.intermediate.dense',
                        'l_148': 'bert.encoder.22.output.dense',
                        'l_149': 'bert.encoder.22.output.dropout',
                        'l_150': 'bert.encoder.22.output.LayerNorm',
                        'l_151': 'bert.encoder.23.attention.self.query',
                        'l_152': 'bert.encoder.23.attention.self.key',
                        'l_153': 'bert.encoder.23.attention.self.value',
                        'l_154': 'bert.encoder.23.attention.self.softmax',
                        'l_155': 'bert.encoder.23.attention.self.dropout',
                        'l_156': 'bert.encoder.23.attention.output.dense',
                        'l_157': 'bert.encoder.23.attention.output.dropout',
                        'l_158': 'bert.encoder.23.attention.output.LayerNorm',
                        'l_159': 'bert.encoder.23.intermediate.dense',
                        'l_160': 'bert.encoder.23.output.dense',
                        'l_161': 'bert.encoder.23.output.dropout',
                        'l_162': 'bert.encoder.23.output.LayerNorm',
                        'l_163': 'bert.pooler.dense',
                        'l_164': 'bert.pooler.activation',
                        'l_165': 'qa_outputs'}
        self.to(self.device)

    def forward(self, *args):
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_0
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_1
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_2
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_3
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertOutput[output]/Linear[dense] <=> self.l_4
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertOutput[output]/Dropout[dropout] <=> self.l_5
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_6
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_7
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_8
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_9
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_10
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_11
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_12
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_13
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_14
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_15
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertOutput[output]/Linear[dense] <=> self.l_16
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertOutput[output]/Dropout[dropout] <=> self.l_17
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[11]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_18
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_19
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_20
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_21
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_22
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_23
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_24
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_25
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_26
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_27
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertOutput[output]/Linear[dense] <=> self.l_28
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertOutput[output]/Dropout[dropout] <=> self.l_29
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[12]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_30
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_31
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_32
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_33
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_34
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_35
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_36
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_37
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_38
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_39
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertOutput[output]/Linear[dense] <=> self.l_40
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertOutput[output]/Dropout[dropout] <=> self.l_41
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[13]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_42
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_43
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_44
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_45
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_46
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_47
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_48
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_49
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_50
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_51
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertOutput[output]/Linear[dense] <=> self.l_52
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertOutput[output]/Dropout[dropout] <=> self.l_53
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[14]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_54
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_55
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_56
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_57
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_58
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_59
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_60
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_61
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_62
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_63
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertOutput[output]/Linear[dense] <=> self.l_64
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertOutput[output]/Dropout[dropout] <=> self.l_65
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[15]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_66
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_67
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_68
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_69
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_70
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_71
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_72
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_73
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_74
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_75
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertOutput[output]/Linear[dense] <=> self.l_76
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertOutput[output]/Dropout[dropout] <=> self.l_77
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[16]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_78
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_79
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_80
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_81
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_82
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_83
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_84
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_85
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_86
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_87
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertOutput[output]/Linear[dense] <=> self.l_88
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertOutput[output]/Dropout[dropout] <=> self.l_89
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[17]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_90
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_91
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_92
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_93
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_94
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_95
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_96
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_97
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_98
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_99
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertOutput[output]/Linear[dense] <=> self.l_100
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertOutput[output]/Dropout[dropout] <=> self.l_101
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[18]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_102
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_103
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_104
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_105
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_106
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_107
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_108
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_109
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_110
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_111
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertOutput[output]/Linear[dense] <=> self.l_112
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertOutput[output]/Dropout[dropout] <=> self.l_113
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[19]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_114
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_115
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_116
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_117
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_118
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_119
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_120
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_121
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_122
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_123
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertOutput[output]/Linear[dense] <=> self.l_124
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertOutput[output]/Dropout[dropout] <=> self.l_125
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[20]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_126
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_127
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_128
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_129
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_130
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_131
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_132
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_133
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_134
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_135
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertOutput[output]/Linear[dense] <=> self.l_136
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertOutput[output]/Dropout[dropout] <=> self.l_137
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[21]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_138
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_139
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_140
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_141
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_142
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_143
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_144
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_145
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_146
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_147
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertOutput[output]/Linear[dense] <=> self.l_148
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertOutput[output]/Dropout[dropout] <=> self.l_149
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[22]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_150
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Linear[query] <=> self.l_151
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Linear[key] <=> self.l_152
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Linear[value] <=> self.l_153
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Softmax[softmax] <=> self.l_154
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfAttention[self]/Dropout[dropout] <=> self.l_155
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfOutput[output]/Linear[dense] <=> self.l_156
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfOutput[output]/Dropout[dropout] <=> self.l_157
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertAttention[attention]/BertSelfOutput[output]/LayerNorm[LayerNorm] <=> self.l_158
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertIntermediate[intermediate]/Linear[dense] <=> self.l_159
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertOutput[output]/Linear[dense] <=> self.l_160
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertOutput[output]/Dropout[dropout] <=> self.l_161
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[23]/BertOutput[output]/LayerNorm[LayerNorm] <=> self.l_162
        # BertForQuestionAnswering/BertModel[bert]/BertPooler[pooler]/Linear[dense] <=> self.l_163
        # BertForQuestionAnswering/BertModel[bert]/BertPooler[pooler]/Tanh[activation] <=> self.l_164
        # BertForQuestionAnswering/Linear[qa_outputs] <=> self.l_165
        # BertForQuestionAnswering/BertModel[bert]/Tensor::__mul___12 <=> x0
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[9]/BertOutput[output]/LayerNorm[LayerNorm] <=> x1
        # BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Tensor::permute_1093 <=> x2
        x0, x1, x2 = unflatten(args,self.input_structure)
        t_0 = x2.contiguous()
        t_1 = t_0.size()
        t_1 = t_1[slice(None, -2, None)]
        t_1 = t_1 + (1024,)
        t_2 = t_1[0]
        t_3 = t_1[1]
        t_1 = t_1[2]
        t_1 = t_0.view(t_2, t_3, t_1)
        t_1 = self.l_0(t_1)
        t_1 = self.l_1(t_1)
        t_1 = t_1 + x1
        t_1 = self.l_2(t_1)
        t_3 = self.l_3(t_1)
        t_3 = torch.nn.functional.gelu(t_3)
        t_3 = self.l_4(t_3)
        t_3 = self.l_5(t_3)
        t_1 = t_3 + t_1
        t_1 = self.l_6(t_1)
        t_3 = self.l_7(t_1)
        t_2 = self.l_8(t_1)
        t_0 = self.l_9(t_1)
        t_4 = t_3.size()
        t_5 = t_2.size()
        t_6 = t_0.size()
        t_4 = t_4[slice(None, -1, None)]
        t_4 = t_4 + (16, 64)
        t_7 = t_4[0]
        t_8 = t_4[1]
        t_9 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_3.view(t_7, t_8, t_9, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_5 = t_5[slice(None, -1, None)]
        t_5 = t_5 + (16, 64)
        t_9 = t_5[0]
        t_8 = t_5[1]
        t_7 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_2.view(t_9, t_8, t_7, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_6 = t_6[slice(None, -1, None)]
        t_6 = t_6 + (16, 64)
        t_7 = t_6[0]
        t_8 = t_6[1]
        t_9 = t_6[2]
        t_6 = t_6[3]
        t_6 = t_0.view(t_7, t_8, t_9, t_6)
        t_6 = t_6.permute(0, 2, 1, 3)
        t_5 = t_5.transpose(-1, -2)
        t_5 = torch.matmul(t_4, t_5)
        t_4 = math.sqrt(64)
        t_4 = t_5 / t_4
        t_4 = t_4 + x0
        t_4 = self.l_10(t_4)
        t_4 = self.l_11(t_4)
        t_6 = torch.matmul(t_4, t_6)
        t_6 = t_6.permute(0, 2, 1, 3)
        t_6 = t_6.contiguous()
        t_4 = t_6.size()
        t_4 = t_4[slice(None, -2, None)]
        t_4 = t_4 + (1024,)
        t_5 = t_4[0]
        t_9 = t_4[1]
        t_4 = t_4[2]
        t_4 = t_6.view(t_5, t_9, t_4)
        t_4 = self.l_12(t_4)
        t_4 = self.l_13(t_4)
        t_1 = t_4 + t_1
        t_1 = self.l_14(t_1)
        t_4 = self.l_15(t_1)
        t_4 = torch.nn.functional.gelu(t_4)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_1 = t_4 + t_1
        t_1 = self.l_18(t_1)
        t_4 = self.l_19(t_1)
        t_9 = self.l_20(t_1)
        t_5 = self.l_21(t_1)
        t_6 = t_4.size()
        t_8 = t_9.size()
        t_7 = t_5.size()
        t_6 = t_6[slice(None, -1, None)]
        t_6 = t_6 + (16, 64)
        t_0 = t_6[0]
        t_2 = t_6[1]
        t_3 = t_6[2]
        t_6 = t_6[3]
        t_6 = t_4.view(t_0, t_2, t_3, t_6)
        t_6 = t_6.permute(0, 2, 1, 3)
        t_8 = t_8[slice(None, -1, None)]
        t_8 = t_8 + (16, 64)
        t_3 = t_8[0]
        t_2 = t_8[1]
        t_0 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_9.view(t_3, t_2, t_0, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_7 = t_7[slice(None, -1, None)]
        t_7 = t_7 + (16, 64)
        t_0 = t_7[0]
        t_2 = t_7[1]
        t_3 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_5.view(t_0, t_2, t_3, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_8 = t_8.transpose(-1, -2)
        t_8 = torch.matmul(t_6, t_8)
        t_6 = math.sqrt(64)
        t_6 = t_8 / t_6
        t_6 = t_6 + x0
        t_6 = self.l_22(t_6)
        t_6 = self.l_23(t_6)
        t_7 = torch.matmul(t_6, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_7 = t_7.contiguous()
        t_6 = t_7.size()
        t_6 = t_6[slice(None, -2, None)]
        t_6 = t_6 + (1024,)
        t_8 = t_6[0]
        t_3 = t_6[1]
        t_6 = t_6[2]
        t_6 = t_7.view(t_8, t_3, t_6)
        t_6 = self.l_24(t_6)
        t_6 = self.l_25(t_6)
        t_1 = t_6 + t_1
        t_1 = self.l_26(t_1)
        t_6 = self.l_27(t_1)
        t_6 = torch.nn.functional.gelu(t_6)
        t_6 = self.l_28(t_6)
        t_6 = self.l_29(t_6)
        t_1 = t_6 + t_1
        t_1 = self.l_30(t_1)
        t_6 = self.l_31(t_1)
        t_3 = self.l_32(t_1)
        t_8 = self.l_33(t_1)
        t_7 = t_6.size()
        t_2 = t_3.size()
        t_0 = t_8.size()
        t_7 = t_7[slice(None, -1, None)]
        t_7 = t_7 + (16, 64)
        t_5 = t_7[0]
        t_9 = t_7[1]
        t_4 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_6.view(t_5, t_9, t_4, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_2 = t_2[slice(None, -1, None)]
        t_2 = t_2 + (16, 64)
        t_4 = t_2[0]
        t_9 = t_2[1]
        t_5 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_4, t_9, t_5, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_0 = t_0[slice(None, -1, None)]
        t_0 = t_0 + (16, 64)
        t_5 = t_0[0]
        t_9 = t_0[1]
        t_4 = t_0[2]
        t_0 = t_0[3]
        t_0 = t_8.view(t_5, t_9, t_4, t_0)
        t_0 = t_0.permute(0, 2, 1, 3)
        t_2 = t_2.transpose(-1, -2)
        t_2 = torch.matmul(t_7, t_2)
        t_7 = math.sqrt(64)
        t_7 = t_2 / t_7
        t_7 = t_7 + x0
        t_7 = self.l_34(t_7)
        t_7 = self.l_35(t_7)
        t_0 = torch.matmul(t_7, t_0)
        t_0 = t_0.permute(0, 2, 1, 3)
        t_0 = t_0.contiguous()
        t_7 = t_0.size()
        t_7 = t_7[slice(None, -2, None)]
        t_7 = t_7 + (1024,)
        t_2 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_0.view(t_2, t_4, t_7)
        t_7 = self.l_36(t_7)
        t_7 = self.l_37(t_7)
        t_1 = t_7 + t_1
        t_1 = self.l_38(t_1)
        t_7 = self.l_39(t_1)
        t_7 = torch.nn.functional.gelu(t_7)
        t_7 = self.l_40(t_7)
        t_7 = self.l_41(t_7)
        t_1 = t_7 + t_1
        t_1 = self.l_42(t_1)
        t_7 = self.l_43(t_1)
        t_4 = self.l_44(t_1)
        t_2 = self.l_45(t_1)
        t_0 = t_7.size()
        t_9 = t_4.size()
        t_5 = t_2.size()
        t_0 = t_0[slice(None, -1, None)]
        t_0 = t_0 + (16, 64)
        t_8 = t_0[0]
        t_3 = t_0[1]
        t_6 = t_0[2]
        t_0 = t_0[3]
        t_0 = t_7.view(t_8, t_3, t_6, t_0)
        t_0 = t_0.permute(0, 2, 1, 3)
        t_9 = t_9[slice(None, -1, None)]
        t_9 = t_9 + (16, 64)
        t_6 = t_9[0]
        t_3 = t_9[1]
        t_8 = t_9[2]
        t_9 = t_9[3]
        t_9 = t_4.view(t_6, t_3, t_8, t_9)
        t_9 = t_9.permute(0, 2, 1, 3)
        t_5 = t_5[slice(None, -1, None)]
        t_5 = t_5 + (16, 64)
        t_8 = t_5[0]
        t_3 = t_5[1]
        t_6 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_2.view(t_8, t_3, t_6, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_9 = t_9.transpose(-1, -2)
        t_9 = torch.matmul(t_0, t_9)
        t_0 = math.sqrt(64)
        t_0 = t_9 / t_0
        t_0 = t_0 + x0
        t_0 = self.l_46(t_0)
        t_0 = self.l_47(t_0)
        t_5 = torch.matmul(t_0, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_5 = t_5.contiguous()
        t_0 = t_5.size()
        t_0 = t_0[slice(None, -2, None)]
        t_0 = t_0 + (1024,)
        t_9 = t_0[0]
        t_6 = t_0[1]
        t_0 = t_0[2]
        t_0 = t_5.view(t_9, t_6, t_0)
        t_0 = self.l_48(t_0)
        t_0 = self.l_49(t_0)
        t_1 = t_0 + t_1
        t_1 = self.l_50(t_1)
        t_0 = self.l_51(t_1)
        t_0 = torch.nn.functional.gelu(t_0)
        t_0 = self.l_52(t_0)
        t_0 = self.l_53(t_0)
        t_1 = t_0 + t_1
        t_1 = self.l_54(t_1)
        t_0 = self.l_55(t_1)
        t_6 = self.l_56(t_1)
        t_9 = self.l_57(t_1)
        t_5 = t_0.size()
        t_3 = t_6.size()
        t_8 = t_9.size()
        t_5 = t_5[slice(None, -1, None)]
        t_5 = t_5 + (16, 64)
        t_2 = t_5[0]
        t_4 = t_5[1]
        t_7 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_0.view(t_2, t_4, t_7, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_3 = t_3[slice(None, -1, None)]
        t_3 = t_3 + (16, 64)
        t_7 = t_3[0]
        t_4 = t_3[1]
        t_2 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_6.view(t_7, t_4, t_2, t_3)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_8 = t_8[slice(None, -1, None)]
        t_8 = t_8 + (16, 64)
        t_2 = t_8[0]
        t_4 = t_8[1]
        t_7 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_9.view(t_2, t_4, t_7, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_3 = t_3.transpose(-1, -2)
        t_3 = torch.matmul(t_5, t_3)
        t_5 = math.sqrt(64)
        t_5 = t_3 / t_5
        t_5 = t_5 + x0
        t_5 = self.l_58(t_5)
        t_5 = self.l_59(t_5)
        t_8 = torch.matmul(t_5, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_8 = t_8.contiguous()
        t_5 = t_8.size()
        t_5 = t_5[slice(None, -2, None)]
        t_5 = t_5 + (1024,)
        t_3 = t_5[0]
        t_7 = t_5[1]
        t_5 = t_5[2]
        t_5 = t_8.view(t_3, t_7, t_5)
        t_5 = self.l_60(t_5)
        t_5 = self.l_61(t_5)
        t_1 = t_5 + t_1
        t_1 = self.l_62(t_1)
        t_5 = self.l_63(t_1)
        t_5 = torch.nn.functional.gelu(t_5)
        t_5 = self.l_64(t_5)
        t_5 = self.l_65(t_5)
        t_1 = t_5 + t_1
        t_1 = self.l_66(t_1)
        t_5 = self.l_67(t_1)
        t_7 = self.l_68(t_1)
        t_3 = self.l_69(t_1)
        t_8 = t_5.size()
        t_4 = t_7.size()
        t_2 = t_3.size()
        t_8 = t_8[slice(None, -1, None)]
        t_8 = t_8 + (16, 64)
        t_9 = t_8[0]
        t_6 = t_8[1]
        t_0 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_5.view(t_9, t_6, t_0, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_4 = t_4[slice(None, -1, None)]
        t_4 = t_4 + (16, 64)
        t_0 = t_4[0]
        t_6 = t_4[1]
        t_9 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_7.view(t_0, t_6, t_9, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_2 = t_2[slice(None, -1, None)]
        t_2 = t_2 + (16, 64)
        t_9 = t_2[0]
        t_6 = t_2[1]
        t_0 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_9, t_6, t_0, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_4 = t_4.transpose(-1, -2)
        t_4 = torch.matmul(t_8, t_4)
        t_8 = math.sqrt(64)
        t_8 = t_4 / t_8
        t_8 = t_8 + x0
        t_8 = self.l_70(t_8)
        t_8 = self.l_71(t_8)
        t_2 = torch.matmul(t_8, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_2 = t_2.contiguous()
        t_8 = t_2.size()
        t_8 = t_8[slice(None, -2, None)]
        t_8 = t_8 + (1024,)
        t_4 = t_8[0]
        t_0 = t_8[1]
        t_8 = t_8[2]
        t_8 = t_2.view(t_4, t_0, t_8)
        t_8 = self.l_72(t_8)
        t_8 = self.l_73(t_8)
        t_1 = t_8 + t_1
        t_1 = self.l_74(t_1)
        t_8 = self.l_75(t_1)
        t_8 = torch.nn.functional.gelu(t_8)
        t_8 = self.l_76(t_8)
        t_8 = self.l_77(t_8)
        t_1 = t_8 + t_1
        t_1 = self.l_78(t_1)
        t_8 = self.l_79(t_1)
        t_0 = self.l_80(t_1)
        t_4 = self.l_81(t_1)
        t_2 = t_8.size()
        t_6 = t_0.size()
        t_9 = t_4.size()
        t_2 = t_2[slice(None, -1, None)]
        t_2 = t_2 + (16, 64)
        t_3 = t_2[0]
        t_7 = t_2[1]
        t_5 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_8.view(t_3, t_7, t_5, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_6 = t_6[slice(None, -1, None)]
        t_6 = t_6 + (16, 64)
        t_5 = t_6[0]
        t_7 = t_6[1]
        t_3 = t_6[2]
        t_6 = t_6[3]
        t_6 = t_0.view(t_5, t_7, t_3, t_6)
        t_6 = t_6.permute(0, 2, 1, 3)
        t_9 = t_9[slice(None, -1, None)]
        t_9 = t_9 + (16, 64)
        t_3 = t_9[0]
        t_7 = t_9[1]
        t_5 = t_9[2]
        t_9 = t_9[3]
        t_9 = t_4.view(t_3, t_7, t_5, t_9)
        t_9 = t_9.permute(0, 2, 1, 3)
        t_6 = t_6.transpose(-1, -2)
        t_6 = torch.matmul(t_2, t_6)
        t_2 = math.sqrt(64)
        t_2 = t_6 / t_2
        t_2 = t_2 + x0
        t_2 = self.l_82(t_2)
        t_2 = self.l_83(t_2)
        t_9 = torch.matmul(t_2, t_9)
        t_9 = t_9.permute(0, 2, 1, 3)
        t_9 = t_9.contiguous()
        t_2 = t_9.size()
        t_2 = t_2[slice(None, -2, None)]
        t_2 = t_2 + (1024,)
        t_6 = t_2[0]
        t_5 = t_2[1]
        t_2 = t_2[2]
        t_2 = t_9.view(t_6, t_5, t_2)
        t_2 = self.l_84(t_2)
        t_2 = self.l_85(t_2)
        t_1 = t_2 + t_1
        t_1 = self.l_86(t_1)
        t_2 = self.l_87(t_1)
        t_2 = torch.nn.functional.gelu(t_2)
        t_2 = self.l_88(t_2)
        t_2 = self.l_89(t_2)
        t_1 = t_2 + t_1
        t_1 = self.l_90(t_1)
        t_2 = self.l_91(t_1)
        t_5 = self.l_92(t_1)
        t_6 = self.l_93(t_1)
        t_9 = t_2.size()
        t_7 = t_5.size()
        t_3 = t_6.size()
        t_9 = t_9[slice(None, -1, None)]
        t_9 = t_9 + (16, 64)
        t_4 = t_9[0]
        t_0 = t_9[1]
        t_8 = t_9[2]
        t_9 = t_9[3]
        t_9 = t_2.view(t_4, t_0, t_8, t_9)
        t_9 = t_9.permute(0, 2, 1, 3)
        t_7 = t_7[slice(None, -1, None)]
        t_7 = t_7 + (16, 64)
        t_8 = t_7[0]
        t_0 = t_7[1]
        t_4 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_5.view(t_8, t_0, t_4, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_3 = t_3[slice(None, -1, None)]
        t_3 = t_3 + (16, 64)
        t_4 = t_3[0]
        t_0 = t_3[1]
        t_8 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_6.view(t_4, t_0, t_8, t_3)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_7 = t_7.transpose(-1, -2)
        t_7 = torch.matmul(t_9, t_7)
        t_9 = math.sqrt(64)
        t_9 = t_7 / t_9
        t_9 = t_9 + x0
        t_9 = self.l_94(t_9)
        t_9 = self.l_95(t_9)
        t_3 = torch.matmul(t_9, t_3)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_3 = t_3.contiguous()
        t_9 = t_3.size()
        t_9 = t_9[slice(None, -2, None)]
        t_9 = t_9 + (1024,)
        t_7 = t_9[0]
        t_8 = t_9[1]
        t_9 = t_9[2]
        t_9 = t_3.view(t_7, t_8, t_9)
        t_9 = self.l_96(t_9)
        t_9 = self.l_97(t_9)
        t_1 = t_9 + t_1
        t_1 = self.l_98(t_1)
        t_9 = self.l_99(t_1)
        t_9 = torch.nn.functional.gelu(t_9)
        t_9 = self.l_100(t_9)
        t_9 = self.l_101(t_9)
        t_1 = t_9 + t_1
        t_1 = self.l_102(t_1)
        t_9 = self.l_103(t_1)
        t_8 = self.l_104(t_1)
        t_7 = self.l_105(t_1)
        t_3 = t_9.size()
        t_0 = t_8.size()
        t_4 = t_7.size()
        t_3 = t_3[slice(None, -1, None)]
        t_3 = t_3 + (16, 64)
        t_6 = t_3[0]
        t_5 = t_3[1]
        t_2 = t_3[2]
        t_3 = t_3[3]
        t_3 = t_9.view(t_6, t_5, t_2, t_3)
        t_3 = t_3.permute(0, 2, 1, 3)
        t_0 = t_0[slice(None, -1, None)]
        t_0 = t_0 + (16, 64)
        t_2 = t_0[0]
        t_5 = t_0[1]
        t_6 = t_0[2]
        t_0 = t_0[3]
        t_0 = t_8.view(t_2, t_5, t_6, t_0)
        t_0 = t_0.permute(0, 2, 1, 3)
        t_4 = t_4[slice(None, -1, None)]
        t_4 = t_4 + (16, 64)
        t_6 = t_4[0]
        t_5 = t_4[1]
        t_2 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_7.view(t_6, t_5, t_2, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_0 = t_0.transpose(-1, -2)
        t_0 = torch.matmul(t_3, t_0)
        t_3 = math.sqrt(64)
        t_3 = t_0 / t_3
        t_3 = t_3 + x0
        t_3 = self.l_106(t_3)
        t_3 = self.l_107(t_3)
        t_4 = torch.matmul(t_3, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_4 = t_4.contiguous()
        t_3 = t_4.size()
        t_3 = t_3[slice(None, -2, None)]
        t_3 = t_3 + (1024,)
        t_0 = t_3[0]
        t_2 = t_3[1]
        t_3 = t_3[2]
        t_3 = t_4.view(t_0, t_2, t_3)
        t_3 = self.l_108(t_3)
        t_3 = self.l_109(t_3)
        t_1 = t_3 + t_1
        t_1 = self.l_110(t_1)
        t_3 = self.l_111(t_1)
        t_3 = torch.nn.functional.gelu(t_3)
        t_3 = self.l_112(t_3)
        t_3 = self.l_113(t_3)
        t_1 = t_3 + t_1
        t_1 = self.l_114(t_1)
        t_3 = self.l_115(t_1)
        t_2 = self.l_116(t_1)
        t_0 = self.l_117(t_1)
        t_4 = t_3.size()
        t_5 = t_2.size()
        t_6 = t_0.size()
        t_4 = t_4[slice(None, -1, None)]
        t_4 = t_4 + (16, 64)
        t_7 = t_4[0]
        t_8 = t_4[1]
        t_9 = t_4[2]
        t_4 = t_4[3]
        t_4 = t_3.view(t_7, t_8, t_9, t_4)
        t_4 = t_4.permute(0, 2, 1, 3)
        t_5 = t_5[slice(None, -1, None)]
        t_5 = t_5 + (16, 64)
        t_9 = t_5[0]
        t_8 = t_5[1]
        t_7 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_2.view(t_9, t_8, t_7, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_6 = t_6[slice(None, -1, None)]
        t_6 = t_6 + (16, 64)
        t_7 = t_6[0]
        t_8 = t_6[1]
        t_9 = t_6[2]
        t_6 = t_6[3]
        t_6 = t_0.view(t_7, t_8, t_9, t_6)
        t_6 = t_6.permute(0, 2, 1, 3)
        t_5 = t_5.transpose(-1, -2)
        t_5 = torch.matmul(t_4, t_5)
        t_4 = math.sqrt(64)
        t_4 = t_5 / t_4
        t_4 = t_4 + x0
        t_4 = self.l_118(t_4)
        t_4 = self.l_119(t_4)
        t_6 = torch.matmul(t_4, t_6)
        t_6 = t_6.permute(0, 2, 1, 3)
        t_6 = t_6.contiguous()
        t_4 = t_6.size()
        t_4 = t_4[slice(None, -2, None)]
        t_4 = t_4 + (1024,)
        t_5 = t_4[0]
        t_9 = t_4[1]
        t_4 = t_4[2]
        t_4 = t_6.view(t_5, t_9, t_4)
        t_4 = self.l_120(t_4)
        t_4 = self.l_121(t_4)
        t_1 = t_4 + t_1
        t_1 = self.l_122(t_1)
        t_4 = self.l_123(t_1)
        t_4 = torch.nn.functional.gelu(t_4)
        t_4 = self.l_124(t_4)
        t_4 = self.l_125(t_4)
        t_1 = t_4 + t_1
        t_1 = self.l_126(t_1)
        t_4 = self.l_127(t_1)
        t_9 = self.l_128(t_1)
        t_5 = self.l_129(t_1)
        t_6 = t_4.size()
        t_8 = t_9.size()
        t_7 = t_5.size()
        t_6 = t_6[slice(None, -1, None)]
        t_6 = t_6 + (16, 64)
        t_0 = t_6[0]
        t_2 = t_6[1]
        t_3 = t_6[2]
        t_6 = t_6[3]
        t_6 = t_4.view(t_0, t_2, t_3, t_6)
        t_6 = t_6.permute(0, 2, 1, 3)
        t_8 = t_8[slice(None, -1, None)]
        t_8 = t_8 + (16, 64)
        t_3 = t_8[0]
        t_2 = t_8[1]
        t_0 = t_8[2]
        t_8 = t_8[3]
        t_8 = t_9.view(t_3, t_2, t_0, t_8)
        t_8 = t_8.permute(0, 2, 1, 3)
        t_7 = t_7[slice(None, -1, None)]
        t_7 = t_7 + (16, 64)
        t_0 = t_7[0]
        t_2 = t_7[1]
        t_3 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_5.view(t_0, t_2, t_3, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_8 = t_8.transpose(-1, -2)
        t_8 = torch.matmul(t_6, t_8)
        t_6 = math.sqrt(64)
        t_6 = t_8 / t_6
        t_6 = t_6 + x0
        t_6 = self.l_130(t_6)
        t_6 = self.l_131(t_6)
        t_7 = torch.matmul(t_6, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_7 = t_7.contiguous()
        t_6 = t_7.size()
        t_6 = t_6[slice(None, -2, None)]
        t_6 = t_6 + (1024,)
        t_8 = t_6[0]
        t_3 = t_6[1]
        t_6 = t_6[2]
        t_6 = t_7.view(t_8, t_3, t_6)
        t_6 = self.l_132(t_6)
        t_6 = self.l_133(t_6)
        t_1 = t_6 + t_1
        t_1 = self.l_134(t_1)
        t_6 = self.l_135(t_1)
        t_6 = torch.nn.functional.gelu(t_6)
        t_6 = self.l_136(t_6)
        t_6 = self.l_137(t_6)
        t_1 = t_6 + t_1
        t_1 = self.l_138(t_1)
        t_6 = self.l_139(t_1)
        t_3 = self.l_140(t_1)
        t_8 = self.l_141(t_1)
        t_7 = t_6.size()
        t_2 = t_3.size()
        t_0 = t_8.size()
        t_7 = t_7[slice(None, -1, None)]
        t_7 = t_7 + (16, 64)
        t_5 = t_7[0]
        t_9 = t_7[1]
        t_4 = t_7[2]
        t_7 = t_7[3]
        t_7 = t_6.view(t_5, t_9, t_4, t_7)
        t_7 = t_7.permute(0, 2, 1, 3)
        t_2 = t_2[slice(None, -1, None)]
        t_2 = t_2 + (16, 64)
        t_4 = t_2[0]
        t_9 = t_2[1]
        t_5 = t_2[2]
        t_2 = t_2[3]
        t_2 = t_3.view(t_4, t_9, t_5, t_2)
        t_2 = t_2.permute(0, 2, 1, 3)
        t_0 = t_0[slice(None, -1, None)]
        t_0 = t_0 + (16, 64)
        t_5 = t_0[0]
        t_9 = t_0[1]
        t_4 = t_0[2]
        t_0 = t_0[3]
        t_0 = t_8.view(t_5, t_9, t_4, t_0)
        t_0 = t_0.permute(0, 2, 1, 3)
        t_2 = t_2.transpose(-1, -2)
        t_2 = torch.matmul(t_7, t_2)
        t_7 = math.sqrt(64)
        t_7 = t_2 / t_7
        t_7 = t_7 + x0
        t_7 = self.l_142(t_7)
        t_7 = self.l_143(t_7)
        t_0 = torch.matmul(t_7, t_0)
        t_0 = t_0.permute(0, 2, 1, 3)
        t_0 = t_0.contiguous()
        t_7 = t_0.size()
        t_7 = t_7[slice(None, -2, None)]
        t_7 = t_7 + (1024,)
        t_2 = t_7[0]
        t_4 = t_7[1]
        t_7 = t_7[2]
        t_7 = t_0.view(t_2, t_4, t_7)
        t_7 = self.l_144(t_7)
        t_7 = self.l_145(t_7)
        t_1 = t_7 + t_1
        t_1 = self.l_146(t_1)
        t_7 = self.l_147(t_1)
        t_7 = torch.nn.functional.gelu(t_7)
        t_7 = self.l_148(t_7)
        t_7 = self.l_149(t_7)
        t_1 = t_7 + t_1
        t_1 = self.l_150(t_1)
        t_7 = self.l_151(t_1)
        t_4 = self.l_152(t_1)
        t_2 = self.l_153(t_1)
        t_0 = t_7.size()
        t_9 = t_4.size()
        t_5 = t_2.size()
        t_0 = t_0[slice(None, -1, None)]
        t_0 = t_0 + (16, 64)
        t_8 = t_0[0]
        t_3 = t_0[1]
        t_6 = t_0[2]
        t_0 = t_0[3]
        t_0 = t_7.view(t_8, t_3, t_6, t_0)
        t_0 = t_0.permute(0, 2, 1, 3)
        t_9 = t_9[slice(None, -1, None)]
        t_9 = t_9 + (16, 64)
        t_6 = t_9[0]
        t_3 = t_9[1]
        t_8 = t_9[2]
        t_9 = t_9[3]
        t_9 = t_4.view(t_6, t_3, t_8, t_9)
        t_9 = t_9.permute(0, 2, 1, 3)
        t_5 = t_5[slice(None, -1, None)]
        t_5 = t_5 + (16, 64)
        t_8 = t_5[0]
        t_3 = t_5[1]
        t_6 = t_5[2]
        t_5 = t_5[3]
        t_5 = t_2.view(t_8, t_3, t_6, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_9 = t_9.transpose(-1, -2)
        t_9 = torch.matmul(t_0, t_9)
        t_0 = math.sqrt(64)
        t_0 = t_9 / t_0
        t_0 = t_0 + x0
        t_0 = self.l_154(t_0)
        t_0 = self.l_155(t_0)
        t_5 = torch.matmul(t_0, t_5)
        t_5 = t_5.permute(0, 2, 1, 3)
        t_5 = t_5.contiguous()
        t_0 = t_5.size()
        t_0 = t_0[slice(None, -2, None)]
        t_0 = t_0 + (1024,)
        t_9 = t_0[0]
        t_6 = t_0[1]
        t_0 = t_0[2]
        t_0 = t_5.view(t_9, t_6, t_0)
        t_0 = self.l_156(t_0)
        t_0 = self.l_157(t_0)
        t_1 = t_0 + t_1
        t_1 = self.l_158(t_1)
        t_0 = self.l_159(t_1)
        t_0 = torch.nn.functional.gelu(t_0)
        t_0 = self.l_160(t_0)
        t_0 = self.l_161(t_0)
        t_1 = t_0 + t_1
        t_1 = self.l_162(t_1)
        t_0 = self.l_165(t_1)
        t_1 = t_1[(slice(None, None, None), 0)]
        t_1 = self.l_163(t_1)
        t_1 = self.l_164(t_1)
        # Returning:
        # BertForQuestionAnswering/Linear[qa_outputs]
        return (t_0,)

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


def traverse_model(module: nn.Module, depth: int, prefix: Optional[str] = None,
                   basic_blocks: Tuple[Type[nn.Module]] = (), full: bool = False) -> Iterator[
    Tuple[nn.Module, str, nn.Module, Optional[bool]]]:
    """
    iterate over model layers yielding the layer,layer_scope,encasing_module
    Parameters:
    -----------
    model:
        the model to iterate over
    depth:
        how far down in the model tree to go
    basic_blocks:
        a list of modules that if encountered will not be broken down
    full:
        whether to yield only layers specified by the depth and basic_block options or to yield all layers
    """
    if prefix is None:
        prefix = type(module).__name__

    for name, sub_module in module.named_children():
        scope = prefix + "/" + type(sub_module).__name__ + f"[{name}]"
        if len(list(sub_module.children())) == 0 or isinstance(sub_module, tuple(basic_blocks)) or depth == 0:
            if full:
                # TODO:
                # is_explicit_block_limit = len(list(sub_module.children())) != 0 and (isinstance(sub_module, tuple(basic_blocks)) or depth == 0)
                yield sub_module, scope, module, True

            else:
                yield sub_module, scope, module
        else:
            if full:
                yield sub_module, scope, module, False
            yield from traverse_model(sub_module, depth - 1, scope, basic_blocks, full)


def layerDict(model: nn.Module, depth=1000, basic_blocks=()) -> Dict[str, nn.Module]:
    return {s: l for l, s, _ in traverse_model(model, depth, basic_blocks=basic_blocks)}


def traverse_params_buffs(module: nn.Module, prefix: Optional[str] = None) -> Iterator[Tuple[torch.tensor, str]]:
    """
    iterate over model's buffers and parameters yielding obj,obj_scope

    Parameters:
    -----------
    model:
        the model to iterate over
    """
    if prefix is None:
        prefix = type(module).__name__

    # params
    for param_name, param in module.named_parameters(recurse=False):
        param_scope = f"{prefix}/{type(param).__name__}[{param_name}]"
        yield param, param_scope

    # buffs
    for buffer_name, buffer in module.named_buffers(recurse=False):
        buffer_scope = f"{prefix}/{type(buffer).__name__}[{buffer_name}]"
        yield buffer, buffer_scope

    # recurse
    for name, sub_module in module.named_children():
        yield from traverse_params_buffs(sub_module, prefix + "/" + type(sub_module).__name__ + f"[{name}]")


def tensorDict(model: nn.Module) -> OrderedDict[str, Tensor]:
    return collections.OrderedDict((s, t) for t, s in traverse_params_buffs(model))


def move_tensors(ts, device):
    def move(t):
        if isinstance(t, (nn.Module, Tensor)):
            return t.to(device)
        return t

    return nested_map(move, ts)


def nested_map(func, ts, full=False):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        return func(ts)
    elif isinstance(ts, (list, tuple, set)):
        return type(ts)(nested_map(func, t, full=full) for t in ts)
    elif isinstance(ts, dict):
        return {k: nested_map(func, v, full=full) for k, v in ts.items()}
    elif isinstance(ts, slice) and full:
        start = nested_map(func, ts.start, full=full)
        stop = nested_map(func, ts.stop, full=full)
        step = nested_map(func, ts.step, full=full)
        return slice(start, stop, step)
    return func(ts)


def flatten(ts):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        yield ts
    elif isinstance(ts, (list, tuple, set)):
        yield from chain(*[flatten(t) for t in ts])
    elif isinstance(ts, dict):
        yield from chain(*[flatten(t) for k, t in sorted(ts.items(), key=lambda t: t[0])])
    else:
        yield ts


def unflatten(xs, structure):
    return _unflatten(xs, structure)[0]


def _unflatten(xs, structure):
    if isinstance(structure, torch.Size):
        # torch.Size is subclass of tuple which is stupid
        return xs[0], 1

    if not isinstance(structure, (list, tuple, set, dict)):
        return xs[0], 1

    if isinstance(structure, (list, tuple, set)):
        offset = 0
        elements = []
        for s in structure:
            e, n = _unflatten(xs[offset:], s)
            elements.append(e)
            offset += n

        return type(structure)(elements), offset

    assert isinstance(structure, dict)
    offset = 0
    elements = dict()
    for k, v in sorted(structure.items(), key=lambda t: t[0]):
        e, n = _unflatten(xs[offset:], v)
        elements[k] = e
        offset += n

    return elements, offset


def state_dict(partition, *args, **kwargs):
    # we return the state dict of this part as it should be in the original model
    state = nn.Module.state_dict(partition, *args, **kwargs)
    lookup = partition.lookup
    result = dict()
    for k, v in state.items():
        if k in lookup:
            result[lookup[k]] = v
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            result[new_k] = v
    return result


def load_state_dict(partition, state_dict, strict=True):
    reverse_lookup = {v: k for k, v in partition.lookup.items()}
    device = partition.device
    keys = list(partition.state_dict(None).keys())
    new_state = dict()
    for k in keys:
        if k in reverse_lookup:
            new_state[reverse_lookup[k]] = state_dict[k].to(device)
            continue
        idx = k.rfind(".")
        to_replace = k[:idx]
        if to_replace in reverse_lookup:
            key = reverse_lookup[to_replace] + k[idx:]
            new_state[key] = state_dict[k].to(device)
    nn.Module.load_state_dict(partition, new_state, strict=strict)


def named_buffers(partition, prefix='', recurse=True):
    # we return the named buffers of this part as it should be in the original model
    params = nn.Module.named_buffers(partition, prefix=prefix, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield lookup[k], v
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield new_k, v


def named_parameters(partition, prefix='', recurse=True):
    # we return the named parameters of this part as it should be in the original model
    params = nn.Module.named_parameters(partition, prefix=prefix, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield lookup[k], v
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield new_k, v


def cpu(partition):
    partition.device = torch.device('cpu')
    return nn.Module.cpu(partition)


def cuda(partition, device=None):
    if device is None:
        device = torch.cuda.current_device()
    partition.device = torch.device(device)
    return nn.Module.cuda(partition, partition.device)


def to(partition, *args, **kwargs):
    device = None
    if 'device' in kwargs:
        device = kwargs['device']
    elif 'tensor' in kwargs:
        device = kwargs['tensor'].device
    if args:
        if isinstance(args[0], (torch.device, int, str)):
            device = args[0]
        if torch.is_tensor(args[0]):
            device = args[0].device
    if not (device is None):
        partition.device = torch.device(device)
    return nn.Module.to(partition, *args, **kwargs)

model_args = {'model_name_or_path': 'bert-large-uncased-whole-word-masking', 'precompute_attention_mask': False, 'max_seq_length': 384, 'max_query_length': 64, 'do_lower_case': True}

def bert_large_uncased_whole_word_maskings_384_2p_bw12_async_pipedream():
    return dict(model_type='bert_squad',
                model_name_or_path='bert-large-uncased-whole-word-masking',
                do_lower_case=True,
                output_past=False,
                stateless_tied=False,
                explicitly_set_dict={'precompute_attention_mask': False, 'return_dict': False},
                do_resize_token_embedding=False,
                )
    
"""analysis summary
-I- Printing Report
warnings:
Partition0 output:BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/Tensor::permute_1093 is not contiguous!
Number of nodes in Computation Graph: 2409
Number of stages: 2
backward times include recomputation
Analysis for async_pipeline=True: last partition will not do recomputation.

Stage parameter count:
 {0: 160893952, 1: 174249986, 'total': 335143938}

GPU parameter count:
 Number of Model Parameters 335.1M
{'total': 335143938}

real times are based on real measurements of execution time (with communication) of generated partitions ms
forward {0: 100.34, 1: 127.64}
backward {0: 272.84, 1: 229.39}

Analysis for T = (1-R)fwd + R*bwd:

Pipeline Slowdown: (compared to sequential execution with no communication, and same recompute policy)
forward 1.130
backward 1.091

Expected utilization by partition
forward {0: 0.77, 1: 1.0}
backward {0: 1.0, 1: 0.83}

worstcase: bwd: 272.835 fwd: 127.639
Expected speedup for 2 partitions is: 1.812
Assuming bandwidth of 12 GBps between GPUs

communication volumes size of activations of each partition
0: input size:'0.00 MB', recieve_time:'0.00 ms', out:'25.18 MB', send time:'2.10 ms'
1: input size:'25.18 MB', recieve_time:'2.10 ms', out:'0.02 MB', send time:'0.00 ms'

Compuatation Communication ratio (comp/(comp+comm)):
forward {0: 0.98, 1: 1.0} 
backward {0: 1.0, 1: 0.99}

Analysis for T = fwd + bwd:
 {'expected_compute_utilization': {0: 1.0, 1: 0.96, 'worstcase_std': 0.0},
 'pipeline_no_comm': {0: 371.08,
                      1: 354.93,
                      'worstcase': 371.08,
                      'worstcase_std': 1.71},
 'pipeline_vs_seq_no_comm': 1.71,
 'pipeline_with_non_parallel_comm': {0: 373.18, 1: 357.03, 'worstcase': 373.18},
 'seq_no_comm_no_recomp': {0: 275.15, 1: 358.29}}

expected_speedup_compared_to_seq_no_recomp_no_comm: 1.582
Analysis max cuda memory used 6.86GB
"""