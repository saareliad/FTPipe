"""AutoGenerated with:
python -m autopipe.partition vision --crop 32 --no_recomputation -b 256 -p 4 --save_memory_mode --partitioning_method pipedream --model wrn_28x10_c100_dr03_gn
"""
import torch
import torch.nn.functional
import torch.functional
from torch import Tensor
import torch.nn as nn
from itertools import chain
from typing import Optional, Tuple, Iterator, Iterable, OrderedDict, Dict
import collections

from typing import Type
from torch.nn.modules.conv import Conv2d
from torch.nn.modules.linear import Linear
from torch.nn.modules.pooling import AvgPool2d
from torch.nn.modules.dropout import Dropout
from torch.nn.modules.activation import ReLU
from torch.nn.modules.normalization import GroupNorm
# this is an auto generated file do not edit unless you know what you are doing


# partition adjacency
# model inputs {0}
# partition 0 {'inputs': {'input0'}, 'outputs': {1}}
# partition 1 {'inputs': {0}, 'outputs': {2}}
# partition 2 {'inputs': {1}, 'outputs': {3}}
# partition 3 {'inputs': {2}, 'outputs': {'output'}}
# model outputs {3}


def create_pipeline_configuration(DEBUG=False, batch_size=256):
    config = {
        'batch_dim': 0,
        'depth': 10000,
        'basic_blocks': (Conv2d,Linear,AvgPool2d,Dropout,ReLU,GroupNorm),
        'model_inputs': {
            'input0': {
                'shape': torch.Size([256, 3, 32, 32]),
                'dtype': torch.float32,
                'is_batched': True,
                'used_by': [0]}},
        'model_outputs': {
            'WideResNet/Linear[fc]': {
                'shape': torch.Size([256, 100]),
                'dtype': torch.float32,
                'is_batched': True,
                'created_by': 3}},
        'stages': {
            0: {
                'stage_cls': Partition0,
                'inputs': {
                    'input0': {
                        'shape': torch.Size([256, 3, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1}},
                'outputs': {
                    'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/torch::add_18': {
                        'shape': torch.Size([256, 160, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]},
                    'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/Dropout[dropout]': {
                        'shape': torch.Size([256, 160, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]}},
                'devices': ['cpu' if DEBUG else 'cuda:0'],
                'stage_depth': 3},
            1: {
                'stage_cls': Partition1,
                'inputs': {
                    'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/torch::add_18': {
                        'shape': torch.Size([256, 160, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/Dropout[dropout]': {
                        'shape': torch.Size([256, 160, 32, 32]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0}},
                'outputs': {
                    'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Conv2d[convShortcut]': {
                        'shape': torch.Size([256, 320, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [2]},
                    'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Conv2d[conv2]': {
                        'shape': torch.Size([256, 320, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [2]}},
                'devices': ['cpu' if DEBUG else 'cuda:1'],
                'stage_depth': 2},
            2: {
                'stage_cls': Partition2,
                'inputs': {
                    'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Conv2d[convShortcut]': {
                        'shape': torch.Size([256, 320, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1},
                    'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Conv2d[conv2]': {
                        'shape': torch.Size([256, 320, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1}},
                'outputs': {
                    'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/torch::add_59': {
                        'shape': torch.Size([256, 320, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [3]},
                    'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/Conv2d[conv2]': {
                        'shape': torch.Size([256, 320, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [3]}},
                'devices': ['cpu' if DEBUG else 'cuda:2'],
                'stage_depth': 1},
            3: {
                'stage_cls': Partition3,
                'inputs': {
                    'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/torch::add_59': {
                        'shape': torch.Size([256, 320, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 2},
                    'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/Conv2d[conv2]': {
                        'shape': torch.Size([256, 320, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 2}},
                'outputs': {
                    'WideResNet/Linear[fc]': {
                        'shape': torch.Size([256, 100]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [-1]}},
                'devices': ['cpu' if DEBUG else 'cuda:3'],
                'stage_depth': 0}}}
    
    
    # switching batch size
    batch_dim = config['batch_dim']
    for d in chain(config['model_inputs'].values(),config['model_outputs'].values()):
        if d['is_batched']:
            shape = d['shape']
            d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    for s in config['stages'].values():
        for d in chain(s['inputs'].values(),s['outputs'].values()):
            if d['is_batched']:
                shape = d['shape']
                d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    return config

class Partition0(nn.Module):
    LAYER_SCOPES = [
            'WideResNet/Conv2d[conv1]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/GroupNorm[bn1]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/ReLU[relu1]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/Conv2d[conv1]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/GroupNorm[bn2]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/ReLU[relu2]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/Dropout[dropout]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/Conv2d[conv2]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/Conv2d[convShortcut]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/GroupNorm[bn1]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/ReLU[relu1]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/Conv2d[conv1]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/GroupNorm[bn2]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/ReLU[relu2]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/Dropout[dropout]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/Conv2d[conv2]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/GroupNorm[bn1]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/ReLU[relu1]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/Conv2d[conv1]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/GroupNorm[bn2]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/ReLU[relu2]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:0'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1]
        self.lookup = {'l_0': 'conv1',
                        'l_1': 'block1.layer.0.bn1',
                        'l_2': 'block1.layer.0.relu1',
                        'l_3': 'block1.layer.0.conv1',
                        'l_4': 'block1.layer.0.bn2',
                        'l_5': 'block1.layer.0.relu2',
                        'l_6': 'block1.layer.0.dropout',
                        'l_7': 'block1.layer.0.conv2',
                        'l_8': 'block1.layer.0.convShortcut',
                        'l_9': 'block1.layer.1.bn1',
                        'l_10': 'block1.layer.1.relu1',
                        'l_11': 'block1.layer.1.conv1',
                        'l_12': 'block1.layer.1.bn2',
                        'l_13': 'block1.layer.1.relu2',
                        'l_14': 'block1.layer.1.dropout',
                        'l_15': 'block1.layer.1.conv2',
                        'l_16': 'block1.layer.2.bn1',
                        'l_17': 'block1.layer.2.relu1',
                        'l_18': 'block1.layer.2.conv1',
                        'l_19': 'block1.layer.2.bn2',
                        'l_20': 'block1.layer.2.relu2',
                        'l_21': 'block1.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # WideResNet/Conv2d[conv1] <=> self.l_0
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/GroupNorm[bn1] <=> self.l_1
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/ReLU[relu1] <=> self.l_2
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/Conv2d[conv1] <=> self.l_3
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/GroupNorm[bn2] <=> self.l_4
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/ReLU[relu2] <=> self.l_5
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/Dropout[dropout] <=> self.l_6
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/Conv2d[conv2] <=> self.l_7
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[0]/Conv2d[convShortcut] <=> self.l_8
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/GroupNorm[bn1] <=> self.l_9
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/ReLU[relu1] <=> self.l_10
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/Conv2d[conv1] <=> self.l_11
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/GroupNorm[bn2] <=> self.l_12
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/ReLU[relu2] <=> self.l_13
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/Dropout[dropout] <=> self.l_14
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/Conv2d[conv2] <=> self.l_15
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/GroupNorm[bn1] <=> self.l_16
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/ReLU[relu1] <=> self.l_17
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/Conv2d[conv1] <=> self.l_18
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/GroupNorm[bn2] <=> self.l_19
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/ReLU[relu2] <=> self.l_20
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/Dropout[dropout] <=> self.l_21
        # input0 <=> x0
        x0 = unflatten(args,self.input_structure)[0]
        t_0 = self.l_0(x0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_1 = self.l_3(t_0)
        t_0 = self.l_8(t_0)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = self.l_7(t_1)
        t_1 = torch.add(t_0, t_1)
        t_0 = self.l_9(t_1)
        t_0 = self.l_10(t_0)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_0 = self.l_14(t_0)
        t_0 = self.l_15(t_0)
        t_0 = torch.add(t_1, t_0)
        t_1 = self.l_16(t_0)
        t_1 = self.l_17(t_1)
        t_1 = self.l_18(t_1)
        t_1 = self.l_19(t_1)
        t_1 = self.l_20(t_1)
        t_1 = self.l_21(t_1)
        # Returning:
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/torch::add_18
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/Dropout[dropout]
        return list(flatten((t_0, t_1)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition1(nn.Module):
    LAYER_SCOPES = [
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/Conv2d[conv2]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[3]/GroupNorm[bn1]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[3]/ReLU[relu1]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[3]/Conv2d[conv1]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[3]/GroupNorm[bn2]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[3]/ReLU[relu2]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[3]/Dropout[dropout]',
            'WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[3]/Conv2d[conv2]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/GroupNorm[bn1]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/ReLU[relu1]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Conv2d[conv1]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/GroupNorm[bn2]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/ReLU[relu2]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Dropout[dropout]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Conv2d[conv2]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Conv2d[convShortcut]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:1'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'block1.layer.2.conv2',
                        'l_1': 'block1.layer.3.bn1',
                        'l_2': 'block1.layer.3.relu1',
                        'l_3': 'block1.layer.3.conv1',
                        'l_4': 'block1.layer.3.bn2',
                        'l_5': 'block1.layer.3.relu2',
                        'l_6': 'block1.layer.3.dropout',
                        'l_7': 'block1.layer.3.conv2',
                        'l_8': 'block2.layer.0.bn1',
                        'l_9': 'block2.layer.0.relu1',
                        'l_10': 'block2.layer.0.conv1',
                        'l_11': 'block2.layer.0.bn2',
                        'l_12': 'block2.layer.0.relu2',
                        'l_13': 'block2.layer.0.dropout',
                        'l_14': 'block2.layer.0.conv2',
                        'l_15': 'block2.layer.0.convShortcut'}
        self.to(self.device)

    def forward(self, *args):
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/Conv2d[conv2] <=> self.l_0
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[3]/GroupNorm[bn1] <=> self.l_1
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[3]/ReLU[relu1] <=> self.l_2
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[3]/Conv2d[conv1] <=> self.l_3
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[3]/GroupNorm[bn2] <=> self.l_4
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[3]/ReLU[relu2] <=> self.l_5
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[3]/Dropout[dropout] <=> self.l_6
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[3]/Conv2d[conv2] <=> self.l_7
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/GroupNorm[bn1] <=> self.l_8
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/ReLU[relu1] <=> self.l_9
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Conv2d[conv1] <=> self.l_10
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/GroupNorm[bn2] <=> self.l_11
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/ReLU[relu2] <=> self.l_12
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Dropout[dropout] <=> self.l_13
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Conv2d[conv2] <=> self.l_14
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Conv2d[convShortcut] <=> self.l_15
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[1]/torch::add_18 <=> x0
        # WideResNet/NetworkBlock[block1]/Sequential[layer]/BasicBlock[2]/Dropout[dropout] <=> x1
        x0, x1 = unflatten(args,self.input_structure)
        t_0 = self.l_0(x1)
        t_0 = torch.add(x0, t_0)
        t_1 = self.l_1(t_0)
        t_1 = self.l_2(t_1)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = self.l_7(t_1)
        t_1 = torch.add(t_0, t_1)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        t_0 = self.l_10(t_1)
        t_1 = self.l_15(t_1)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_0 = self.l_14(t_0)
        # Returning:
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Conv2d[convShortcut]
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Conv2d[conv2]
        return list(flatten((t_1, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition2(nn.Module):
    LAYER_SCOPES = [
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[1]/GroupNorm[bn1]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[1]/ReLU[relu1]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[1]/Conv2d[conv1]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[1]/GroupNorm[bn2]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[1]/ReLU[relu2]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[1]/Dropout[dropout]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[1]/Conv2d[conv2]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/GroupNorm[bn1]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/ReLU[relu1]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/Conv2d[conv1]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/GroupNorm[bn2]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/ReLU[relu2]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/Dropout[dropout]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/Conv2d[conv2]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/GroupNorm[bn1]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/ReLU[relu1]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/Conv2d[conv1]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/GroupNorm[bn2]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/ReLU[relu2]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/Dropout[dropout]',
            'WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/Conv2d[conv2]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:2'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'block2.layer.1.bn1',
                        'l_1': 'block2.layer.1.relu1',
                        'l_2': 'block2.layer.1.conv1',
                        'l_3': 'block2.layer.1.bn2',
                        'l_4': 'block2.layer.1.relu2',
                        'l_5': 'block2.layer.1.dropout',
                        'l_6': 'block2.layer.1.conv2',
                        'l_7': 'block2.layer.2.bn1',
                        'l_8': 'block2.layer.2.relu1',
                        'l_9': 'block2.layer.2.conv1',
                        'l_10': 'block2.layer.2.bn2',
                        'l_11': 'block2.layer.2.relu2',
                        'l_12': 'block2.layer.2.dropout',
                        'l_13': 'block2.layer.2.conv2',
                        'l_14': 'block2.layer.3.bn1',
                        'l_15': 'block2.layer.3.relu1',
                        'l_16': 'block2.layer.3.conv1',
                        'l_17': 'block2.layer.3.bn2',
                        'l_18': 'block2.layer.3.relu2',
                        'l_19': 'block2.layer.3.dropout',
                        'l_20': 'block2.layer.3.conv2'}
        self.to(self.device)

    def forward(self, *args):
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[1]/GroupNorm[bn1] <=> self.l_0
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[1]/ReLU[relu1] <=> self.l_1
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[1]/Conv2d[conv1] <=> self.l_2
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[1]/GroupNorm[bn2] <=> self.l_3
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[1]/ReLU[relu2] <=> self.l_4
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[1]/Dropout[dropout] <=> self.l_5
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[1]/Conv2d[conv2] <=> self.l_6
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/GroupNorm[bn1] <=> self.l_7
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/ReLU[relu1] <=> self.l_8
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/Conv2d[conv1] <=> self.l_9
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/GroupNorm[bn2] <=> self.l_10
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/ReLU[relu2] <=> self.l_11
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/Dropout[dropout] <=> self.l_12
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/Conv2d[conv2] <=> self.l_13
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/GroupNorm[bn1] <=> self.l_14
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/ReLU[relu1] <=> self.l_15
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/Conv2d[conv1] <=> self.l_16
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/GroupNorm[bn2] <=> self.l_17
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/ReLU[relu2] <=> self.l_18
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/Dropout[dropout] <=> self.l_19
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/Conv2d[conv2] <=> self.l_20
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Conv2d[convShortcut] <=> x0
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[0]/Conv2d[conv2] <=> x1
        x0, x1 = unflatten(args,self.input_structure)
        t_0 = torch.add(x0, x1)
        t_1 = self.l_0(t_0)
        t_1 = self.l_1(t_1)
        t_1 = self.l_2(t_1)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = torch.add(t_0, t_1)
        t_0 = self.l_7(t_1)
        t_0 = self.l_8(t_0)
        t_0 = self.l_9(t_0)
        t_0 = self.l_10(t_0)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_0 = torch.add(t_1, t_0)
        t_1 = self.l_14(t_0)
        t_1 = self.l_15(t_1)
        t_1 = self.l_16(t_1)
        t_1 = self.l_17(t_1)
        t_1 = self.l_18(t_1)
        t_1 = self.l_19(t_1)
        t_1 = self.l_20(t_1)
        # Returning:
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/torch::add_59
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/Conv2d[conv2]
        return list(flatten((t_0, t_1)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition3(nn.Module):
    LAYER_SCOPES = [
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/GroupNorm[bn1]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/ReLU[relu1]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/Conv2d[conv1]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/GroupNorm[bn2]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/ReLU[relu2]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/Dropout[dropout]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/Conv2d[conv2]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/Conv2d[convShortcut]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[1]/GroupNorm[bn1]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[1]/ReLU[relu1]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[1]/Conv2d[conv1]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[1]/GroupNorm[bn2]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[1]/ReLU[relu2]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[1]/Dropout[dropout]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[1]/Conv2d[conv2]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[2]/GroupNorm[bn1]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[2]/ReLU[relu1]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[2]/Conv2d[conv1]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[2]/GroupNorm[bn2]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[2]/ReLU[relu2]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[2]/Dropout[dropout]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[2]/Conv2d[conv2]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[3]/GroupNorm[bn1]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[3]/ReLU[relu1]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[3]/Conv2d[conv1]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[3]/GroupNorm[bn2]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[3]/ReLU[relu2]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[3]/Dropout[dropout]',
            'WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[3]/Conv2d[conv2]',
            'WideResNet/GroupNorm[bn1]',
            'WideResNet/ReLU[relu]',
            'WideResNet/AvgPool2d[avg_pool]',
            'WideResNet/Linear[fc]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:3'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'block3.layer.0.bn1',
                        'l_1': 'block3.layer.0.relu1',
                        'l_2': 'block3.layer.0.conv1',
                        'l_3': 'block3.layer.0.bn2',
                        'l_4': 'block3.layer.0.relu2',
                        'l_5': 'block3.layer.0.dropout',
                        'l_6': 'block3.layer.0.conv2',
                        'l_7': 'block3.layer.0.convShortcut',
                        'l_8': 'block3.layer.1.bn1',
                        'l_9': 'block3.layer.1.relu1',
                        'l_10': 'block3.layer.1.conv1',
                        'l_11': 'block3.layer.1.bn2',
                        'l_12': 'block3.layer.1.relu2',
                        'l_13': 'block3.layer.1.dropout',
                        'l_14': 'block3.layer.1.conv2',
                        'l_15': 'block3.layer.2.bn1',
                        'l_16': 'block3.layer.2.relu1',
                        'l_17': 'block3.layer.2.conv1',
                        'l_18': 'block3.layer.2.bn2',
                        'l_19': 'block3.layer.2.relu2',
                        'l_20': 'block3.layer.2.dropout',
                        'l_21': 'block3.layer.2.conv2',
                        'l_22': 'block3.layer.3.bn1',
                        'l_23': 'block3.layer.3.relu1',
                        'l_24': 'block3.layer.3.conv1',
                        'l_25': 'block3.layer.3.bn2',
                        'l_26': 'block3.layer.3.relu2',
                        'l_27': 'block3.layer.3.dropout',
                        'l_28': 'block3.layer.3.conv2',
                        'l_29': 'bn1',
                        'l_30': 'relu',
                        'l_31': 'avg_pool',
                        'l_32': 'fc'}
        self.to(self.device)

    def forward(self, *args):
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/GroupNorm[bn1] <=> self.l_0
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/ReLU[relu1] <=> self.l_1
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/Conv2d[conv1] <=> self.l_2
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/GroupNorm[bn2] <=> self.l_3
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/ReLU[relu2] <=> self.l_4
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/Dropout[dropout] <=> self.l_5
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/Conv2d[conv2] <=> self.l_6
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[0]/Conv2d[convShortcut] <=> self.l_7
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[1]/GroupNorm[bn1] <=> self.l_8
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[1]/ReLU[relu1] <=> self.l_9
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[1]/Conv2d[conv1] <=> self.l_10
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[1]/GroupNorm[bn2] <=> self.l_11
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[1]/ReLU[relu2] <=> self.l_12
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[1]/Dropout[dropout] <=> self.l_13
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[1]/Conv2d[conv2] <=> self.l_14
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[2]/GroupNorm[bn1] <=> self.l_15
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[2]/ReLU[relu1] <=> self.l_16
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[2]/Conv2d[conv1] <=> self.l_17
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[2]/GroupNorm[bn2] <=> self.l_18
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[2]/ReLU[relu2] <=> self.l_19
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[2]/Dropout[dropout] <=> self.l_20
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[2]/Conv2d[conv2] <=> self.l_21
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[3]/GroupNorm[bn1] <=> self.l_22
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[3]/ReLU[relu1] <=> self.l_23
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[3]/Conv2d[conv1] <=> self.l_24
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[3]/GroupNorm[bn2] <=> self.l_25
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[3]/ReLU[relu2] <=> self.l_26
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[3]/Dropout[dropout] <=> self.l_27
        # WideResNet/NetworkBlock[block3]/Sequential[layer]/BasicBlock[3]/Conv2d[conv2] <=> self.l_28
        # WideResNet/GroupNorm[bn1] <=> self.l_29
        # WideResNet/ReLU[relu] <=> self.l_30
        # WideResNet/AvgPool2d[avg_pool] <=> self.l_31
        # WideResNet/Linear[fc] <=> self.l_32
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[2]/torch::add_59 <=> x0
        # WideResNet/NetworkBlock[block2]/Sequential[layer]/BasicBlock[3]/Conv2d[conv2] <=> x1
        x0, x1 = unflatten(args,self.input_structure)
        t_0 = torch.add(x0, x1)
        t_0 = self.l_0(t_0)
        t_0 = self.l_1(t_0)
        t_1 = self.l_2(t_0)
        t_0 = self.l_7(t_0)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = torch.add(t_0, t_1)
        t_0 = self.l_8(t_1)
        t_0 = self.l_9(t_0)
        t_0 = self.l_10(t_0)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_0 = self.l_14(t_0)
        t_0 = torch.add(t_1, t_0)
        t_1 = self.l_15(t_0)
        t_1 = self.l_16(t_1)
        t_1 = self.l_17(t_1)
        t_1 = self.l_18(t_1)
        t_1 = self.l_19(t_1)
        t_1 = self.l_20(t_1)
        t_1 = self.l_21(t_1)
        t_1 = torch.add(t_0, t_1)
        t_0 = self.l_22(t_1)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = self.l_25(t_0)
        t_0 = self.l_26(t_0)
        t_0 = self.l_27(t_0)
        t_0 = self.l_28(t_0)
        t_0 = torch.add(t_1, t_0)
        t_0 = self.l_29(t_0)
        t_0 = self.l_30(t_0)
        t_0 = self.l_31(t_0)
        t_0 = t_0.view(-1, 640)
        t_0 = self.l_32(t_0)
        # Returning:
        # WideResNet/Linear[fc]
        return (t_0,)

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


def traverse_model(module: nn.Module, depth: int, prefix: Optional[str] = None,
                   basic_blocks: Tuple[Type[nn.Module]] = (), full: bool = False) -> Iterator[
    Tuple[nn.Module, str, nn.Module, Optional[bool]]]:
    """
    iterate over model layers yielding the layer,layer_scope,encasing_module
    Parameters:
    -----------
    model:
        the model to iterate over
    depth:
        how far down in the model tree to go
    basic_blocks:
        a list of modules that if encountered will not be broken down
    full:
        whether to yield only layers specified by the depth and basic_block options or to yield all layers
    """
    if prefix is None:
        prefix = type(module).__name__

    for name, sub_module in module.named_children():
        scope = prefix + "/" + type(sub_module).__name__ + f"[{name}]"
        if len(list(sub_module.children())) == 0 or isinstance(sub_module, tuple(basic_blocks)) or depth == 0:
            if full:
                # TODO:
                # is_explicit_block_limit = len(list(sub_module.children())) != 0 and (isinstance(sub_module, tuple(basic_blocks)) or depth == 0)
                yield sub_module, scope, module, True

            else:
                yield sub_module, scope, module
        else:
            if full:
                yield sub_module, scope, module, False
            yield from traverse_model(sub_module, depth - 1, scope, basic_blocks, full)


def layerDict(model: nn.Module, depth=1000, basic_blocks=()) -> Dict[str, nn.Module]:
    return {s: l for l, s, _ in traverse_model(model, depth, basic_blocks=basic_blocks)}


def traverse_params_buffs(module: nn.Module, prefix: Optional[str] = None) -> Iterator[Tuple[torch.tensor, str]]:
    """
    iterate over model's buffers and parameters yielding obj,obj_scope

    Parameters:
    -----------
    model:
        the model to iterate over
    """
    if prefix is None:
        prefix = type(module).__name__

    # params
    for param_name, param in module.named_parameters(recurse=False):
        param_scope = f"{prefix}/{type(param).__name__}[{param_name}]"
        yield param, param_scope

    # buffs
    for buffer_name, buffer in module.named_buffers(recurse=False):
        buffer_scope = f"{prefix}/{type(buffer).__name__}[{buffer_name}]"
        yield buffer, buffer_scope

    # recurse
    for name, sub_module in module.named_children():
        yield from traverse_params_buffs(sub_module, prefix + "/" + type(sub_module).__name__ + f"[{name}]")


def tensorDict(model: nn.Module) -> OrderedDict[str, Tensor]:
    return collections.OrderedDict((s, t) for t, s in traverse_params_buffs(model))


def move_tensors(ts, device):
    def move(t):
        if isinstance(t, (nn.Module, Tensor)):
            return t.to(device)
        return t

    return nested_map(move, ts)


def nested_map(func, ts, full=False):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        return func(ts)
    elif isinstance(ts, (list, tuple, set)):
        return type(ts)(nested_map(func, t, full=full) for t in ts)
    elif isinstance(ts, dict):
        return {k: nested_map(func, v, full=full) for k, v in ts.items()}
    elif isinstance(ts, slice) and full:
        start = nested_map(func, ts.start, full=full)
        stop = nested_map(func, ts.stop, full=full)
        step = nested_map(func, ts.step, full=full)
        return slice(start, stop, step)
    return func(ts)


def flatten(ts):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        yield ts
    elif isinstance(ts, (list, tuple, set)):
        yield from chain(*[flatten(t) for t in ts])
    elif isinstance(ts, dict):
        yield from chain(*[flatten(t) for k, t in sorted(ts.items(), key=lambda t: t[0])])
    else:
        yield ts


def unflatten(xs, structure):
    return _unflatten(xs, structure)[0]


def _unflatten(xs, structure):
    if isinstance(structure, torch.Size):
        # torch.Size is subclass of tuple which is stupid
        return xs[0], 1

    if not isinstance(structure, (list, tuple, set, dict)):
        return xs[0], 1

    if isinstance(structure, (list, tuple, set)):
        offset = 0
        elements = []
        for s in structure:
            e, n = _unflatten(xs[offset:], s)
            elements.append(e)
            offset += n

        return type(structure)(elements), offset

    assert isinstance(structure, dict)
    offset = 0
    elements = dict()
    for k, v in sorted(structure.items(), key=lambda t: t[0]):
        e, n = _unflatten(xs[offset:], v)
        elements[k] = e
        offset += n

    return elements, offset


def state_dict(partition, *args, **kwargs):
    # we return the state dict of this part as it should be in the original model
    state = nn.Module.state_dict(partition, *args, **kwargs)
    lookup = partition.lookup
    result = dict()
    for k, v in state.items():
        if k in lookup:
            result[lookup[k]] = v
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            result[new_k] = v
    return result


def load_state_dict(partition, state_dict, strict=True):
    reverse_lookup = {v: k for k, v in partition.lookup.items()}
    device = partition.device
    keys = list(partition.state_dict(None).keys())
    new_state = dict()
    for k in keys:
        if k in reverse_lookup:
            new_state[reverse_lookup[k]] = state_dict[k].to(device)
            continue
        idx = k.rfind(".")
        to_replace = k[:idx]
        if to_replace in reverse_lookup:
            key = reverse_lookup[to_replace] + k[idx:]
            new_state[key] = state_dict[k].to(device)
    nn.Module.load_state_dict(partition, new_state, strict=strict)


def named_buffers(partition, prefix='', recurse=True):
    # we return the named buffers of this part as it should be in the original model
    params = nn.Module.named_buffers(partition, prefix=prefix, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield lookup[k], v
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield new_k, v


def named_parameters(partition, prefix='', recurse=True):
    # we return the named parameters of this part as it should be in the original model
    params = nn.Module.named_parameters(partition, prefix=prefix, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield lookup[k], v
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield new_k, v


def cpu(partition):
    partition.device = torch.device('cpu')
    return nn.Module.cpu(partition)


def cuda(partition, device=None):
    if device is None:
        device = torch.cuda.current_device()
    partition.device = torch.device(device)
    return nn.Module.cuda(partition, partition.device)


def to(partition, *args, **kwargs):
    device = None
    if 'device' in kwargs:
        device = kwargs['device']
    elif 'tensor' in kwargs:
        device = kwargs['tensor'].device
    if args:
        if isinstance(args[0], (torch.device, int, str)):
            device = args[0]
        if torch.is_tensor(args[0]):
            device = args[0].device
    if not (device is None):
        partition.device = torch.device(device)
    return nn.Module.to(partition, *args, **kwargs)

model_args = {'model': 'wrn_28x10_c100_dr03_gn'}
"""analysis summary
-I- Printing Report
Number of nodes in Computation Graph: 108
Number of stages: 4
backward times do not include recomputation

Stage parameter count:
 {0: 949264, 1: 2126400, 2: 5533440, 3: 27927780, 'total': 36536884}

GPU parameter count:
 Number of Model Parameters 36.5M
{'total': 36536884}

real times are based on real measurements of execution time (with communication) of generated partitions ms
forward {0: 10.94, 1: 8.26, 2: 5.63, 3: 6.47}
backward {0: 9.52, 1: 13.11, 2: 9.56, 3: 14.06}

Analysis for T = (1-R)fwd + R*bwd:

Pipeline Slowdown: (compared to sequential execution with no communication, and same recompute policy)
forward 1.800
backward 1.432

Expected utilization by partition
forward {0: 0.68, 1: 0.6, 2: 0.36, 3: 0.59}
backward {0: 0.68, 1: 0.68, 2: 0.56, 3: 0.88}

worstcase: bwd: 14.060 fwd: 10.936
Expected speedup for 4 partitions is: 2.511
Assuming bandwidth of 12 GBps between GPUs

communication volumes size of activations of each partition
0: input size:'0.00 MB', recieve_time:'0.00 ms', out:'41.94 MB', send time:'3.50 ms'
1: input size:'41.94 MB', recieve_time:'3.50 ms', out:'20.97 MB', send time:'1.75 ms'
2: input size:'20.97 MB', recieve_time:'1.75 ms', out:'20.97 MB', send time:'1.75 ms'
3: input size:'20.97 MB', recieve_time:'1.75 ms', out:'0.01 MB', send time:'0.00 ms'

Compuatation Communication ratio (comp/(comp+comm)):
forward {0: 0.68, 1: 0.79, 2: 0.69, 3: 1.0} 
backward {0: 1.0, 1: 0.73, 2: 0.82, 3: 0.88}

Analysis for T = fwd + bwd:
 {'expected_compute_utilization': {0: 0.9,
                                  1: 0.86,
                                  2: 0.62,
                                  3: 1.0,
                                  'worstcase_std': 0.01},
 'pipeline_no_comm': {0: 16.97,
                      1: 16.12,
                      2: 11.7,
                      3: 18.78,
                      'worstcase': 18.78,
                      'worstcase_std': 0.15},
 'pipeline_vs_seq_no_comm': 3.39,
 'pipeline_with_non_parallel_comm': {0: 20.46,
                                     1: 21.36,
                                     2: 15.19,
                                     3: 20.53,
                                     'worstcase': 21.36},
 'seq_no_comm_no_recomp': {0: 17.0, 1: 16.14, 2: 11.72, 3: 18.81}}

expected_speedup_compared_to_seq_no_recomp_no_comm: 2.547
Analysis max cuda memory used 0.48GB
"""