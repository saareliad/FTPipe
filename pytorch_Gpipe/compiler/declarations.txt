# taken from torchGpipe https://github.com/kakaobrain/torchgpipe



#  @generated from torch/__init__.pyi.in

from typing import List, Tuple, Optional, Union, Any, ContextManager, Callable, overload
from torch._six import inf

import builtins

import numpy

# These identifiers are reexported from other modules.  These modules
# are not mypy-clean yet, so in order to use this stub file usefully
# from mypy you will need to specify --follow-imports=silent.
# Not all is lost: these imports still enable IDEs like PyCharm to offer
# autocomplete.
#
# Note: Why does the syntax here look so strange?  Import visibility
# rules in stubs are different from normal Python files!  You must use
# 'from ... import ... as ...' syntax to cause an identifier to be
# exposed (or use a wildcard); regular syntax is not exposed.
# from .random import set_rng_state as set_rng_state, get_rng_state as get_rng_state, manual_seed as manual_seed, initial_seed as initial_seed, seed as seed
# from ._Tensor_str import set_printoptions as set_printoptions
# from .functional import *
# from .serialization import save as save, load as load
# from .autograd import no_grad as no_grad, enable_grad as enable_grad, set_grad_enabled as set_grad_enabled
# from . import cuda as cuda
# from . import optim as optim
# from . import nn as nn

#MODIFIED BY TORCHGPIPE
# from . import backends
# from . import version
#END
# class dtype: ...

# class layout: ...

# strided : _layout = ...

# class memory_format: ...

# contiguous_format: memory_format = ...

# class qscheme: ...

# per_Tensor_affine: qscheme = ...

# See https://github.com/python/mypy/issues/4146 for why these workarounds
# is necessary
_int = builtins.int
_float = builtins.float
_bool = builtins.bool
_str = builtins.str
_slice = builtins.slice

# float32: dtype = ...
# float: dtype = ...
# float64: dtype = ...
# double: dtype = ...
# float16: dtype = ...
# half: dtype = ...
# uint8: dtype = ...
# int8: dtype = ...
# int16: dtype = ...
# short: dtype = ...
# int32: dtype = ...
# int: dtype = ...
# int64: dtype = ...
# long: dtype = ...
#  complex32: dtype = ...
#  complex64: dtype = ...
#  complex128: dtype = ...
#  quint8: dtype = ...
#  qint8: dtype = ...
#  qint32: dtype = ...
# bool: dtype = ...

# See https://github.com/python/mypy/issues/4146 for why these workarounds
# is necessary
_Tensor = torch.Tensor
_dtype = torch.dtype
_device = torch.device
_qscheme = torch.qscheme
_Size = torch.Size
_memory_format=torch.memory_format
_size = Union[_Size, List, Tuple]

# Meta-type for "numeric" things; matches our docs
Number = Union[_int, _float, _bool]

_layout = torch.layout
_ndarray = numpy.ndarray
_Storage = torch.Storage
_Generator = torch.Generator
# class device:
#     type: _str
#     index: _int

#MODIFIED BY TORCHGPIPE
#     @overload
#     def __init__(self: device, device: device) -> None: ...
#END

#     @overload
#     def __init__(self: device, device: Union[_int, _str]) -> None: ...

#     @overload
#     def __init__(self: device, type: _str, index: _int) -> None: ...

# class Size(tuple):
#MODIFIED BY TORCHGPIPE
#     def numel(self: Size) -> _int: ...
#END

# class Storage:
#MODIFIED BY TORCHGPIPE
#     def size(self: Storage) -> _int: ...
#     def element_size(self: Storage) -> _int: ...
#END



# class Generator:
#     device: _device = ...

#     @overload
#     def __init__(self: Generator, device: Optional[_device]=None) -> None: ...

#     @overload
#     def __init__(self: Generator, device: Union[_int, _str]) -> None: ...

# TODO: One downside of doing it this way, is direct use of
# torch.tensor.Tensor doesn't get type annotations.  Nobody
# should really do that, so maybe this is not so bad.
# class Tensor:
    ##TENSOR
    # dtype: _dtype = ...
    # shape: Size = ...
    # device: _device = ...
    # requires_grad: _bool = ...
    # grad: Optional[_Tensor] = ...
    # is_cuda: _bool
    # is_leaf: _bool

    #the T indicates start of Tensor functions
    def T__abs__(self: _Tensor) -> Tensor: ...
    def __add__(self: _Tensor, other: Any) -> Tensor: ...
    @overload
    def __and__(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def __and__(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def __and__(self: _Tensor, other: Any) -> Tensor: ...
    def __bool__(self: _Tensor) -> builtins.bool: ...
    def __div__(self: _Tensor, other: Any) -> Tensor: ...
    def __eq__(self: _Tensor, other: Any) -> Tensor: ...  # type: ignore
    def __float__(self: _Tensor) -> builtins.float: ...
    def __floordiv__(self: _Tensor, other: Any) -> Tensor: ...
    def __ge__(self: _Tensor, other: Any) -> Tensor: ...  # type: ignore
    def __getitem__(self: _Tensor, indices: Optional[Union[_int, _slice, _Tensor, List, Tuple]]) -> Tensor: ...
    def __gt__(self: _Tensor, other: Any) -> Tensor: ...  # type: ignore
    def __iadd__(self: _Tensor, other: Any) -> Tensor: ...
    @overload
    def __iand__(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def __iand__(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def __iand__(self: _Tensor, other: Any) -> Tensor: ...
    def __idiv__(self: _Tensor, other: Any) -> Tensor: ...
    @overload
    def __ilshift__(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def __ilshift__(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def __ilshift__(self: _Tensor, other: Any) -> Tensor: ...
    def __imul__(self: _Tensor, other: Any) -> Tensor: ...
    def __index__(self: _Tensor) -> builtins.int: ...
    def __int__(self: _Tensor) -> builtins.int: ...
    def __invert__(self: _Tensor) -> Tensor: ...
    @overload
    def __ior__(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def __ior__(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def __ior__(self: _Tensor, other: Any) -> Tensor: ...
    @overload
    def __irshift__(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def __irshift__(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def __irshift__(self: _Tensor, other: Any) -> Tensor: ...
    def __isub__(self: _Tensor, other: Any) -> Tensor: ...
    def __itruediv__(self: _Tensor, other: Any) -> Tensor: ...
    @overload
    def __ixor__(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def __ixor__(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def __ixor__(self: _Tensor, other: Any) -> Tensor: ...
    def __le__(self: _Tensor, other: Any) -> Tensor: ...  # type: ignore
    def __long__(self: _Tensor) -> builtins.int: ...
    @overload
    def __lshift__(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def __lshift__(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def __lshift__(self: _Tensor, other: Any) -> Tensor: ...
    def __lt__(self: _Tensor, other: Any) -> Tensor: ...  # type: ignore
    def __matmul__(self: _Tensor, other: Any) -> Tensor: ...
    def __mod__(self: _Tensor, other: Any) -> Tensor: ...
    def __mul__(self: _Tensor, other: Any) -> Tensor: ...
    def __ne__(self: _Tensor, other: Any) -> Tensor: ...  # type: ignore
    def __neg__(self: _Tensor) -> Tensor: ...
    def __nonzero__(self: _Tensor) -> builtins.bool: ...
    @overload
    def __or__(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def __or__(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def __or__(self: _Tensor, other: Any) -> Tensor: ...
    def __pow__(self: _Tensor, other: Any) -> Tensor: ...
    def __radd__(self: _Tensor, other: Any) -> Tensor: ...
    def __rfloordiv__(self: _Tensor, other: Any) -> Tensor: ...
    def __rmul__(self: _Tensor, other: Any) -> Tensor: ...
    @overload
    def __rshift__(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def __rshift__(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def __rshift__(self: _Tensor, other: Any) -> Tensor: ...
    def __setitem__(self: _Tensor, indices: Optional[Union[_int, _slice, _Tensor, List, Tuple]], val: Union[_Tensor, Number]) -> None: ...
    def __sub__(self: _Tensor, other: Any) -> Tensor: ...
    def __truediv__(self: _Tensor, other: Any) -> Tensor: ...
    @overload
    def __xor__(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def __xor__(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def __xor__(self: _Tensor, other: Any) -> Tensor: ...
    def _coalesced_(self: _Tensor, coalesced: _bool) -> Tensor: ...
    def _dimI(self: _Tensor) -> _int: ...
    def _dimV(self: _Tensor) -> _int: ...
    def _indices(self: _Tensor) -> Tensor: ...
    def _nnz(self: _Tensor) -> _int: ...
    def _values(self: _Tensor) -> Tensor: ...
    def abs(self: _Tensor) -> Tensor: ...
    def abs_(self: _Tensor) -> Tensor: ...
    def acos(self: _Tensor) -> Tensor: ...
    def acos_(self: _Tensor) -> Tensor: ...
    def addbmm(self: _Tensor, batch1: _Tensor, batch2: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
    def addbmm_(self: _Tensor, batch1: _Tensor, batch2: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
    def addcdiv(self: _Tensor, tensor1: _Tensor, tensor2: _Tensor, *, value: Number=1) -> Tensor: ...
    def addcdiv_(self: _Tensor, tensor1: _Tensor, tensor2: _Tensor, *, value: Number=1) -> Tensor: ...
    def addcmul(self: _Tensor, tensor1: _Tensor, tensor2: _Tensor, *, value: Number=1) -> Tensor: ...
    def addcmul_(self: _Tensor, tensor1: _Tensor, tensor2: _Tensor, *, value: Number=1) -> Tensor: ...
    def addmm(self: _Tensor, mat1: _Tensor, mat2: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
    def addmm_(self: _Tensor, mat1: _Tensor, mat2: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
    def addmv(self: _Tensor, mat: _Tensor, vec: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
    def addmv_(self: _Tensor, mat: _Tensor, vec: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
    def addr(self: _Tensor, vec1: _Tensor, vec2: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
    def addr_(self: _Tensor, vec1: _Tensor, vec2: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
    @overload
    def all(self: _Tensor, dim: _int, keepdim: _bool=False) -> Tensor: ...
    @overload
    def all(self: _Tensor) -> Tensor: ...
    def allclose(self: _Tensor, other: _Tensor, rtol: _float=1e-05, atol: _float=1e-08, equal_nan: _bool=False) -> _bool: ...
    @overload
    def any(self: _Tensor, dim: _int, keepdim: _bool=False) -> Tensor: ...
    @overload
    def any(self: _Tensor) -> Tensor: ...
    def apply_(self: _Tensor, callable: Callable) -> Tensor: ...
    def argmax(self: _Tensor, dim: Optional[_int]=None, keepdim: _bool=False) -> Tensor: ...
    def argmin(self: _Tensor, dim: Optional[_int]=None, keepdim: _bool=False) -> Tensor: ...
    def argsort(self: _Tensor, dim: _int=-1, descending: _bool=False) -> Tensor: ...
    def as_strided(self: _Tensor, size: _size, stride: _size, storage_offset: Optional[_int]=None) -> Tensor: ...
    def as_strided_(self: _Tensor, size: _size, stride: _size, storage_offset: Optional[_int]=None) -> Tensor: ...
    def asin(self: _Tensor) -> Tensor: ...
    def asin_(self: _Tensor) -> Tensor: ...
    def atan(self: _Tensor) -> Tensor: ...
    def atan2(self: _Tensor, other: _Tensor) -> Tensor: ...
    def atan2_(self: _Tensor, other: _Tensor) -> Tensor: ...
    def atan_(self: _Tensor) -> Tensor: ...
    def backward(self: _Tensor, gradient: Optional[_Tensor]=None, keep_graph: _bool=False, create_graph: _bool=False) -> None: ...
    def baddbmm(self: _Tensor, batch1: _Tensor, batch2: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
    def baddbmm_(self: _Tensor, batch1: _Tensor, batch2: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
    @overload
    def bernoulli(self: _Tensor, *, generator: Optional[_Generator]=None) -> Tensor: ...
    @overload
    def bernoulli(self: _Tensor, p: _float, *, generator: Optional[_Generator]=None) -> Tensor: ...
    @overload
    def bernoulli_(self: _Tensor, p: _Tensor, *, generator: Optional[_Generator]=None) -> Tensor: ...
    @overload
    def bernoulli_(self: _Tensor, p: _float=0.5, *, generator: Optional[_Generator]=None) -> Tensor: ...
    def bincount(self: _Tensor, weights: Optional[_Tensor]=None, minlength: _int=0) -> Tensor: ...
    def bitwise_not(self: _Tensor) -> Tensor: ...
    def bitwise_not_(self: _Tensor) -> Tensor: ...
    def bmm(self: _Tensor, mat2: _Tensor) -> Tensor: ...
    def bool(self: _Tensor) -> Tensor: ...
    def byte(self: _Tensor) -> Tensor: ...
    def cauchy_(self: _Tensor, median: _float=0, sigma: _float=1, *, generator: Optional[_Generator]=None) -> Tensor: ...
    def ceil(self: _Tensor) -> Tensor: ...
    def ceil_(self: _Tensor) -> Tensor: ...
    def char(self: _Tensor) -> Tensor: ...
    def cholesky(self: _Tensor, upper: _bool=False) -> Tensor: ...
    def cholesky_inverse(self: _Tensor, upper: _bool=False) -> Tensor: ...
    def cholesky_solve(self: _Tensor, input2: _Tensor, upper: _bool=False) -> Tensor: ...
    def chunk(self: _Tensor, chunks: _int, dim: _int=0) -> Union[Tuple, List]: ...
    def clamp(self: _Tensor, min: _float=-inf, max: _float=inf, *, out: Optional[_Tensor]=None) -> Tensor: ...
    def clamp_(self: _Tensor, min: _float=-inf, max: _float=inf) -> Tensor: ...
    def clamp_max(self: _Tensor, max: Number) -> Tensor: ...
    def clamp_max_(self: _Tensor, max: Number) -> Tensor: ...
    def clamp_min(self: _Tensor, min: Number) -> Tensor: ...
    def clamp_min_(self: _Tensor, min: Number) -> Tensor: ...
    def clone(self: _Tensor) -> Tensor: ...
    def coalesce(self: _Tensor) -> Tensor: ...
    def contiguous(self: _Tensor) -> Tensor: ...
    def cos(self: _Tensor) -> Tensor: ...
    def cos_(self: _Tensor) -> Tensor: ...
    def cosh(self: _Tensor) -> Tensor: ...
    def cosh_(self: _Tensor) -> Tensor: ...
    def cpu(self: _Tensor) -> Tensor: ...
    def cross(self: _Tensor, other: _Tensor, dim: Optional[_int]=None) -> Tensor: ...
    def cuda(self: _Tensor, device: Optional[_device]=None, non_blocking: _bool=False) -> Tensor: ...
    def cumprod(self: _Tensor, dim: _int, *, dtype: Optional[_dtype]=None) -> Tensor: ...
    def cumsum(self: _Tensor, dim: _int, *, dtype: Optional[_dtype]=None) -> Tensor: ...
    def dense_dim(self: _Tensor) -> _int: ...
    def dequantize(self: _Tensor) -> Tensor: ...
    def det(self: _Tensor) -> Tensor: ...
    def detach(self: _Tensor) -> Tensor: ...
    def detach_(self: _Tensor) -> Tensor: ...
    def diag(self: _Tensor, diagonal: _int=0) -> Tensor: ...
    def diag_embed(self: _Tensor, offset: _int=0, dim1: _int=-2, dim2: _int=-1) -> Tensor: ...
    def diagflat(self: _Tensor, offset: _int=0) -> Tensor: ...
    def diagonal(self: _Tensor, offset: _int=0, dim1: _int=0, dim2: _int=1) -> Tensor: ...
    def digamma(self: _Tensor) -> Tensor: ...
    def digamma_(self: _Tensor) -> Tensor: ...
    def dim(self: _Tensor) -> _int: ...
    def dist(self: _Tensor, other: _Tensor, p: Number=2) -> Tensor: ...
    def dot(self: _Tensor, tensor: _Tensor) -> Tensor: ...
    def double(self: _Tensor) -> Tensor: ...
    def eig(self: _Tensor, eigenvectors: _bool=False) -> Tuple[_Tensor, Tensor]: ...
    def element_size(self: _Tensor) -> _int: ...
    @overload
    def eq(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def eq(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def eq_(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def eq_(self: _Tensor, other: _Tensor) -> Tensor: ...
    def equal(self: _Tensor, other: _Tensor) -> _bool: ...
    def erf(self: _Tensor) -> Tensor: ...
    def erf_(self: _Tensor) -> Tensor: ...
    def erfc(self: _Tensor) -> Tensor: ...
    def erfc_(self: _Tensor) -> Tensor: ...
    def erfinv(self: _Tensor) -> Tensor: ...
    def erfinv_(self: _Tensor) -> Tensor: ...
    def exp(self: _Tensor) -> Tensor: ...
    def exp_(self: _Tensor) -> Tensor: ...
    @overload
    def expand(self: _Tensor, size: _size, *, implicit: _bool=False) -> Tensor: ...
    @overload
    def expand(self: _Tensor, *size: _int, implicit: _bool=False) -> Tensor: ...
    def expand_as(self: _Tensor, other: _Tensor) -> Tensor: ...
    def expm1(self: _Tensor) -> Tensor: ...
    def expm1_(self: _Tensor) -> Tensor: ...
    def exponential_(self: _Tensor, lambd: _float=1, *, generator: Optional[_Generator]=None) -> Tensor: ...
    def fft(self: _Tensor, signal_ndim: _int, normalized: _bool=False) -> Tensor: ...
    @overload
    def fill_(self: _Tensor, value: Number) -> Tensor: ...
    @overload
    def fill_(self: _Tensor, value: _Tensor) -> Tensor: ...
    def fill_diagonal_(self: _Tensor, fill_value: Number, wrap: _bool=False) -> Tensor: ...
    def flatten(self: _Tensor, start_dim: _int=0, end_dim: _int=-1) -> Tensor: ...
    @overload
    def flip(self: _Tensor, dims: _size) -> Tensor: ...
    @overload
    def flip(self: _Tensor, *dims: _int) -> Tensor: ...
    def float(self: _Tensor) -> Tensor: ...
    def floor(self: _Tensor) -> Tensor: ...
    def floor_(self: _Tensor) -> Tensor: ...
    @overload
    def fmod(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def fmod(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def fmod_(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def fmod_(self: _Tensor, other: _Tensor) -> Tensor: ...
    def frac(self: _Tensor) -> Tensor: ...
    def frac_(self: _Tensor) -> Tensor: ...
    def gather(self: _Tensor, dim: _int, index: _Tensor, *, sparse_grad: _bool=False) -> Tensor: ...
    @overload
    def ge(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def ge(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def ge_(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def ge_(self: _Tensor, other: _Tensor) -> Tensor: ...
    def geometric_(self: _Tensor, p: _float, *, generator: Optional[_Generator]=None) -> Tensor: ...
    def geqrf(self: _Tensor) -> Tuple[_Tensor, Tensor]: ...
    def ger(self: _Tensor, vec2: _Tensor) -> Tensor: ...
    def get_device(self: _Tensor) -> _int: ...
    @overload
    def gt(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def gt(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def gt_(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def gt_(self: _Tensor, other: _Tensor) -> Tensor: ...
    def half(self: _Tensor) -> Tensor: ...
    def hardshrink(self: _Tensor, lambd: Number=0.5) -> Tensor: ...
    def histc(self: _Tensor, bins: _int=100, min: Number=0, max: Number=0) -> Tensor: ...
    def ifft(self: _Tensor, signal_ndim: _int, normalized: _bool=False) -> Tensor: ...
    def index_add(self: _Tensor, dim: _int, index: _Tensor, source: _Tensor) -> Tensor: ...
    def index_add_(self: _Tensor, dim: _int, index: _Tensor, source: _Tensor) -> Tensor: ...
    def index_copy(self: _Tensor, dim: _int, index: _Tensor, source: _Tensor) -> Tensor: ...
    def index_copy_(self: _Tensor, dim: _int, index: _Tensor, source: _Tensor) -> Tensor: ...
    @overload
    def index_fill(self: _Tensor, dim: _int, index: _Tensor, value: Number) -> Tensor: ...
    @overload
    def index_fill(self: _Tensor, dim: _int, index: _Tensor, value: _Tensor) -> Tensor: ...
    @overload
    def index_fill_(self: _Tensor, dim: _int, index: _Tensor, value: Number) -> Tensor: ...
    @overload
    def index_fill_(self: _Tensor, dim: _int, index: _Tensor, value: _Tensor) -> Tensor: ...
    def index_put(self: _Tensor, indices: Optional[Union[Tuple, List]], values: _Tensor, accumulate: _bool=False) -> Tensor: ...
    def index_put_(self: _Tensor, indices: Optional[Union[Tuple, List]], values: _Tensor, accumulate: _bool=False) -> Tensor: ...
    def index_select(self: _Tensor, dim: _int, index: _Tensor) -> Tensor: ...
    def indices(self: _Tensor) -> Tensor: ...
    def int(self: _Tensor) -> Tensor: ...
    def int_repr(self: _Tensor) -> Tensor: ...
    def inverse(self: _Tensor) -> Tensor: ...
    def irfft(self: _Tensor, signal_ndim: _int, normalized: _bool=False, onesided: _bool=True, signal_sizes: _size=()) -> Tensor: ...
    def is_coalesced(self: _Tensor) -> _bool: ...
    def is_complex(self: _Tensor) -> _bool: ...
    def is_contiguous(self: _Tensor) -> _bool: ...
    def is_distributed(self: _Tensor) -> _bool: ...
    def is_floating_point(self: _Tensor) -> _bool: ...
    def is_nonzero(self: _Tensor) -> _bool: ...
    def is_same_size(self: _Tensor, other: _Tensor) -> _bool: ...
    def is_set_to(self: _Tensor, tensor: _Tensor) -> _bool: ...
    def is_signed(self: _Tensor) -> _bool: ...
    def isclose(self: _Tensor, other: _Tensor, rtol: _float=1e-05, atol: _float=1e-08, equal_nan: _bool=False) -> Tensor: ...
    def item(self: _Tensor) -> Number: ...
    def kthvalue(self: _Tensor, k: _int, dim: _int=-1, keepdim: _bool=False) -> Tuple[_Tensor, Tensor]: ...
    @overload
    def le(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def le(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def le_(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def le_(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def lerp(self: _Tensor, end: _Tensor, weight: Number) -> Tensor: ...
    @overload
    def lerp(self: _Tensor, end: _Tensor, weight: _Tensor) -> Tensor: ...
    @overload
    def lerp_(self: _Tensor, end: _Tensor, weight: Number) -> Tensor: ...
    @overload
    def lerp_(self: _Tensor, end: _Tensor, weight: _Tensor) -> Tensor: ...
    def lgamma(self: _Tensor) -> Tensor: ...
    def lgamma_(self: _Tensor) -> Tensor: ...
    def log(self: _Tensor) -> Tensor: ...
    def log10(self: _Tensor) -> Tensor: ...
    def log10_(self: _Tensor) -> Tensor: ...
    def log1p(self: _Tensor) -> Tensor: ...
    def log1p_(self: _Tensor) -> Tensor: ...
    def log2(self: _Tensor) -> Tensor: ...
    def log2_(self: _Tensor) -> Tensor: ...
    def log_(self: _Tensor) -> Tensor: ...
    def log_normal_(self: _Tensor, mean: _float=1, std: _float=2, *, generator: Optional[_Generator]=None) -> Tensor: ...
    def log_softmax(self: _Tensor, dim: _int, dtype: Optional[_dtype]=None) -> Tensor: ...
    def logdet(self: _Tensor) -> Tensor: ...
    def logsumexp(self: _Tensor, dim: Union[_int, _size], keepdim: _bool=False) -> Tensor: ...
    def long(self: _Tensor) -> Tensor: ...
    def lstsq(self: _Tensor, A: _Tensor) -> Tuple[_Tensor, Tensor]: ...
    @overload
    def lt(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def lt(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def lt_(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def lt_(self: _Tensor, other: _Tensor) -> Tensor: ...
    def lu_solve(self: _Tensor, LU_data: _Tensor, LU_pivots: _Tensor) -> Tensor: ...
    def map_(tensor: _Tensor, callable: Callable) -> Tensor: ...
    @overload
    def masked_fill(self: _Tensor, mask: _Tensor, value: Number) -> Tensor: ...
    @overload
    def masked_fill(self: _Tensor, mask: _Tensor, value: _Tensor) -> Tensor: ...
    @overload
    def masked_fill_(self: _Tensor, mask: _Tensor, value: Number) -> Tensor: ...
    @overload
    def masked_fill_(self: _Tensor, mask: _Tensor, value: _Tensor) -> Tensor: ...
    def masked_scatter(self: _Tensor, mask: _Tensor, source: _Tensor) -> Tensor: ...
    def masked_scatter_(self: _Tensor, mask: _Tensor, source: _Tensor) -> Tensor: ...
    def masked_select(self: _Tensor, mask: _Tensor) -> Tensor: ...
    def matmul(self: _Tensor, other: _Tensor) -> Tensor: ...
    def matrix_power(self: _Tensor, n: _int) -> Tensor: ...
    @overload
    def max(self: _Tensor, dim: _int, keepdim: _bool=False) -> Tuple[_Tensor, Tensor]: ...
    @overload
    def max(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def max(self: _Tensor) -> Tensor: ...
    @overload
    def mean(self: _Tensor, *, dtype: Optional[_dtype]=None) -> Tensor: ...
    @overload
    def mean(self: _Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, dtype: Optional[_dtype]=None) -> Tensor: ...
    @overload
    def median(self: _Tensor, dim: _int, keepdim: _bool=False) -> Tuple[_Tensor, Tensor]: ...
    @overload
    def median(self: _Tensor) -> Tensor: ...
    @overload
    def min(self: _Tensor, dim: _int, keepdim: _bool=False) -> Tuple[_Tensor, Tensor]: ...
    @overload
    def min(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def min(self: _Tensor) -> Tensor: ...
    def mm(self: _Tensor, mat2: _Tensor) -> Tensor: ...
    def mode(self: _Tensor, dim: _int=-1, keepdim: _bool=False) -> Tuple[_Tensor, Tensor]: ...
    def multinomial(self: _Tensor, num_samples: _int, replacement: _bool=False, *, generator: Optional[_Generator]=None) -> Tensor: ...
    def mv(self: _Tensor, vec: _Tensor) -> Tensor: ...
    def mvlgamma(self: _Tensor, p: _int) -> Tensor: ...
    def mvlgamma_(self: _Tensor, p: _int) -> Tensor: ...
    def narrow(self: _Tensor, dim: _int, start: _int, length: _int) -> Tensor: ...
    def narrow_copy(self: _Tensor, dim: _int, start: _int, length: _int) -> Tensor: ...
    def ndimension(self: _Tensor) -> _int: ...
    @overload
    def ne(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def ne(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def ne_(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def ne_(self: _Tensor, other: _Tensor) -> Tensor: ...
    def neg(self: _Tensor) -> Tensor: ...
    def neg_(self: _Tensor) -> Tensor: ...
    def nelement(self: _Tensor) -> _int: ...
    def new_empty(self: _Tensor, size: _size, dtype: Optional[_dtype]=None, device: Optional[Union[_device, _str]]=None, requires_grad: _bool=False) -> Tensor: ...
    def new_full(self: _Tensor, size: _size, value: Number, dtype: Optional[_dtype]=None, device: Optional[Union[_device, _str]]=None, requires_grad: _bool=False) -> Tensor: ...
    def new_ones(self: _Tensor, size: _size, dtype: Optional[_dtype]=None, device: Optional[Union[_device, _str]]=None, requires_grad: _bool=False) -> Tensor: ...
    def new_tensor(self: _Tensor, data: Any, dtype: Optional[_dtype]=None, device: Optional[Union[_device, _str]]=None, requires_grad: _bool=False) -> Tensor: ...
    def new_zeros(self: _Tensor, size: _size, dtype: Optional[_dtype]=None, device: Optional[Union[_device, _str]]=None, requires_grad: _bool=False) -> Tensor: ...
    def normal_(self: _Tensor, mean: _float=0, std: _float=1, *, generator: Optional[_Generator]=None) -> Tensor: ...
    def numel(self: _Tensor) -> _int: ...
    def numpy(self: _Tensor) -> Any: ...
    def orgqr(self: _Tensor, input2: _Tensor) -> Tensor: ...
    def ormqr(self: _Tensor, input2: _Tensor, input3: _Tensor, left: _bool=True, transpose: _bool=False) -> Tensor: ...
    @overload
    def permute(self: _Tensor, dims: _size) -> Tensor: ...
    @overload
    def permute(self: _Tensor, *dims: _int) -> Tensor: ...
    def pin_memory(self: _Tensor) -> Tensor: ...
    def pinverse(self: _Tensor, rcond: _float=1e-15) -> Tensor: ...
    def polygamma(self: _Tensor, n: _int) -> Tensor: ...
    def polygamma_(self: _Tensor, n: _int) -> Tensor: ...
    @overload
    def pow(self: _Tensor, exponent: Number) -> Tensor: ...
    @overload
    def pow(self: _Tensor, exponent: _Tensor) -> Tensor: ...
    @overload
    def pow_(self: _Tensor, exponent: Number) -> Tensor: ...
    @overload
    def pow_(self: _Tensor, exponent: _Tensor) -> Tensor: ...
    def prelu(self: _Tensor, weight: _Tensor) -> Tensor: ...
    @overload
    def prod(self: _Tensor, *, dtype: Optional[_dtype]=None) -> Tensor: ...
    @overload
    def prod(self: _Tensor, dim: _int, keepdim: _bool=False, *, dtype: Optional[_dtype]=None) -> Tensor: ...
    def put_(self: _Tensor, index: _Tensor, source: _Tensor, accumulate: _bool=False) -> Tensor: ...
    def q_scale(self: _Tensor) -> _float: ...
    def q_zero_point(self: _Tensor) -> _int: ...
    def qr(self: _Tensor, some: _bool=True) -> Tuple[_Tensor, Tensor]: ...
    def qscheme(self: _Tensor) -> _qscheme: ...
    @overload
    def random_(self: _Tensor, from_: _int, to: _int, *, generator: Optional[_Generator]=None) -> Tensor: ...
    @overload
    def random_(self: _Tensor, to: _int, *, generator: Optional[_Generator]=None) -> Tensor: ...
    @overload
    def random_(self: _Tensor, *, generator: Optional[_Generator]=None) -> Tensor: ...
    def reciprocal(self: _Tensor) -> Tensor: ...
    def reciprocal_(self: _Tensor) -> Tensor: ...
    def relu(self: _Tensor) -> Tensor: ...
    def relu_(self: _Tensor) -> Tensor: ...
    @overload
    def remainder(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def remainder(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def remainder_(self: _Tensor, other: Number) -> Tensor: ...
    @overload
    def remainder_(self: _Tensor, other: _Tensor) -> Tensor: ...
    def renorm(self: _Tensor, p: Number, dim: _int, maxnorm: Number) -> Tensor: ...
    def renorm_(self: _Tensor, p: Number, dim: _int, maxnorm: Number) -> Tensor: ...
    @overload
    def repeat(self: _Tensor, repeats: _size) -> Tensor: ...
    @overload
    def repeat(self: _Tensor, *repeats: _int) -> Tensor: ...
    @overload
    def repeat_interleave(self: _Tensor, repeats: _Tensor, dim: Optional[_int]=None) -> Tensor: ...
    @overload
    def repeat_interleave(self: _Tensor, repeats: _int, dim: Optional[_int]=None) -> Tensor: ...
    def requires_grad_(self: _Tensor, mode: _bool=True) -> Tensor: ...
    @overload
    def reshape(self: _Tensor, shape: _size) -> Tensor: ...
    @overload
    def reshape(self: _Tensor, *shape: _int) -> Tensor: ...
    def reshape_as(self: _Tensor, other: _Tensor) -> Tensor: ...
    @overload
    def resize_(self: _Tensor, size: _size) -> Tensor: ...
    @overload
    def resize_(self: _Tensor, *size: _int) -> Tensor: ...
    def resize_as_(self: _Tensor, the_template: _Tensor) -> Tensor: ...
    def rfft(self: _Tensor, signal_ndim: _int, normalized: _bool=False, onesided: _bool=True) -> Tensor: ...
    def roll(self: _Tensor, shifts: Union[_int, _size], dims: Union[_int, _size]=()) -> Tensor: ...
    def rot90(self: _Tensor, k: _int=1, dims: _size=(0,1)) -> Tensor: ...
    def round(self: _Tensor) -> Tensor: ...
    def round_(self: _Tensor) -> Tensor: ...
    def rsqrt(self: _Tensor) -> Tensor: ...
    def rsqrt_(self: _Tensor) -> Tensor: ...
    @overload
    def scatter(self: _Tensor, dim: _int, index: _Tensor, src: _Tensor) -> Tensor: ...
    @overload
    def scatter(self: _Tensor, dim: _int, index: _Tensor, value: Number) -> Tensor: ...
    @overload
    def scatter_(self: _Tensor, dim: _int, index: _Tensor, src: _Tensor) -> Tensor: ...
    @overload
    def scatter_(self: _Tensor, dim: _int, index: _Tensor, value: Number) -> Tensor: ...
    def scatter_add(self: _Tensor, dim: _int, index: _Tensor, src: _Tensor) -> Tensor: ...
    def scatter_add_(self: _Tensor, dim: _int, index: _Tensor, src: _Tensor) -> Tensor: ...
    def select(self: _Tensor, dim: _int, index: _int) -> Tensor: ...
    @overload
    def set_(self: _Tensor, source: _Storage) -> Tensor: ...
    @overload
    def set_(self: _Tensor, source: _Storage, storage_offset: _int, size: _size, stride: _size=()) -> Tensor: ...
    @overload
    def set_(self: _Tensor, source: _Tensor) -> Tensor: ...
    @overload
    def set_(self: _Tensor) -> Tensor: ...
    def short(self: _Tensor) -> Tensor: ...
    def sigmoid(self: _Tensor) -> Tensor: ...
    def sigmoid_(self: _Tensor) -> Tensor: ...
    def sign(self: _Tensor) -> Tensor: ...
    def sign_(self: _Tensor) -> Tensor: ...
    def sin(self: _Tensor) -> Tensor: ...
    def sin_(self: _Tensor) -> Tensor: ...
    def sinh(self: _Tensor) -> Tensor: ...
    def sinh_(self: _Tensor) -> Tensor: ...
    # @overload
    def size(self: _Tensor) -> Size: ...
    # @overload
    def size(self: _Tensor, dim:_int) -> _int: ...
    def slogdet(self: _Tensor) -> Tuple[_Tensor, Tensor]: ...
    def smm(self: _Tensor, mat2: _Tensor) -> Tensor: ...
    def softmax(self: _Tensor, dim: _int, dtype: Optional[_dtype]=None) -> Tensor: ...
    def solve(self: _Tensor, A: _Tensor) -> Tuple[_Tensor, Tensor]: ...
    def sort(self: _Tensor, dim: _int=-1, descending: _bool=False) -> Tuple[_Tensor, Tensor]: ...
    def sparse_dim(self: _Tensor) -> _int: ...
    def sparse_mask(self: _Tensor, mask: _Tensor) -> Tensor: ...
    def sparse_resize_(self: _Tensor, size: _size, sparse_dim: _int, dense_dim: _int) -> Tensor: ...
    def sparse_resize_and_clear_(self: _Tensor, size: _size, sparse_dim: _int, dense_dim: _int) -> Tensor: ...
    def split_with_sizes(self: _Tensor, split_sizes: _size, dim: _int=0) -> Union[Tuple[_Tensor, ...], List]: ...
    def sqrt(self: _Tensor) -> Tensor: ...
    def sqrt_(self: _Tensor) -> Tensor: ...
    @overload
    def squeeze(self: _Tensor) -> Tensor: ...
    @overload
    def squeeze(self: _Tensor, dim: _int) -> Tensor: ...
    @overload
    def squeeze_(self: _Tensor) -> Tensor: ...
    @overload
    def squeeze_(self: _Tensor, dim: _int) -> Tensor: ...
    def sspaddmm(self: _Tensor, mat1: _Tensor, mat2: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
    @overload
    def std(self: _Tensor, unbiased: _bool=True) -> Tensor: ...
    @overload
    def std(self: _Tensor, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False) -> Tensor: ...
    def storage(self: _Tensor) -> Storage: ...
    def storage_offset(self: _Tensor) -> _int: ...
    @overload
    def stride(self: _Tensor) -> Tuple[_int]: ...
    @overload
    def stride(self: _Tensor,dim: _int) -> _int: ...
    @overload
    def sum(self: _Tensor, *, dtype: Optional[_dtype]=None) -> Tensor: ...
    @overload
    def sum(self: _Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, dtype: Optional[_dtype]=None) -> Tensor: ...
    @overload
    def sum_to_size(self: _Tensor, size: _size) -> Tensor: ...
    @overload
    def sum_to_size(self: _Tensor, *size: _int) -> Tensor: ...
    def svd(self: _Tensor, some: _bool=True, compute_uv: _bool=True) -> Tuple[_Tensor, Tensor, Tensor]: ...
    def symeig(self: _Tensor, eigenvectors: _bool=False, upper: _bool=True) -> Tuple[_Tensor, Tensor]: ...
    def t(self: _Tensor) -> Tensor: ...
    def t_(self: _Tensor) -> Tensor: ...
    def take(self: _Tensor, index: _Tensor) -> Tensor: ...
    def tan(self: _Tensor) -> Tensor: ...
    def tan_(self: _Tensor) -> Tensor: ...
    def tanh(self: _Tensor) -> Tensor: ...
    def tanh_(self: _Tensor) -> Tensor: ...
    @overload
    def to(self: _Tensor, dtype: _dtype, non_blocking: _bool=False, copy: _bool=False) -> Tensor: ...
    @overload
    def to(self: _Tensor, device: Optional[Union[_device, _str]]=None, dtype: Optional[_dtype]=None, non_blocking: _bool=False, copy: _bool=False) -> Tensor: ...
    @overload
    def to(self: _Tensor, other: _Tensor, non_blocking: _bool=False, copy: _bool=False) -> Tensor: ...
    def to_dense(self: _Tensor) -> Tensor: ...
    def to_mkldnn(self: _Tensor) -> Tensor: ...
    @overload
    def to_sparse(self: _Tensor, sparse_dim: _int) -> Tensor: ...
    @overload
    def to_sparse(self: _Tensor) -> Tensor: ...
    def tolist(self: _Tensor) -> List: ...
    def topk(self: _Tensor, k: _int, dim: _int=-1, largest: _bool=True, sorted: _bool=True) -> Tuple[_Tensor, Tensor]: ...
    def trace(self: _Tensor) -> Tensor: ...
    def transpose(self: _Tensor, dim0: _int, dim1: _int) -> Tensor: ...
    def transpose_(self: _Tensor, dim0: _int, dim1: _int) -> Tensor: ...
    def triangular_solve(self: _Tensor, A: _Tensor, upper: _bool=True, transpose: _bool=False, unitriangular: _bool=False) -> Tuple[_Tensor, Tensor]: ...
    def tril(self: _Tensor, diagonal: _int=0) -> Tensor: ...
    def tril_(self: _Tensor, diagonal: _int=0) -> Tensor: ...
    def triu(self: _Tensor, diagonal: _int=0) -> Tensor: ...
    def triu_(self: _Tensor, diagonal: _int=0) -> Tensor: ...
    def trunc(self: _Tensor) -> Tensor: ...
    def trunc_(self: _Tensor) -> Tensor: ...
    def type(self: _Tensor, dtype: Optional[Union[_str, _dtype]]=None, non_blocking: _bool=False) -> Union[_str, Tensor]: ...
    def type_as(self: _Tensor, other: _Tensor) -> Tensor: ...
    def unbind(self: _Tensor, dim: _int=0) -> Union[Tuple[_Tensor, ...], List]: ...
    def unfold(self: _Tensor, dimension: _int, size: _int, step: _int) -> Tensor: ...
    def uniform_(self: _Tensor, from_: _float=0, to: _float=1, *, generator: Optional[_Generator]=None) -> Tensor: ...
    def unsqueeze(self: _Tensor, dim: _int) -> Tensor: ...
    def unsqueeze_(self: _Tensor, dim: _int) -> Tensor: ...
    def values(self: _Tensor) -> Tensor: ...
    @overload
    def var(self: _Tensor, unbiased: _bool=True) -> Tensor: ...
    @overload
    def var(self: _Tensor, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False) -> Tensor: ...
    @overload
    def view(self: _Tensor, size: _size) -> Tensor: ...
    @overload
    def view(self: _Tensor, *size: _int) -> Tensor: ...
    def view_as(self: _Tensor, other: _Tensor) -> Tensor: ...
    def where(self: _Tensor, condition: _Tensor, other: _Tensor) -> Tensor: ...
    def zero_(self: _Tensor) -> Tensor: ...
    # @overload
    # def zeros_like_(self: _Tensor, other: Union[_Tensor, Number]) -> Tensor: ...
    # @overload
    # def zeros_like_(self: _Tensor, value: Number, other: Union[_Tensor, Number]) -> Tensor: ...
    # @overload
    # def zeros_like_(self: _Tensor, other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...
    # @overload
    # def zeros_like_(self: _Tensor, value: Number, other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...
    # @overload
    # def zeros_like__(self: _Tensor, other: Union[_Tensor, Number]) -> Tensor: ...
    # @overload
    # def zeros_like__(self: _Tensor, value: Number, other: Union[_Tensor, Number]) -> Tensor: ...
    # @overload
    # def zeros_like__(self: _Tensor, other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...
    # @overload
    # def zeros_like__(self: _Tensor, value: Number, other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...
    # @overload
    # def zeros_like___(self: _Tensor, other: Union[_Tensor, Number]) -> Tensor: ...
    # @overload
    # def zeros_like___(self: _Tensor, value: Number, other: Union[_Tensor, Number]) -> Tensor: ...
    # @overload
    # def zeros_like___(self: _Tensor, other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...
    # @overload
    # def zeros_like___(self: _Tensor, value: Number, other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...
    # @overload
    # def zeros_like____(self: _Tensor, other: Union[_Tensor, Number]) -> Tensor: ...
    # @overload
    # def zeros_like____(self: _Tensor, value: Number, other: Union[_Tensor, Number]) -> Tensor: ...
    # @overload
    # def zeros_like____(self: _Tensor, other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...
    # @overload
    # def zeros_like____(self: _Tensor, value: Number, other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...

    # Manually defined methods from torch/tensor.py
    def register_hook(self: _Tensor, hook: Callable) -> Any: ...
    def retain_grad(self: _Tensor) -> None: ...
    def is_pinned(self: _Tensor) -> _bool: ...
    def is_shared(self: _Tensor) -> _bool: ...
    def share_memory_(self: _Tensor) -> None: ...
    # TODO: fill in the types for these, or otherwise figure out some
    # way to not have to write these out again...
    def nonzero(self: _Tensor, *, as_tuple:_bool=True): ...
    def norm(self: _Tensor, p: _str="fro", dim: Optional[_int]=None, keepdim: _bool=False): ...
    def stft(self: _Tensor, n_fft: _int, hop_length: Optional[_int] = None, win_length: Optional[_int]=None, window: Optional[_Tensor]=None, center: _bool=True, pad_mode: _str='reflect', normalized: _bool=False, onesided: _bool=True): ...
    def split(self: _Tensor, split_size: Union[_int,List], dim :_int=0): ...
    def unique(self: _Tensor, sorted: _bool=True, return_inverse: _bool=False, dim: Optional[_int]=None): ...
    def unique_consecutive(self: _Tensor, sorted: _bool=True, return_inverse: _bool=False, return_counts: _bool=False, dim: Optional[_int]=None): ...
    def lu(self: _Tensor, pivot: _bool=True, get_infos: _bool=False): ...


# #MODIFIED BY TORCHGPIPE
#     from .cuda import Stream
#     _Stream=Stream
#     def record_stream(self: _Tensor, stream: Optional[_Stream]) -> None: ...
# #END



# @overload
# def __and__(self: _Tensor, other: Number) -> Tensor: ...
# @overload
# def __and__(self: _Tensor, other: _Tensor) -> Tensor: ...
# @overload
# def __lshift__(self: _Tensor, other: Number) -> Tensor: ...
# @overload
# def __lshift__(self: _Tensor, other: _Tensor) -> Tensor: ...
# @overload
# def __or__(self: _Tensor, other: Number) -> Tensor: ...
# @overload
# def __or__(self: _Tensor, other: _Tensor) -> Tensor: ...
# @overload
# def __rshift__(self: _Tensor, other: Number) -> Tensor: ...
# @overload
# def __rshift__(self: _Tensor, other: _Tensor) -> Tensor: ...
# @overload
# def __xor__(self: _Tensor, other: Number) -> Tensor: ...
# @overload
# def __xor__(self: _Tensor, other: _Tensor) -> Tensor: ...
# the T is a marker that we now parse torch
def T_adaptive_avg_pool2d(self: _Tensor, output_size: Union[_int, _size]) -> Tensor: ...
def _addmm(self: _Tensor, mat1: _Tensor, mat2: _Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[_Tensor]=None) -> Tensor: ...
def _addmm_(self: _Tensor, mat1: _Tensor, mat2: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
def _addr(self: _Tensor, vec1: _Tensor, vec2: _Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[_Tensor]=None) -> Tensor: ...
def _addr_(self: _Tensor, vec1: _Tensor, vec2: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
def _baddbmm_mkl_(self: _Tensor, batch1: _Tensor, batch2: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
def _batch_norm_impl_index(input: _Tensor, weight: Optional[_Tensor], bias: Optional[_Tensor], running_mean: Optional[_Tensor], running_var: Optional[_Tensor], training: _bool, momentum: _float, eps: _float, cudnn_enabled: _bool) -> Tuple[_Tensor, Tensor, Tensor, _int]: ...
def _cast_Byte(self: _Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cast_Char(self: _Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cast_Double(self: _Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cast_Float(self: _Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cast_Half(self: _Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cast_Int(self: _Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cast_Long(self: _Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cast_Short(self: _Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cat(tensors: Union[Tuple, List], dim: _int=0, *, out: Optional[_Tensor]=None) -> Tensor: ...
def _convolution(input: _Tensor, weight: _Tensor, bias: Optional[_Tensor], stride: _size, padding: _size, dilation: _size, transposed: _bool, output_padding: _size, groups: _int, benchmark: _bool, deterministic: _bool, cudnn_enabled: _bool) -> Tensor: ...
def _convolution_nogroup(input: _Tensor, weight: _Tensor, bias: Optional[_Tensor], stride: _size, padding: _size, dilation: _size, transposed: _bool, output_padding: _size) -> Tensor: ...
def _copy_from(self: _Tensor, dst: _Tensor, non_blocking: _bool=False) -> Tensor: ...
def _ctc_loss(log_probs: _Tensor, targets: _Tensor, input_lengths: _size, target_lengths: _size, blank: _int=0, zero_infinity: _bool=False) -> Tuple[_Tensor, Tensor]: ...
def _cudnn_ctc_loss(log_probs: _Tensor, targets: _Tensor, input_lengths: _size, target_lengths: _size, blank: _int, deterministic: _bool, zero_infinity: _bool) -> Tuple[_Tensor, Tensor]: ...
def _cudnn_init_dropout_state(dropout: _float, train: _bool, dropout_seed: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def _cudnn_rnn(input: _Tensor, weight: Union[Tuple, List], weight_stride0: _int, weight_buf: Optional[_Tensor], hx: _Tensor, cx: Optional[_Tensor], mode: _int, hidden_size: _int, num_layers: _int, batch_first: _bool, dropout: _float, train: _bool, bidirectional: _bool, batch_sizes: _size, dropout_state: Optional[_Tensor]) -> Tuple[_Tensor, Tensor, Tensor, Tensor, Tensor]: ...
def _cudnn_rnn_flatten_weight(weight_arr: Union[Tuple, List], weight_stride0: _int, input_size: _int, mode: _int, hidden_size: _int, num_layers: _int, batch_first: _bool, bidirectional: _bool) -> Tensor: ...
def _cufft_clear_plan_cache(device_index: _int) -> None: ...
def _cufft_get_plan_cache_max_size(device_index: _int) -> _int: ...
def _cufft_get_plan_cache_size(device_index: _int) -> _int: ...
def _cufft_set_plan_cache_max_size(device_index: _int, max_size: _int) -> None: ...
def _debug_has_internal_overlap(self: _Tensor) -> _int: ...
def _dequantize_linear(self: _Tensor, scale: _float, zero_point: _int, dtype: _dtype) -> Tensor: ...
def _dim_arange(like: _Tensor, dim: _int) -> Tensor: ...
def _dirichlet_grad(x: _Tensor, alpha: _Tensor, total: _Tensor) -> Tensor: ...
def _embedding_bag(weight: _Tensor, indices: _Tensor, offsets: _Tensor, scale_grad_by_freq: _bool=False, mode: _int=0, sparse: _bool=False, per_sample_weights: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor, Tensor, Tensor]: ...
@overload
def _empty_affine_quantized(size: _size, *, scale: _float=1, zero_point: _int=0, memory_format: Optional[_memory_format]=contiguous_format, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def _empty_affine_quantized(*size: _int, scale: _float=1, zero_point: _int=0, memory_format: Optional[_memory_format]=contiguous_format, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def _fft_with_size(self: _Tensor, signal_ndim: _int, complex_input: _bool, complex_output: _bool, inverse: _bool, checked_signal_sizes: _size, normalized: _bool, onesided: _bool, output_sizes: _size) -> Tensor: ...
def _fused_dropout(self: _Tensor, p: _float, generator: Optional[_Generator]=None) -> Tuple[_Tensor, Tensor]: ...
def _has_compatible_shallow_copy_type(self: _Tensor, from_: _Tensor) -> _bool: ...
def _index_copy_(self: _Tensor, dim: _int, index: _Tensor, source: _Tensor) -> Tensor: ...
def _index_put_impl_(self: _Tensor, indices: Optional[Union[Tuple, List]], values: _Tensor, accumulate: _bool=False, unsafe: _bool=False) -> Tensor: ...
def _log_softmax(self: _Tensor, dim: _int, half_to_float: _bool) -> Tensor: ...
def _log_softmax_backward_data(grad_output: _Tensor, output: _Tensor, dim: _int, self: _Tensor) -> Tensor: ...
def _lu_solve_helper(self: _Tensor, LU_data: _Tensor, LU_pivots: _Tensor) -> Tensor: ...
def _lu_with_info(self: _Tensor, pivot: _bool=True, check_errors: _bool=True) -> Tuple[_Tensor, Tensor, Tensor]: ...
def _masked_scale(self: _Tensor, mask: _Tensor, scale: _float) -> Tensor: ...
def _max(self: _Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
def _min(self: _Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
def _mkldnn_reshape(self: _Tensor, shape: _size) -> Tensor: ...
def _mkldnn_transpose(self: _Tensor, dim0: _int, dim1: _int) -> Tensor: ...
def _mkldnn_transpose_(self: _Tensor, dim0: _int, dim1: _int) -> Tensor: ...
def _mode(self: _Tensor, dim: _int=-1, keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
def _multinomial_alias_draw(J: _Tensor, q: _Tensor, num_samples: _int, *, generator: Optional[_Generator]=None) -> Tensor: ...
def _multinomial_alias_setup(probs: _Tensor) -> Tuple[_Tensor, Tensor]: ...
# def _nnpack_available() -> _bool: ...
def _nnpack_spatial_convolution(input: _Tensor, weight: _Tensor, bias: Optional[_Tensor], padding: Union[_int, _size]) -> Tensor: ...
def _pack_padded_sequence(input: _Tensor, lengths: _Tensor, batch_first: _bool) -> Tuple[_Tensor, Tensor]: ...
def _pad_packed_sequence(data: _Tensor, batch_sizes: _Tensor, batch_first: _bool, padding_value: Number, total_length: _int) -> Tuple[_Tensor, Tensor]: ...
def _per_tensor_affine_qtensor(self: _Tensor, scale: _float, zero_point: _int) -> Tensor: ...
def _reshape_from_tensor(self: _Tensor, shape: _Tensor) -> Tensor: ...
def _s_where(condition: _Tensor, self: _Tensor, other: _Tensor) -> Tensor: ...
def _sample_dirichlet(self: _Tensor, generator: Optional[_Generator]=None) -> Tensor: ...
def _shape_as_tensor(self: _Tensor) -> Tensor: ...
def _sobol_engine_draw(quasi: _Tensor, n: _int, sobolstate: _Tensor, dimension: _int, num_generated: _int, dtype: Optional[_dtype]) -> Tuple[_Tensor, Tensor]: ...
def _sobol_engine_ff_(self: _Tensor, n: _int, sobolstate: _Tensor, dimension: _int, num_generated: _int) -> Tensor: ...
def _sobol_engine_initialize_state_(self: _Tensor, dimension: _int) -> Tensor: ...
def _sobol_engine_scramble_(self: _Tensor, ltm: _Tensor, dimension: _int) -> Tensor: ...
def _softmax(self: _Tensor, dim: _int, half_to_float: _bool) -> Tensor: ...
def _softmax_backward_data(grad_output: _Tensor, output: _Tensor, dim: _int, self: _Tensor) -> Tensor: ...
def _sparse_addmm(self: _Tensor, sparse: _Tensor, dense: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
def _sparse_mm(sparse: _Tensor, dense: _Tensor) -> Tensor: ...
@overload
def _sparse_sum(self: _Tensor) -> Tensor: ...
@overload
def _sparse_sum(self: _Tensor, *, dtype: _dtype) -> Tensor: ...
@overload
def _sparse_sum(self: _Tensor, dim: Union[_int, _size]) -> Tensor: ...
@overload
def _sparse_sum(self: _Tensor, dim: Union[_int, _size], *, dtype: _dtype) -> Tensor: ...
def _standard_gamma(self: _Tensor, generator: Optional[_Generator]=None) -> Tensor: ...
def _standard_gamma_grad(self: _Tensor, output: _Tensor) -> Tensor: ...
def _std(self: _Tensor, unbiased: _bool=True) -> Tensor: ...
def _trilinear(i1: _Tensor, i2: _Tensor, i3: _Tensor, expand1: _size, expand2: _size, expand3: _size, sumdim: _size, unroll_dim: _int=1) -> Tensor: ...
def _unique(self: _Tensor, sorted: _bool=True, return_inverse: _bool=False) -> Tuple[_Tensor, Tensor]: ...
def _unique2(self: _Tensor, sorted: _bool=True, return_inverse: _bool=False, return_counts: _bool=False) -> Tuple[_Tensor, Tensor, Tensor]: ...
def _var(self: _Tensor, unbiased: _bool=True) -> Tensor: ...
def _weight_norm(v: _Tensor, g: _Tensor, dim: _int=0) -> Tensor: ...
def _weight_norm_cuda_interface(v: _Tensor, g: _Tensor, dim: _int=0) -> Tuple[_Tensor, Tensor]: ...
def abs(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def abs_(self: _Tensor) -> Tensor: ...
def acos(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def acos_(self: _Tensor) -> Tensor: ...
def adaptive_avg_pool1d(self: _Tensor, output_size: Union[_int, _size]) -> Tensor: ...
def adaptive_max_pool1d(self: _Tensor, output_size: Union[_int, _size]) -> Tuple[_Tensor, Tensor]: ...
@overload
def add(input: Union[_Tensor, Number], other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def add(input: Union[_Tensor, Number], value: Number, other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def add(self: _Tensor, alpha: Number, other: _Tensor) -> Tensor: ...
@overload
def add(self: _Tensor, alpha: Number, other: _Tensor, *, out: _Tensor) -> Tensor: ...
@overload
def addbmm(self: _Tensor, batch1: _Tensor, batch2: _Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def addbmm(beta: Number, self: _Tensor, alpha: Number, batch1: _Tensor, batch2: _Tensor) -> Tensor: ...
@overload
def addbmm(beta: Number, self: _Tensor, alpha: Number, batch1: _Tensor, batch2: _Tensor, *, out: _Tensor) -> Tensor: ...
@overload
def addbmm(beta: Number, self: _Tensor, batch1: _Tensor, batch2: _Tensor) -> Tensor: ...
@overload
def addbmm(beta: Number, self: _Tensor, batch1: _Tensor, batch2: _Tensor, *, out: _Tensor) -> Tensor: ...
@overload
def addcdiv(self: _Tensor, tensor1: _Tensor, tensor2: _Tensor, *, value: Number=1, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def addcdiv(self: _Tensor, value: Number, tensor1: _Tensor, tensor2: _Tensor) -> Tensor: ...
@overload
def addcdiv(self: _Tensor, value: Number, tensor1: _Tensor, tensor2: _Tensor, *, out: _Tensor) -> Tensor: ...
@overload
def addcmul(self: _Tensor, tensor1: _Tensor, tensor2: _Tensor, *, value: Number=1, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def addcmul(self: _Tensor, value: Number, tensor1: _Tensor, tensor2: _Tensor) -> Tensor: ...
@overload
def addcmul(self: _Tensor, value: Number, tensor1: _Tensor, tensor2: _Tensor, *, out: _Tensor) -> Tensor: ...
@overload
def addmm(self: _Tensor, mat1: _Tensor, mat2: _Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def addmm(beta: Number, self: _Tensor, alpha: Number, mat1: _Tensor, mat2: _Tensor) -> Tensor: ...
@overload
def addmm(beta: Number, self: _Tensor, alpha: Number, mat1: _Tensor, mat2: _Tensor, *, out: _Tensor) -> Tensor: ...
@overload
def addmm(beta: Number, self: _Tensor, mat1: _Tensor, mat2: _Tensor) -> Tensor: ...
@overload
def addmm(beta: Number, self: _Tensor, mat1: _Tensor, mat2: _Tensor, *, out: _Tensor) -> Tensor: ...
@overload
def addmv(self: _Tensor, mat: _Tensor, vec: _Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def addmv(beta: Number, self: _Tensor, alpha: Number, mat: _Tensor, vec: _Tensor) -> Tensor: ...
@overload
def addmv(beta: Number, self: _Tensor, alpha: Number, mat: _Tensor, vec: _Tensor, *, out: _Tensor) -> Tensor: ...
@overload
def addmv(beta: Number, self: _Tensor, mat: _Tensor, vec: _Tensor) -> Tensor: ...
@overload
def addmv(beta: Number, self: _Tensor, mat: _Tensor, vec: _Tensor, *, out: _Tensor) -> Tensor: ...
def addmv_(self: _Tensor, mat: _Tensor, vec: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
@overload
def addr(self: _Tensor, vec1: _Tensor, vec2: _Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def addr(beta: Number, self: _Tensor, alpha: Number, vec1: _Tensor, vec2: _Tensor) -> Tensor: ...
@overload
def addr(beta: Number, self: _Tensor, alpha: Number, vec1: _Tensor, vec2: _Tensor, *, out: _Tensor) -> Tensor: ...
@overload
def addr(beta: Number, self: _Tensor, vec1: _Tensor, vec2: _Tensor) -> Tensor: ...
@overload
def addr(beta: Number, self: _Tensor, vec1: _Tensor, vec2: _Tensor, *, out: _Tensor) -> Tensor: ...
def affine_grid_generator(theta: _Tensor, size: _size) -> Tensor: ...
@overload
def all(self: _Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def all(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def allclose(self: _Tensor, other: _Tensor, rtol: _float=1e-05, atol: _float=1e-08, equal_nan: _bool=False) -> _bool: ...
def alpha_dropout(input: _Tensor, p: _float, train: _bool) -> Tensor: ...
def alpha_dropout_(self: _Tensor, p: _float, train: _bool) -> Tensor: ...
@overload
def any(self: _Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def any(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def arange(start: Number, end: Number, step: Number, *, out: Optional[_Tensor]=None, dtype: Optional[_dtype]=None, device: Optional[Union[_device, _str]]=None, requires_grad: _bool=False) -> Tensor: ...
@overload
def arange(start: Number, end: Number, *, out: Optional[_Tensor]=None, dtype: Optional[_dtype]=None, device: Optional[Union[_device, _str]]=None, requires_grad: _bool=False) -> Tensor: ...
@overload
def arange(end: Number, *, out: Optional[_Tensor]=None, dtype: Optional[_dtype]=None, device: Optional[Union[_device, _str]]=None, requires_grad: _bool=False) -> Tensor: ...
def argmax(self: _Tensor, dim: Optional[_int]=None, keepdim: _bool=False) -> Tensor: ...
def argmin(self: _Tensor, dim: Optional[_int]=None, keepdim: _bool=False) -> Tensor: ...
def argsort(self: _Tensor, dim: _int=-1, descending: _bool=False) -> Tensor: ...
def as_strided(self: _Tensor, size: _size, stride: _size, storage_offset: Optional[_int]=None) -> Tensor: ...
def as_strided_(self: _Tensor, size: _size, stride: _size, storage_offset: Optional[_int]=None) -> Tensor: ...
def as_tensor(data: Any, dtype: _dtype=None, device: Optional[_device]=None) -> Tensor: ...
def asin(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def asin_(self: _Tensor) -> Tensor: ...
def atan(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def atan2(self: _Tensor, other: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def atan_(self: _Tensor) -> Tensor: ...
def avg_pool1d(self: _Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, ceil_mode: _bool=False, count_include_pad: _bool=True) -> Tensor: ...
@overload
def baddbmm(self: _Tensor, batch1: _Tensor, batch2: _Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def baddbmm(beta: Number, self: _Tensor, alpha: Number, batch1: _Tensor, batch2: _Tensor) -> Tensor: ...
@overload
def baddbmm(beta: Number, self: _Tensor, alpha: Number, batch1: _Tensor, batch2: _Tensor, *, out: _Tensor) -> Tensor: ...
@overload
def baddbmm(beta: Number, self: _Tensor, batch1: _Tensor, batch2: _Tensor) -> Tensor: ...
@overload
def baddbmm(beta: Number, self: _Tensor, batch1: _Tensor, batch2: _Tensor, *, out: _Tensor) -> Tensor: ...
@overload
def bartlett_window(window_length: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def bartlett_window(window_length: _int, periodic: _bool, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def batch_norm(input: _Tensor, weight: Optional[_Tensor], bias: Optional[_Tensor], running_mean: Optional[_Tensor], running_var: Optional[_Tensor], training: _bool, momentum: _float, eps: _float, cudnn_enabled: _bool) -> Tensor: ...
def batch_norm_backward_elemt(grad_out: _Tensor, input: _Tensor, mean: _Tensor, invstd: _Tensor, weight: Optional[_Tensor], mean_dy: _Tensor, mean_dy_xmu: _Tensor) -> Tensor: ...
def batch_norm_backward_reduce(grad_out: _Tensor, input: _Tensor, mean: _Tensor, invstd: _Tensor, input_g: _bool, weight_g: _bool, bias_g: _bool) -> Tuple[_Tensor, Tensor, Tensor, Tensor]: ...
def batch_norm_elemt(input: _Tensor, weight: Optional[_Tensor], bias: Optional[_Tensor], mean: _Tensor, invstd: _Tensor, eps: _float) -> Tensor: ...
def batch_norm_gather_stats(input: _Tensor, mean: _Tensor, invstd: _Tensor, running_mean: Optional[_Tensor], running_var: Optional[_Tensor], momentum: _float, eps: _float, count: _int) -> Tuple[_Tensor, Tensor]: ...
def batch_norm_gather_stats_with_counts(input: _Tensor, mean: _Tensor, invstd: _Tensor, running_mean: Optional[_Tensor], running_var: Optional[_Tensor], momentum: _float, eps: _float, counts: _size) -> Tuple[_Tensor, Tensor]: ...
def batch_norm_stats(input: _Tensor, eps: _float) -> Tuple[_Tensor, Tensor]: ...
def batch_norm_update_stats(input: _Tensor, running_mean: Optional[_Tensor], running_var: Optional[_Tensor], momentum: _float) -> Tuple[_Tensor, Tensor]: ...
@overload
def bernoulli(self: _Tensor, *, generator: Optional[_Generator]=None, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def bernoulli(self: _Tensor, p: _float, *, generator: Optional[_Generator]=None, out: Optional[_Tensor]=None) -> Tensor: ...
def bilinear(input1: _Tensor, input2: _Tensor, weight: _Tensor, bias: Optional[_Tensor]) -> Tensor: ...
def bincount(self: _Tensor, weights: Optional[_Tensor]=None, minlength: _int=0) -> Tensor: ...
def bitwise_not(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def blackman_window(window_length: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def blackman_window(window_length: _int, periodic: _bool, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def bmm(self: _Tensor, mat2: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def cat(tensors: Union[Tuple, List], dim: _int=0, *, out: Optional[_Tensor]=None) -> Tensor: ...
def cdist(x1: _Tensor, x2: _Tensor, p: _float=2) -> Tensor: ...
def ceil(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def ceil_(self: _Tensor) -> Tensor: ...
def celu(self: _Tensor, alpha: Number=1.0) -> Tensor: ...
def celu_(self: _Tensor, alpha: Number=1.0) -> Tensor: ...
def cholesky(self: _Tensor, upper: _bool=False, *, out: Optional[_Tensor]=None) -> Tensor: ...
def cholesky_inverse(self: _Tensor, upper: _bool=False, *, out: Optional[_Tensor]=None) -> Tensor: ...
def cholesky_solve(self: _Tensor, input2: _Tensor, upper: _bool=False, *, out: Optional[_Tensor]=None) -> Tensor: ...
def chunk(self: _Tensor, chunks: _int, dim: _int=0) -> Union[Tuple, List]: ...
def clamp(self: _Tensor, min: _float=-inf, max: _float=inf, *, out: Optional[_Tensor]=None) -> Tensor: ...
def clamp_max(self: _Tensor, max: Number, *, out: Optional[_Tensor]=None) -> Tensor: ...
def clamp_max_(self: _Tensor, max: Number) -> Tensor: ...
def clamp_min(self: _Tensor, min: Number, *, out: Optional[_Tensor]=None) -> Tensor: ...
def clamp_min_(self: _Tensor, min: Number) -> Tensor: ...
def clone(self: _Tensor) -> Tensor: ...
def combinations(self: _Tensor, r: _int=2, with_replacement: _bool=False) -> Tensor: ...
def constant_pad_nd(self: _Tensor, pad: _size, value: Number=0) -> Tensor: ...
def conv1d(input: _Tensor, weight: _Tensor, bias: Optional[_Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, groups: _int=1) -> Tensor: ...
def conv2d(input: _Tensor, weight: _Tensor, bias: Optional[_Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, groups: _int=1) -> Tensor: ...
def conv3d(input: _Tensor, weight: _Tensor, bias: Optional[_Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, groups: _int=1) -> Tensor: ...
def conv_tbc(self: _Tensor, weight: _Tensor, bias: _Tensor, pad: _int=0) -> Tensor: ...
def conv_transpose1d(input: _Tensor, weight: _Tensor, bias: Optional[_Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, output_padding: Union[_int, _size]=0, groups: _int=1, dilation: Union[_int, _size]=1) -> Tensor: ...
def conv_transpose2d(input: _Tensor, weight: _Tensor, bias: Optional[_Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, output_padding: Union[_int, _size]=0, groups: _int=1, dilation: Union[_int, _size]=1) -> Tensor: ...
def conv_transpose3d(input: _Tensor, weight: _Tensor, bias: Optional[_Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, output_padding: Union[_int, _size]=0, groups: _int=1, dilation: Union[_int, _size]=1) -> Tensor: ...
def convolution(input: _Tensor, weight: _Tensor, bias: Optional[_Tensor], stride: _size, padding: _size, dilation: _size, transposed: _bool, output_padding: _size, groups: _int) -> Tensor: ...
def cos(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def cos_(self: _Tensor) -> Tensor: ...
def cosh(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def cosh_(self: _Tensor) -> Tensor: ...
def cosine_similarity(x1: _Tensor, x2: _Tensor, dim: _int=1, eps: _float=1e-08) -> Tensor: ...
def cross(self: _Tensor, other: _Tensor, dim: Optional[_int]=None, *, out: Optional[_Tensor]=None) -> Tensor: ...
def cudnn_affine_grid_generator(theta: _Tensor, N: _int, C: _int, H: _int, W: _int) -> Tensor: ...
def cudnn_batch_norm(input: _Tensor, weight: _Tensor, bias: Optional[_Tensor], running_mean: Optional[_Tensor], running_var: Optional[_Tensor], training: _bool, exponential_average_factor: _float, epsilon: _float) -> Tuple[_Tensor, Tensor, Tensor]: ...
def cudnn_convolution(self: _Tensor, weight: _Tensor, bias: Optional[_Tensor], padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...
def cudnn_convolution_transpose(self: _Tensor, weight: _Tensor, bias: Optional[_Tensor], padding: _size, output_padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...
def cudnn_grid_sampler(self: _Tensor, grid: _Tensor) -> Tensor: ...
def cudnn_is_acceptable(self: _Tensor) -> _bool: ...
def cumprod(self: _Tensor, dim: _int, *, dtype: Optional[_dtype]=None, out: Optional[_Tensor]=None) -> Tensor: ...
def cumsum(self: _Tensor, dim: _int, *, dtype: Optional[_dtype]=None, out: Optional[_Tensor]=None) -> Tensor: ...
def dequantize(self: _Tensor) -> Tensor: ...
def det(self: _Tensor) -> Tensor: ...
def detach(self: _Tensor) -> Tensor: ...
def detach_(self: _Tensor) -> Tensor: ...
def diag(self: _Tensor, diagonal: _int=0, *, out: Optional[_Tensor]=None) -> Tensor: ...
def diag_embed(self: _Tensor, offset: _int=0, dim1: _int=-2, dim2: _int=-1) -> Tensor: ...
def diagflat(self: _Tensor, offset: _int=0) -> Tensor: ...
def diagonal(self: _Tensor, offset: _int=0, dim1: _int=0, dim2: _int=1) -> Tensor: ...
def digamma(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def dist(self: _Tensor, other: _Tensor, p: Number=2) -> Tensor: ...
@overload
def div(input: Union[_Tensor, Number], other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def div(input: Union[_Tensor, Number], value: Number, other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...
def dot(self: _Tensor, tensor: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def dropout(input: _Tensor, p: _float, train: _bool) -> Tensor: ...
def dropout_(self: _Tensor, p: _float, train: _bool) -> Tensor: ...
def eig(self: _Tensor, eigenvectors: _bool=False, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
def embedding(weight: _Tensor, indices: _Tensor, padding_idx: _int=-1, scale_grad_by_freq: _bool=False, sparse: _bool=False) -> Tensor: ...
def embedding_bag(weight: _Tensor, indices: _Tensor, offsets: _Tensor, scale_grad_by_freq: _bool=False, mode: _int=0, sparse: _bool=False, per_sample_weights: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor, Tensor, Tensor]: ...
def embedding_renorm_(self: _Tensor, indices: _Tensor, max_norm: _float, norm_type: _float) -> Tensor: ...
@overload
def empty(size: _size, *, memory_format: Optional[_memory_format]=None, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def empty(*size: _int, memory_format: Optional[_memory_format]=None, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def empty_like(self: _Tensor) -> Tensor: ...
@overload
def empty_like(self: _Tensor, *, memory_format: Optional[_memory_format]=contiguous_format, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def empty_strided(size: _size, stride: _size, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def eq(self: _Tensor, other: Number, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def eq(self: _Tensor, other: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def equal(self: _Tensor, other: _Tensor) -> _bool: ...
def erf(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def erf_(self: _Tensor) -> Tensor: ...
def erfc(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def erfc_(self: _Tensor) -> Tensor: ...
def erfinv(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def exp(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def exp_(self: _Tensor) -> Tensor: ...
def expm1(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def expm1_(self: _Tensor) -> Tensor: ...
@overload
def eye(n: _int, *, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def eye(n: _int, m: _int, *, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def fake_quantize_per_tensor_affine(self: _Tensor, scale: _float, zero_point: _int, quant_min: _int, quant_max: _int) -> Tensor: ...
# def fbgemm_is_cpu_supported() -> _bool: ...
def fbgemm_linear_fp16_weight(input: _Tensor, packed_weight: _Tensor, bias: _Tensor) -> Tensor: ...
def fbgemm_linear_int8_weight(input: _Tensor, weight: _Tensor, packed: _Tensor, col_offsets: _Tensor, weight_scale: Number, weight_zero_point: Number, bias: _Tensor) -> Tensor: ...
def fbgemm_linear_quantize_weight(input: _Tensor) -> Tuple[_Tensor, Tensor, _float, _int]: ...
def fbgemm_pack_gemm_matrix_fp16(input: _Tensor) -> Tensor: ...
def fbgemm_pack_quantized_matrix(input: _Tensor, K: _int, N: _int) -> Tensor: ...
def feature_alpha_dropout(input: _Tensor, p: _float, train: _bool) -> Tensor: ...
def feature_alpha_dropout_(self: _Tensor, p: _float, train: _bool) -> Tensor: ...
def feature_dropout(input: _Tensor, p: _float, train: _bool) -> Tensor: ...
def feature_dropout_(self: _Tensor, p: _float, train: _bool) -> Tensor: ...
def fft(self: _Tensor, signal_ndim: _int, normalized: _bool=False) -> Tensor: ...
@overload
def fill_(self: _Tensor, value: Number) -> Tensor: ...
@overload
def fill_(self: _Tensor, value: _Tensor) -> Tensor: ...
def flatten(self: _Tensor, start_dim: _int=0, end_dim: _int=-1) -> Tensor: ...
def flip(self: _Tensor, dims: _size) -> Tensor: ...
def floor(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def floor_(self: _Tensor) -> Tensor: ...
@overload
def fmod(self: _Tensor, other: Number, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def fmod(self: _Tensor, other: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def frac(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def frac_(self: _Tensor) -> Tensor: ...
@overload
def frobenius_norm(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def frobenius_norm(self: _Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tensor: ...
def from_file(filename: _str, shared: Optional[_bool]=None, size: Optional[_int]=0, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def from_numpy(ndarray: _ndarray) -> Tensor: ...
def full(size: _size, fill_value: Number, *, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def full_like(self: _Tensor, fill_value: Number) -> Tensor: ...
@overload
def full_like(self: _Tensor, fill_value: Number, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def gather(self: _Tensor, dim: _int, index: _Tensor, *, sparse_grad: _bool=False, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def ge(self: _Tensor, other: Number, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def ge(self: _Tensor, other: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def geqrf(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
def ger(self: _Tensor, vec2: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
# def get_default_dtype() -> _dtype: ...
# def get_num_interop_threads() -> _int: ...
# def get_num_threads() -> _int: ...
def grid_sampler(input: _Tensor, grid: _Tensor, interpolation_mode: _int, padding_mode: _int) -> Tensor: ...
def grid_sampler_2d(input: _Tensor, grid: _Tensor, interpolation_mode: _int, padding_mode: _int) -> Tensor: ...
def grid_sampler_3d(input: _Tensor, grid: _Tensor, interpolation_mode: _int, padding_mode: _int) -> Tensor: ...
def group_norm(input: _Tensor, num_groups: _int, weight: Optional[_Tensor]=None, bias: Optional[_Tensor]=None, eps: _float=1e-05, cudnn_enabled: _bool=True) -> Tensor: ...
@overload
def gru(input: _Tensor, hx: _Tensor, params: Union[Tuple, List], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[_Tensor, Tensor]: ...
@overload
def gru(data: _Tensor, batch_sizes: _Tensor, hx: _Tensor, params: Union[Tuple, List], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[_Tensor, Tensor]: ...
def gru_cell(input: _Tensor, hx: _Tensor, w_ih: _Tensor, w_hh: _Tensor, b_ih: Optional[_Tensor]=None, b_hh: Optional[_Tensor]=None) -> Tensor: ...
@overload
def gt(self: _Tensor, other: Number, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def gt(self: _Tensor, other: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def hamming_window(window_length: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def hamming_window(window_length: _int, periodic: _bool, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def hamming_window(window_length: _int, periodic: _bool, alpha: _float, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def hamming_window(window_length: _int, periodic: _bool, alpha: _float, beta: _float, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def hann_window(window_length: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def hann_window(window_length: _int, periodic: _bool, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def hardshrink(self: _Tensor, lambd: Number=0.5) -> Tensor: ...
def histc(self: _Tensor, bins: _int=100, min: Number=0, max: Number=0, *, out: Optional[_Tensor]=None) -> Tensor: ...
def hspmm(mat1: _Tensor, mat2: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def ifft(self: _Tensor, signal_ndim: _int, normalized: _bool=False) -> Tensor: ...
def index_add(self: _Tensor, dim: _int, index: _Tensor, source: _Tensor) -> Tensor: ...
def index_copy(self: _Tensor, dim: _int, index: _Tensor, source: _Tensor) -> Tensor: ...
@overload
def index_fill(self: _Tensor, dim: _int, index: _Tensor, value: Number) -> Tensor: ...
@overload
def index_fill(self: _Tensor, dim: _int, index: _Tensor, value: _Tensor) -> Tensor: ...
def index_put(self: _Tensor, indices: Optional[Union[Tuple, List]], values: _Tensor, accumulate: _bool=False) -> Tensor: ...
def index_put_(self: _Tensor, indices: Optional[Union[Tuple, List]], values: _Tensor, accumulate: _bool=False) -> Tensor: ...
def index_select(self: _Tensor, dim: _int, index: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def instance_norm(input: _Tensor, weight: Optional[_Tensor], bias: Optional[_Tensor], running_mean: Optional[_Tensor], running_var: Optional[_Tensor], use_input_stats: _bool, momentum: _float, eps: _float, cudnn_enabled: _bool) -> Tensor: ...
def int_repr(self: _Tensor) -> Tensor: ...
def inverse(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def irfft(self: _Tensor, signal_ndim: _int, normalized: _bool=False, onesided: _bool=True, signal_sizes: _size=()) -> Tensor: ...
def is_complex(self: _Tensor) -> _bool: ...
def is_distributed(self: _Tensor) -> _bool: ...
def is_floating_point(self: _Tensor) -> _bool: ...
def is_nonzero(self: _Tensor) -> _bool: ...
def is_same_size(self: _Tensor, other: _Tensor) -> _bool: ...
def is_signed(self: _Tensor) -> _bool: ...
def isclose(self: _Tensor, other: _Tensor, rtol: _float=1e-05, atol: _float=1e-08, equal_nan: _bool=False) -> Tensor: ...
def isnan(self: _Tensor) -> Tensor: ...
def kthvalue(self: _Tensor, k: _int, dim: _int=-1, keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
def layer_norm(input: _Tensor, normalized_shape: _size, weight: Optional[_Tensor]=None, bias: Optional[_Tensor]=None, eps: _float=1e-05, cudnn_enable: _bool=True) -> Tensor: ...
@overload
def le(self: _Tensor, other: Number, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def le(self: _Tensor, other: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def lerp(self: _Tensor, end: _Tensor, weight: Number, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def lerp(self: _Tensor, end: _Tensor, weight: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def lgamma(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def linspace(start: Number, end: Number, steps: _int=100, *, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def log(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def log10(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def log10_(self: _Tensor) -> Tensor: ...
def log1p(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def log1p_(self: _Tensor) -> Tensor: ...
def log2(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def log2_(self: _Tensor) -> Tensor: ...
def log_(self: _Tensor) -> Tensor: ...
def log_softmax(self: _Tensor, dim: _int, dtype: Optional[_dtype]=None) -> Tensor: ...
def logdet(self: _Tensor) -> Tensor: ...
def logspace(start: Number, end: Number, steps: _int=100, base: _float=10.0, *, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def logsumexp(self: _Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def lstm(input: _Tensor, hx: Union[Tuple, List], params: Union[Tuple, List], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[_Tensor, Tensor, Tensor]: ...
@overload
def lstm(data: _Tensor, batch_sizes: _Tensor, hx: Union[Tuple, List], params: Union[Tuple, List], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[_Tensor, Tensor, Tensor]: ...
def lstm_cell(input: _Tensor, hx: Union[Tuple, List], w_ih: _Tensor, w_hh: _Tensor, b_ih: Optional[_Tensor]=None, b_hh: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
def lstsq(self: _Tensor, A: _Tensor, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
@overload
def lt(self: _Tensor, other: Number, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def lt(self: _Tensor, other: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def lu_solve(self: _Tensor, LU_data: _Tensor, LU_pivots: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def masked_fill(self: _Tensor, mask: _Tensor, value: Number) -> Tensor: ...
@overload
def masked_fill(self: _Tensor, mask: _Tensor, value: _Tensor) -> Tensor: ...
def masked_scatter(self: _Tensor, mask: _Tensor, source: _Tensor) -> Tensor: ...
def masked_select(self: _Tensor, mask: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def matmul(self: _Tensor, other: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def matrix_power(self: _Tensor, n: _int) -> Tensor: ...
@overload
def matrix_rank(self: _Tensor, tol: _float, symmetric: _bool=False) -> Tensor: ...
@overload
def matrix_rank(self: _Tensor, symmetric: _bool=False) -> Tensor: ...
@overload
def max(self: _Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
@overload
def max(self: _Tensor, other: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def max(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def max_pool1d(self: _Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tensor: ...
def max_pool1d_with_indices(self: _Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tuple[_Tensor, Tensor]: ...
def max_pool2d(self: _Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tensor: ...
def max_pool3d(self: _Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tensor: ...
@overload
def mean(self: _Tensor, *, dtype: Optional[_dtype]=None, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def mean(self: _Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, dtype: Optional[_dtype]=None, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def median(self: _Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
@overload
def median(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def min(self: _Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
@overload
def min(self: _Tensor, other: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def min(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def miopen_batch_norm(input: _Tensor, weight: _Tensor, bias: Optional[_Tensor], running_mean: Optional[_Tensor], running_var: Optional[_Tensor], training: _bool, exponential_average_factor: _float, epsilon: _float) -> Tuple[_Tensor, Tensor, Tensor]: ...
def miopen_convolution(self: _Tensor, weight: _Tensor, bias: Optional[_Tensor], padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...
def miopen_convolution_transpose(self: _Tensor, weight: _Tensor, bias: Optional[_Tensor], padding: _size, output_padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...
def miopen_depthwise_convolution(self: _Tensor, weight: _Tensor, bias: Optional[_Tensor], padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...
def miopen_rnn(input: _Tensor, weight: Union[Tuple, List], weight_stride0: _int, hx: _Tensor, cx: Optional[_Tensor], mode: _int, hidden_size: _int, num_layers: _int, batch_first: _bool, dropout: _float, train: _bool, bidirectional: _bool, batch_sizes: _size, dropout_state: Optional[_Tensor]) -> Tuple[_Tensor, Tensor, Tensor, Tensor, Tensor]: ...
def mkldnn_adaptive_avg_pool2d(self: _Tensor, output_size: Union[_int, _size]) -> Tensor: ...
def mkldnn_convolution(self: _Tensor, weight: _Tensor, bias: Optional[_Tensor], padding: _size, stride: _size, dilation: _size, groups: _int) -> Tensor: ...
def mkldnn_convolution_backward_weights(weight_size: _size, grad_output: _Tensor, self: _Tensor, padding: _size, stride: _size, dilation: _size, groups: _int, bias_defined: _bool) -> Tuple[_Tensor, Tensor]: ...
def mkldnn_max_pool2d(self: _Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tensor: ...
def mm(self: _Tensor, mat2: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def mode(self: _Tensor, dim: _int=-1, keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
@overload
def mul(input: Union[_Tensor, Number], other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def mul(input: Union[_Tensor, Number], value: Number, other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...
def multinomial(self: _Tensor, num_samples: _int, replacement: _bool=False, *, generator: Optional[_Generator]=None, out: Optional[_Tensor]=None) -> Tensor: ...
def mv(self: _Tensor, vec: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def mvlgamma(self: _Tensor, p: _int) -> Tensor: ...
def narrow(self: _Tensor, dim: _int, start: _int, length: _int) -> Tensor: ...
def native_batch_norm(input: _Tensor, weight: Optional[_Tensor], bias: Optional[_Tensor], running_mean: Optional[_Tensor], running_var: Optional[_Tensor], training: _bool, momentum: _float, eps: _float) -> Tuple[_Tensor, Tensor, Tensor]: ...
def native_layer_norm(input: _Tensor, weight: Optional[_Tensor], bias: Optional[_Tensor], M: _int, N: _int, eps: _float) -> Tuple[_Tensor, Tensor, Tensor]: ...
def native_norm(self: _Tensor, p: Number=2) -> Tensor: ...
@overload
def ne(self: _Tensor, other: Number, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def ne(self: _Tensor, other: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def neg(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def neg_(self: _Tensor) -> Tensor: ...
def norm_except_dim(v: _Tensor, pow: _int=2, dim: _int=0) -> Tensor: ...
@overload
def normal(mean: _Tensor, std: _float=1, *, generator: Optional[_Generator]=None, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def normal(mean: _float, std: _Tensor, *, generator: Optional[_Generator]=None, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def normal(mean: _Tensor, std: _Tensor, *, generator: Optional[_Generator]=None, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def normal(mean: _float, std: _float, size: _size, *, generator: Optional[_Generator]=None, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def nuclear_norm(self: _Tensor, keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def nuclear_norm(self: _Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tensor: ...
def numel(self: _Tensor) -> _int: ...
@overload
def ones(size: _size, *, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def ones(*size: _int, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def ones_like(self: _Tensor) -> Tensor: ...
@overload
def ones_like(self: _Tensor, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def orgqr(self: _Tensor, input2: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def ormqr(self: _Tensor, input2: _Tensor, input3: _Tensor, left: _bool=True, transpose: _bool=False, *, out: Optional[_Tensor]=None) -> Tensor: ...
def pairwise_distance(x1: _Tensor, x2: _Tensor, p: _float=2, eps: _float=1e-06, keepdim: _bool=False) -> Tensor: ...
def pdist(self: _Tensor, p: _float=2) -> Tensor: ...
def pin_memory(self: _Tensor) -> Tensor: ...
def pinverse(self: _Tensor, rcond: _float=1e-15) -> Tensor: ...
def pixel_shuffle(self: _Tensor, upscale_factor: _int) -> Tensor: ...
def poisson(self: _Tensor, generator: Optional[_Generator]=None) -> Tensor: ...
def poisson_nll_loss(input: _Tensor, target: _Tensor, log_input: _bool, full: _bool, eps: _float, reduction: _int) -> Tensor: ...
def polygamma(n: _int, self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def pow(self: _Tensor, exponent: Number, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def pow(self: _Tensor, exponent: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def pow(self: Number, exponent: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def prelu(self: _Tensor, weight: _Tensor) -> Tensor: ...
@overload
def prod(self: _Tensor, *, dtype: Optional[_dtype]=None, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def prod(self: _Tensor, dim: _int, keepdim: _bool=False, *, dtype: Optional[_dtype]=None, out: Optional[_Tensor]=None) -> Tensor: ...
def q_scale(self: _Tensor) -> _float: ...
def q_zero_point(self: _Tensor) -> _int: ...
def qr(self: _Tensor, some: _bool=True, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
def quantize_linear(self: _Tensor, scale: _float, zero_point: _int, dtype: _dtype) -> Tensor: ...
def quantize_linear_per_channel(self: _Tensor, scales: _Tensor, zero_points: _Tensor, axis: _size, dtype: _dtype) -> Tensor: ...
@overload
def quantized_gru(input: _Tensor, hx: _Tensor, params: Union[Tuple, List], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[_Tensor, Tensor]: ...
@overload
def quantized_gru(data: _Tensor, batch_sizes: _Tensor, hx: _Tensor, params: Union[Tuple, List], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[_Tensor, Tensor]: ...
def quantized_gru_cell(input: _Tensor, hx: _Tensor, w_ih: _Tensor, w_hh: _Tensor, b_ih: _Tensor, b_hh: _Tensor, packed_ih: _Tensor, packed_hh: _Tensor, col_offsets_ih: _Tensor, col_offsets_hh: _Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tensor: ...
def quantized_lstm(input: _Tensor, hx: Union[Tuple, List], params: Union[Tuple, List], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool, *, dtype: Optional[_dtype]=None) -> Tuple[_Tensor, Tensor, Tensor]: ...
def quantized_lstm_cell(input: _Tensor, hx: Union[Tuple, List], w_ih: _Tensor, w_hh: _Tensor, b_ih: _Tensor, b_hh: _Tensor, packed_ih: _Tensor, packed_hh: _Tensor, col_offsets_ih: _Tensor, col_offsets_hh: _Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tuple[_Tensor, Tensor]: ...
def quantized_max_pool2d(self: _Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1) -> Tensor: ...
def quantized_rnn_relu_cell(input: _Tensor, hx: _Tensor, w_ih: _Tensor, w_hh: _Tensor, b_ih: _Tensor, b_hh: _Tensor, packed_ih: _Tensor, packed_hh: _Tensor, col_offsets_ih: _Tensor, col_offsets_hh: _Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tensor: ...
def quantized_rnn_tanh_cell(input: _Tensor, hx: _Tensor, w_ih: _Tensor, w_hh: _Tensor, b_ih: _Tensor, b_hh: _Tensor, packed_ih: _Tensor, packed_hh: _Tensor, col_offsets_ih: _Tensor, col_offsets_hh: _Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tensor: ...
@overload
def rand(size: _size, *, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def rand(*size: _int, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def rand(size: _size, *, generator: _Generator, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def rand(*size: _int, generator: _Generator, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def rand_like(self: _Tensor) -> Tensor: ...
@overload
def rand_like(self: _Tensor, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randint(low: _int, high: _int, size: _size, *, dtype: Optional[_dtype]=None, device: Optional[Union[_device, _str]]=None, requires_grad: _bool=False) -> Tensor: ...
@overload
def randint(high: _int, size: _size, *, dtype: Optional[_dtype]=None, device: Optional[Union[_device, _str]]=None, requires_grad: _bool=False) -> Tensor: ...
@overload
def randint_like(self: _Tensor, high: _int) -> Tensor: ...
@overload
def randint_like(self: _Tensor, low: _int, high: _int) -> Tensor: ...
@overload
def randint_like(self: _Tensor, high: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randint_like(self: _Tensor, low: _int, high: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randn(size: _size, *, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randn(*size: _int, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randn(size: _size, *, generator: _Generator, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randn(*size: _int, generator: _Generator, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randn_like(self: _Tensor) -> Tensor: ...
@overload
def randn_like(self: _Tensor, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randperm(n: _int, *, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randperm(n: _int, *, generator: _Generator, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def range(start: Number, end: Number, step: Number=1, *, out: Optional[_Tensor]=None, dtype: Optional[_dtype]=None, device: Optional[Union[_device, _str]]=None, requires_grad: _bool=False) -> Tensor: ...
def reciprocal(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def reciprocal_(self: _Tensor) -> Tensor: ...
def relu(self: _Tensor) -> Tensor: ...
def relu_(self: _Tensor) -> Tensor: ...
@overload
def remainder(self: _Tensor, other: Number, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def remainder(self: _Tensor, other: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def renorm(self: _Tensor, p: Number, dim: _int, maxnorm: Number, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def repeat_interleave(repeats: _Tensor) -> Tensor: ...
@overload
def repeat_interleave(self: _Tensor, repeats: _Tensor, dim: Optional[_int]=None) -> Tensor: ...
@overload
def repeat_interleave(self: _Tensor, repeats: _int, dim: Optional[_int]=None) -> Tensor: ...
def reshape(self: _Tensor, shape: _size) -> Tensor: ...
def resize_as_(self: _Tensor, the_template: _Tensor) -> Tensor: ...
def rfft(self: _Tensor, signal_ndim: _int, normalized: _bool=False, onesided: _bool=True) -> Tensor: ...
@overload
def rnn_relu(input: _Tensor, hx: _Tensor, params: Union[Tuple, List], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[_Tensor, Tensor]: ...
@overload
def rnn_relu(data: _Tensor, batch_sizes: _Tensor, hx: _Tensor, params: Union[Tuple, List], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[_Tensor, Tensor]: ...
def rnn_relu_cell(input: _Tensor, hx: _Tensor, w_ih: _Tensor, w_hh: _Tensor, b_ih: Optional[_Tensor]=None, b_hh: Optional[_Tensor]=None) -> Tensor: ...
@overload
def rnn_tanh(input: _Tensor, hx: _Tensor, params: Union[Tuple, List], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[_Tensor, Tensor]: ...
@overload
def rnn_tanh(data: _Tensor, batch_sizes: _Tensor, hx: _Tensor, params: Union[Tuple, List], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[_Tensor, Tensor]: ...
def rnn_tanh_cell(input: _Tensor, hx: _Tensor, w_ih: _Tensor, w_hh: _Tensor, b_ih: Optional[_Tensor]=None, b_hh: Optional[_Tensor]=None) -> Tensor: ...
def roll(self: _Tensor, shifts: Union[_int, _size], dims: Union[_int, _size]=()) -> Tensor: ...
def rot90(self: _Tensor, k: _int=1, dims: _size=(0,1)) -> Tensor: ...
def round(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def round_(self: _Tensor) -> Tensor: ...
def rrelu(self: _Tensor, lower: Number=0.125, upper: Number=0.3333333333333333, training: _bool=False, generator: Optional[_Generator]=None) -> Tensor: ...
def rrelu_(self: _Tensor, lower: Number=0.125, upper: Number=0.3333333333333333, training: _bool=False, generator: Optional[_Generator]=None) -> Tensor: ...
def rsqrt(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def rsqrt_(self: _Tensor) -> Tensor: ...
@overload
def rsub(self: _Tensor, other: _Tensor, *, alpha: Number=1) -> Tensor: ...
@overload
def rsub(self: _Tensor, other: Number, alpha: Number=1) -> Tensor: ...
def s_native_addmm(self: _Tensor, mat1: _Tensor, mat2: _Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[_Tensor]=None) -> Tensor: ...
def s_native_addmm_(self: _Tensor, mat1: _Tensor, mat2: _Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
def scalar_tensor(s: Number, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def scatter(self: _Tensor, dim: _int, index: _Tensor, src: _Tensor) -> Tensor: ...
@overload
def scatter(self: _Tensor, dim: _int, index: _Tensor, value: Number) -> Tensor: ...
def scatter_add(self: _Tensor, dim: _int, index: _Tensor, src: _Tensor) -> Tensor: ...
def select(self: _Tensor, dim: _int, index: _int) -> Tensor: ...
def selu(self: _Tensor) -> Tensor: ...
def selu_(self: _Tensor) -> Tensor: ...
def set_flush_denormal(mode: _bool) -> _bool: ...
def set_num_interop_threads(num: _int) -> None: ...
def set_num_threads(num: _int) -> None: ...
def sigmoid(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def sigmoid_(self: _Tensor) -> Tensor: ...
def sign(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def sin(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def sin_(self: _Tensor) -> Tensor: ...
def sinh(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def sinh_(self: _Tensor) -> Tensor: ...
def slogdet(self: _Tensor) -> Tuple[_Tensor, Tensor]: ...
def smm(self: _Tensor, mat2: _Tensor) -> Tensor: ...
def softmax(self: _Tensor, dim: _int, dtype: Optional[_dtype]=None) -> Tensor: ...
def solve(self: _Tensor, A: _Tensor, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
def sort(self: _Tensor, dim: _int=-1, descending: _bool=False, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
def sparse_coo_tensor(indices: _Tensor, values: Union[_Tensor,List], size: Optional[_size]=None, *, dtype: Optional[_dtype]=None, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def split_with_sizes(self: _Tensor, split_sizes: _size, dim: _int=0) -> Union[Tuple[_Tensor, ...], List]: ...
def sqrt(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def sqrt_(self: _Tensor) -> Tensor: ...
@overload
def squeeze(self: _Tensor) -> Tensor: ...
@overload
def squeeze(self: _Tensor, dim: _int) -> Tensor: ...
@overload
def sspaddmm(self: _Tensor, mat1: _Tensor, mat2: _Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def sspaddmm(beta: Number, self: _Tensor, alpha: Number, mat1: _Tensor, mat2: _Tensor) -> Tensor: ...
@overload
def sspaddmm(beta: Number, self: _Tensor, mat1: _Tensor, mat2: _Tensor) -> Tensor: ...
def stack(tensors: Union[Tuple, List], dim: _int=0, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def std(self: _Tensor, unbiased: _bool=True, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def std(self: _Tensor, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def std_mean(self: _Tensor, unbiased: _bool=True) -> Tuple[_Tensor, Tensor]: ...
@overload
def std_mean(self: _Tensor, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False) -> Tuple[_Tensor, Tensor]: ...
@overload
def sub(input: Union[_Tensor, Number], other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def sub(input: Union[_Tensor, Number], value: Number, other: Union[_Tensor, Number], *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def sub(self: _Tensor, alpha: Number, other: _Tensor) -> Tensor: ...
@overload
def sub(self: _Tensor, alpha: Number, other: _Tensor, *, out: _Tensor) -> Tensor: ...
@overload
def sum(self: _Tensor, *, dtype: Optional[_dtype]=None, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def sum(self: _Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, dtype: Optional[_dtype]=None, out: Optional[_Tensor]=None) -> Tensor: ...
def svd(self: _Tensor, some: _bool=True, compute_uv: _bool=True, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor, Tensor]: ...
def symeig(self: _Tensor, eigenvectors: _bool=False, upper: _bool=True, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
def t(self: _Tensor) -> Tensor: ...
def take(self: _Tensor, index: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def tan(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def tan_(self: _Tensor) -> Tensor: ...
def tanh(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def tanh_(self: _Tensor) -> Tensor: ...
def tensor(data: Any, dtype: Optional[_dtype]=None, device: Optional[Union[_device, _str]]=None, requires_grad: _bool=False) -> Tensor: ...
def threshold(self: _Tensor, threshold: Number, value: Number, *, out: Optional[_Tensor]=None) -> Tensor: ...
def threshold_(self: _Tensor, threshold: Number, value: Number) -> Tensor: ...
def topk(self: _Tensor, k: _int, dim: _int=-1, largest: _bool=True, sorted: _bool=True, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
def trace(self: _Tensor) -> Tensor: ...
def transpose(self: _Tensor, dim0: _int, dim1: _int) -> Tensor: ...
@overload
def trapz(y: _Tensor, x: _Tensor, *, dim: _int=-1) -> Tensor: ...
@overload
def trapz(y: _Tensor, *, dx: _float=1, dim: _int=-1) -> Tensor: ...
def triangular_solve(self: _Tensor, A: _Tensor, upper: _bool=True, transpose: _bool=False, unitriangular: _bool=False, *, out: Optional[_Tensor]=None) -> Tuple[_Tensor, Tensor]: ...
def tril(self: _Tensor, diagonal: _int=0, *, out: Optional[_Tensor]=None) -> Tensor: ...
def tril_indices(row: _int, col: _int, offset: _int=0, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def triu(self: _Tensor, diagonal: _int=0, *, out: Optional[_Tensor]=None) -> Tensor: ...
def triu_indices(row: _int, col: _int, offset: _int=0, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
def trunc(self: _Tensor, *, out: Optional[_Tensor]=None) -> Tensor: ...
def trunc_(self: _Tensor) -> Tensor: ...
def unbind(self: _Tensor, dim: _int=0) -> Union[Tuple[_Tensor, ...], List]: ...
# def unique_dim(self: _Tensor, dim: _int, sorted: _bool=True, return_inverse: _bool=False, return_counts: _bool=False) -> Tuple[_Tensor, Tensor, Tensor]: ...
def unsqueeze(self: _Tensor, dim: _int) -> Tensor: ...
@overload
def var(self: _Tensor, unbiased: _bool=True, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def var(self: _Tensor, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False, *, out: Optional[_Tensor]=None) -> Tensor: ...
@overload
def var_mean(self: _Tensor, unbiased: _bool=True) -> Tuple[_Tensor, Tensor]: ...
@overload
def var_mean(self: _Tensor, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False) -> Tuple[_Tensor, Tensor]: ...
@overload
def where(condition: _Tensor, self: _Tensor, other: _Tensor) -> Tensor: ...
@overload
def where(condition: _Tensor) -> Union[Tuple[_Tensor, ...], List]: ...
def zero_(self: _Tensor) -> Tensor: ...
@overload
def zeros(size: _size, *, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def zeros(*size: _int, out: Optional[_Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def zeros_like(self: _Tensor) -> Tensor: ...
@overload
def zeros_like(self: _Tensor, *, dtype: _dtype=None, layout: _layout=strided, device: Optional[Union[_device, _str]]=None, requires_grad:_bool=False) -> Tensor: ...

# class DoubleStorage(Storage): ...
# class FloatStorage(Storage): ...
# class LongStorage(Storage): ...
# class IntStorage(Storage): ...
# class ShortStorage(Storage): ...
# class CharStorage(Storage): ...
# class ByteStorage(Storage): ...
# class BoolStorage(Storage): ...
# class DoubleTensor(Tensor): ...
# class FloatTensor(Tensor): ...
# class LongTensor(Tensor): ...
# class IntTensor(Tensor): ...
# class ShortTensor(Tensor): ...
# class CharTensor(Tensor): ...
# class ByteTensor(Tensor): ...
# class BoolTensor(Tensor): ...

# Pure Python functions defined in torch/__init__.py

# def typename(obj: Any) -> _str: ...
# def is_Tensor(obj: Any) -> _bool: ...
# def is_Storage(obj: Any) -> _bool: ...
# def set_default_Tensor_type(type) -> None: ...  # ick, what a bad legacy API
# def set_default_dtype(d: _dtype) -> None: ...
# def manager_path() -> _str: ...
# def compiled_with_cxx11_abi() -> _bool: ...

# The return value of this function depends on the value of `as_tuple`,
# (similar to `unique`, `lu`, etc.); as such, it is not
# possible to type correctly
def nonzero(input: _Tensor, *, out: Optional[_Tensor]=None, as_tuple: Optional[_bool]=None): ...

#MODIFIED BY TORCHGPIPE
# def is_grad_enabled() -> _bool: ...
# __version__: _str = ...
#END
