#HACK loss functions have 2 deprecated args (reduce,size_average) so we removed them
#HACK for nll_loss and cross_entropy we switched the order between reduction and ignore_index args to match the torch._C call
#HACK reduction is actually a string which is translated to an enum the trace has the enum value so we change the arg tupe to int


def fractional_max_pool2d_with_indices(input: Tensor, kernel_size: _size, output_size: Optional[_size]=None, output_ratio: Optional[float]=None, return_indices: bool=False, _random_samples: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...

def fractional_max_pool3d_with_indices(input: Tensor, kernel_size: _size, output_size: Optional[_size]=None, output_ratio: Optional[float]=None, return_indices: bool=False, _random_samples: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...

def max_pool1d_with_indices(input: Tensor, kernel_size: _size, stride: Optional[_size]=None, padding: _size=0, dilation: _size=1, ceil_mode: bool=False, return_indices: bool=False) -> Tuple[Tensor, Tensor]: ...

def max_pool2d_with_indices(input: Tensor, kernel_size: _size, stride: Optional[_size]=None, padding: _size=0, dilation: _size=1, ceil_mode: bool=False, return_indices: bool=False) -> Tuple[Tensor, Tensor]: ...

def max_pool3d_with_indices(input: Tensor, kernel_size: _size, stride: Optional[_size]=None, padding: _size=0, dilation: _size=1, ceil_mode: bool=False, return_indices: bool=False) -> Tuple[Tensor, Tensor]: ...

def max_unpool1d(input: Tensor, indices: Tensor, kernel_size: _size, stride: Optional[_size]=None, padding: _size=0, output_size: Optional[_size]=None) -> Tensor: ...

def max_unpool2d(input: Tensor, indices: Tensor, kernel_size: _size, stride: Optional[_size]=None, padding: _size=0, output_size: Optional[_size]=None) -> Tensor: ...

def max_unpool3d(input: Tensor, indices: Tensor, kernel_size: _size, stride: Optional[_size]=None, padding: _size=0, output_size: Optional[_size]=None) -> Tensor: ...

def lp_pool2d(input: Tensor, norm_type: float, kernel_size: int, stride: Optional[_size]=None, ceil_mode: bool=False) -> Tensor: ...

def lp_pool1d(input: Tensor, norm_type: float, kernel_size: int, stride: Optional[_size]=None, ceil_mode: bool=False) -> Tensor: ...

def adaptive_max_pool1d_with_indices(input: Tensor, output_size: _size, return_indices: bool=False) -> Tuple[Tensor, Tensor]: ...

def adaptive_max_pool2d_with_indices(input: Tensor, output_size: _size, return_indices: bool=False) -> Tuple[Tensor, Tensor]: ...

def adaptive_max_pool3d_with_indices(input: Tensor, output_size: _size, return_indices: bool=False) -> Tuple[Tensor, Tensor]: ...

def adaptive_avg_pool2d(input: Tensor, output_size: _size) -> Tensor: ...

def adaptive_avg_pool3d(input: Tensor, output_size: _size) -> Tensor: ...

def dropout(input: Tensor, p: float=0.5, training: bool=True, inplace: bool=False) -> Tensor: ...

def alpha_dropout(input: Tensor, p: float=0.5, training: bool=False, inplace: bool=False) -> Tensor: ...

def dropout2d(input: Tensor, p: float=0.5, training: bool=True, inplace: bool=False) -> Tensor: ...

def dropout3d(input: Tensor, p: float=0.5, training: bool=True, inplace: bool=False) -> Tensor: ...

def feature_alpha_dropout(input: Tensor, p: float=0.5, training: bool=False, inplace: bool=False) -> Tensor: ...

def threshold(input: Tensor, threshold: float, value: float, inplace: bool=False) -> Tensor: ...

def relu(input: Tensor, inplace: bool=False) -> Tensor: ...

def glu(input: Tensor, dim: int=-1) -> Tensor: ...

def hardtanh(input: Tensor, min_val: float=-1., max_val: float=1., inplace: bool=False) -> Tensor: ...

def relu6(input: Tensor, inplace: bool=False) -> Tensor: ...

def elu(input: Tensor, alpha: float=1., inplace: bool=False) -> Tensor: ...

def selu(input: Tensor, inplace: bool=False) -> Tensor: ...

def celu(input: Tensor, alpha: float=1., inplace: bool=False) -> Tensor: ...

def leaky_relu(input: Tensor, negative_slope: float=0.01, inplace: bool=False) -> Tensor: ...

def prelu(input: Tensor, weight: Tensor) -> Tensor: ...

def rrelu(input: Tensor, lower: float=1./8, upper: float=1./3, training: bool=False, inplace: bool=False) -> Tensor: ...

def gelu(input: Any): ...

def hardshrink(input: Tensor, lambd: float=0.5) -> Tensor: ...

def tanhshrink(input: Any): ...

def softsign(input: Any): ...

def softmin(input: Tensor, dim: Optional[int]=None, _stacklevel: int=3, dtype: Optional[int]=None) -> Tensor: ...

def softmax(input: Tensor, dim: Optional[int]=None, _stacklevel: int=3, dtype: Optional[int]=None) -> Tensor: ...

def gumbel_softmax(logits: Tensor, tau: float=1., hard: bool=False, eps: float=1e-10, dim: int=-1) -> Tensor: ...

def log_softmax(input: Tensor, dim: Optional[int]=None, _stacklevel: int=3, dtype: Optional[int]=None) -> Tensor: ...

def tanh(input: Any): ...

def sigmoid(input: Any): ...

def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor]=None) -> Tensor: ...

def bilinear(input1: Tensor, input2: Tensor, weight: Tensor, bias: Optional[Tensor]=None) -> Tensor: ...

def embedding(input: Tensor, weight: Tensor, padding_idx: Optional[int]=None, max_norm: Optional[float]=None, norm_type: float=2., scale_grad_by_freq: bool=False, sparse: bool=False) -> Tensor: ...

def embedding_bag(input: Tensor, weight: Tensor, offsets: Optional[Tensor]=None, max_norm: Optional[float]=None, norm_type: float=2., scale_grad_by_freq: bool=False, mode: str='mean', sparse: bool=False) -> Tensor: ...

def batch_norm(input: Tensor, running_mean: Optional[Tensor]=None, running_var: Optional[Tensor]=None, weight: Optional[Tensor]=None, bias: Optional[Tensor]=None, training: bool=False, momentum: float=0.1, eps: float=1e-5) -> Tensor: ...

def instance_norm(input: Tensor, running_mean: Optional[Tensor]=None, running_var: Optional[Tensor]=None, weight: Optional[Tensor]=None, bias: Optional[Tensor]=None, use_input_stats: bool=True, momentum: float=0.1, eps: float=1e-5) -> Tensor: ...

def layer_norm(input: Tensor, normalized_shape: List, weight: Optional[Tensor]=None, bias: Optional[Tensor]=None, eps: float=1e-5) -> Tensor: ...

def group_norm(input: Tensor, num_groups: int, weight: Optional[Tensor]=None, bias: Optional[Tensor]=None, eps: float=1e-5) -> Tensor: ...

def local_response_norm(input: Tensor, size: int, alpha: float=1e-4, beta: float=0.75, k: float=1.) -> Tensor: ...

def ctc_loss(log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor, blank: int=0, reduction: int='mean', zero_infinity: bool=False) -> Tensor: ...

def nll_loss(input: Tensor, target: Tensor, weight: Optional[Tensor]=None, reduction: int='mean', ignore_index: int=-100) -> Tensor: ...

def poisson_nll_loss(input: Tensor, target: Tensor, log_input: bool=True, full: bool=False, eps: float=1e-8, reduction: int='mean') -> Tensor: ...

def kl_div(input: Tensor, target: Tensor, reduction: int='mean') -> Tensor: ...

def cross_entropy(input: Tensor, target: Tensor, weight: Optional[Tensor]=None, reduction: int='mean', ignore_index: int=-100) -> Tensor: ...

def binary_cross_entropy(input: Tensor, target: Tensor, weight: Optional[Tensor]=None, reduction: int='mean') -> Tensor: ...

def binary_cross_entropy_with_logits(input: Tensor, target: Tensor, weight: Optional[Tensor]=None, reduction: int='mean', pos_weight: Optional[Tensor]=None) -> Tensor: ...

def smooth_l1_loss(input: Tensor, target: Tensor, reduction: int='mean') -> Tensor: ...

def l1_loss(input: Tensor, target: Tensor, reduction: int='mean') -> Tensor: ...

def mse_loss(input: Tensor, target: Tensor, reduction: int='mean') -> Tensor: ...

def margin_ranking_loss(input1: Tensor, input2: Tensor, target: Tensor, margin: float=0., reduction: int='mean') -> Tensor: ...

def hinge_embedding_loss(input: Tensor, target: Tensor, margin: float=1., reduction: int='mean') -> Tensor: ...

def multilabel_margin_loss(input: Tensor, target: Tensor, reduction: int='mean') -> Tensor: ...

def soft_margin_loss(input: Tensor, target: Tensor, reduction: int='mean') -> Tensor: ...

def multilabel_soft_margin_loss(input: Tensor, target: Tensor, weight: Optional[Tensor]=None, reduction: int='mean') -> Tensor: ...

def cosine_embedding_loss(input1: Tensor, input2: Tensor, target: Tensor, margin: float=0., reduction: int='mean') -> Tensor: ...

def multi_margin_loss(input: Tensor, target: Tensor, p: int=1, margin: float=1., weight: Optional[Tensor]=None, reduction: int='mean') -> Tensor: ...

def upsample(input: Any, size: Optional[Any]=None, scale_factor: Optional[Any]=None, mode: str='nearest', align_corners: Optional[Any]=None): ...

def interpolate(input: Any, size: Optional[Any]=None, scale_factor: Optional[Any]=None, mode: str='nearest', align_corners: Optional[Any]=None): ...

def upsample_nearest(input: Any, size: Optional[Any]=None, scale_factor: Optional[Any]=None): ...

def upsample_bilinear(input: Any, size: Optional[Any]=None, scale_factor: Optional[Any]=None): ...

def grid_sample(input: Tensor, grid: Tensor, mode: str='bilinear', padding_mode: str='zeros') -> Tensor: ...

def affine_grid(theta: Tensor, size: List) -> Tensor: ...

def pad(input: Tensor, pad: List, mode: str='constant', value: float=0.) -> Tensor: ...

def pairwise_distance(x1: Tensor, x2: Tensor, p: float=2., eps: float=1e-6, keepdim: bool=False) -> Tensor: ...

def triplet_margin_loss(anchor: Tensor, positive: Tensor, negative: Tensor, margin: float=1.0, p: float=2., eps: float=1e-6, swap: bool=False, reduction: int='mean') -> Tensor: ...

def normalize(input: Tensor, p: float=2., dim: int=1, eps: float=1e-12, out: Optional[Tensor]=None) -> Tensor: ...

def assert_int_or_pair(arg: Any, arg_name: Any, message: Any) -> None: ...

def unfold(input: Tensor, kernel_size: _size, dilation: _size=1, padding: _size=0, stride: _size=1) -> Tensor: ...

def fold(input: Tensor, output_size: _size, kernel_size: _size, dilation: _size=1, padding: _size=0, stride: _size=1) -> Tensor: ...