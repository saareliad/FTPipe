"""AutoGenerated with:
python partitioning_script.py --partitioning_task t5 --t5_task squad1 --lmhead --n_iter 10 --n_partitions 32 --analysis_batch_size 1 --partitioning_batch_size 1 --precompute_masks --stateless_tied --model_name_or_path t5-11b --save_memory_mode --max_seq_length 128 --answer_max_seq_length 16 --basic_blocks T5LayerSelfAttention T5LayerCrossAttention --constraint memory --objective stage_time --profiles_cache_name t5_11b_T5Attentions_cache
"""
import math
import torch
import torch.nn.functional
import torch.functional
from torch import Tensor
import torch.nn as nn
from itertools import chain
from typing import Optional, Tuple, Iterator, Iterable, OrderedDict, Dict
import collections

from torch.nn.modules.dropout import Dropout
from models.normal.NLP_models.modeling_t5 import T5LayerNorm
from models.normal.NLP_models.modeling_t5 import T5LayerCrossAttention
from models.normal.NLP_models.stateless import StatelessEmbedding
from models.normal.NLP_models.modeling_t5 import T5LayerSelfAttention
from torch.nn.modules.linear import Linear
from torch.nn.modules.loss import CrossEntropyLoss
# this is an auto generated file do not edit unless you know what you are doing


# partition adjacency
# model inputs {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31}
# partition 0 {'inputs': {'attention_mask', 'decoder_attention_mask', 'input_ids', 'decoder_input_ids'}, 'outputs': {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}}
# partition 1 {'inputs': {'attention_mask', 0}, 'outputs': {2}}
# partition 2 {'inputs': {'attention_mask', 0, 1}, 'outputs': {3}}
# partition 3 {'inputs': {'attention_mask', 0, 2}, 'outputs': {4}}
# partition 4 {'inputs': {'attention_mask', 0, 3}, 'outputs': {5}}
# partition 5 {'inputs': {'attention_mask', 0, 4}, 'outputs': {6}}
# partition 6 {'inputs': {'attention_mask', 0, 5}, 'outputs': {7}}
# partition 7 {'inputs': {'attention_mask', 0, 6}, 'outputs': {8, 13, 14}}
# partition 8 {'inputs': {'attention_mask', 0, 7}, 'outputs': {9}}
# partition 9 {'inputs': {'attention_mask', 0, 8}, 'outputs': {10}}
# partition 10 {'inputs': {'attention_mask', 0, 9}, 'outputs': {11}}
# partition 11 {'inputs': {'attention_mask', 0, 10}, 'outputs': {12}}
# partition 12 {'inputs': {'attention_mask', 0, 11}, 'outputs': {13}}
# partition 13 {'inputs': {0, 7, 'attention_mask', 12, 'inverted_encoder_attention_mask'}, 'outputs': {14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31}}
# partition 14 {'inputs': {'decoder_attention_mask', 13, 'inverted_encoder_attention_mask', 7}, 'outputs': {15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31}}
# partition 15 {'inputs': {'decoder_attention_mask', 13, 'inverted_encoder_attention_mask', 14}, 'outputs': {16}}
# partition 16 {'inputs': {13, 14, 15, 'inverted_encoder_attention_mask', 'decoder_attention_mask'}, 'outputs': {17}}
# partition 17 {'inputs': {13, 14, 16, 'inverted_encoder_attention_mask', 'decoder_attention_mask'}, 'outputs': {18}}
# partition 18 {'inputs': {13, 14, 17, 'inverted_encoder_attention_mask', 'decoder_attention_mask'}, 'outputs': {19}}
# partition 19 {'inputs': {13, 14, 18, 'inverted_encoder_attention_mask', 'decoder_attention_mask'}, 'outputs': {20}}
# partition 20 {'inputs': {13, 14, 19, 'inverted_encoder_attention_mask', 'decoder_attention_mask'}, 'outputs': {21}}
# partition 21 {'inputs': {13, 14, 20, 'inverted_encoder_attention_mask', 'decoder_attention_mask'}, 'outputs': {22}}
# partition 22 {'inputs': {13, 14, 21, 'inverted_encoder_attention_mask', 'decoder_attention_mask'}, 'outputs': {23}}
# partition 23 {'inputs': {13, 14, 22, 'inverted_encoder_attention_mask', 'decoder_attention_mask'}, 'outputs': {24}}
# partition 24 {'inputs': {13, 14, 'inverted_encoder_attention_mask', 23, 'decoder_attention_mask'}, 'outputs': {25}}
# partition 25 {'inputs': {13, 14, 'inverted_encoder_attention_mask', 24, 'decoder_attention_mask'}, 'outputs': {26}}
# partition 26 {'inputs': {13, 14, 'inverted_encoder_attention_mask', 'decoder_attention_mask', 25}, 'outputs': {27}}
# partition 27 {'inputs': {13, 14, 'inverted_encoder_attention_mask', 'decoder_attention_mask', 26}, 'outputs': {28}}
# partition 28 {'inputs': {13, 14, 'inverted_encoder_attention_mask', 'decoder_attention_mask', 27}, 'outputs': {29}}
# partition 29 {'inputs': {13, 14, 'inverted_encoder_attention_mask', 'decoder_attention_mask', 28}, 'outputs': {30}}
# partition 30 {'inputs': {13, 14, 'inverted_encoder_attention_mask', 'decoder_attention_mask', 29}, 'outputs': {31}}
# partition 31 {'inputs': {13, 14, 'lm_labels', 'inverted_encoder_attention_mask', 'decoder_attention_mask', 30}, 'outputs': {'output'}}
# model outputs {31}


def create_pipeline_configuration(DEBUG=False, batch_size=1):
    config = {
        'batch_dim': 0,
        'depth': 10000,
        'basic_blocks': (Dropout,T5LayerNorm,T5LayerCrossAttention,StatelessEmbedding,T5LayerSelfAttention,Linear,CrossEntropyLoss),
        'model_inputs': {
            'attention_mask': {
                'shape': torch.Size([1, 1, 1, 128]),
                'dtype': torch.float32,
                'is_batched': True,
                'used_by': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]},
            'decoder_attention_mask': {
                'shape': torch.Size([1, 1, 16, 16]),
                'dtype': torch.float32,
                'is_batched': True,
                'used_by': [0, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]},
            'decoder_input_ids': {
                'shape': torch.Size([1, 16]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0]},
            'input_ids': {
                'shape': torch.Size([1, 128]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0]},
            'inverted_encoder_attention_mask': {
                'shape': torch.Size([1, 1, 1, 128]),
                'dtype': torch.float32,
                'is_batched': True,
                'used_by': [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]},
            'lm_labels': {
                'shape': torch.Size([1, 16]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [31]}},
        'model_outputs': {
            'T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]': {
                'shape': torch.Size([1]),
                'dtype': torch.float32,
                'is_batched': False,
                'created_by': 31}},
        'stages': {
            0: {
                'stage_cls': Partition0,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'decoder_input_ids': {
                        'shape': torch.Size([1, 16]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'input_ids': {
                        'shape': torch.Size([1, 128]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_1': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]_0': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [7]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]_1': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [7]}},
                'devices': ['cpu' if DEBUG else 'cuda:0']},
            1: {
                'stage_cls': Partition1,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_1': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_2': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [2]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [2]}},
                'devices': ['cpu' if DEBUG else 'cuda:1']},
            2: {
                'stage_cls': Partition2,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_2': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_3': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [3]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/Tensor::__add___68': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [3]}},
                'devices': ['cpu' if DEBUG else 'cuda:2']},
            3: {
                'stage_cls': Partition3,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_3': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 2},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/Tensor::__add___68': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 2}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_4': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [4]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [4]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]': {
                        'shape': torch.Size([1, 128, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [4]}},
                'devices': ['cpu' if DEBUG else 'cuda:3']},
            4: {
                'stage_cls': Partition4,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_4': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 3},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 3},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]': {
                        'shape': torch.Size([1, 128, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 3}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_5': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [5]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [5]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [5]}},
                'devices': ['cpu' if DEBUG else 'cuda:4']},
            5: {
                'stage_cls': Partition5,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_5': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 4},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 4},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 4}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_6': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [6]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [6]}},
                'devices': ['cpu' if DEBUG else 'cuda:5']},
            6: {
                'stage_cls': Partition6,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_6': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 5},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 5}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_7': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [7]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/Tensor::__add___131': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [7]}},
                'devices': ['cpu' if DEBUG else 'cuda:6']},
            7: {
                'stage_cls': Partition7,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_7': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 6},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/Tensor::__add___131': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 6},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]_0': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]_1': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_8': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [8]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [8]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]': {
                        'shape': torch.Size([1, 128, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [8]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___253': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___255': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]}},
                'devices': ['cpu' if DEBUG else 'cuda:7']},
            8: {
                'stage_cls': Partition8,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_8': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 7},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 7},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]': {
                        'shape': torch.Size([1, 128, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 7}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_9': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [9]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [9]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 128, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [9]}},
                'devices': ['cpu' if DEBUG else 'cuda:8']},
            9: {
                'stage_cls': Partition9,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_9': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 8},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 8},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 128, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 8}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_10': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [10]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [10]}},
                'devices': ['cpu' if DEBUG else 'cuda:9']},
            10: {
                'stage_cls': Partition10,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_10': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 9},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 9}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_11': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/Tensor::__add___194': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]}},
                'devices': ['cpu' if DEBUG else 'cuda:10']},
            11: {
                'stage_cls': Partition11,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_11': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/Tensor::__add___194': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_12': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]': {
                        'shape': torch.Size([1, 128, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]}},
                'devices': ['cpu' if DEBUG else 'cuda:11']},
            12: {
                'stage_cls': Partition12,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_12': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]': {
                        'shape': torch.Size([1, 128, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_13': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 128, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]}},
                'devices': ['cpu' if DEBUG else 'cuda:12']},
            13: {
                'stage_cls': Partition13,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_13': {
                        'shape': torch.Size([1, 128, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 12},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 12},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 128, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 12},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___253': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 7}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_14': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]_0': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]_1': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]}},
                'devices': ['cpu' if DEBUG else 'cuda:13']},
            14: {
                'stage_cls': Partition14,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_14': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 13},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___255': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 7},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]_0': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 13},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]_1': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 13}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_15': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_15': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_15': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 16, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]}},
                'devices': ['cpu' if DEBUG else 'cuda:14']},
            15: {
                'stage_cls': Partition15,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_15': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_15': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_15': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 16, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_16': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [16]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_16': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [16]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_16': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [16]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/Tensor::__add___295': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [16]}},
                'devices': ['cpu' if DEBUG else 'cuda:15']},
            16: {
                'stage_cls': Partition16,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_16': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 15},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_16': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 15},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_16': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 15},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/Tensor::__add___295': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 15}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_17': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [17]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_17': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [17]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_17': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [17]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [17]}},
                'devices': ['cpu' if DEBUG else 'cuda:16']},
            17: {
                'stage_cls': Partition17,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_17': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 16},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_17': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 16},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_17': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 16},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 16}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_18': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [18]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_18': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [18]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_18': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [18]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [18]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/torch.nn.functional::relu_321': {
                        'shape': torch.Size([1, 16, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [18]}},
                'devices': ['cpu' if DEBUG else 'cuda:17']},
            18: {
                'stage_cls': Partition18,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_18': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 17},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_18': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 17},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_18': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 17},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 17},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/torch.nn.functional::relu_321': {
                        'shape': torch.Size([1, 16, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 17}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_19': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [19]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_19': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [19]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_19': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [19]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/Tensor::__add___335': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [19]}},
                'devices': ['cpu' if DEBUG else 'cuda:18']},
            19: {
                'stage_cls': Partition19,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_19': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 18},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_19': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 18},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_19': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 18},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/Tensor::__add___335': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 18}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_20': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [20]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_20': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [20]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_20': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [20]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [20]}},
                'devices': ['cpu' if DEBUG else 'cuda:19']},
            20: {
                'stage_cls': Partition20,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_20': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 19},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_20': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 19},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_20': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 19},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 19}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_21': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [21]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_21': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [21]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_21': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [21]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [21]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 16, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [21]}},
                'devices': ['cpu' if DEBUG else 'cuda:20']},
            21: {
                'stage_cls': Partition21,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_21': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 20},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_21': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 20},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_21': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 20},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 20},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 16, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 20}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_22': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [22]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_22': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [22]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_22': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [22]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [22]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/Dropout[dropout]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [22]}},
                'devices': ['cpu' if DEBUG else 'cuda:21']},
            22: {
                'stage_cls': Partition22,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_22': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 21},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_22': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 21},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_22': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 21},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 21},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/Dropout[dropout]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 21}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_23': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [23]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_23': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [23]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_23': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [23]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [23]}},
                'devices': ['cpu' if DEBUG else 'cuda:22']},
            23: {
                'stage_cls': Partition23,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_23': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 22},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_23': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 22},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_23': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 22},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 22}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_24': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [24]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_24': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [24]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_24': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [24]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [24]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 16, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [24]}},
                'devices': ['cpu' if DEBUG else 'cuda:23']},
            24: {
                'stage_cls': Partition24,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_24': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 23},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_24': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 23},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_24': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 23},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 23},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 16, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 23}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_25': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [25]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_25': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [25]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_25': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [25]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [25]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [25]}},
                'devices': ['cpu' if DEBUG else 'cuda:24']},
            25: {
                'stage_cls': Partition25,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_25': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 24},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_25': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 24},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_25': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 24},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 24},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 24}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_26': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [26]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_26': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [26]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_26': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [26]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [26]}},
                'devices': ['cpu' if DEBUG else 'cuda:25']},
            26: {
                'stage_cls': Partition26,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_26': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 25},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_26': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 25},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_26': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 25},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 25}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_27': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [27]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_27': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [27]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_27': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [27]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [27]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 16, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [27]}},
                'devices': ['cpu' if DEBUG else 'cuda:26']},
            27: {
                'stage_cls': Partition27,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_27': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 26},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_27': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 26},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_27': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 26},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 26},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]': {
                        'shape': torch.Size([1, 16, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 26}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_28': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [28]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_28': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [28]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_28': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [28]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [28]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/Dropout[dropout]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [28]}},
                'devices': ['cpu' if DEBUG else 'cuda:27']},
            28: {
                'stage_cls': Partition28,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_28': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 27},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_28': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 27},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_28': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 27},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 27},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/Dropout[dropout]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 27}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_29': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [29]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_29': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [29]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_29': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [29]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [29]}},
                'devices': ['cpu' if DEBUG else 'cuda:28']},
            29: {
                'stage_cls': Partition29,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_29': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 28},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_29': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 28},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_29': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 28},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 28}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_30': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [30]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_30': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [30]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_30': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [30]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [30]}},
                'devices': ['cpu' if DEBUG else 'cuda:29']},
            30: {
                'stage_cls': Partition30,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_30': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 29},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_30': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 29},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_30': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 29},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 29}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_31': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [31]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_31': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [31]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_31': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [31]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [31]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]': {
                        'shape': torch.Size([1, 16, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [31]}},
                'devices': ['cpu' if DEBUG else 'cuda:30']},
            31: {
                'stage_cls': Partition31,
                'inputs': {
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'inverted_encoder_attention_mask': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'lm_labels': {
                        'shape': torch.Size([1, 16]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_31': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 30},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_31': {
                        'shape': torch.Size([1, 128, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 30},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_31': {
                        'shape': torch.Size([1, 128, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 30},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 30},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]': {
                        'shape': torch.Size([1, 16, 65536]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 30}},
                'outputs': {
                    'T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]': {
                        'shape': torch.Size([1]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [-1]}},
                'devices': ['cpu' if DEBUG else 'cuda:31']}}}
    
    
    # switching batch size
    batch_dim = config['batch_dim']
    for d in chain(config['model_inputs'].values(),config['model_outputs'].values()):
        if d['is_batched']:
            shape = d['shape']
            d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    for s in config['stages'].values():
        for d in chain(s['inputs'].values(),s['outputs'].values()):
            if d['is_batched']:
                shape = d['shape']
                d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    return config

class Partition0(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/StatelessEmbedding[embed_tokens]',
            'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/StatelessEmbedding[embed_tokens]',
            'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]',
        ]
    TENSORS=[
            'T5ForConditionalGeneration/Parameter[shared_embed_weight]',
        ]
    def __init__(self, layers, tensors, device='cuda:0'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.embed_tokens',
                        'l_1': 'encoder.dropout',
                        'l_2': 'encoder.0.0',
                        'l_3': 'encoder.0.1.layer_norm',
                        'l_4': 'encoder.0.1.DenseReluDense.wi',
                        'l_5': 'encoder.0.1.DenseReluDense.dropout',
                        'l_6': 'encoder.0.1.DenseReluDense.wo',
                        'l_7': 'encoder.0.1.dropout',
                        'l_8': 'encoder.1.0',
                        'l_9': 'encoder.1.1.layer_norm',
                        'l_10': 'decoder.embed_tokens',
                        'l_11': 'decoder.dropout',
                        'l_12': 'decoder.0.0',
                        'p_0': 'shared_embed_weight'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/StatelessEmbedding[embed_tokens] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/StatelessEmbedding[embed_tokens] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0] <=> self.l_12
        # T5ForConditionalGeneration/Parameter[shared_embed_weight] <=> self.p_0
        # input0 <=> attention_mask
        # input1 <=> decoder_attention_mask
        # input2 <=> decoder_input_ids
        # input3 <=> input_ids

        # moving inputs to current device no op if already on the correct device
        attention_mask, decoder_attention_mask, decoder_input_ids, input_ids = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = input_ids.size()
        t_0 = t_0[-1]
        t_0 = input_ids.view(-1, t_0)
        t_0 = self.l_0(self.p_0, t_0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0, attention_mask=attention_mask, position_bias=None)
        t_1 = t_0[0]
        t_0 = t_0[1]
        t_2 = self.l_3(t_1)
        t_2 = self.l_4(t_2)
        t_2 = torch.nn.functional.relu(t_2, inplace=False)
        t_2 = self.l_5(t_2)
        t_2 = self.l_6(t_2)
        t_2 = self.l_7(t_2)
        t_2 = t_1 + t_2
        t_0 = (t_2, t_0)
        t_2 = t_0[0]
        t_0 = t_0[1]
        t_2 = self.l_8(t_2, attention_mask=attention_mask, position_bias=t_0)
        t_1 = self.l_9(t_2)
        t_3 = decoder_input_ids.size()
        t_3 = t_3[-1]
        t_3 = decoder_input_ids.view(-1, t_3)
        t_3 = self.l_10(self.p_0, t_3)
        t_3 = self.l_11(t_3)
        t_3 = self.l_12(t_3, attention_mask=decoder_attention_mask, position_bias=None)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5LayerNorm[layer_norm]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]
        return list(flatten((t_0, t_2, t_1, t_3)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition1(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:1'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.1.1.DenseReluDense.wi',
                        'l_1': 'encoder.1.1.DenseReluDense.dropout',
                        'l_2': 'encoder.1.1.DenseReluDense.wo',
                        'l_3': 'encoder.1.1.dropout',
                        'l_4': 'encoder.2.0',
                        'l_5': 'encoder.2.1.layer_norm',
                        'l_6': 'encoder.2.1.DenseReluDense.wi',
                        'l_7': 'encoder.2.1.DenseReluDense.dropout',
                        'l_8': 'encoder.2.1.DenseReluDense.wo',
                        'l_9': 'encoder.2.1.dropout',
                        'l_10': 'encoder.3.0'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/Dropout[dropout] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0] <=> self.l_10
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0] <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> x2

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x2)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = x1 + t_0
        t_0 = self.l_4(t_0, attention_mask=attention_mask, position_bias=x0)
        t_1 = self.l_5(t_0)
        t_1 = self.l_6(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_7(t_1)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_10(t_1, attention_mask=attention_mask, position_bias=x0)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]
        return list(flatten((x0, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition2(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:2'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.3.1.layer_norm',
                        'l_1': 'encoder.3.1.DenseReluDense.wi',
                        'l_2': 'encoder.3.1.DenseReluDense.dropout',
                        'l_3': 'encoder.3.1.DenseReluDense.wo',
                        'l_4': 'encoder.3.1.dropout',
                        'l_5': 'encoder.4.0',
                        'l_6': 'encoder.4.1.layer_norm',
                        'l_7': 'encoder.4.1.DenseReluDense.wi',
                        'l_8': 'encoder.4.1.DenseReluDense.dropout',
                        'l_9': 'encoder.4.1.DenseReluDense.wo',
                        'l_10': 'encoder.4.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/Dropout[dropout] <=> self.l_10
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0] <=> x1

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1)
        t_0 = self.l_1(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x1 + t_0
        t_0 = self.l_5(t_0, attention_mask=attention_mask, position_bias=x0)
        t_1 = self.l_6(t_0)
        t_1 = self.l_7(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = t_0 + t_1
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/Tensor::__add___68
        return list(flatten((x0, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition3(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:3'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.5.0',
                        'l_1': 'encoder.5.1.layer_norm',
                        'l_2': 'encoder.5.1.DenseReluDense.wi',
                        'l_3': 'encoder.5.1.DenseReluDense.dropout',
                        'l_4': 'encoder.5.1.DenseReluDense.wo',
                        'l_5': 'encoder.5.1.dropout',
                        'l_6': 'encoder.6.0',
                        'l_7': 'encoder.6.1.layer_norm',
                        'l_8': 'encoder.6.1.DenseReluDense.wi',
                        'l_9': 'encoder.6.1.DenseReluDense.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/Tensor::__add___68 <=> x1

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1, attention_mask=attention_mask, position_bias=x0)
        t_1 = self.l_1(t_0)
        t_1 = self.l_2(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_6(t_1, attention_mask=attention_mask, position_bias=x0)
        t_0 = self.l_7(t_1)
        t_0 = self.l_8(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_9(t_0)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]
        return list(flatten((x0, t_1, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition4(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:4'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.6.1.DenseReluDense.wo',
                        'l_1': 'encoder.6.1.dropout',
                        'l_2': 'encoder.7.0',
                        'l_3': 'encoder.7.1.layer_norm',
                        'l_4': 'encoder.7.1.DenseReluDense.wi',
                        'l_5': 'encoder.7.1.DenseReluDense.dropout',
                        'l_6': 'encoder.7.1.DenseReluDense.wo',
                        'l_7': 'encoder.7.1.dropout',
                        'l_8': 'encoder.8.0',
                        'l_9': 'encoder.8.1.layer_norm'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_9
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0] <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> x2

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x2)
        t_0 = self.l_1(t_0)
        t_0 = x1 + t_0
        t_0 = self.l_2(t_0, attention_mask=attention_mask, position_bias=x0)
        t_1 = self.l_3(t_0)
        t_1 = self.l_4(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = self.l_7(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_8(t_1, attention_mask=attention_mask, position_bias=x0)
        t_0 = self.l_9(t_1)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5LayerNorm[layer_norm]
        return list(flatten((x0, t_1, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition5(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:5'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.8.1.DenseReluDense.wi',
                        'l_1': 'encoder.8.1.DenseReluDense.dropout',
                        'l_2': 'encoder.8.1.DenseReluDense.wo',
                        'l_3': 'encoder.8.1.dropout',
                        'l_4': 'encoder.9.0',
                        'l_5': 'encoder.9.1.layer_norm',
                        'l_6': 'encoder.9.1.DenseReluDense.wi',
                        'l_7': 'encoder.9.1.DenseReluDense.dropout',
                        'l_8': 'encoder.9.1.DenseReluDense.wo',
                        'l_9': 'encoder.9.1.dropout',
                        'l_10': 'encoder.10.0'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/Dropout[dropout] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0] <=> self.l_10
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0] <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> x2

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x2)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = x1 + t_0
        t_0 = self.l_4(t_0, attention_mask=attention_mask, position_bias=x0)
        t_1 = self.l_5(t_0)
        t_1 = self.l_6(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_7(t_1)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_10(t_1, attention_mask=attention_mask, position_bias=x0)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]
        return list(flatten((x0, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition6(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:6'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.10.1.layer_norm',
                        'l_1': 'encoder.10.1.DenseReluDense.wi',
                        'l_2': 'encoder.10.1.DenseReluDense.dropout',
                        'l_3': 'encoder.10.1.DenseReluDense.wo',
                        'l_4': 'encoder.10.1.dropout',
                        'l_5': 'encoder.11.0',
                        'l_6': 'encoder.11.1.layer_norm',
                        'l_7': 'encoder.11.1.DenseReluDense.wi',
                        'l_8': 'encoder.11.1.DenseReluDense.dropout',
                        'l_9': 'encoder.11.1.DenseReluDense.wo',
                        'l_10': 'encoder.11.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/Dropout[dropout] <=> self.l_10
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0] <=> x1

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1)
        t_0 = self.l_1(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x1 + t_0
        t_0 = self.l_5(t_0, attention_mask=attention_mask, position_bias=x0)
        t_1 = self.l_6(t_0)
        t_1 = self.l_7(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = t_0 + t_1
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/Tensor::__add___131
        return list(flatten((x0, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition7(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:7'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, (1, 1)]
        self.lookup = { 'l_0': 'encoder.12.0',
                        'l_1': 'encoder.12.1.layer_norm',
                        'l_2': 'encoder.12.1.DenseReluDense.wi',
                        'l_3': 'encoder.12.1.DenseReluDense.dropout',
                        'l_4': 'encoder.12.1.DenseReluDense.wo',
                        'l_5': 'encoder.12.1.dropout',
                        'l_6': 'encoder.13.0',
                        'l_7': 'encoder.13.1.layer_norm',
                        'l_8': 'encoder.13.1.DenseReluDense.wi',
                        'l_9': 'encoder.13.1.DenseReluDense.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/Tensor::__add___131 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0] <=> x2

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1, attention_mask=attention_mask, position_bias=x0)
        t_1 = self.l_1(t_0)
        t_1 = self.l_2(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_6(t_1, attention_mask=attention_mask, position_bias=x0)
        t_0 = self.l_7(t_1)
        t_0 = self.l_8(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_9(t_0)
        t_2 = x2[0]
        t_3 = x2[1]
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___253
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___255
        return list(flatten((x0, t_1, t_0, t_2, t_3)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition8(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:8'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.13.1.DenseReluDense.wo',
                        'l_1': 'encoder.13.1.dropout',
                        'l_2': 'encoder.14.0',
                        'l_3': 'encoder.14.1.layer_norm',
                        'l_4': 'encoder.14.1.DenseReluDense.wi',
                        'l_5': 'encoder.14.1.DenseReluDense.dropout',
                        'l_6': 'encoder.14.1.DenseReluDense.wo',
                        'l_7': 'encoder.14.1.dropout',
                        'l_8': 'encoder.15.0',
                        'l_9': 'encoder.15.1.layer_norm',
                        'l_10': 'encoder.15.1.DenseReluDense.wi'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_10
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0] <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> x2

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x2)
        t_0 = self.l_1(t_0)
        t_0 = x1 + t_0
        t_0 = self.l_2(t_0, attention_mask=attention_mask, position_bias=x0)
        t_1 = self.l_3(t_0)
        t_1 = self.l_4(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = self.l_7(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_8(t_1, attention_mask=attention_mask, position_bias=x0)
        t_0 = self.l_9(t_1)
        t_0 = self.l_10(t_0)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]
        return list(flatten((x0, t_1, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition9(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:9'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.15.1.DenseReluDense.dropout',
                        'l_1': 'encoder.15.1.DenseReluDense.wo',
                        'l_2': 'encoder.15.1.dropout',
                        'l_3': 'encoder.16.0',
                        'l_4': 'encoder.16.1.layer_norm',
                        'l_5': 'encoder.16.1.DenseReluDense.wi',
                        'l_6': 'encoder.16.1.DenseReluDense.dropout',
                        'l_7': 'encoder.16.1.DenseReluDense.wo',
                        'l_8': 'encoder.16.1.dropout',
                        'l_9': 'encoder.17.0'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0] <=> self.l_9
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0] <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> x2

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = torch.nn.functional.relu(x2, inplace=False)
        t_0 = self.l_0(t_0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = x1 + t_0
        t_0 = self.l_3(t_0, attention_mask=attention_mask, position_bias=x0)
        t_1 = self.l_4(t_0)
        t_1 = self.l_5(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_6(t_1)
        t_1 = self.l_7(t_1)
        t_1 = self.l_8(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_9(t_1, attention_mask=attention_mask, position_bias=x0)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]
        return list(flatten((x0, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition10(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:10'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.17.1.layer_norm',
                        'l_1': 'encoder.17.1.DenseReluDense.wi',
                        'l_2': 'encoder.17.1.DenseReluDense.dropout',
                        'l_3': 'encoder.17.1.DenseReluDense.wo',
                        'l_4': 'encoder.17.1.dropout',
                        'l_5': 'encoder.18.0',
                        'l_6': 'encoder.18.1.layer_norm',
                        'l_7': 'encoder.18.1.DenseReluDense.wi',
                        'l_8': 'encoder.18.1.DenseReluDense.dropout',
                        'l_9': 'encoder.18.1.DenseReluDense.wo',
                        'l_10': 'encoder.18.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/Dropout[dropout] <=> self.l_10
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0] <=> x1

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1)
        t_0 = self.l_1(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x1 + t_0
        t_0 = self.l_5(t_0, attention_mask=attention_mask, position_bias=x0)
        t_1 = self.l_6(t_0)
        t_1 = self.l_7(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = t_0 + t_1
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/Tensor::__add___194
        return list(flatten((x0, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition11(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:11'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'encoder.19.0',
                        'l_1': 'encoder.19.1.layer_norm',
                        'l_2': 'encoder.19.1.DenseReluDense.wi',
                        'l_3': 'encoder.19.1.DenseReluDense.dropout',
                        'l_4': 'encoder.19.1.DenseReluDense.wo',
                        'l_5': 'encoder.19.1.dropout',
                        'l_6': 'encoder.20.0',
                        'l_7': 'encoder.20.1.layer_norm',
                        'l_8': 'encoder.20.1.DenseReluDense.wi',
                        'l_9': 'encoder.20.1.DenseReluDense.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/Tensor::__add___194 <=> x1

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1, attention_mask=attention_mask, position_bias=x0)
        t_1 = self.l_1(t_0)
        t_1 = self.l_2(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_6(t_1, attention_mask=attention_mask, position_bias=x0)
        t_0 = self.l_7(t_1)
        t_0 = self.l_8(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_9(t_0)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]
        return list(flatten((x0, t_1, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition12(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:12'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.20.1.DenseReluDense.wo',
                        'l_1': 'encoder.20.1.dropout',
                        'l_2': 'encoder.21.0',
                        'l_3': 'encoder.21.1.layer_norm',
                        'l_4': 'encoder.21.1.DenseReluDense.wi',
                        'l_5': 'encoder.21.1.DenseReluDense.dropout',
                        'l_6': 'encoder.21.1.DenseReluDense.wo',
                        'l_7': 'encoder.21.1.dropout',
                        'l_8': 'encoder.22.0',
                        'l_9': 'encoder.22.1.layer_norm',
                        'l_10': 'encoder.22.1.DenseReluDense.wi'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_10
        # input0 <=> attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0] <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> x2

        # moving inputs to current device no op if already on the correct device
        attention_mask, x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x2)
        t_0 = self.l_1(t_0)
        t_0 = x1 + t_0
        t_0 = self.l_2(t_0, attention_mask=attention_mask, position_bias=x0)
        t_1 = self.l_3(t_0)
        t_1 = self.l_4(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = self.l_7(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_8(t_1, attention_mask=attention_mask, position_bias=x0)
        t_0 = self.l_9(t_1)
        t_0 = self.l_10(t_0)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]
        return list(flatten((x0, t_1, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition13(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5LayerNorm[final_layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:13'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'encoder.22.1.DenseReluDense.dropout',
                        'l_1': 'encoder.22.1.DenseReluDense.wo',
                        'l_2': 'encoder.22.1.dropout',
                        'l_3': 'encoder.23.0',
                        'l_4': 'encoder.23.1.layer_norm',
                        'l_5': 'encoder.23.1.DenseReluDense.wi',
                        'l_6': 'encoder.23.1.DenseReluDense.dropout',
                        'l_7': 'encoder.23.1.DenseReluDense.wo',
                        'l_8': 'encoder.23.1.dropout',
                        'l_9': 'encoder.final_layer_norm',
                        'l_10': 'encoder.dropout',
                        'l_11': 'decoder.0.1'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5LayerNorm[final_layer_norm] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1] <=> self.l_11
        # input0 <=> attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0] <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___253 <=> x3

        # moving inputs to current device no op if already on the correct device
        attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = torch.nn.functional.relu(x2, inplace=False)
        t_0 = self.l_0(t_0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = x1 + t_0
        t_0 = self.l_3(t_0, attention_mask=attention_mask, position_bias=x0)
        t_1 = self.l_4(t_0)
        t_1 = self.l_5(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_6(t_1)
        t_1 = self.l_7(t_1)
        t_1 = self.l_8(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_0 = self.l_11(x3, kv=t_1, attention_mask=inverted_encoder_attention_mask, position_bias=None)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]
        return list(flatten((t_1, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition14(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:14'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, (1, 1)]
        self.lookup = { 'l_0': 'decoder.0.2.layer_norm',
                        'l_1': 'decoder.0.2.DenseReluDense.wi',
                        'l_2': 'decoder.0.2.DenseReluDense.dropout',
                        'l_3': 'decoder.0.2.DenseReluDense.wo',
                        'l_4': 'decoder.0.2.dropout',
                        'l_5': 'decoder.1.0',
                        'l_6': 'decoder.1.1',
                        'l_7': 'decoder.1.2.layer_norm',
                        'l_8': 'decoder.1.2.DenseReluDense.wi'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___255 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1] <=> x2

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = x2[0]
        t_1 = x2[1]
        t_2 = self.l_0(t_0)
        t_2 = self.l_1(t_2)
        t_2 = torch.nn.functional.relu(t_2, inplace=False)
        t_2 = self.l_2(t_2)
        t_2 = self.l_3(t_2)
        t_2 = self.l_4(t_2)
        t_2 = t_0 + t_2
        t_1 = (t_2, x1, t_1)
        t_2 = t_1[0]
        t_0 = t_1[1]
        t_1 = t_1[2]
        t_2 = self.l_5(t_2, attention_mask=decoder_attention_mask, position_bias=t_0)
        t_2 = self.l_6(t_2, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=t_1)
        t_3 = self.l_7(t_2)
        t_3 = self.l_8(t_3)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]
        return list(flatten((x0, t_0, t_1, t_2, t_3)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition15(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:15'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.1.2.DenseReluDense.dropout',
                        'l_1': 'decoder.1.2.DenseReluDense.wo',
                        'l_2': 'decoder.1.2.dropout',
                        'l_3': 'decoder.2.0',
                        'l_4': 'decoder.2.1',
                        'l_5': 'decoder.2.2.layer_norm',
                        'l_6': 'decoder.2.2.DenseReluDense.wi',
                        'l_7': 'decoder.2.2.DenseReluDense.dropout',
                        'l_8': 'decoder.2.2.DenseReluDense.wo',
                        'l_9': 'decoder.2.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/Dropout[dropout] <=> self.l_9
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1] <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> x4

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = torch.nn.functional.relu(x4, inplace=False)
        t_0 = self.l_0(t_0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = x3 + t_0
        t_0 = self.l_3(t_0, attention_mask=decoder_attention_mask, position_bias=x1)
        t_0 = self.l_4(t_0, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_5(t_0)
        t_1 = self.l_6(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_7(t_1)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        t_1 = t_0 + t_1
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/Tensor::__add___295
        return list(flatten((x0, x1, x2, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition16(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:16'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.3.0',
                        'l_1': 'decoder.3.1',
                        'l_2': 'decoder.3.2.layer_norm',
                        'l_3': 'decoder.3.2.DenseReluDense.wi',
                        'l_4': 'decoder.3.2.DenseReluDense.dropout',
                        'l_5': 'decoder.3.2.DenseReluDense.wo',
                        'l_6': 'decoder.3.2.dropout',
                        'l_7': 'decoder.4.0',
                        'l_8': 'decoder.4.1'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1] <=> self.l_8
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/Tensor::__add___295 <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, attention_mask=decoder_attention_mask, position_bias=x1)
        t_0 = self.l_1(t_0, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_2(t_0)
        t_1 = self.l_3(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_7(t_1, attention_mask=decoder_attention_mask, position_bias=x1)
        t_1 = self.l_8(t_1, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]
        return list(flatten((x0, x1, x2, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition17(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:17'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.4.2.layer_norm',
                        'l_1': 'decoder.4.2.DenseReluDense.wi',
                        'l_2': 'decoder.4.2.DenseReluDense.dropout',
                        'l_3': 'decoder.4.2.DenseReluDense.wo',
                        'l_4': 'decoder.4.2.dropout',
                        'l_5': 'decoder.5.0',
                        'l_6': 'decoder.5.1',
                        'l_7': 'decoder.5.2.layer_norm',
                        'l_8': 'decoder.5.2.DenseReluDense.wi'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3)
        t_0 = self.l_1(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x3 + t_0
        t_0 = self.l_5(t_0, attention_mask=decoder_attention_mask, position_bias=x1)
        t_0 = self.l_6(t_0, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_7(t_0)
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/torch.nn.functional::relu_321
        return list(flatten((x0, x1, x2, t_0, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition18(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:18'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.5.2.DenseReluDense.dropout',
                        'l_1': 'decoder.5.2.DenseReluDense.wo',
                        'l_2': 'decoder.5.2.dropout',
                        'l_3': 'decoder.6.0',
                        'l_4': 'decoder.6.1',
                        'l_5': 'decoder.6.2.layer_norm',
                        'l_6': 'decoder.6.2.DenseReluDense.wi',
                        'l_7': 'decoder.6.2.DenseReluDense.dropout',
                        'l_8': 'decoder.6.2.DenseReluDense.wo',
                        'l_9': 'decoder.6.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/Dropout[dropout] <=> self.l_9
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1] <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/torch.nn.functional::relu_321 <=> x4

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x4)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = x3 + t_0
        t_0 = self.l_3(t_0, attention_mask=decoder_attention_mask, position_bias=x1)
        t_0 = self.l_4(t_0, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_5(t_0)
        t_1 = self.l_6(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_7(t_1)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        t_1 = t_0 + t_1
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/Tensor::__add___335
        return list(flatten((x0, x1, x2, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition19(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:19'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.7.0',
                        'l_1': 'decoder.7.1',
                        'l_2': 'decoder.7.2.layer_norm',
                        'l_3': 'decoder.7.2.DenseReluDense.wi',
                        'l_4': 'decoder.7.2.DenseReluDense.dropout',
                        'l_5': 'decoder.7.2.DenseReluDense.wo',
                        'l_6': 'decoder.7.2.dropout',
                        'l_7': 'decoder.8.0',
                        'l_8': 'decoder.8.1'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1] <=> self.l_8
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/Tensor::__add___335 <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, attention_mask=decoder_attention_mask, position_bias=x1)
        t_0 = self.l_1(t_0, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_2(t_0)
        t_1 = self.l_3(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_7(t_1, attention_mask=decoder_attention_mask, position_bias=x1)
        t_1 = self.l_8(t_1, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]
        return list(flatten((x0, x1, x2, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition20(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:20'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.8.2.layer_norm',
                        'l_1': 'decoder.8.2.DenseReluDense.wi',
                        'l_2': 'decoder.8.2.DenseReluDense.dropout',
                        'l_3': 'decoder.8.2.DenseReluDense.wo',
                        'l_4': 'decoder.8.2.dropout',
                        'l_5': 'decoder.9.0',
                        'l_6': 'decoder.9.1',
                        'l_7': 'decoder.9.2.layer_norm',
                        'l_8': 'decoder.9.2.DenseReluDense.wi'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3)
        t_0 = self.l_1(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x3 + t_0
        t_0 = self.l_5(t_0, attention_mask=decoder_attention_mask, position_bias=x1)
        t_0 = self.l_6(t_0, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_7(t_0)
        t_1 = self.l_8(t_1)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]
        return list(flatten((x0, x1, x2, t_0, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition21(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:21'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.9.2.DenseReluDense.dropout',
                        'l_1': 'decoder.9.2.DenseReluDense.wo',
                        'l_2': 'decoder.9.2.dropout',
                        'l_3': 'decoder.10.0',
                        'l_4': 'decoder.10.1',
                        'l_5': 'decoder.10.2.layer_norm',
                        'l_6': 'decoder.10.2.DenseReluDense.wi',
                        'l_7': 'decoder.10.2.DenseReluDense.dropout',
                        'l_8': 'decoder.10.2.DenseReluDense.wo',
                        'l_9': 'decoder.10.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/Dropout[dropout] <=> self.l_9
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1] <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> x4

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = torch.nn.functional.relu(x4, inplace=False)
        t_0 = self.l_0(t_0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = x3 + t_0
        t_0 = self.l_3(t_0, attention_mask=decoder_attention_mask, position_bias=x1)
        t_0 = self.l_4(t_0, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_5(t_0)
        t_1 = self.l_6(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_7(t_1)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/Dropout[dropout]
        return list(flatten((x0, x1, x2, t_0, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition22(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:22'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.11.0',
                        'l_1': 'decoder.11.1',
                        'l_2': 'decoder.11.2.layer_norm',
                        'l_3': 'decoder.11.2.DenseReluDense.wi',
                        'l_4': 'decoder.11.2.DenseReluDense.dropout',
                        'l_5': 'decoder.11.2.DenseReluDense.wo',
                        'l_6': 'decoder.11.2.dropout',
                        'l_7': 'decoder.12.0'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0] <=> self.l_7
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1] <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/Dropout[dropout] <=> x4

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = x3 + x4
        t_0 = self.l_0(t_0, attention_mask=decoder_attention_mask, position_bias=x1)
        t_0 = self.l_1(t_0, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_2(t_0)
        t_1 = self.l_3(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_7(t_1, attention_mask=decoder_attention_mask, position_bias=x1)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]
        return list(flatten((x0, x1, x2, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition23(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:23'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.12.1',
                        'l_1': 'decoder.12.2.layer_norm',
                        'l_2': 'decoder.12.2.DenseReluDense.wi',
                        'l_3': 'decoder.12.2.DenseReluDense.dropout',
                        'l_4': 'decoder.12.2.DenseReluDense.wo',
                        'l_5': 'decoder.12.2.dropout',
                        'l_6': 'decoder.13.0',
                        'l_7': 'decoder.13.1',
                        'l_8': 'decoder.13.2.layer_norm',
                        'l_9': 'decoder.13.2.DenseReluDense.wi'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_9
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_1(t_0)
        t_1 = self.l_2(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_6(t_1, attention_mask=decoder_attention_mask, position_bias=x1)
        t_1 = self.l_7(t_1, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_0 = self.l_8(t_1)
        t_0 = self.l_9(t_0)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]
        return list(flatten((x0, x1, x2, t_1, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition24(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:24'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.13.2.DenseReluDense.dropout',
                        'l_1': 'decoder.13.2.DenseReluDense.wo',
                        'l_2': 'decoder.13.2.dropout',
                        'l_3': 'decoder.14.0',
                        'l_4': 'decoder.14.1',
                        'l_5': 'decoder.14.2.layer_norm',
                        'l_6': 'decoder.14.2.DenseReluDense.wi',
                        'l_7': 'decoder.14.2.DenseReluDense.dropout',
                        'l_8': 'decoder.14.2.DenseReluDense.wo'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_8
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1] <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> x4

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = torch.nn.functional.relu(x4, inplace=False)
        t_0 = self.l_0(t_0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = x3 + t_0
        t_0 = self.l_3(t_0, attention_mask=decoder_attention_mask, position_bias=x1)
        t_0 = self.l_4(t_0, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_5(t_0)
        t_1 = self.l_6(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_7(t_1)
        t_1 = self.l_8(t_1)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]
        return list(flatten((x0, x1, x2, t_0, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition25(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:25'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.14.2.dropout',
                        'l_1': 'decoder.15.0',
                        'l_2': 'decoder.15.1',
                        'l_3': 'decoder.15.2.layer_norm',
                        'l_4': 'decoder.15.2.DenseReluDense.wi',
                        'l_5': 'decoder.15.2.DenseReluDense.dropout',
                        'l_6': 'decoder.15.2.DenseReluDense.wo',
                        'l_7': 'decoder.15.2.dropout',
                        'l_8': 'decoder.16.0'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/Dropout[dropout] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0] <=> self.l_8
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1] <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> x4

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x4)
        t_0 = x3 + t_0
        t_0 = self.l_1(t_0, attention_mask=decoder_attention_mask, position_bias=x1)
        t_0 = self.l_2(t_0, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_3(t_0)
        t_1 = self.l_4(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = self.l_7(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_8(t_1, attention_mask=decoder_attention_mask, position_bias=x1)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]
        return list(flatten((x0, x1, x2, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition26(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:26'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.16.1',
                        'l_1': 'decoder.16.2.layer_norm',
                        'l_2': 'decoder.16.2.DenseReluDense.wi',
                        'l_3': 'decoder.16.2.DenseReluDense.dropout',
                        'l_4': 'decoder.16.2.DenseReluDense.wo',
                        'l_5': 'decoder.16.2.dropout',
                        'l_6': 'decoder.17.0',
                        'l_7': 'decoder.17.1',
                        'l_8': 'decoder.17.2.layer_norm',
                        'l_9': 'decoder.17.2.DenseReluDense.wi'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_9
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_1(t_0)
        t_1 = self.l_2(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_6(t_1, attention_mask=decoder_attention_mask, position_bias=x1)
        t_1 = self.l_7(t_1, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_0 = self.l_8(t_1)
        t_0 = self.l_9(t_0)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]
        return list(flatten((x0, x1, x2, t_1, t_0)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition27(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:27'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.17.2.DenseReluDense.dropout',
                        'l_1': 'decoder.17.2.DenseReluDense.wo',
                        'l_2': 'decoder.17.2.dropout',
                        'l_3': 'decoder.18.0',
                        'l_4': 'decoder.18.1',
                        'l_5': 'decoder.18.2.layer_norm',
                        'l_6': 'decoder.18.2.DenseReluDense.wi',
                        'l_7': 'decoder.18.2.DenseReluDense.dropout',
                        'l_8': 'decoder.18.2.DenseReluDense.wo',
                        'l_9': 'decoder.18.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/Dropout[dropout] <=> self.l_9
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1] <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> x4

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = torch.nn.functional.relu(x4, inplace=False)
        t_0 = self.l_0(t_0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = x3 + t_0
        t_0 = self.l_3(t_0, attention_mask=decoder_attention_mask, position_bias=x1)
        t_0 = self.l_4(t_0, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_5(t_0)
        t_1 = self.l_6(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_7(t_1)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/Dropout[dropout]
        return list(flatten((x0, x1, x2, t_0, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition28(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:28'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.19.0',
                        'l_1': 'decoder.19.1',
                        'l_2': 'decoder.19.2.layer_norm',
                        'l_3': 'decoder.19.2.DenseReluDense.wi',
                        'l_4': 'decoder.19.2.DenseReluDense.dropout',
                        'l_5': 'decoder.19.2.DenseReluDense.wo',
                        'l_6': 'decoder.19.2.dropout',
                        'l_7': 'decoder.20.0'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0] <=> self.l_7
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1] <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/Dropout[dropout] <=> x4

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = x3 + x4
        t_0 = self.l_0(t_0, attention_mask=decoder_attention_mask, position_bias=x1)
        t_0 = self.l_1(t_0, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_2(t_0)
        t_1 = self.l_3(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_7(t_1, attention_mask=decoder_attention_mask, position_bias=x1)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]
        return list(flatten((x0, x1, x2, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition29(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:29'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.20.1',
                        'l_1': 'decoder.20.2.layer_norm',
                        'l_2': 'decoder.20.2.DenseReluDense.wi',
                        'l_3': 'decoder.20.2.DenseReluDense.dropout',
                        'l_4': 'decoder.20.2.DenseReluDense.wo',
                        'l_5': 'decoder.20.2.dropout',
                        'l_6': 'decoder.21.0',
                        'l_7': 'decoder.21.1'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1] <=> self.l_7
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_1(t_0)
        t_1 = self.l_2(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_6(t_1, attention_mask=decoder_attention_mask, position_bias=x1)
        t_1 = self.l_7(t_1, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]
        return list(flatten((x0, x1, x2, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition30(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:30'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.21.2.layer_norm',
                        'l_1': 'decoder.21.2.DenseReluDense.wi',
                        'l_2': 'decoder.21.2.DenseReluDense.dropout',
                        'l_3': 'decoder.21.2.DenseReluDense.wo',
                        'l_4': 'decoder.21.2.dropout',
                        'l_5': 'decoder.22.0',
                        'l_6': 'decoder.22.1',
                        'l_7': 'decoder.22.2.layer_norm',
                        'l_8': 'decoder.22.2.DenseReluDense.wi',
                        'l_9': 'decoder.22.2.DenseReluDense.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1] <=> x3

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x3)
        t_0 = self.l_1(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x3 + t_0
        t_0 = self.l_5(t_0, attention_mask=decoder_attention_mask, position_bias=x1)
        t_0 = self.l_6(t_0, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_7(t_0)
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]
        return list(flatten((x0, x1, x2, t_0, t_1)))

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition31(nn.Module):
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5LayerNorm[final_layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/Linear[lm_head]',
            'T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:31'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.22.2.DenseReluDense.wo',
                        'l_1': 'decoder.22.2.dropout',
                        'l_2': 'decoder.23.0',
                        'l_3': 'decoder.23.1',
                        'l_4': 'decoder.23.2.layer_norm',
                        'l_5': 'decoder.23.2.DenseReluDense.wi',
                        'l_6': 'decoder.23.2.DenseReluDense.dropout',
                        'l_7': 'decoder.23.2.DenseReluDense.wo',
                        'l_8': 'decoder.23.2.dropout',
                        'l_9': 'decoder.final_layer_norm',
                        'l_10': 'decoder.dropout',
                        'l_11': 'lm_head',
                        'l_12': 'lm_loss'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5LayerNorm[final_layer_norm] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout] <=> self.l_10
        # T5ForConditionalGeneration/Linear[lm_head] <=> self.l_11
        # T5ForConditionalGeneration/CrossEntropyLoss[lm_loss] <=> self.l_12
        # input1 <=> decoder_attention_mask
        # input4 <=> inverted_encoder_attention_mask
        # input5 <=> lm_labels
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1] <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> x4

        # moving inputs to current device no op if already on the correct device
        decoder_attention_mask, inverted_encoder_attention_mask, lm_labels, x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x4)
        t_0 = self.l_1(t_0)
        t_0 = x3 + t_0
        t_0 = self.l_2(t_0, attention_mask=decoder_attention_mask, position_bias=x1)
        t_0 = self.l_3(t_0, kv=x0, attention_mask=inverted_encoder_attention_mask, position_bias=x2)
        t_1 = self.l_4(t_0)
        t_1 = self.l_5(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_6(t_1)
        t_1 = self.l_7(t_1)
        t_1 = self.l_8(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = t_1 * 0.03125
        t_1 = self.l_11(t_1)
        t_0 = t_1.size(-1)
        t_0 = t_1.view(-1, t_0)
        t_1 = lm_labels.view(-1)
        t_1 = self.l_12(t_0, t_1)
        # returning:
        # T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]
        return (t_1,)

    def state_dict(self,*args,**kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,*args,**kwargs)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


def traverse_model(module: nn.Module, depth: int, prefix: Optional[str] = None,
                   basic_blocks: Tuple[nn.Module] = (), full: bool = False) -> Iterator[Tuple[nn.Module, str, nn.Module]]:
    '''
    iterate over model layers yielding the layer,layer_scope,encasing_module
    Parameters:
    -----------
    model:
        the model to iterate over
    depth:
        how far down in the model tree to go
    basic_blocks:
        a list of modules that if encountered will not be broken down
    full:
        whether to yield only layers specified by the depth and basick_block options or to yield all layers
    '''
    if prefix is None:
        prefix = type(module).__name__

    for name, sub_module in module.named_children():
        scope = prefix + "/" + type(sub_module).__name__ + f"[{name}]"
        if len(list(sub_module.children())) == 0 or isinstance(sub_module, tuple(basic_blocks)) or depth == 0:
            if full:
                yield sub_module, scope, module, True
            else:
                yield sub_module, scope, module
        else:
            if full:
                yield sub_module, scope, module, False
            yield from traverse_model(sub_module, depth - 1, scope, basic_blocks, full)


def layerDict(model: nn.Module, depth=1000, basic_blocks=()) -> Dict[str, nn.Module]:
    return {s: l for l, s, _ in traverse_model(model, depth, basic_blocks=basic_blocks)}


def traverse_params_buffs(module: nn.Module, prefix: Optional[str] = None) -> Iterator[Tuple[torch.tensor, str]]:
    '''
    iterate over model's buffers and parameters yielding obj,obj_scope

    Parameters:
    -----------
    model:
        the model to iterate over
    '''
    if prefix is None:
        prefix = type(module).__name__

    # params
    for param_name, param in module.named_parameters(recurse=False):
        param_scope = f"{prefix}/{type(param).__name__}[{param_name}]"
        yield param, param_scope

    # buffs
    for buffer_name, buffer in module.named_buffers(recurse=False):
        buffer_scope = f"{prefix}/{type(buffer).__name__}[{buffer_name}]"
        yield buffer, buffer_scope

    # recurse
    for name, sub_module in module.named_children():
        yield from traverse_params_buffs(sub_module, prefix + "/" + type(sub_module).__name__ + f"[{name}]")


def tensorDict(model: nn.Module) -> OrderedDict[str, Tensor]:
    return collections.OrderedDict((s, t)for t, s in traverse_params_buffs(model))


def move_tensors(ts, device):
    def move(t):
        if isinstance(t, (nn.Module, Tensor)):
            return t.to(device)
        return t

    return nested_map(move, ts)


def nested_map(func, ts,full=False):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        return func(ts)
    elif isinstance(ts, (list, tuple, set)):
        return type(ts)(nested_map(func, t,full=full) for t in ts)
    elif isinstance(ts, dict):
        return {k: nested_map(func, v,full=full) for k, v in ts.items()}
    elif isinstance(ts, slice) and full:
        start = nested_map(func, ts.start,full=full)
        stop = nested_map(func, ts.stop,full=full)
        step = nested_map(func, ts.step,full=full)
        return slice(start, stop, step)
    return func(ts)


def flatten(ts):
    if isinstance(ts,torch.Size):
        # size is inheriting from tuple which is stupid
        yield ts
    elif isinstance(ts, (list, tuple, set)):
        yield from chain(*[flatten(t) for t in ts])
    elif isinstance(ts, dict):
        yield from chain(*[flatten(t) for k,t in sorted(ts.items(),key=lambda t:t[0])])
    else:
        yield ts


def unflatten(xs,structure):
    return _unflatten(xs,structure)[0]


def _unflatten(xs,structure):
    if isinstance(structure,torch.Size):
        #torch.Size is subclass of tuple which is stupid
        return xs[0],1

    if not isinstance(structure,(list,tuple,set,dict)):
        return xs[0],1
    
    if isinstance(structure,(list,tuple,set)):
        offset=0
        elements = []
        for s in structure:
            e,n = _unflatten(xs[offset:],s)
            elements.append(e)
            offset += n
        
        return type(structure)(elements),offset
    
    assert isinstance(structure,dict)
    offset = 0
    elements = dict()
    for k,v in sorted(structure.items(),key=lambda t: t[0]):
        e,n = _unflatten(xs[offset:],v)
        elements[k] = e
        offset += n
    
    return elements,offset


def state_dict(partition, *args,**kwargs):
    # we return the state dict of this part as it should be in the original model
    state = nn.Module.state_dict(partition,*args,**kwargs)
    lookup = partition.lookup
    result = dict()
    for k, v in state.items():
        if k in lookup:
            result[lookup[k]] = v
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            result[new_k] = v
    return result


def load_state_dict(partition, state):
    reverse_lookup = {v: k for k, v in partition.lookup.items()}
    device = partition.device
    keys = list(partition.state_dict(None).keys())
    new_state = dict()
    for k in keys:
        if k in reverse_lookup:
            new_state[reverse_lookup[k]] = state[k].to(device)
            continue
        idx = k.rfind(".")
        to_replace = k[:idx]
        if to_replace in reverse_lookup:
            key = reverse_lookup[to_replace] + k[idx:]
            new_state[key] = state[k].to(device)
    nn.Module.load_state_dict(partition, new_state, strict=True)


def named_buffers(partition, recurse=True):
    # we return the named buffers of this part as it should be in the original model
    params = nn.Module.named_buffers(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def named_parameters(partition, recurse=True):
    # we return the named parameters of this part as it should be in the original model
    params = nn.Module.named_parameters(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def cpu(partition):
    partition.device = torch.device('cpu')
    return nn.Module.cpu(partition)


def cuda(partition, device=None):
    if device is None:
        device = torch.cuda.current_device()
    partition.device = torch.device(device)
    return nn.Module.cuda(partition, partition.device)


def to(partition, *args, **kwargs):
    device = None
    if 'device' in kwargs:
        device = kwargs['device']
    elif 'tensor' in kwargs:
        device = kwargs['tensor'].device
    if args:
        if isinstance(args[0], (torch.device, int, str)):
            device = args[0]
        if torch.is_tensor(args[0]):
            device = args[0].device
    if not (device is None):
        partition.device = torch.device(device)
    return nn.Module.to(partition, *args, **kwargs)

model_args = {'model_name_or_path': 't5-11b', 'max_seq_length': 128, 'answer_max_seq_length': 16, 'stateless_tied': True, 'lmhead': True, 'precompute_masks': True}
"""analysis summary
-I- Printing Report
warnings:
Partition0 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_1 is not contiguous!
Partition0 output:T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]_1 is not contiguous!
Partition1 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_2 is not contiguous!
Partition2 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_3 is not contiguous!
Partition3 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_4 is not contiguous!
Partition4 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_5 is not contiguous!
Partition5 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_6 is not contiguous!
Partition6 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_7 is not contiguous!
Partition7 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_8 is not contiguous!
Partition7 output:T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___255 is not contiguous!
Partition8 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_9 is not contiguous!
Partition9 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_10 is not contiguous!
Partition10 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_11 is not contiguous!
Partition11 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_12 is not contiguous!
Partition12 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___32_13 is not contiguous!
Partition13 output:T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]_1 is not contiguous!
Partition14 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_15 is not contiguous!
Partition14 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_15 is not contiguous!
Partition15 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_16 is not contiguous!
Partition15 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_16 is not contiguous!
Partition16 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_17 is not contiguous!
Partition16 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_17 is not contiguous!
Partition17 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_18 is not contiguous!
Partition17 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_18 is not contiguous!
Partition18 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_19 is not contiguous!
Partition18 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_19 is not contiguous!
Partition19 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_20 is not contiguous!
Partition19 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_20 is not contiguous!
Partition20 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_21 is not contiguous!
Partition20 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_21 is not contiguous!
Partition21 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_22 is not contiguous!
Partition21 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_22 is not contiguous!
Partition22 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_23 is not contiguous!
Partition22 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_23 is not contiguous!
Partition23 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_24 is not contiguous!
Partition23 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_24 is not contiguous!
Partition24 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_25 is not contiguous!
Partition24 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_25 is not contiguous!
Partition25 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_26 is not contiguous!
Partition25 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_26 is not contiguous!
Partition26 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_27 is not contiguous!
Partition26 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_27 is not contiguous!
Partition27 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_28 is not contiguous!
Partition27 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_28 is not contiguous!
Partition28 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_29 is not contiguous!
Partition28 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_29 is not contiguous!
Partition29 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_30 is not contiguous!
Partition29 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_30 is not contiguous!
Partition30 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___273_31 is not contiguous!
Partition30 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___275_31 is not contiguous!
Number of stages: 32
cutting edges are edges between partitions
number of cutting edges: 164

backward times include recomputation

Stage parameter count:
 {0: 368456704,
 1: 402656256,
 2: 335547392,
 3: 335548416,
 4: 335548416,
 5: 402656256,
 6: 335547392,
 7: 335548416,
 8: 402657280,
 9: 335547392,
 10: 335547392,
 11: 335548416,
 12: 402657280,
 13: 335552512,
 14: 335548416,
 15: 335547392,
 16: 402658304,
 17: 335548416,
 18: 335547392,
 19: 402658304,
 20: 335548416,
 21: 335547392,
 22: 335548416,
 23: 402658304,
 24: 335547392,
 25: 335548416,
 26: 402658304,
 27: 335547392,
 28: 335548416,
 29: 335548416,
 30: 335548416,
 31: 368447488,
 'total': 11340224512}

real times are based on real measurements of execution time (with communication) of generated partitions ms
forward {0: 14.45, 1: 15.89, 2: 13.1, 3: 16.91, 4: 14.09, 5: 15.75, 6: 13.1, 7: 17.08, 8: 18.72, 9: 14.15, 10: 13.19, 11: 17.04, 12: 18.4, 13: 12.61, 14: 6.25, 15: 6.05, 16: 7.99, 17: 6.3, 18: 6.02, 19: 8.07, 20: 6.21, 21: 6.05, 22: 6.09, 23: 8.12, 24: 6.01, 25: 6.16, 26: 8.32, 27: 6.05, 28: 6.21, 29: 6.99, 30: 6.29, 31: 6.29}
backward {0: 37.21, 1: 41.21, 2: 33.58, 3: 36.59, 4: 39.26, 5: 41.16, 6: 32.98, 7: 36.9, 8: 44.52, 9: 39.08, 10: 33.74, 11: 36.95, 12: 43.92, 13: 37.39, 14: 15.44, 15: 16.04, 16: 21.21, 17: 15.6, 18: 15.97, 19: 21.24, 20: 15.43, 21: 16.05, 22: 15.86, 23: 20.58, 24: 15.95, 25: 15.89, 26: 20.76, 27: 16.01, 28: 16.12, 29: 18.46, 30: 15.58, 31: 17.27}

Assuming bandwidth of 12 GBps between GPUs

communication volumes size of activations of each partition
0: input size:'0.00 MB', recieve_time:'0.00 ms', out:'9.63 MB', send time:'0.80 ms'
1: input size:'9.44 MB', recieve_time:'0.79 ms', out:'8.91 MB', send time:'0.74 ms'
2: input size:'8.91 MB', recieve_time:'0.74 ms', out:'8.91 MB', send time:'0.74 ms'
3: input size:'8.91 MB', recieve_time:'0.74 ms', out:'42.47 MB', send time:'3.54 ms'
4: input size:'42.47 MB', recieve_time:'3.54 ms', out:'9.44 MB', send time:'0.79 ms'
5: input size:'9.44 MB', recieve_time:'0.79 ms', out:'8.91 MB', send time:'0.74 ms'
6: input size:'8.91 MB', recieve_time:'0.74 ms', out:'8.91 MB', send time:'0.74 ms'
7: input size:'9.11 MB', recieve_time:'0.76 ms', out:'42.66 MB', send time:'3.56 ms'
8: input size:'42.47 MB', recieve_time:'3.54 ms', out:'42.47 MB', send time:'3.54 ms'
9: input size:'42.47 MB', recieve_time:'3.54 ms', out:'8.91 MB', send time:'0.74 ms'
10: input size:'8.91 MB', recieve_time:'0.74 ms', out:'8.91 MB', send time:'0.74 ms'
11: input size:'8.91 MB', recieve_time:'0.74 ms', out:'42.47 MB', send time:'3.54 ms'
12: input size:'42.47 MB', recieve_time:'3.54 ms', out:'42.47 MB', send time:'3.54 ms'
13: input size:'42.53 MB', recieve_time:'3.54 ms', out:'1.64 MB', send time:'0.14 ms'
14: input size:'1.77 MB', recieve_time:'0.15 ms', out:'5.96 MB', send time:'0.50 ms'
15: input size:'5.97 MB', recieve_time:'0.50 ms', out:'1.77 MB', send time:'0.15 ms'
16: input size:'1.77 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
17: input size:'1.77 MB', recieve_time:'0.15 ms', out:'5.96 MB', send time:'0.50 ms'
18: input size:'5.97 MB', recieve_time:'0.50 ms', out:'1.77 MB', send time:'0.15 ms'
19: input size:'1.77 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
20: input size:'1.77 MB', recieve_time:'0.15 ms', out:'5.96 MB', send time:'0.50 ms'
21: input size:'5.97 MB', recieve_time:'0.50 ms', out:'1.84 MB', send time:'0.15 ms'
22: input size:'1.84 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
23: input size:'1.77 MB', recieve_time:'0.15 ms', out:'5.96 MB', send time:'0.50 ms'
24: input size:'5.97 MB', recieve_time:'0.50 ms', out:'1.84 MB', send time:'0.15 ms'
25: input size:'1.84 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
26: input size:'1.77 MB', recieve_time:'0.15 ms', out:'5.96 MB', send time:'0.50 ms'
27: input size:'5.97 MB', recieve_time:'0.50 ms', out:'1.84 MB', send time:'0.15 ms'
28: input size:'1.84 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
29: input size:'1.77 MB', recieve_time:'0.15 ms', out:'1.77 MB', send time:'0.15 ms'
30: input size:'1.77 MB', recieve_time:'0.15 ms', out:'5.96 MB', send time:'0.50 ms'
31: input size:'5.97 MB', recieve_time:'0.50 ms', out:'0.00 MB', send time:'0.00 ms'

Compuatation Communication ratio (comp/(comp+comm)):
forward {0: 0.94, 1: 0.95, 2: 0.94, 3: 0.79, 4: 0.94, 5: 0.95, 6: 0.94, 7: 0.79, 8: 0.81, 9: 0.95, 10: 0.94, 11: 0.79, 12: 0.81, 13: 0.99, 14: 0.92, 15: 0.98, 16: 0.98, 17: 0.92, 18: 0.98, 19: 0.98, 20: 0.92, 21: 0.97, 22: 0.98, 23: 0.94, 24: 0.97, 25: 0.98, 26: 0.94, 27: 0.97, 28: 0.98, 29: 0.98, 30: 0.92, 31: 1.0} 
backward {0: 1.0, 1: 0.98, 2: 0.98, 3: 0.98, 4: 0.91, 5: 0.98, 6: 0.98, 7: 0.98, 8: 0.92, 9: 0.91, 10: 0.98, 11: 0.98, 12: 0.92, 13: 0.91, 14: 0.99, 15: 0.97, 16: 0.99, 17: 0.99, 18: 0.97, 19: 0.99, 20: 0.99, 21: 0.97, 22: 0.99, 23: 0.99, 24: 0.97, 25: 0.99, 26: 0.99, 27: 0.97, 28: 0.99, 29: 0.99, 30: 0.99, 31: 0.97}

Analysis for T = fwd + bwd:
 {'expected_compute_utilization': {0: 0.91,
                                  1: 0.99,
                                  2: 0.8,
                                  3: 0.88,
                                  4: 0.87,
                                  5: 0.99,
                                  6: 0.79,
                                  7: 0.88,
                                  8: 1.0,
                                  9: 0.87,
                                  10: 0.81,
                                  11: 0.89,
                                  12: 0.98,
                                  13: 0.82,
                                  14: 0.37,
                                  15: 0.38,
                                  16: 0.51,
                                  17: 0.38,
                                  18: 0.38,
                                  19: 0.52,
                                  20: 0.37,
                                  21: 0.38,
                                  22: 0.39,
                                  23: 0.5,
                                  24: 0.38,
                                  25: 0.39,
                                  26: 0.51,
                                  27: 0.38,
                                  28: 0.39,
                                  29: 0.45,
                                  30: 0.38,
                                  31: 0.41},
 'pipeline_no_comm': {0: 50.86,
                      1: 55.57,
                      2: 45.19,
                      3: 49.22,
                      4: 49.03,
                      5: 55.38,
                      6: 44.6,
                      7: 49.66,
                      8: 56.17,
                      9: 48.94,
                      10: 45.44,
                      11: 49.72,
                      12: 55.24,
                      13: 46.32,
                      14: 21.04,
                      15: 21.44,
                      16: 28.91,
                      17: 21.26,
                      18: 21.35,
                      19: 29.01,
                      20: 21.0,
                      21: 21.45,
                      22: 21.65,
                      23: 28.06,
                      24: 21.31,
                      25: 21.75,
                      26: 28.43,
                      27: 21.41,
                      28: 22.03,
                      29: 25.16,
                      30: 21.23,
                      31: 23.06,
                      'worstcase': 56.17},
 'pipeline_vs_seq_no_comm': 15.0,
 'pipeline_with_non_parallel_comm': {0: 51.67,
                                     1: 57.1,
                                     2: 46.68,
                                     3: 53.5,
                                     4: 53.36,
                                     5: 56.91,
                                     6: 46.08,
                                     7: 53.98,
                                     8: 63.24,
                                     9: 53.22,
                                     10: 46.93,
                                     11: 54.0,
                                     12: 62.32,
                                     13: 50.0,
                                     14: 21.69,
                                     15: 22.09,
                                     16: 29.2,
                                     17: 21.9,
                                     18: 21.99,
                                     19: 29.31,
                                     20: 21.64,
                                     21: 22.1,
                                     22: 21.95,
                                     23: 28.71,
                                     24: 21.96,
                                     25: 22.05,
                                     26: 29.08,
                                     27: 22.06,
                                     28: 22.33,
                                     29: 25.45,
                                     30: 21.87,
                                     31: 23.56,
                                     'worstcase': 63.24},
 'seq_no_comm_no_recomp': {0: 36.62,
                           1: 43.79,
                           2: 33.8,
                           3: 37.17,
                           4: 37.48,
                           5: 42.62,
                           6: 34.83,
                           7: 37.41,
                           8: 42.03,
                           9: 37.56,
                           10: 33.95,
                           11: 36.9,
                           12: 41.07,
                           13: 35.44,
                           14: 15.45,
                           15: 15.99,
                           16: 21.62,
                           17: 15.64,
                           18: 15.65,
                           19: 21.33,
                           20: 15.89,
                           21: 16.11,
                           22: 16.26,
                           23: 20.85,
                           24: 15.98,
                           25: 16.52,
                           26: 20.9,
                           27: 15.92,
                           28: 16.25,
                           29: 18.5,
                           30: 15.7,
                           31: 17.21}}

Analysis for T = (1-R)fwd + R*bwd:

Pipeline Slowdown: (compared to sequential executation with no communication, and same recompute policy)
forward 1.961
backward 1.747

Expected utilization by partition
forward {0: 0.73, 1: 0.81, 2: 0.66, 3: 0.71, 4: 0.71, 5: 0.8, 6: 0.66, 7: 0.72, 8: 0.81, 9: 0.72, 10: 0.66, 11: 0.72, 12: 0.79, 13: 0.66, 14: 0.3, 15: 0.31, 16: 0.42, 17: 0.31, 18: 0.31, 19: 0.42, 20: 0.3, 21: 0.31, 22: 0.32, 23: 0.4, 24: 0.31, 25: 0.32, 26: 0.41, 27: 0.31, 28: 0.32, 29: 0.36, 30: 0.31, 31: 0.34}
backward {0: 0.84, 1: 0.91, 2: 0.73, 3: 0.8, 4: 0.8, 5: 0.9, 6: 0.72, 7: 0.81, 8: 0.92, 9: 0.8, 10: 0.74, 11: 0.81, 12: 0.91, 13: 0.76, 14: 0.35, 15: 0.35, 16: 0.48, 17: 0.35, 18: 0.35, 19: 0.48, 20: 0.35, 21: 0.35, 22: 0.36, 23: 0.46, 24: 0.35, 25: 0.36, 26: 0.47, 27: 0.35, 28: 0.36, 29: 0.41, 30: 0.35, 31: 0.38}

worstcase: bwd: 44.524 fwd: 18.719
expected_speedup_compared_to_seq_no_recomp_no_comm: 13.321
Expected speedup for 32 partitions is: 17.674
max cuda memory used 3.43GB
"""