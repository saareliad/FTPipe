"""AutoGenerated with:
python partitioning_script.py --partitioning_task megatron --arch transformer_lm_megatron
"""
import torch.functional
import torch
import math
import torch.nn.functional
from torch import Tensor
import torch.nn as nn
from itertools import chain
from typing import Optional, Tuple, Iterator, Iterable, OrderedDict, Dict
import collections

from fairseq.modules.fairseq_dropout import FairseqDropout
from fairseq.model_parallel.modules.multihead_attention import ModelParallelMultiheadAttention
from fairseq.model_parallel.megatron.mpu.layers import RowParallelLinear
from fairseq.modules.layer_norm import FusedLayerNorm
from torch.nn.modules.linear import Linear
from fairseq.model_parallel.megatron.mpu.layers import ColumnParallelLinear
from fairseq.modules.sinusoidal_positional_embedding import SinusoidalPositionalEmbedding
from fairseq.model_parallel.megatron.mpu.layers import VocabParallelEmbedding
# this is an auto generated file do not edit unless you know what you are doing


# partition adjacency
# model inputs {0}
# partition 0 {'inputs': {'src_tokens'}, 'outputs': {1}}
# partition 1 {'inputs': {0}, 'outputs': {2}}
# partition 2 {'inputs': {1}, 'outputs': {3}}
# partition 3 {'inputs': {2}, 'outputs': {4}}
# partition 4 {'inputs': {3}, 'outputs': {5}}
# partition 5 {'inputs': {4}, 'outputs': {6}}
# partition 6 {'inputs': {5}, 'outputs': {7}}
# partition 7 {'inputs': {6}, 'outputs': {8}}
# partition 8 {'inputs': {7}, 'outputs': {9}}
# partition 9 {'inputs': {8}, 'outputs': {10}}
# partition 10 {'inputs': {9}, 'outputs': {11}}
# partition 11 {'inputs': {10}, 'outputs': {12}}
# partition 12 {'inputs': {11}, 'outputs': {13}}
# partition 13 {'inputs': {12}, 'outputs': {14}}
# partition 14 {'inputs': {13}, 'outputs': {15}}
# partition 15 {'inputs': {14}, 'outputs': {'output'}}
# model outputs {15}


def create_pipeline_configuration(DEBUG=False, batch_size=1):
    config = {
        'batch_dim': 0,
        'depth': 10000,
        'basic_blocks': (FairseqDropout,ModelParallelMultiheadAttention,RowParallelLinear,FusedLayerNorm,Linear,ColumnParallelLinear,SinusoidalPositionalEmbedding,VocabParallelEmbedding),
        'model_inputs': {
            'src_tokens': {
                'shape': torch.Size([1, 1024]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0]}},
        'model_outputs': {
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Linear[output_projection]': {
                'shape': torch.Size([1, 1024, 51200]),
                'dtype': torch.float32,
                'is_batched': True,
                'created_by': 15}},
        'stages': {
            0: {
                'stage_cls': Partition0,
                'inputs': {
                    'src_tokens': {
                        'shape': torch.Size([1, 1024]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/Tensor::__add___129': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [1]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_132': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [1]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/ModelParallelMultiheadAttention[self_attn]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [1]}},
                'devices': ['cpu' if DEBUG else 'cuda:0']},
            1: {
                'stage_cls': Partition1,
                'inputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/Tensor::__add___129': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 0},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_132': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 0},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/ModelParallelMultiheadAttention[self_attn]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 0}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/Tensor::__add___264': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [2]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_267': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [2]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::__getitem___275': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [2]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/FusedLayerNorm[self_attn_layer_norm]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [2]}},
                'devices': ['cpu' if DEBUG else 'cuda:1']},
            2: {
                'stage_cls': Partition2,
                'inputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/Tensor::__add___264': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 1},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_267': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 1},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::__getitem___275': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 1},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/FusedLayerNorm[self_attn_layer_norm]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 1}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/Tensor::__add___399': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [3]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_402': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [3]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_405': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [3]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_408': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [3]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/FusedLayerNorm[self_attn_layer_norm]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [3]}},
                'devices': ['cpu' if DEBUG else 'cuda:2']},
            3: {
                'stage_cls': Partition3,
                'inputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/Tensor::__add___399': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 2},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_402': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 2},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_405': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 2},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_408': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 2},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/FusedLayerNorm[self_attn_layer_norm]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 2}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_510': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [4]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/Tensor::__add___525': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [4]}},
                'devices': ['cpu' if DEBUG else 'cuda:3']},
            4: {
                'stage_cls': Partition4,
                'inputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_510': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 3},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/Tensor::__add___525': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 3}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/Tensor::__add___642': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [5]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_645': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [5]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/FairseqDropout[dropout_module]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [5]}},
                'devices': ['cpu' if DEBUG else 'cuda:4']},
            5: {
                'stage_cls': Partition5,
                'inputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/Tensor::__add___642': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 4},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_645': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 4},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/FairseqDropout[dropout_module]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 4}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/Tensor::__add___777': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [6]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_780': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [6]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_783': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [6]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_786': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [6]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/FusedLayerNorm[self_attn_layer_norm]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [6]}},
                'devices': ['cpu' if DEBUG else 'cuda:5']},
            6: {
                'stage_cls': Partition6,
                'inputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/Tensor::__add___777': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 5},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_780': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 5},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_783': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 5},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_786': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 5},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/FusedLayerNorm[self_attn_layer_norm]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 5}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/Tensor::__add___885': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [7]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_888': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [7]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_891': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [7]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_894': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [7]}},
                'devices': ['cpu' if DEBUG else 'cuda:6']},
            7: {
                'stage_cls': Partition7,
                'inputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/Tensor::__add___885': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 6},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_888': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 6},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_891': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 6},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_894': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 6}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/Tensor::__add___1020': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [8]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1023': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [8]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1026': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [8]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1029': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [8]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/FusedLayerNorm[self_attn_layer_norm]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [8]}},
                'devices': ['cpu' if DEBUG else 'cuda:7']},
            8: {
                'stage_cls': Partition8,
                'inputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/Tensor::__add___1020': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 7},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1023': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 7},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1026': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 7},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1029': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 7},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/FusedLayerNorm[self_attn_layer_norm]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 7}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1131': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [9]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/Tensor::__add___1146': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [9]}},
                'devices': ['cpu' if DEBUG else 'cuda:8']},
            9: {
                'stage_cls': Partition9,
                'inputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1131': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 8},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/Tensor::__add___1146': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 8}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/Tensor::__add___1263': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [10]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1266': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [10]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1269': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [10]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1272': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [10]}},
                'devices': ['cpu' if DEBUG else 'cuda:9']},
            10: {
                'stage_cls': Partition10,
                'inputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/Tensor::__add___1263': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 9},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1266': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 9},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1269': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 9},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1272': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 9}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/Tensor::__add___1371': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [11]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1374': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [11]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/FairseqDropout[dropout_module]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [11]}},
                'devices': ['cpu' if DEBUG else 'cuda:10']},
            11: {
                'stage_cls': Partition11,
                'inputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/Tensor::__add___1371': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 10},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1374': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 10},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/FairseqDropout[dropout_module]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 10}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/Tensor::__add___1506': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [12]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1509': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [12]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1512': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [12]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1515': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [12]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/FusedLayerNorm[self_attn_layer_norm]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [12]}},
                'devices': ['cpu' if DEBUG else 'cuda:11']},
            12: {
                'stage_cls': Partition12,
                'inputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/Tensor::__add___1506': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 11},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1509': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 11},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1512': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 11},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1515': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 11},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/FusedLayerNorm[self_attn_layer_norm]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 11}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1617': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [13]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/Tensor::__add___1632': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [13]}},
                'devices': ['cpu' if DEBUG else 'cuda:12']},
            13: {
                'stage_cls': Partition13,
                'inputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1617': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 12},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/Tensor::__add___1632': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 12}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/Tensor::__add___1749': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [14]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1752': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [14]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1755': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [14]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1758': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [14]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/FusedLayerNorm[self_attn_layer_norm]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [14]}},
                'devices': ['cpu' if DEBUG else 'cuda:13']},
            14: {
                'stage_cls': Partition14,
                'inputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/Tensor::__add___1749': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 13},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1752': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 13},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1755': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 13},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1758': {
                        'shape': None,
                        'dtype': slice,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 13},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/FusedLayerNorm[self_attn_layer_norm]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 13}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/Tensor::__add___1857': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [15]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1860': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [15]},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/ModelParallelMultiheadAttention[self_attn]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [15]}},
                'devices': ['cpu' if DEBUG else 'cuda:14']},
            15: {
                'stage_cls': Partition15,
                'inputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/Tensor::__add___1857': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 14},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1860': {
                        'shape': torch.Size([1024, 1024]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 14},
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/ModelParallelMultiheadAttention[self_attn]': {
                        'shape': torch.Size([1024, 1, 3072]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 14}},
                'outputs': {
                    'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Linear[output_projection]': {
                        'shape': torch.Size([1, 1024, 51200]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [-1]}},
                'devices': ['cpu' if DEBUG else 'cuda:15']}}}
    
    
    # switching batch size
    batch_dim = config['batch_dim']
    for d in chain(config['model_inputs'].values(),config['model_outputs'].values()):
        if d['is_batched']:
            shape = d['shape']
            d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    for s in config['stages'].values():
        for d in chain(s['inputs'].values(),s['outputs'].values()):
            if d['is_batched']:
                shape = d['shape']
                d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    return config

class Partition0(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/SinusoidalPositionalEmbedding[embed_positions]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/VocabParallelEmbedding[embed_tokens]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/ModelParallelMultiheadAttention[self_attn]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:0'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1]
        self.lookup = { 'l_0': 'decoder.embed_positions',
                        'l_1': 'decoder.embed_tokens',
                        'l_2': 'decoder.dropout_module',
                        'l_3': 'decoder.0.self_attn_layer_norm',
                        'l_4': 'decoder.0.self_attn',
                        'l_5': 'decoder.0.dropout_module',
                        'l_6': 'decoder.0.final_layer_norm',
                        'l_7': 'decoder.0.fc1',
                        'l_8': 'decoder.0.activation_dropout_module',
                        'l_9': 'decoder.0.fc2',
                        'l_10': 'decoder.0.dropout_module',
                        'l_11': 'decoder.1.self_attn_layer_norm',
                        'l_12': 'decoder.1.self_attn',
                        'l_13': 'decoder.1.dropout_module',
                        'l_14': 'decoder.1.final_layer_norm',
                        'l_15': 'decoder.1.fc1',
                        'l_16': 'decoder.1.activation_dropout_module',
                        'l_17': 'decoder.1.fc2',
                        'l_18': 'decoder.1.dropout_module',
                        'l_19': 'decoder.2.self_attn_layer_norm',
                        'l_20': 'decoder.2.self_attn',
                        'l_21': 'decoder.2.dropout_module',
                        'l_22': 'decoder.2.final_layer_norm',
                        'l_23': 'decoder.2.fc1',
                        'l_24': 'decoder.2.activation_dropout_module',
                        'l_25': 'decoder.2.fc2',
                        'l_26': 'decoder.2.dropout_module',
                        'l_27': 'decoder.3.self_attn_layer_norm',
                        'l_28': 'decoder.3.self_attn',
                        'l_29': 'decoder.3.dropout_module',
                        'l_30': 'decoder.3.final_layer_norm',
                        'l_31': 'decoder.3.fc1',
                        'l_32': 'decoder.3.activation_dropout_module',
                        'l_33': 'decoder.3.fc2',
                        'l_34': 'decoder.3.dropout_module',
                        'l_35': 'decoder.4.self_attn_layer_norm',
                        'l_36': 'decoder.4.self_attn'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/SinusoidalPositionalEmbedding[embed_positions] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/VocabParallelEmbedding[embed_tokens] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/FairseqDropout[dropout_module] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/ModelParallelMultiheadAttention[self_attn] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/FairseqDropout[dropout_module] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/FusedLayerNorm[final_layer_norm] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/ColumnParallelLinear[fc1] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/FairseqDropout[activation_dropout_module] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/RowParallelLinear[fc2] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[0]/FairseqDropout[dropout_module] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/ModelParallelMultiheadAttention[self_attn] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/FairseqDropout[dropout_module] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/FusedLayerNorm[final_layer_norm] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/ColumnParallelLinear[fc1] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/FairseqDropout[activation_dropout_module] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/RowParallelLinear[fc2] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[1]/FairseqDropout[dropout_module] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/ModelParallelMultiheadAttention[self_attn] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/FairseqDropout[dropout_module] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/FusedLayerNorm[final_layer_norm] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/ColumnParallelLinear[fc1] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/FairseqDropout[activation_dropout_module] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/RowParallelLinear[fc2] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[2]/FairseqDropout[dropout_module] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/ModelParallelMultiheadAttention[self_attn] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/FairseqDropout[dropout_module] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/FusedLayerNorm[final_layer_norm] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/ColumnParallelLinear[fc1] <=> self.l_31
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/FairseqDropout[activation_dropout_module] <=> self.l_32
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/RowParallelLinear[fc2] <=> self.l_33
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/FairseqDropout[dropout_module] <=> self.l_34
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_35
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/ModelParallelMultiheadAttention[self_attn] <=> self.l_36
        # input0 <=> src_tokens

        # moving inputs to current device no op if already on the correct device
        src_tokens = move_tensors(unflatten(args,self.input_structure), self.device)[0]
        t_0 = self.l_0(src_tokens, incremental_state=None)
        t_1 = self.l_1(src_tokens)
        t_1 = 55.42562584220407 * t_1
        t_1 += t_0
        t_0 = t_1
        t_0 = self.l_2(t_0)
        t_0 = t_0.transpose(0, 1)
        t_1 = src_tokens.eq(1)
        t_1 = t_1.any()
        t_2 = t_0.size(0)
        t_3 = [t_2, t_2]
        t_3 = torch.zeros(t_3)
        t_4 = t_3.float()
        t_4 = t_4.fill_(float('-inf'))
        t_3 = t_4.type_as(t_3)
        t_3 = torch.triu(t_3, 1)
        t_3 = t_3.to(t_0)
        t_4 = slice(None, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_4, t_2)
        t_2 = t_3[t_2]
        t_4 = self.l_3(t_0)
        t_2 = self.l_4(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_2)
        t_2 = self.l_5(t_2)
        t_2 = t_0 + t_2
        t_0 = self.l_6(t_2)
        t_0 = self.l_7(t_0)
        t_5 = t_0.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_0 = t_5.type_as(t_0)
        t_0 = self.l_8(t_0)
        t_0 = self.l_9(t_0)
        t_0 = self.l_10(t_0)
        t_0 = t_2 + t_0
        t_2 = t_0.size(0)
        t_3 = t_3.to(t_0)
        t_5 = slice(None, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_5, t_2)
        t_2 = t_3[t_2]
        t_5 = self.l_11(t_0)
        t_2 = self.l_12(query=t_5, key=t_5, value=t_5, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_2)
        t_2 = self.l_13(t_2)
        t_2 = t_0 + t_2
        t_0 = self.l_14(t_2)
        t_0 = self.l_15(t_0)
        t_6 = t_0.float()
        t_6 = torch.nn.functional.gelu(t_6)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_16(t_0)
        t_0 = self.l_17(t_0)
        t_0 = self.l_18(t_0)
        t_0 = t_2 + t_0
        t_2 = t_0.size(0)
        t_3 = t_3.to(t_0)
        t_6 = slice(None, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_6, t_2)
        t_2 = t_3[t_2]
        t_6 = self.l_19(t_0)
        t_2 = self.l_20(query=t_6, key=t_6, value=t_6, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_2)
        t_2 = self.l_21(t_2)
        t_2 = t_0 + t_2
        t_0 = self.l_22(t_2)
        t_0 = self.l_23(t_0)
        t_7 = t_0.float()
        t_7 = torch.nn.functional.gelu(t_7)
        t_0 = t_7.type_as(t_0)
        t_0 = self.l_24(t_0)
        t_0 = self.l_25(t_0)
        t_0 = self.l_26(t_0)
        t_0 = t_2 + t_0
        t_2 = t_0.size(0)
        t_3 = t_3.to(t_0)
        t_7 = slice(None, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_7, t_2)
        t_2 = t_3[t_2]
        t_7 = self.l_27(t_0)
        t_2 = self.l_28(query=t_7, key=t_7, value=t_7, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_2)
        t_2 = self.l_29(t_2)
        t_2 = t_0 + t_2
        t_0 = self.l_30(t_2)
        t_0 = self.l_31(t_0)
        t_8 = t_0.float()
        t_8 = torch.nn.functional.gelu(t_8)
        t_0 = t_8.type_as(t_0)
        t_0 = self.l_32(t_0)
        t_0 = self.l_33(t_0)
        t_0 = self.l_34(t_0)
        t_0 = t_2 + t_0
        t_2 = t_0.size(0)
        t_3 = t_3.to(t_0)
        t_8 = slice(None, t_2, None)
        t_2 = slice(None, t_2, None)
        t_2 = (t_8, t_2)
        t_2 = t_3[t_2]
        t_8 = self.l_35(t_0)
        t_2 = self.l_36(query=t_8, key=t_8, value=t_8, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_2)
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/Tensor::__add___129
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_132
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/ModelParallelMultiheadAttention[self_attn]
        return list(flatten((t_0, t_3, t_2)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition1(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/FusedLayerNorm[self_attn_layer_norm]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:1'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'decoder.4.dropout_module',
                        'l_1': 'decoder.4.final_layer_norm',
                        'l_2': 'decoder.4.fc1',
                        'l_3': 'decoder.4.activation_dropout_module',
                        'l_4': 'decoder.4.fc2',
                        'l_5': 'decoder.4.dropout_module',
                        'l_6': 'decoder.5.self_attn_layer_norm',
                        'l_7': 'decoder.5.self_attn',
                        'l_8': 'decoder.5.dropout_module',
                        'l_9': 'decoder.5.final_layer_norm',
                        'l_10': 'decoder.5.fc1',
                        'l_11': 'decoder.5.activation_dropout_module',
                        'l_12': 'decoder.5.fc2',
                        'l_13': 'decoder.5.dropout_module',
                        'l_14': 'decoder.6.self_attn_layer_norm',
                        'l_15': 'decoder.6.self_attn',
                        'l_16': 'decoder.6.dropout_module',
                        'l_17': 'decoder.6.final_layer_norm',
                        'l_18': 'decoder.6.fc1',
                        'l_19': 'decoder.6.activation_dropout_module',
                        'l_20': 'decoder.6.fc2',
                        'l_21': 'decoder.6.dropout_module',
                        'l_22': 'decoder.7.self_attn_layer_norm',
                        'l_23': 'decoder.7.self_attn',
                        'l_24': 'decoder.7.dropout_module',
                        'l_25': 'decoder.7.final_layer_norm',
                        'l_26': 'decoder.7.fc1',
                        'l_27': 'decoder.7.activation_dropout_module',
                        'l_28': 'decoder.7.fc2',
                        'l_29': 'decoder.7.dropout_module',
                        'l_30': 'decoder.8.self_attn_layer_norm',
                        'l_31': 'decoder.8.self_attn',
                        'l_32': 'decoder.8.dropout_module',
                        'l_33': 'decoder.8.final_layer_norm',
                        'l_34': 'decoder.8.fc1',
                        'l_35': 'decoder.8.activation_dropout_module',
                        'l_36': 'decoder.8.fc2',
                        'l_37': 'decoder.8.dropout_module',
                        'l_38': 'decoder.9.self_attn_layer_norm'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/FairseqDropout[dropout_module] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/FusedLayerNorm[final_layer_norm] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/ColumnParallelLinear[fc1] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/FairseqDropout[activation_dropout_module] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/RowParallelLinear[fc2] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/FairseqDropout[dropout_module] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/ModelParallelMultiheadAttention[self_attn] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/FairseqDropout[dropout_module] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/FusedLayerNorm[final_layer_norm] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/ColumnParallelLinear[fc1] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/FairseqDropout[activation_dropout_module] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/RowParallelLinear[fc2] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[5]/FairseqDropout[dropout_module] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/ModelParallelMultiheadAttention[self_attn] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/FairseqDropout[dropout_module] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/FusedLayerNorm[final_layer_norm] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/ColumnParallelLinear[fc1] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/FairseqDropout[activation_dropout_module] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/RowParallelLinear[fc2] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[6]/FairseqDropout[dropout_module] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/ModelParallelMultiheadAttention[self_attn] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/FairseqDropout[dropout_module] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/FusedLayerNorm[final_layer_norm] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/ColumnParallelLinear[fc1] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/FairseqDropout[activation_dropout_module] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/RowParallelLinear[fc2] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[7]/FairseqDropout[dropout_module] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/ModelParallelMultiheadAttention[self_attn] <=> self.l_31
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/FairseqDropout[dropout_module] <=> self.l_32
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/FusedLayerNorm[final_layer_norm] <=> self.l_33
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/ColumnParallelLinear[fc1] <=> self.l_34
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/FairseqDropout[activation_dropout_module] <=> self.l_35
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/RowParallelLinear[fc2] <=> self.l_36
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/FairseqDropout[dropout_module] <=> self.l_37
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_38
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[3]/Tensor::__add___129 <=> x0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_132 <=> x1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[4]/ModelParallelMultiheadAttention[self_attn] <=> x2

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x2)
        t_0 = x0 + t_0
        t_1 = self.l_1(t_0)
        t_1 = self.l_2(t_1)
        t_2 = t_1.float()
        t_2 = torch.nn.functional.gelu(t_2)
        t_1 = t_2.type_as(t_1)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = x1.to(t_1)
        t_3 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_3, t_0)
        t_0 = t_2[t_0]
        t_3 = self.l_6(t_1)
        t_0 = self.l_7(query=t_3, key=t_3, value=t_3, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_8(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_9(t_0)
        t_1 = self.l_10(t_1)
        t_4 = t_1.float()
        t_4 = torch.nn.functional.gelu(t_4)
        t_1 = t_4.type_as(t_1)
        t_1 = self.l_11(t_1)
        t_1 = self.l_12(t_1)
        t_1 = self.l_13(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_4 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_4, t_0)
        t_0 = t_2[t_0]
        t_4 = self.l_14(t_1)
        t_0 = self.l_15(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_16(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_17(t_0)
        t_1 = self.l_18(t_1)
        t_5 = t_1.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_1 = t_5.type_as(t_1)
        t_1 = self.l_19(t_1)
        t_1 = self.l_20(t_1)
        t_1 = self.l_21(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_5 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_5, t_0)
        t_0 = t_2[t_0]
        t_5 = self.l_22(t_1)
        t_0 = self.l_23(query=t_5, key=t_5, value=t_5, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_24(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_25(t_0)
        t_1 = self.l_26(t_1)
        t_6 = t_1.float()
        t_6 = torch.nn.functional.gelu(t_6)
        t_1 = t_6.type_as(t_1)
        t_1 = self.l_27(t_1)
        t_1 = self.l_28(t_1)
        t_1 = self.l_29(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_6 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_6, t_0)
        t_0 = t_2[t_0]
        t_6 = self.l_30(t_1)
        t_0 = self.l_31(query=t_6, key=t_6, value=t_6, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_32(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_33(t_0)
        t_1 = self.l_34(t_1)
        t_7 = t_1.float()
        t_7 = torch.nn.functional.gelu(t_7)
        t_1 = t_7.type_as(t_1)
        t_1 = self.l_35(t_1)
        t_1 = self.l_36(t_1)
        t_1 = self.l_37(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_7 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_7, t_0)
        t_0 = t_2[t_0]
        t_7 = self.l_38(t_1)
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/Tensor::__add___264
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_267
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::__getitem___275
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/FusedLayerNorm[self_attn_layer_norm]
        return list(flatten((t_1, t_2, t_0, t_7)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition2(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/FusedLayerNorm[self_attn_layer_norm]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:2'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.9.self_attn',
                        'l_1': 'decoder.9.dropout_module',
                        'l_2': 'decoder.9.final_layer_norm',
                        'l_3': 'decoder.9.fc1',
                        'l_4': 'decoder.9.activation_dropout_module',
                        'l_5': 'decoder.9.fc2',
                        'l_6': 'decoder.9.dropout_module',
                        'l_7': 'decoder.10.self_attn_layer_norm',
                        'l_8': 'decoder.10.self_attn',
                        'l_9': 'decoder.10.dropout_module',
                        'l_10': 'decoder.10.final_layer_norm',
                        'l_11': 'decoder.10.fc1',
                        'l_12': 'decoder.10.activation_dropout_module',
                        'l_13': 'decoder.10.fc2',
                        'l_14': 'decoder.10.dropout_module',
                        'l_15': 'decoder.11.self_attn_layer_norm',
                        'l_16': 'decoder.11.self_attn',
                        'l_17': 'decoder.11.dropout_module',
                        'l_18': 'decoder.11.final_layer_norm',
                        'l_19': 'decoder.11.fc1',
                        'l_20': 'decoder.11.activation_dropout_module',
                        'l_21': 'decoder.11.fc2',
                        'l_22': 'decoder.11.dropout_module',
                        'l_23': 'decoder.12.self_attn_layer_norm',
                        'l_24': 'decoder.12.self_attn',
                        'l_25': 'decoder.12.dropout_module',
                        'l_26': 'decoder.12.final_layer_norm',
                        'l_27': 'decoder.12.fc1',
                        'l_28': 'decoder.12.activation_dropout_module',
                        'l_29': 'decoder.12.fc2',
                        'l_30': 'decoder.12.dropout_module',
                        'l_31': 'decoder.13.self_attn_layer_norm',
                        'l_32': 'decoder.13.self_attn',
                        'l_33': 'decoder.13.dropout_module',
                        'l_34': 'decoder.13.final_layer_norm',
                        'l_35': 'decoder.13.fc1',
                        'l_36': 'decoder.13.activation_dropout_module',
                        'l_37': 'decoder.13.fc2',
                        'l_38': 'decoder.13.dropout_module',
                        'l_39': 'decoder.14.self_attn_layer_norm'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/ModelParallelMultiheadAttention[self_attn] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/FairseqDropout[dropout_module] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/FusedLayerNorm[final_layer_norm] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/ColumnParallelLinear[fc1] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/FairseqDropout[activation_dropout_module] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/RowParallelLinear[fc2] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/FairseqDropout[dropout_module] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/ModelParallelMultiheadAttention[self_attn] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/FairseqDropout[dropout_module] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/FusedLayerNorm[final_layer_norm] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/ColumnParallelLinear[fc1] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/FairseqDropout[activation_dropout_module] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/RowParallelLinear[fc2] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[10]/FairseqDropout[dropout_module] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/ModelParallelMultiheadAttention[self_attn] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/FairseqDropout[dropout_module] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/FusedLayerNorm[final_layer_norm] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/ColumnParallelLinear[fc1] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/FairseqDropout[activation_dropout_module] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/RowParallelLinear[fc2] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[11]/FairseqDropout[dropout_module] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/ModelParallelMultiheadAttention[self_attn] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/FairseqDropout[dropout_module] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/FusedLayerNorm[final_layer_norm] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/ColumnParallelLinear[fc1] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/FairseqDropout[activation_dropout_module] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/RowParallelLinear[fc2] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[12]/FairseqDropout[dropout_module] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_31
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/ModelParallelMultiheadAttention[self_attn] <=> self.l_32
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/FairseqDropout[dropout_module] <=> self.l_33
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/FusedLayerNorm[final_layer_norm] <=> self.l_34
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/ColumnParallelLinear[fc1] <=> self.l_35
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/FairseqDropout[activation_dropout_module] <=> self.l_36
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/RowParallelLinear[fc2] <=> self.l_37
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/FairseqDropout[dropout_module] <=> self.l_38
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_39
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[8]/Tensor::__add___264 <=> x0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_267 <=> x1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::__getitem___275 <=> x2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[9]/FusedLayerNorm[self_attn_layer_norm] <=> x3

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(query=x3, key=x3, value=x3, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=x2)
        t_0 = self.l_1(t_0)
        t_0 = x0 + t_0
        t_1 = self.l_2(t_0)
        t_1 = self.l_3(t_1)
        t_2 = t_1.float()
        t_2 = torch.nn.functional.gelu(t_2)
        t_1 = t_2.type_as(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = x1.to(t_1)
        t_3 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_3, t_0)
        t_0 = t_2[t_0]
        t_3 = self.l_7(t_1)
        t_0 = self.l_8(query=t_3, key=t_3, value=t_3, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_9(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_10(t_0)
        t_1 = self.l_11(t_1)
        t_4 = t_1.float()
        t_4 = torch.nn.functional.gelu(t_4)
        t_1 = t_4.type_as(t_1)
        t_1 = self.l_12(t_1)
        t_1 = self.l_13(t_1)
        t_1 = self.l_14(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_4 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_4, t_0)
        t_0 = t_2[t_0]
        t_4 = self.l_15(t_1)
        t_0 = self.l_16(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_17(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_18(t_0)
        t_1 = self.l_19(t_1)
        t_5 = t_1.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_1 = t_5.type_as(t_1)
        t_1 = self.l_20(t_1)
        t_1 = self.l_21(t_1)
        t_1 = self.l_22(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_5 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_5, t_0)
        t_0 = t_2[t_0]
        t_5 = self.l_23(t_1)
        t_0 = self.l_24(query=t_5, key=t_5, value=t_5, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_25(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_26(t_0)
        t_1 = self.l_27(t_1)
        t_6 = t_1.float()
        t_6 = torch.nn.functional.gelu(t_6)
        t_1 = t_6.type_as(t_1)
        t_1 = self.l_28(t_1)
        t_1 = self.l_29(t_1)
        t_1 = self.l_30(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_6 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_6, t_0)
        t_0 = t_2[t_0]
        t_6 = self.l_31(t_1)
        t_0 = self.l_32(query=t_6, key=t_6, value=t_6, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_33(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_34(t_0)
        t_1 = self.l_35(t_1)
        t_7 = t_1.float()
        t_7 = torch.nn.functional.gelu(t_7)
        t_1 = t_7.type_as(t_1)
        t_1 = self.l_36(t_1)
        t_1 = self.l_37(t_1)
        t_1 = self.l_38(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_7 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_8 = self.l_39(t_1)
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/Tensor::__add___399
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_402
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_405
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_408
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/FusedLayerNorm[self_attn_layer_norm]
        return list(flatten((t_1, t_2, t_7, t_0, t_8)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition3(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/FairseqDropout[dropout_module]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:3'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.14.self_attn',
                        'l_1': 'decoder.14.dropout_module',
                        'l_2': 'decoder.14.final_layer_norm',
                        'l_3': 'decoder.14.fc1',
                        'l_4': 'decoder.14.activation_dropout_module',
                        'l_5': 'decoder.14.fc2',
                        'l_6': 'decoder.14.dropout_module',
                        'l_7': 'decoder.15.self_attn_layer_norm',
                        'l_8': 'decoder.15.self_attn',
                        'l_9': 'decoder.15.dropout_module',
                        'l_10': 'decoder.15.final_layer_norm',
                        'l_11': 'decoder.15.fc1',
                        'l_12': 'decoder.15.activation_dropout_module',
                        'l_13': 'decoder.15.fc2',
                        'l_14': 'decoder.15.dropout_module',
                        'l_15': 'decoder.16.self_attn_layer_norm',
                        'l_16': 'decoder.16.self_attn',
                        'l_17': 'decoder.16.dropout_module',
                        'l_18': 'decoder.16.final_layer_norm',
                        'l_19': 'decoder.16.fc1',
                        'l_20': 'decoder.16.activation_dropout_module',
                        'l_21': 'decoder.16.fc2',
                        'l_22': 'decoder.16.dropout_module',
                        'l_23': 'decoder.17.self_attn_layer_norm',
                        'l_24': 'decoder.17.self_attn',
                        'l_25': 'decoder.17.dropout_module',
                        'l_26': 'decoder.17.final_layer_norm',
                        'l_27': 'decoder.17.fc1',
                        'l_28': 'decoder.17.activation_dropout_module',
                        'l_29': 'decoder.17.fc2',
                        'l_30': 'decoder.17.dropout_module',
                        'l_31': 'decoder.18.self_attn_layer_norm',
                        'l_32': 'decoder.18.self_attn',
                        'l_33': 'decoder.18.dropout_module'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/ModelParallelMultiheadAttention[self_attn] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/FairseqDropout[dropout_module] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/FusedLayerNorm[final_layer_norm] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/ColumnParallelLinear[fc1] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/FairseqDropout[activation_dropout_module] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/RowParallelLinear[fc2] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/FairseqDropout[dropout_module] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/ModelParallelMultiheadAttention[self_attn] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/FairseqDropout[dropout_module] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/FusedLayerNorm[final_layer_norm] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/ColumnParallelLinear[fc1] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/FairseqDropout[activation_dropout_module] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/RowParallelLinear[fc2] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[15]/FairseqDropout[dropout_module] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/ModelParallelMultiheadAttention[self_attn] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/FairseqDropout[dropout_module] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/FusedLayerNorm[final_layer_norm] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/ColumnParallelLinear[fc1] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/FairseqDropout[activation_dropout_module] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/RowParallelLinear[fc2] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[16]/FairseqDropout[dropout_module] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/ModelParallelMultiheadAttention[self_attn] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/FairseqDropout[dropout_module] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/FusedLayerNorm[final_layer_norm] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/ColumnParallelLinear[fc1] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/FairseqDropout[activation_dropout_module] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/RowParallelLinear[fc2] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[17]/FairseqDropout[dropout_module] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_31
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/ModelParallelMultiheadAttention[self_attn] <=> self.l_32
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/FairseqDropout[dropout_module] <=> self.l_33
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[13]/Tensor::__add___399 <=> x0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_402 <=> x1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_405 <=> x2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_408 <=> x3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[14]/FusedLayerNorm[self_attn_layer_norm] <=> x4

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = (x2, x3)
        t_0 = x1[t_0]
        t_0 = self.l_0(query=x4, key=x4, value=x4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_1(t_0)
        t_0 = x0 + t_0
        t_1 = self.l_2(t_0)
        t_1 = self.l_3(t_1)
        t_2 = t_1.float()
        t_2 = torch.nn.functional.gelu(t_2)
        t_1 = t_2.type_as(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = x1.to(t_1)
        t_3 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_3, t_0)
        t_0 = t_2[t_0]
        t_3 = self.l_7(t_1)
        t_0 = self.l_8(query=t_3, key=t_3, value=t_3, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_9(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_10(t_0)
        t_1 = self.l_11(t_1)
        t_4 = t_1.float()
        t_4 = torch.nn.functional.gelu(t_4)
        t_1 = t_4.type_as(t_1)
        t_1 = self.l_12(t_1)
        t_1 = self.l_13(t_1)
        t_1 = self.l_14(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_4 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_4, t_0)
        t_0 = t_2[t_0]
        t_4 = self.l_15(t_1)
        t_0 = self.l_16(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_17(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_18(t_0)
        t_1 = self.l_19(t_1)
        t_5 = t_1.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_1 = t_5.type_as(t_1)
        t_1 = self.l_20(t_1)
        t_1 = self.l_21(t_1)
        t_1 = self.l_22(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_5 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_5, t_0)
        t_0 = t_2[t_0]
        t_5 = self.l_23(t_1)
        t_0 = self.l_24(query=t_5, key=t_5, value=t_5, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_25(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_26(t_0)
        t_1 = self.l_27(t_1)
        t_6 = t_1.float()
        t_6 = torch.nn.functional.gelu(t_6)
        t_1 = t_6.type_as(t_1)
        t_1 = self.l_28(t_1)
        t_1 = self.l_29(t_1)
        t_1 = self.l_30(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_6 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_6, t_0)
        t_0 = t_2[t_0]
        t_6 = self.l_31(t_1)
        t_0 = self.l_32(query=t_6, key=t_6, value=t_6, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_33(t_0)
        t_0 = t_1 + t_0
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_510
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/Tensor::__add___525
        return list(flatten((t_2, t_0)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition4(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/FairseqDropout[dropout_module]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:4'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = { 'l_0': 'decoder.18.final_layer_norm',
                        'l_1': 'decoder.18.fc1',
                        'l_2': 'decoder.18.activation_dropout_module',
                        'l_3': 'decoder.18.fc2',
                        'l_4': 'decoder.18.dropout_module',
                        'l_5': 'decoder.19.self_attn_layer_norm',
                        'l_6': 'decoder.19.self_attn',
                        'l_7': 'decoder.19.dropout_module',
                        'l_8': 'decoder.19.final_layer_norm',
                        'l_9': 'decoder.19.fc1',
                        'l_10': 'decoder.19.activation_dropout_module',
                        'l_11': 'decoder.19.fc2',
                        'l_12': 'decoder.19.dropout_module',
                        'l_13': 'decoder.20.self_attn_layer_norm',
                        'l_14': 'decoder.20.self_attn',
                        'l_15': 'decoder.20.dropout_module',
                        'l_16': 'decoder.20.final_layer_norm',
                        'l_17': 'decoder.20.fc1',
                        'l_18': 'decoder.20.activation_dropout_module',
                        'l_19': 'decoder.20.fc2',
                        'l_20': 'decoder.20.dropout_module',
                        'l_21': 'decoder.21.self_attn_layer_norm',
                        'l_22': 'decoder.21.self_attn',
                        'l_23': 'decoder.21.dropout_module',
                        'l_24': 'decoder.21.final_layer_norm',
                        'l_25': 'decoder.21.fc1',
                        'l_26': 'decoder.21.activation_dropout_module',
                        'l_27': 'decoder.21.fc2',
                        'l_28': 'decoder.21.dropout_module',
                        'l_29': 'decoder.22.self_attn_layer_norm',
                        'l_30': 'decoder.22.self_attn',
                        'l_31': 'decoder.22.dropout_module',
                        'l_32': 'decoder.22.final_layer_norm',
                        'l_33': 'decoder.22.fc1',
                        'l_34': 'decoder.22.activation_dropout_module',
                        'l_35': 'decoder.22.fc2',
                        'l_36': 'decoder.22.dropout_module',
                        'l_37': 'decoder.23.self_attn_layer_norm',
                        'l_38': 'decoder.23.self_attn',
                        'l_39': 'decoder.23.dropout_module'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/FusedLayerNorm[final_layer_norm] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/ColumnParallelLinear[fc1] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/FairseqDropout[activation_dropout_module] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/RowParallelLinear[fc2] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/FairseqDropout[dropout_module] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/ModelParallelMultiheadAttention[self_attn] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/FairseqDropout[dropout_module] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/FusedLayerNorm[final_layer_norm] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/ColumnParallelLinear[fc1] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/FairseqDropout[activation_dropout_module] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/RowParallelLinear[fc2] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[19]/FairseqDropout[dropout_module] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/ModelParallelMultiheadAttention[self_attn] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/FairseqDropout[dropout_module] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/FusedLayerNorm[final_layer_norm] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/ColumnParallelLinear[fc1] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/FairseqDropout[activation_dropout_module] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/RowParallelLinear[fc2] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[20]/FairseqDropout[dropout_module] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/ModelParallelMultiheadAttention[self_attn] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/FairseqDropout[dropout_module] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/FusedLayerNorm[final_layer_norm] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/ColumnParallelLinear[fc1] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/FairseqDropout[activation_dropout_module] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/RowParallelLinear[fc2] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[21]/FairseqDropout[dropout_module] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/ModelParallelMultiheadAttention[self_attn] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/FairseqDropout[dropout_module] <=> self.l_31
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/FusedLayerNorm[final_layer_norm] <=> self.l_32
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/ColumnParallelLinear[fc1] <=> self.l_33
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/FairseqDropout[activation_dropout_module] <=> self.l_34
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/RowParallelLinear[fc2] <=> self.l_35
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/FairseqDropout[dropout_module] <=> self.l_36
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_37
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/ModelParallelMultiheadAttention[self_attn] <=> self.l_38
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/FairseqDropout[dropout_module] <=> self.l_39
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_510 <=> x0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[18]/Tensor::__add___525 <=> x1

        # moving inputs to current device no op if already on the correct device
        x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1)
        t_0 = self.l_1(t_0)
        t_1 = t_0.float()
        t_1 = torch.nn.functional.gelu(t_1)
        t_0 = t_1.type_as(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x1 + t_0
        t_1 = t_0.size(0)
        t_2 = x0.to(t_0)
        t_3 = slice(None, t_1, None)
        t_1 = slice(None, t_1, None)
        t_1 = (t_3, t_1)
        t_1 = t_2[t_1]
        t_3 = self.l_5(t_0)
        t_1 = self.l_6(query=t_3, key=t_3, value=t_3, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_1)
        t_1 = self.l_7(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_8(t_1)
        t_0 = self.l_9(t_0)
        t_4 = t_0.float()
        t_4 = torch.nn.functional.gelu(t_4)
        t_0 = t_4.type_as(t_0)
        t_0 = self.l_10(t_0)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = t_1 + t_0
        t_1 = t_0.size(0)
        t_2 = t_2.to(t_0)
        t_4 = slice(None, t_1, None)
        t_1 = slice(None, t_1, None)
        t_1 = (t_4, t_1)
        t_1 = t_2[t_1]
        t_4 = self.l_13(t_0)
        t_1 = self.l_14(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_1)
        t_1 = self.l_15(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_16(t_1)
        t_0 = self.l_17(t_0)
        t_5 = t_0.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_0 = t_5.type_as(t_0)
        t_0 = self.l_18(t_0)
        t_0 = self.l_19(t_0)
        t_0 = self.l_20(t_0)
        t_0 = t_1 + t_0
        t_1 = t_0.size(0)
        t_2 = t_2.to(t_0)
        t_5 = slice(None, t_1, None)
        t_1 = slice(None, t_1, None)
        t_1 = (t_5, t_1)
        t_1 = t_2[t_1]
        t_5 = self.l_21(t_0)
        t_1 = self.l_22(query=t_5, key=t_5, value=t_5, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_1)
        t_1 = self.l_23(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_24(t_1)
        t_0 = self.l_25(t_0)
        t_6 = t_0.float()
        t_6 = torch.nn.functional.gelu(t_6)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_26(t_0)
        t_0 = self.l_27(t_0)
        t_0 = self.l_28(t_0)
        t_0 = t_1 + t_0
        t_1 = t_0.size(0)
        t_2 = t_2.to(t_0)
        t_6 = slice(None, t_1, None)
        t_1 = slice(None, t_1, None)
        t_1 = (t_6, t_1)
        t_1 = t_2[t_1]
        t_6 = self.l_29(t_0)
        t_1 = self.l_30(query=t_6, key=t_6, value=t_6, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_1)
        t_1 = self.l_31(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_32(t_1)
        t_0 = self.l_33(t_0)
        t_7 = t_0.float()
        t_7 = torch.nn.functional.gelu(t_7)
        t_0 = t_7.type_as(t_0)
        t_0 = self.l_34(t_0)
        t_0 = self.l_35(t_0)
        t_0 = self.l_36(t_0)
        t_0 = t_1 + t_0
        t_1 = t_0.size(0)
        t_2 = t_2.to(t_0)
        t_7 = slice(None, t_1, None)
        t_1 = slice(None, t_1, None)
        t_1 = (t_7, t_1)
        t_1 = t_2[t_1]
        t_7 = self.l_37(t_0)
        t_1 = self.l_38(query=t_7, key=t_7, value=t_7, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_1)
        t_1 = self.l_39(t_1)
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/Tensor::__add___642
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_645
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/FairseqDropout[dropout_module]
        return list(flatten((t_0, t_2, t_1)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition5(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/FusedLayerNorm[self_attn_layer_norm]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:5'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'decoder.23.final_layer_norm',
                        'l_1': 'decoder.23.fc1',
                        'l_2': 'decoder.23.activation_dropout_module',
                        'l_3': 'decoder.23.fc2',
                        'l_4': 'decoder.23.dropout_module',
                        'l_5': 'decoder.24.self_attn_layer_norm',
                        'l_6': 'decoder.24.self_attn',
                        'l_7': 'decoder.24.dropout_module',
                        'l_8': 'decoder.24.final_layer_norm',
                        'l_9': 'decoder.24.fc1',
                        'l_10': 'decoder.24.activation_dropout_module',
                        'l_11': 'decoder.24.fc2',
                        'l_12': 'decoder.24.dropout_module',
                        'l_13': 'decoder.25.self_attn_layer_norm',
                        'l_14': 'decoder.25.self_attn',
                        'l_15': 'decoder.25.dropout_module',
                        'l_16': 'decoder.25.final_layer_norm',
                        'l_17': 'decoder.25.fc1',
                        'l_18': 'decoder.25.activation_dropout_module',
                        'l_19': 'decoder.25.fc2',
                        'l_20': 'decoder.25.dropout_module',
                        'l_21': 'decoder.26.self_attn_layer_norm',
                        'l_22': 'decoder.26.self_attn',
                        'l_23': 'decoder.26.dropout_module',
                        'l_24': 'decoder.26.final_layer_norm',
                        'l_25': 'decoder.26.fc1',
                        'l_26': 'decoder.26.activation_dropout_module',
                        'l_27': 'decoder.26.fc2',
                        'l_28': 'decoder.26.dropout_module',
                        'l_29': 'decoder.27.self_attn_layer_norm',
                        'l_30': 'decoder.27.self_attn',
                        'l_31': 'decoder.27.dropout_module',
                        'l_32': 'decoder.27.final_layer_norm',
                        'l_33': 'decoder.27.fc1',
                        'l_34': 'decoder.27.activation_dropout_module',
                        'l_35': 'decoder.27.fc2',
                        'l_36': 'decoder.27.dropout_module',
                        'l_37': 'decoder.28.self_attn_layer_norm'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/FusedLayerNorm[final_layer_norm] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/ColumnParallelLinear[fc1] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/FairseqDropout[activation_dropout_module] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/RowParallelLinear[fc2] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/FairseqDropout[dropout_module] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/ModelParallelMultiheadAttention[self_attn] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/FairseqDropout[dropout_module] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/FusedLayerNorm[final_layer_norm] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/ColumnParallelLinear[fc1] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/FairseqDropout[activation_dropout_module] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/RowParallelLinear[fc2] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[24]/FairseqDropout[dropout_module] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/ModelParallelMultiheadAttention[self_attn] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/FairseqDropout[dropout_module] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/FusedLayerNorm[final_layer_norm] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/ColumnParallelLinear[fc1] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/FairseqDropout[activation_dropout_module] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/RowParallelLinear[fc2] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[25]/FairseqDropout[dropout_module] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/ModelParallelMultiheadAttention[self_attn] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/FairseqDropout[dropout_module] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/FusedLayerNorm[final_layer_norm] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/ColumnParallelLinear[fc1] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/FairseqDropout[activation_dropout_module] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/RowParallelLinear[fc2] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[26]/FairseqDropout[dropout_module] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/ModelParallelMultiheadAttention[self_attn] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/FairseqDropout[dropout_module] <=> self.l_31
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/FusedLayerNorm[final_layer_norm] <=> self.l_32
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/ColumnParallelLinear[fc1] <=> self.l_33
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/FairseqDropout[activation_dropout_module] <=> self.l_34
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/RowParallelLinear[fc2] <=> self.l_35
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/FairseqDropout[dropout_module] <=> self.l_36
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_37
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[22]/Tensor::__add___642 <=> x0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_645 <=> x1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[23]/FairseqDropout[dropout_module] <=> x2

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = x0 + x2
        t_1 = self.l_0(t_0)
        t_1 = self.l_1(t_1)
        t_2 = t_1.float()
        t_2 = torch.nn.functional.gelu(t_2)
        t_1 = t_2.type_as(t_1)
        t_1 = self.l_2(t_1)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = x1.to(t_1)
        t_3 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_3, t_0)
        t_0 = t_2[t_0]
        t_3 = self.l_5(t_1)
        t_0 = self.l_6(query=t_3, key=t_3, value=t_3, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_7(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_8(t_0)
        t_1 = self.l_9(t_1)
        t_4 = t_1.float()
        t_4 = torch.nn.functional.gelu(t_4)
        t_1 = t_4.type_as(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = self.l_12(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_4 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_4, t_0)
        t_0 = t_2[t_0]
        t_4 = self.l_13(t_1)
        t_0 = self.l_14(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_15(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_16(t_0)
        t_1 = self.l_17(t_1)
        t_5 = t_1.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_1 = t_5.type_as(t_1)
        t_1 = self.l_18(t_1)
        t_1 = self.l_19(t_1)
        t_1 = self.l_20(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_5 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_5, t_0)
        t_0 = t_2[t_0]
        t_5 = self.l_21(t_1)
        t_0 = self.l_22(query=t_5, key=t_5, value=t_5, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_23(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_24(t_0)
        t_1 = self.l_25(t_1)
        t_6 = t_1.float()
        t_6 = torch.nn.functional.gelu(t_6)
        t_1 = t_6.type_as(t_1)
        t_1 = self.l_26(t_1)
        t_1 = self.l_27(t_1)
        t_1 = self.l_28(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_6 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_6, t_0)
        t_0 = t_2[t_0]
        t_6 = self.l_29(t_1)
        t_0 = self.l_30(query=t_6, key=t_6, value=t_6, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_31(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_32(t_0)
        t_1 = self.l_33(t_1)
        t_7 = t_1.float()
        t_7 = torch.nn.functional.gelu(t_7)
        t_1 = t_7.type_as(t_1)
        t_1 = self.l_34(t_1)
        t_1 = self.l_35(t_1)
        t_1 = self.l_36(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_7 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_8 = self.l_37(t_1)
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/Tensor::__add___777
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_780
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_783
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_786
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/FusedLayerNorm[self_attn_layer_norm]
        return list(flatten((t_1, t_2, t_7, t_0, t_8)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition6(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/FairseqDropout[dropout_module]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:6'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.28.self_attn',
                        'l_1': 'decoder.28.dropout_module',
                        'l_2': 'decoder.28.final_layer_norm',
                        'l_3': 'decoder.28.fc1',
                        'l_4': 'decoder.28.activation_dropout_module',
                        'l_5': 'decoder.28.fc2',
                        'l_6': 'decoder.28.dropout_module',
                        'l_7': 'decoder.29.self_attn_layer_norm',
                        'l_8': 'decoder.29.self_attn',
                        'l_9': 'decoder.29.dropout_module',
                        'l_10': 'decoder.29.final_layer_norm',
                        'l_11': 'decoder.29.fc1',
                        'l_12': 'decoder.29.activation_dropout_module',
                        'l_13': 'decoder.29.fc2',
                        'l_14': 'decoder.29.dropout_module',
                        'l_15': 'decoder.30.self_attn_layer_norm',
                        'l_16': 'decoder.30.self_attn',
                        'l_17': 'decoder.30.dropout_module',
                        'l_18': 'decoder.30.final_layer_norm',
                        'l_19': 'decoder.30.fc1',
                        'l_20': 'decoder.30.activation_dropout_module',
                        'l_21': 'decoder.30.fc2',
                        'l_22': 'decoder.30.dropout_module',
                        'l_23': 'decoder.31.self_attn_layer_norm',
                        'l_24': 'decoder.31.self_attn',
                        'l_25': 'decoder.31.dropout_module',
                        'l_26': 'decoder.31.final_layer_norm',
                        'l_27': 'decoder.31.fc1',
                        'l_28': 'decoder.31.activation_dropout_module',
                        'l_29': 'decoder.31.fc2',
                        'l_30': 'decoder.31.dropout_module'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/ModelParallelMultiheadAttention[self_attn] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/FairseqDropout[dropout_module] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/FusedLayerNorm[final_layer_norm] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/ColumnParallelLinear[fc1] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/FairseqDropout[activation_dropout_module] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/RowParallelLinear[fc2] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/FairseqDropout[dropout_module] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/ModelParallelMultiheadAttention[self_attn] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/FairseqDropout[dropout_module] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/FusedLayerNorm[final_layer_norm] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/ColumnParallelLinear[fc1] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/FairseqDropout[activation_dropout_module] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/RowParallelLinear[fc2] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[29]/FairseqDropout[dropout_module] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/ModelParallelMultiheadAttention[self_attn] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/FairseqDropout[dropout_module] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/FusedLayerNorm[final_layer_norm] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/ColumnParallelLinear[fc1] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/FairseqDropout[activation_dropout_module] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/RowParallelLinear[fc2] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[30]/FairseqDropout[dropout_module] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/ModelParallelMultiheadAttention[self_attn] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/FairseqDropout[dropout_module] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/FusedLayerNorm[final_layer_norm] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/ColumnParallelLinear[fc1] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/FairseqDropout[activation_dropout_module] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/RowParallelLinear[fc2] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/FairseqDropout[dropout_module] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[27]/Tensor::__add___777 <=> x0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_780 <=> x1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_783 <=> x2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_786 <=> x3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[28]/FusedLayerNorm[self_attn_layer_norm] <=> x4

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = (x2, x3)
        t_0 = x1[t_0]
        t_0 = self.l_0(query=x4, key=x4, value=x4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_1(t_0)
        t_0 = x0 + t_0
        t_1 = self.l_2(t_0)
        t_1 = self.l_3(t_1)
        t_2 = t_1.float()
        t_2 = torch.nn.functional.gelu(t_2)
        t_1 = t_2.type_as(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = x1.to(t_1)
        t_3 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_3, t_0)
        t_0 = t_2[t_0]
        t_3 = self.l_7(t_1)
        t_0 = self.l_8(query=t_3, key=t_3, value=t_3, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_9(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_10(t_0)
        t_1 = self.l_11(t_1)
        t_4 = t_1.float()
        t_4 = torch.nn.functional.gelu(t_4)
        t_1 = t_4.type_as(t_1)
        t_1 = self.l_12(t_1)
        t_1 = self.l_13(t_1)
        t_1 = self.l_14(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_4 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_4, t_0)
        t_0 = t_2[t_0]
        t_4 = self.l_15(t_1)
        t_0 = self.l_16(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_17(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_18(t_0)
        t_1 = self.l_19(t_1)
        t_5 = t_1.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_1 = t_5.type_as(t_1)
        t_1 = self.l_20(t_1)
        t_1 = self.l_21(t_1)
        t_1 = self.l_22(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_5 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_5, t_0)
        t_0 = t_2[t_0]
        t_5 = self.l_23(t_1)
        t_0 = self.l_24(query=t_5, key=t_5, value=t_5, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_25(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_26(t_0)
        t_1 = self.l_27(t_1)
        t_6 = t_1.float()
        t_6 = torch.nn.functional.gelu(t_6)
        t_1 = t_6.type_as(t_1)
        t_1 = self.l_28(t_1)
        t_1 = self.l_29(t_1)
        t_1 = self.l_30(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_6 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/Tensor::__add___885
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_888
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_891
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_894
        return list(flatten((t_1, t_2, t_6, t_0)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition7(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/FusedLayerNorm[self_attn_layer_norm]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:7'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.32.self_attn_layer_norm',
                        'l_1': 'decoder.32.self_attn',
                        'l_2': 'decoder.32.dropout_module',
                        'l_3': 'decoder.32.final_layer_norm',
                        'l_4': 'decoder.32.fc1',
                        'l_5': 'decoder.32.activation_dropout_module',
                        'l_6': 'decoder.32.fc2',
                        'l_7': 'decoder.32.dropout_module',
                        'l_8': 'decoder.33.self_attn_layer_norm',
                        'l_9': 'decoder.33.self_attn',
                        'l_10': 'decoder.33.dropout_module',
                        'l_11': 'decoder.33.final_layer_norm',
                        'l_12': 'decoder.33.fc1',
                        'l_13': 'decoder.33.activation_dropout_module',
                        'l_14': 'decoder.33.fc2',
                        'l_15': 'decoder.33.dropout_module',
                        'l_16': 'decoder.34.self_attn_layer_norm',
                        'l_17': 'decoder.34.self_attn',
                        'l_18': 'decoder.34.dropout_module',
                        'l_19': 'decoder.34.final_layer_norm',
                        'l_20': 'decoder.34.fc1',
                        'l_21': 'decoder.34.activation_dropout_module',
                        'l_22': 'decoder.34.fc2',
                        'l_23': 'decoder.34.dropout_module',
                        'l_24': 'decoder.35.self_attn_layer_norm',
                        'l_25': 'decoder.35.self_attn',
                        'l_26': 'decoder.35.dropout_module',
                        'l_27': 'decoder.35.final_layer_norm',
                        'l_28': 'decoder.35.fc1',
                        'l_29': 'decoder.35.activation_dropout_module',
                        'l_30': 'decoder.35.fc2',
                        'l_31': 'decoder.35.dropout_module',
                        'l_32': 'decoder.36.self_attn_layer_norm',
                        'l_33': 'decoder.36.self_attn',
                        'l_34': 'decoder.36.dropout_module',
                        'l_35': 'decoder.36.final_layer_norm',
                        'l_36': 'decoder.36.fc1',
                        'l_37': 'decoder.36.activation_dropout_module',
                        'l_38': 'decoder.36.fc2',
                        'l_39': 'decoder.36.dropout_module',
                        'l_40': 'decoder.37.self_attn_layer_norm'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/ModelParallelMultiheadAttention[self_attn] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/FairseqDropout[dropout_module] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/FusedLayerNorm[final_layer_norm] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/ColumnParallelLinear[fc1] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/FairseqDropout[activation_dropout_module] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/RowParallelLinear[fc2] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[32]/FairseqDropout[dropout_module] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/ModelParallelMultiheadAttention[self_attn] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/FairseqDropout[dropout_module] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/FusedLayerNorm[final_layer_norm] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/ColumnParallelLinear[fc1] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/FairseqDropout[activation_dropout_module] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/RowParallelLinear[fc2] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[33]/FairseqDropout[dropout_module] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/ModelParallelMultiheadAttention[self_attn] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/FairseqDropout[dropout_module] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/FusedLayerNorm[final_layer_norm] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/ColumnParallelLinear[fc1] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/FairseqDropout[activation_dropout_module] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/RowParallelLinear[fc2] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[34]/FairseqDropout[dropout_module] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/ModelParallelMultiheadAttention[self_attn] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/FairseqDropout[dropout_module] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/FusedLayerNorm[final_layer_norm] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/ColumnParallelLinear[fc1] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/FairseqDropout[activation_dropout_module] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/RowParallelLinear[fc2] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[35]/FairseqDropout[dropout_module] <=> self.l_31
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_32
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/ModelParallelMultiheadAttention[self_attn] <=> self.l_33
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/FairseqDropout[dropout_module] <=> self.l_34
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/FusedLayerNorm[final_layer_norm] <=> self.l_35
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/ColumnParallelLinear[fc1] <=> self.l_36
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/FairseqDropout[activation_dropout_module] <=> self.l_37
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/RowParallelLinear[fc2] <=> self.l_38
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/FairseqDropout[dropout_module] <=> self.l_39
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_40
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[31]/Tensor::__add___885 <=> x0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_888 <=> x1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_891 <=> x2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_894 <=> x3

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = (x2, x3)
        t_0 = x1[t_0]
        t_1 = self.l_0(x0)
        t_0 = self.l_1(query=t_1, key=t_1, value=t_1, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_2(t_0)
        t_0 = x0 + t_0
        t_2 = self.l_3(t_0)
        t_2 = self.l_4(t_2)
        t_3 = t_2.float()
        t_3 = torch.nn.functional.gelu(t_3)
        t_2 = t_3.type_as(t_2)
        t_2 = self.l_5(t_2)
        t_2 = self.l_6(t_2)
        t_2 = self.l_7(t_2)
        t_2 = t_0 + t_2
        t_0 = t_2.size(0)
        t_3 = x1.to(t_2)
        t_4 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_4, t_0)
        t_0 = t_3[t_0]
        t_4 = self.l_8(t_2)
        t_0 = self.l_9(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_10(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_11(t_0)
        t_2 = self.l_12(t_2)
        t_5 = t_2.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_2 = t_5.type_as(t_2)
        t_2 = self.l_13(t_2)
        t_2 = self.l_14(t_2)
        t_2 = self.l_15(t_2)
        t_2 = t_0 + t_2
        t_0 = t_2.size(0)
        t_3 = t_3.to(t_2)
        t_5 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_5, t_0)
        t_0 = t_3[t_0]
        t_5 = self.l_16(t_2)
        t_0 = self.l_17(query=t_5, key=t_5, value=t_5, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_18(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_19(t_0)
        t_2 = self.l_20(t_2)
        t_6 = t_2.float()
        t_6 = torch.nn.functional.gelu(t_6)
        t_2 = t_6.type_as(t_2)
        t_2 = self.l_21(t_2)
        t_2 = self.l_22(t_2)
        t_2 = self.l_23(t_2)
        t_2 = t_0 + t_2
        t_0 = t_2.size(0)
        t_3 = t_3.to(t_2)
        t_6 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_6, t_0)
        t_0 = t_3[t_0]
        t_6 = self.l_24(t_2)
        t_0 = self.l_25(query=t_6, key=t_6, value=t_6, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_26(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_27(t_0)
        t_2 = self.l_28(t_2)
        t_7 = t_2.float()
        t_7 = torch.nn.functional.gelu(t_7)
        t_2 = t_7.type_as(t_2)
        t_2 = self.l_29(t_2)
        t_2 = self.l_30(t_2)
        t_2 = self.l_31(t_2)
        t_2 = t_0 + t_2
        t_0 = t_2.size(0)
        t_3 = t_3.to(t_2)
        t_7 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_7, t_0)
        t_0 = t_3[t_0]
        t_7 = self.l_32(t_2)
        t_0 = self.l_33(query=t_7, key=t_7, value=t_7, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_34(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_35(t_0)
        t_2 = self.l_36(t_2)
        t_8 = t_2.float()
        t_8 = torch.nn.functional.gelu(t_8)
        t_2 = t_8.type_as(t_2)
        t_2 = self.l_37(t_2)
        t_2 = self.l_38(t_2)
        t_2 = self.l_39(t_2)
        t_2 = t_0 + t_2
        t_0 = t_2.size(0)
        t_3 = t_3.to(t_2)
        t_8 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_9 = self.l_40(t_2)
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/Tensor::__add___1020
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1023
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1026
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1029
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/FusedLayerNorm[self_attn_layer_norm]
        return list(flatten((t_2, t_3, t_8, t_0, t_9)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition8(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/FairseqDropout[dropout_module]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:8'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.37.self_attn',
                        'l_1': 'decoder.37.dropout_module',
                        'l_2': 'decoder.37.final_layer_norm',
                        'l_3': 'decoder.37.fc1',
                        'l_4': 'decoder.37.activation_dropout_module',
                        'l_5': 'decoder.37.fc2',
                        'l_6': 'decoder.37.dropout_module',
                        'l_7': 'decoder.38.self_attn_layer_norm',
                        'l_8': 'decoder.38.self_attn',
                        'l_9': 'decoder.38.dropout_module',
                        'l_10': 'decoder.38.final_layer_norm',
                        'l_11': 'decoder.38.fc1',
                        'l_12': 'decoder.38.activation_dropout_module',
                        'l_13': 'decoder.38.fc2',
                        'l_14': 'decoder.38.dropout_module',
                        'l_15': 'decoder.39.self_attn_layer_norm',
                        'l_16': 'decoder.39.self_attn',
                        'l_17': 'decoder.39.dropout_module',
                        'l_18': 'decoder.39.final_layer_norm',
                        'l_19': 'decoder.39.fc1',
                        'l_20': 'decoder.39.activation_dropout_module',
                        'l_21': 'decoder.39.fc2',
                        'l_22': 'decoder.39.dropout_module',
                        'l_23': 'decoder.40.self_attn_layer_norm',
                        'l_24': 'decoder.40.self_attn',
                        'l_25': 'decoder.40.dropout_module',
                        'l_26': 'decoder.40.final_layer_norm',
                        'l_27': 'decoder.40.fc1',
                        'l_28': 'decoder.40.activation_dropout_module',
                        'l_29': 'decoder.40.fc2',
                        'l_30': 'decoder.40.dropout_module',
                        'l_31': 'decoder.41.self_attn_layer_norm',
                        'l_32': 'decoder.41.self_attn',
                        'l_33': 'decoder.41.dropout_module'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/ModelParallelMultiheadAttention[self_attn] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/FairseqDropout[dropout_module] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/FusedLayerNorm[final_layer_norm] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/ColumnParallelLinear[fc1] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/FairseqDropout[activation_dropout_module] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/RowParallelLinear[fc2] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/FairseqDropout[dropout_module] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/ModelParallelMultiheadAttention[self_attn] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/FairseqDropout[dropout_module] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/FusedLayerNorm[final_layer_norm] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/ColumnParallelLinear[fc1] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/FairseqDropout[activation_dropout_module] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/RowParallelLinear[fc2] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[38]/FairseqDropout[dropout_module] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/ModelParallelMultiheadAttention[self_attn] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/FairseqDropout[dropout_module] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/FusedLayerNorm[final_layer_norm] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/ColumnParallelLinear[fc1] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/FairseqDropout[activation_dropout_module] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/RowParallelLinear[fc2] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[39]/FairseqDropout[dropout_module] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/ModelParallelMultiheadAttention[self_attn] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/FairseqDropout[dropout_module] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/FusedLayerNorm[final_layer_norm] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/ColumnParallelLinear[fc1] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/FairseqDropout[activation_dropout_module] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/RowParallelLinear[fc2] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[40]/FairseqDropout[dropout_module] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_31
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/ModelParallelMultiheadAttention[self_attn] <=> self.l_32
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/FairseqDropout[dropout_module] <=> self.l_33
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[36]/Tensor::__add___1020 <=> x0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1023 <=> x1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1026 <=> x2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1029 <=> x3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[37]/FusedLayerNorm[self_attn_layer_norm] <=> x4

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = (x2, x3)
        t_0 = x1[t_0]
        t_0 = self.l_0(query=x4, key=x4, value=x4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_1(t_0)
        t_0 = x0 + t_0
        t_1 = self.l_2(t_0)
        t_1 = self.l_3(t_1)
        t_2 = t_1.float()
        t_2 = torch.nn.functional.gelu(t_2)
        t_1 = t_2.type_as(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = x1.to(t_1)
        t_3 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_3, t_0)
        t_0 = t_2[t_0]
        t_3 = self.l_7(t_1)
        t_0 = self.l_8(query=t_3, key=t_3, value=t_3, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_9(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_10(t_0)
        t_1 = self.l_11(t_1)
        t_4 = t_1.float()
        t_4 = torch.nn.functional.gelu(t_4)
        t_1 = t_4.type_as(t_1)
        t_1 = self.l_12(t_1)
        t_1 = self.l_13(t_1)
        t_1 = self.l_14(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_4 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_4, t_0)
        t_0 = t_2[t_0]
        t_4 = self.l_15(t_1)
        t_0 = self.l_16(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_17(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_18(t_0)
        t_1 = self.l_19(t_1)
        t_5 = t_1.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_1 = t_5.type_as(t_1)
        t_1 = self.l_20(t_1)
        t_1 = self.l_21(t_1)
        t_1 = self.l_22(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_5 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_5, t_0)
        t_0 = t_2[t_0]
        t_5 = self.l_23(t_1)
        t_0 = self.l_24(query=t_5, key=t_5, value=t_5, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_25(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_26(t_0)
        t_1 = self.l_27(t_1)
        t_6 = t_1.float()
        t_6 = torch.nn.functional.gelu(t_6)
        t_1 = t_6.type_as(t_1)
        t_1 = self.l_28(t_1)
        t_1 = self.l_29(t_1)
        t_1 = self.l_30(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_6 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_6, t_0)
        t_0 = t_2[t_0]
        t_6 = self.l_31(t_1)
        t_0 = self.l_32(query=t_6, key=t_6, value=t_6, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_33(t_0)
        t_0 = t_1 + t_0
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1131
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/Tensor::__add___1146
        return list(flatten((t_2, t_0)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition9(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/FairseqDropout[dropout_module]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:9'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = { 'l_0': 'decoder.41.final_layer_norm',
                        'l_1': 'decoder.41.fc1',
                        'l_2': 'decoder.41.activation_dropout_module',
                        'l_3': 'decoder.41.fc2',
                        'l_4': 'decoder.41.dropout_module',
                        'l_5': 'decoder.42.self_attn_layer_norm',
                        'l_6': 'decoder.42.self_attn',
                        'l_7': 'decoder.42.dropout_module',
                        'l_8': 'decoder.42.final_layer_norm',
                        'l_9': 'decoder.42.fc1',
                        'l_10': 'decoder.42.activation_dropout_module',
                        'l_11': 'decoder.42.fc2',
                        'l_12': 'decoder.42.dropout_module',
                        'l_13': 'decoder.43.self_attn_layer_norm',
                        'l_14': 'decoder.43.self_attn',
                        'l_15': 'decoder.43.dropout_module',
                        'l_16': 'decoder.43.final_layer_norm',
                        'l_17': 'decoder.43.fc1',
                        'l_18': 'decoder.43.activation_dropout_module',
                        'l_19': 'decoder.43.fc2',
                        'l_20': 'decoder.43.dropout_module',
                        'l_21': 'decoder.44.self_attn_layer_norm',
                        'l_22': 'decoder.44.self_attn',
                        'l_23': 'decoder.44.dropout_module',
                        'l_24': 'decoder.44.final_layer_norm',
                        'l_25': 'decoder.44.fc1',
                        'l_26': 'decoder.44.activation_dropout_module',
                        'l_27': 'decoder.44.fc2',
                        'l_28': 'decoder.44.dropout_module',
                        'l_29': 'decoder.45.self_attn_layer_norm',
                        'l_30': 'decoder.45.self_attn',
                        'l_31': 'decoder.45.dropout_module',
                        'l_32': 'decoder.45.final_layer_norm',
                        'l_33': 'decoder.45.fc1',
                        'l_34': 'decoder.45.activation_dropout_module',
                        'l_35': 'decoder.45.fc2',
                        'l_36': 'decoder.45.dropout_module'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/FusedLayerNorm[final_layer_norm] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/ColumnParallelLinear[fc1] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/FairseqDropout[activation_dropout_module] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/RowParallelLinear[fc2] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/FairseqDropout[dropout_module] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/ModelParallelMultiheadAttention[self_attn] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/FairseqDropout[dropout_module] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/FusedLayerNorm[final_layer_norm] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/ColumnParallelLinear[fc1] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/FairseqDropout[activation_dropout_module] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/RowParallelLinear[fc2] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[42]/FairseqDropout[dropout_module] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/ModelParallelMultiheadAttention[self_attn] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/FairseqDropout[dropout_module] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/FusedLayerNorm[final_layer_norm] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/ColumnParallelLinear[fc1] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/FairseqDropout[activation_dropout_module] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/RowParallelLinear[fc2] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[43]/FairseqDropout[dropout_module] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/ModelParallelMultiheadAttention[self_attn] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/FairseqDropout[dropout_module] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/FusedLayerNorm[final_layer_norm] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/ColumnParallelLinear[fc1] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/FairseqDropout[activation_dropout_module] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/RowParallelLinear[fc2] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[44]/FairseqDropout[dropout_module] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/ModelParallelMultiheadAttention[self_attn] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/FairseqDropout[dropout_module] <=> self.l_31
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/FusedLayerNorm[final_layer_norm] <=> self.l_32
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/ColumnParallelLinear[fc1] <=> self.l_33
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/FairseqDropout[activation_dropout_module] <=> self.l_34
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/RowParallelLinear[fc2] <=> self.l_35
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/FairseqDropout[dropout_module] <=> self.l_36
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1131 <=> x0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[41]/Tensor::__add___1146 <=> x1

        # moving inputs to current device no op if already on the correct device
        x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1)
        t_0 = self.l_1(t_0)
        t_1 = t_0.float()
        t_1 = torch.nn.functional.gelu(t_1)
        t_0 = t_1.type_as(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x1 + t_0
        t_1 = t_0.size(0)
        t_2 = x0.to(t_0)
        t_3 = slice(None, t_1, None)
        t_1 = slice(None, t_1, None)
        t_1 = (t_3, t_1)
        t_1 = t_2[t_1]
        t_3 = self.l_5(t_0)
        t_1 = self.l_6(query=t_3, key=t_3, value=t_3, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_1)
        t_1 = self.l_7(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_8(t_1)
        t_0 = self.l_9(t_0)
        t_4 = t_0.float()
        t_4 = torch.nn.functional.gelu(t_4)
        t_0 = t_4.type_as(t_0)
        t_0 = self.l_10(t_0)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = t_1 + t_0
        t_1 = t_0.size(0)
        t_2 = t_2.to(t_0)
        t_4 = slice(None, t_1, None)
        t_1 = slice(None, t_1, None)
        t_1 = (t_4, t_1)
        t_1 = t_2[t_1]
        t_4 = self.l_13(t_0)
        t_1 = self.l_14(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_1)
        t_1 = self.l_15(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_16(t_1)
        t_0 = self.l_17(t_0)
        t_5 = t_0.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_0 = t_5.type_as(t_0)
        t_0 = self.l_18(t_0)
        t_0 = self.l_19(t_0)
        t_0 = self.l_20(t_0)
        t_0 = t_1 + t_0
        t_1 = t_0.size(0)
        t_2 = t_2.to(t_0)
        t_5 = slice(None, t_1, None)
        t_1 = slice(None, t_1, None)
        t_1 = (t_5, t_1)
        t_1 = t_2[t_1]
        t_5 = self.l_21(t_0)
        t_1 = self.l_22(query=t_5, key=t_5, value=t_5, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_1)
        t_1 = self.l_23(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_24(t_1)
        t_0 = self.l_25(t_0)
        t_6 = t_0.float()
        t_6 = torch.nn.functional.gelu(t_6)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_26(t_0)
        t_0 = self.l_27(t_0)
        t_0 = self.l_28(t_0)
        t_0 = t_1 + t_0
        t_1 = t_0.size(0)
        t_2 = t_2.to(t_0)
        t_6 = slice(None, t_1, None)
        t_1 = slice(None, t_1, None)
        t_1 = (t_6, t_1)
        t_1 = t_2[t_1]
        t_6 = self.l_29(t_0)
        t_1 = self.l_30(query=t_6, key=t_6, value=t_6, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_1)
        t_1 = self.l_31(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_32(t_1)
        t_0 = self.l_33(t_0)
        t_7 = t_0.float()
        t_7 = torch.nn.functional.gelu(t_7)
        t_0 = t_7.type_as(t_0)
        t_0 = self.l_34(t_0)
        t_0 = self.l_35(t_0)
        t_0 = self.l_36(t_0)
        t_0 = t_1 + t_0
        t_1 = t_0.size(0)
        t_2 = t_2.to(t_0)
        t_7 = slice(None, t_1, None)
        t_1 = slice(None, t_1, None)
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/Tensor::__add___1263
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1266
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1269
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1272
        return list(flatten((t_0, t_2, t_7, t_1)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition10(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/FairseqDropout[dropout_module]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:10'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.46.self_attn_layer_norm',
                        'l_1': 'decoder.46.self_attn',
                        'l_2': 'decoder.46.dropout_module',
                        'l_3': 'decoder.46.final_layer_norm',
                        'l_4': 'decoder.46.fc1',
                        'l_5': 'decoder.46.activation_dropout_module',
                        'l_6': 'decoder.46.fc2',
                        'l_7': 'decoder.46.dropout_module',
                        'l_8': 'decoder.47.self_attn_layer_norm',
                        'l_9': 'decoder.47.self_attn',
                        'l_10': 'decoder.47.dropout_module',
                        'l_11': 'decoder.47.final_layer_norm',
                        'l_12': 'decoder.47.fc1',
                        'l_13': 'decoder.47.activation_dropout_module',
                        'l_14': 'decoder.47.fc2',
                        'l_15': 'decoder.47.dropout_module',
                        'l_16': 'decoder.48.self_attn_layer_norm',
                        'l_17': 'decoder.48.self_attn',
                        'l_18': 'decoder.48.dropout_module',
                        'l_19': 'decoder.48.final_layer_norm',
                        'l_20': 'decoder.48.fc1',
                        'l_21': 'decoder.48.activation_dropout_module',
                        'l_22': 'decoder.48.fc2',
                        'l_23': 'decoder.48.dropout_module',
                        'l_24': 'decoder.49.self_attn_layer_norm',
                        'l_25': 'decoder.49.self_attn',
                        'l_26': 'decoder.49.dropout_module',
                        'l_27': 'decoder.49.final_layer_norm',
                        'l_28': 'decoder.49.fc1',
                        'l_29': 'decoder.49.activation_dropout_module',
                        'l_30': 'decoder.49.fc2',
                        'l_31': 'decoder.49.dropout_module',
                        'l_32': 'decoder.50.self_attn_layer_norm',
                        'l_33': 'decoder.50.self_attn',
                        'l_34': 'decoder.50.dropout_module'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/ModelParallelMultiheadAttention[self_attn] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/FairseqDropout[dropout_module] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/FusedLayerNorm[final_layer_norm] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/ColumnParallelLinear[fc1] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/FairseqDropout[activation_dropout_module] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/RowParallelLinear[fc2] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[46]/FairseqDropout[dropout_module] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/ModelParallelMultiheadAttention[self_attn] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/FairseqDropout[dropout_module] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/FusedLayerNorm[final_layer_norm] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/ColumnParallelLinear[fc1] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/FairseqDropout[activation_dropout_module] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/RowParallelLinear[fc2] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[47]/FairseqDropout[dropout_module] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/ModelParallelMultiheadAttention[self_attn] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/FairseqDropout[dropout_module] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/FusedLayerNorm[final_layer_norm] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/ColumnParallelLinear[fc1] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/FairseqDropout[activation_dropout_module] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/RowParallelLinear[fc2] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[48]/FairseqDropout[dropout_module] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/ModelParallelMultiheadAttention[self_attn] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/FairseqDropout[dropout_module] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/FusedLayerNorm[final_layer_norm] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/ColumnParallelLinear[fc1] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/FairseqDropout[activation_dropout_module] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/RowParallelLinear[fc2] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/FairseqDropout[dropout_module] <=> self.l_31
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_32
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/ModelParallelMultiheadAttention[self_attn] <=> self.l_33
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/FairseqDropout[dropout_module] <=> self.l_34
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[45]/Tensor::__add___1263 <=> x0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1266 <=> x1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1269 <=> x2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1272 <=> x3

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = (x2, x3)
        t_0 = x1[t_0]
        t_1 = self.l_0(x0)
        t_0 = self.l_1(query=t_1, key=t_1, value=t_1, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_2(t_0)
        t_0 = x0 + t_0
        t_2 = self.l_3(t_0)
        t_2 = self.l_4(t_2)
        t_3 = t_2.float()
        t_3 = torch.nn.functional.gelu(t_3)
        t_2 = t_3.type_as(t_2)
        t_2 = self.l_5(t_2)
        t_2 = self.l_6(t_2)
        t_2 = self.l_7(t_2)
        t_2 = t_0 + t_2
        t_0 = t_2.size(0)
        t_3 = x1.to(t_2)
        t_4 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_4, t_0)
        t_0 = t_3[t_0]
        t_4 = self.l_8(t_2)
        t_0 = self.l_9(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_10(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_11(t_0)
        t_2 = self.l_12(t_2)
        t_5 = t_2.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_2 = t_5.type_as(t_2)
        t_2 = self.l_13(t_2)
        t_2 = self.l_14(t_2)
        t_2 = self.l_15(t_2)
        t_2 = t_0 + t_2
        t_0 = t_2.size(0)
        t_3 = t_3.to(t_2)
        t_5 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_5, t_0)
        t_0 = t_3[t_0]
        t_5 = self.l_16(t_2)
        t_0 = self.l_17(query=t_5, key=t_5, value=t_5, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_18(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_19(t_0)
        t_2 = self.l_20(t_2)
        t_6 = t_2.float()
        t_6 = torch.nn.functional.gelu(t_6)
        t_2 = t_6.type_as(t_2)
        t_2 = self.l_21(t_2)
        t_2 = self.l_22(t_2)
        t_2 = self.l_23(t_2)
        t_2 = t_0 + t_2
        t_0 = t_2.size(0)
        t_3 = t_3.to(t_2)
        t_6 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_6, t_0)
        t_0 = t_3[t_0]
        t_6 = self.l_24(t_2)
        t_0 = self.l_25(query=t_6, key=t_6, value=t_6, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_26(t_0)
        t_0 = t_2 + t_0
        t_2 = self.l_27(t_0)
        t_2 = self.l_28(t_2)
        t_7 = t_2.float()
        t_7 = torch.nn.functional.gelu(t_7)
        t_2 = t_7.type_as(t_2)
        t_2 = self.l_29(t_2)
        t_2 = self.l_30(t_2)
        t_2 = self.l_31(t_2)
        t_2 = t_0 + t_2
        t_0 = t_2.size(0)
        t_3 = t_3.to(t_2)
        t_7 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_7, t_0)
        t_0 = t_3[t_0]
        t_7 = self.l_32(t_2)
        t_0 = self.l_33(query=t_7, key=t_7, value=t_7, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_34(t_0)
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/Tensor::__add___1371
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1374
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/FairseqDropout[dropout_module]
        return list(flatten((t_2, t_3, t_0)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition11(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/FusedLayerNorm[self_attn_layer_norm]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:11'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'decoder.50.final_layer_norm',
                        'l_1': 'decoder.50.fc1',
                        'l_2': 'decoder.50.activation_dropout_module',
                        'l_3': 'decoder.50.fc2',
                        'l_4': 'decoder.50.dropout_module',
                        'l_5': 'decoder.51.self_attn_layer_norm',
                        'l_6': 'decoder.51.self_attn',
                        'l_7': 'decoder.51.dropout_module',
                        'l_8': 'decoder.51.final_layer_norm',
                        'l_9': 'decoder.51.fc1',
                        'l_10': 'decoder.51.activation_dropout_module',
                        'l_11': 'decoder.51.fc2',
                        'l_12': 'decoder.51.dropout_module',
                        'l_13': 'decoder.52.self_attn_layer_norm',
                        'l_14': 'decoder.52.self_attn',
                        'l_15': 'decoder.52.dropout_module',
                        'l_16': 'decoder.52.final_layer_norm',
                        'l_17': 'decoder.52.fc1',
                        'l_18': 'decoder.52.activation_dropout_module',
                        'l_19': 'decoder.52.fc2',
                        'l_20': 'decoder.52.dropout_module',
                        'l_21': 'decoder.53.self_attn_layer_norm',
                        'l_22': 'decoder.53.self_attn',
                        'l_23': 'decoder.53.dropout_module',
                        'l_24': 'decoder.53.final_layer_norm',
                        'l_25': 'decoder.53.fc1',
                        'l_26': 'decoder.53.activation_dropout_module',
                        'l_27': 'decoder.53.fc2',
                        'l_28': 'decoder.53.dropout_module',
                        'l_29': 'decoder.54.self_attn_layer_norm',
                        'l_30': 'decoder.54.self_attn',
                        'l_31': 'decoder.54.dropout_module',
                        'l_32': 'decoder.54.final_layer_norm',
                        'l_33': 'decoder.54.fc1',
                        'l_34': 'decoder.54.activation_dropout_module',
                        'l_35': 'decoder.54.fc2',
                        'l_36': 'decoder.54.dropout_module',
                        'l_37': 'decoder.55.self_attn_layer_norm'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/FusedLayerNorm[final_layer_norm] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/ColumnParallelLinear[fc1] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/FairseqDropout[activation_dropout_module] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/RowParallelLinear[fc2] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/FairseqDropout[dropout_module] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/ModelParallelMultiheadAttention[self_attn] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/FairseqDropout[dropout_module] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/FusedLayerNorm[final_layer_norm] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/ColumnParallelLinear[fc1] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/FairseqDropout[activation_dropout_module] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/RowParallelLinear[fc2] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[51]/FairseqDropout[dropout_module] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/ModelParallelMultiheadAttention[self_attn] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/FairseqDropout[dropout_module] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/FusedLayerNorm[final_layer_norm] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/ColumnParallelLinear[fc1] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/FairseqDropout[activation_dropout_module] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/RowParallelLinear[fc2] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[52]/FairseqDropout[dropout_module] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/ModelParallelMultiheadAttention[self_attn] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/FairseqDropout[dropout_module] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/FusedLayerNorm[final_layer_norm] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/ColumnParallelLinear[fc1] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/FairseqDropout[activation_dropout_module] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/RowParallelLinear[fc2] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[53]/FairseqDropout[dropout_module] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/ModelParallelMultiheadAttention[self_attn] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/FairseqDropout[dropout_module] <=> self.l_31
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/FusedLayerNorm[final_layer_norm] <=> self.l_32
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/ColumnParallelLinear[fc1] <=> self.l_33
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/FairseqDropout[activation_dropout_module] <=> self.l_34
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/RowParallelLinear[fc2] <=> self.l_35
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/FairseqDropout[dropout_module] <=> self.l_36
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_37
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[49]/Tensor::__add___1371 <=> x0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1374 <=> x1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[50]/FairseqDropout[dropout_module] <=> x2

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = x0 + x2
        t_1 = self.l_0(t_0)
        t_1 = self.l_1(t_1)
        t_2 = t_1.float()
        t_2 = torch.nn.functional.gelu(t_2)
        t_1 = t_2.type_as(t_1)
        t_1 = self.l_2(t_1)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = x1.to(t_1)
        t_3 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_3, t_0)
        t_0 = t_2[t_0]
        t_3 = self.l_5(t_1)
        t_0 = self.l_6(query=t_3, key=t_3, value=t_3, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_7(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_8(t_0)
        t_1 = self.l_9(t_1)
        t_4 = t_1.float()
        t_4 = torch.nn.functional.gelu(t_4)
        t_1 = t_4.type_as(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = self.l_12(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_4 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_4, t_0)
        t_0 = t_2[t_0]
        t_4 = self.l_13(t_1)
        t_0 = self.l_14(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_15(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_16(t_0)
        t_1 = self.l_17(t_1)
        t_5 = t_1.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_1 = t_5.type_as(t_1)
        t_1 = self.l_18(t_1)
        t_1 = self.l_19(t_1)
        t_1 = self.l_20(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_5 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_5, t_0)
        t_0 = t_2[t_0]
        t_5 = self.l_21(t_1)
        t_0 = self.l_22(query=t_5, key=t_5, value=t_5, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_23(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_24(t_0)
        t_1 = self.l_25(t_1)
        t_6 = t_1.float()
        t_6 = torch.nn.functional.gelu(t_6)
        t_1 = t_6.type_as(t_1)
        t_1 = self.l_26(t_1)
        t_1 = self.l_27(t_1)
        t_1 = self.l_28(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_6 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_6, t_0)
        t_0 = t_2[t_0]
        t_6 = self.l_29(t_1)
        t_0 = self.l_30(query=t_6, key=t_6, value=t_6, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_31(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_32(t_0)
        t_1 = self.l_33(t_1)
        t_7 = t_1.float()
        t_7 = torch.nn.functional.gelu(t_7)
        t_1 = t_7.type_as(t_1)
        t_1 = self.l_34(t_1)
        t_1 = self.l_35(t_1)
        t_1 = self.l_36(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_7 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_8 = self.l_37(t_1)
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/Tensor::__add___1506
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1509
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1512
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1515
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/FusedLayerNorm[self_attn_layer_norm]
        return list(flatten((t_1, t_2, t_7, t_0, t_8)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition12(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/FairseqDropout[dropout_module]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:12'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.55.self_attn',
                        'l_1': 'decoder.55.dropout_module',
                        'l_2': 'decoder.55.final_layer_norm',
                        'l_3': 'decoder.55.fc1',
                        'l_4': 'decoder.55.activation_dropout_module',
                        'l_5': 'decoder.55.fc2',
                        'l_6': 'decoder.55.dropout_module',
                        'l_7': 'decoder.56.self_attn_layer_norm',
                        'l_8': 'decoder.56.self_attn',
                        'l_9': 'decoder.56.dropout_module',
                        'l_10': 'decoder.56.final_layer_norm',
                        'l_11': 'decoder.56.fc1',
                        'l_12': 'decoder.56.activation_dropout_module',
                        'l_13': 'decoder.56.fc2',
                        'l_14': 'decoder.56.dropout_module',
                        'l_15': 'decoder.57.self_attn_layer_norm',
                        'l_16': 'decoder.57.self_attn',
                        'l_17': 'decoder.57.dropout_module',
                        'l_18': 'decoder.57.final_layer_norm',
                        'l_19': 'decoder.57.fc1',
                        'l_20': 'decoder.57.activation_dropout_module',
                        'l_21': 'decoder.57.fc2',
                        'l_22': 'decoder.57.dropout_module',
                        'l_23': 'decoder.58.self_attn_layer_norm',
                        'l_24': 'decoder.58.self_attn',
                        'l_25': 'decoder.58.dropout_module',
                        'l_26': 'decoder.58.final_layer_norm',
                        'l_27': 'decoder.58.fc1',
                        'l_28': 'decoder.58.activation_dropout_module',
                        'l_29': 'decoder.58.fc2',
                        'l_30': 'decoder.58.dropout_module',
                        'l_31': 'decoder.59.self_attn_layer_norm',
                        'l_32': 'decoder.59.self_attn',
                        'l_33': 'decoder.59.dropout_module'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/ModelParallelMultiheadAttention[self_attn] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/FairseqDropout[dropout_module] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/FusedLayerNorm[final_layer_norm] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/ColumnParallelLinear[fc1] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/FairseqDropout[activation_dropout_module] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/RowParallelLinear[fc2] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/FairseqDropout[dropout_module] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/ModelParallelMultiheadAttention[self_attn] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/FairseqDropout[dropout_module] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/FusedLayerNorm[final_layer_norm] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/ColumnParallelLinear[fc1] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/FairseqDropout[activation_dropout_module] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/RowParallelLinear[fc2] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[56]/FairseqDropout[dropout_module] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/ModelParallelMultiheadAttention[self_attn] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/FairseqDropout[dropout_module] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/FusedLayerNorm[final_layer_norm] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/ColumnParallelLinear[fc1] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/FairseqDropout[activation_dropout_module] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/RowParallelLinear[fc2] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[57]/FairseqDropout[dropout_module] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/ModelParallelMultiheadAttention[self_attn] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/FairseqDropout[dropout_module] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/FusedLayerNorm[final_layer_norm] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/ColumnParallelLinear[fc1] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/FairseqDropout[activation_dropout_module] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/RowParallelLinear[fc2] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[58]/FairseqDropout[dropout_module] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_31
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/ModelParallelMultiheadAttention[self_attn] <=> self.l_32
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/FairseqDropout[dropout_module] <=> self.l_33
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[54]/Tensor::__add___1506 <=> x0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1509 <=> x1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1512 <=> x2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1515 <=> x3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[55]/FusedLayerNorm[self_attn_layer_norm] <=> x4

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = (x2, x3)
        t_0 = x1[t_0]
        t_0 = self.l_0(query=x4, key=x4, value=x4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_1(t_0)
        t_0 = x0 + t_0
        t_1 = self.l_2(t_0)
        t_1 = self.l_3(t_1)
        t_2 = t_1.float()
        t_2 = torch.nn.functional.gelu(t_2)
        t_1 = t_2.type_as(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = x1.to(t_1)
        t_3 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_3, t_0)
        t_0 = t_2[t_0]
        t_3 = self.l_7(t_1)
        t_0 = self.l_8(query=t_3, key=t_3, value=t_3, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_9(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_10(t_0)
        t_1 = self.l_11(t_1)
        t_4 = t_1.float()
        t_4 = torch.nn.functional.gelu(t_4)
        t_1 = t_4.type_as(t_1)
        t_1 = self.l_12(t_1)
        t_1 = self.l_13(t_1)
        t_1 = self.l_14(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_4 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_4, t_0)
        t_0 = t_2[t_0]
        t_4 = self.l_15(t_1)
        t_0 = self.l_16(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_17(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_18(t_0)
        t_1 = self.l_19(t_1)
        t_5 = t_1.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_1 = t_5.type_as(t_1)
        t_1 = self.l_20(t_1)
        t_1 = self.l_21(t_1)
        t_1 = self.l_22(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_5 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_5, t_0)
        t_0 = t_2[t_0]
        t_5 = self.l_23(t_1)
        t_0 = self.l_24(query=t_5, key=t_5, value=t_5, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_25(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_26(t_0)
        t_1 = self.l_27(t_1)
        t_6 = t_1.float()
        t_6 = torch.nn.functional.gelu(t_6)
        t_1 = t_6.type_as(t_1)
        t_1 = self.l_28(t_1)
        t_1 = self.l_29(t_1)
        t_1 = self.l_30(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_6 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_6, t_0)
        t_0 = t_2[t_0]
        t_6 = self.l_31(t_1)
        t_0 = self.l_32(query=t_6, key=t_6, value=t_6, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_33(t_0)
        t_0 = t_1 + t_0
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1617
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/Tensor::__add___1632
        return list(flatten((t_2, t_0)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition13(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/FusedLayerNorm[self_attn_layer_norm]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:13'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = { 'l_0': 'decoder.59.final_layer_norm',
                        'l_1': 'decoder.59.fc1',
                        'l_2': 'decoder.59.activation_dropout_module',
                        'l_3': 'decoder.59.fc2',
                        'l_4': 'decoder.59.dropout_module',
                        'l_5': 'decoder.60.self_attn_layer_norm',
                        'l_6': 'decoder.60.self_attn',
                        'l_7': 'decoder.60.dropout_module',
                        'l_8': 'decoder.60.final_layer_norm',
                        'l_9': 'decoder.60.fc1',
                        'l_10': 'decoder.60.activation_dropout_module',
                        'l_11': 'decoder.60.fc2',
                        'l_12': 'decoder.60.dropout_module',
                        'l_13': 'decoder.61.self_attn_layer_norm',
                        'l_14': 'decoder.61.self_attn',
                        'l_15': 'decoder.61.dropout_module',
                        'l_16': 'decoder.61.final_layer_norm',
                        'l_17': 'decoder.61.fc1',
                        'l_18': 'decoder.61.activation_dropout_module',
                        'l_19': 'decoder.61.fc2',
                        'l_20': 'decoder.61.dropout_module',
                        'l_21': 'decoder.62.self_attn_layer_norm',
                        'l_22': 'decoder.62.self_attn',
                        'l_23': 'decoder.62.dropout_module',
                        'l_24': 'decoder.62.final_layer_norm',
                        'l_25': 'decoder.62.fc1',
                        'l_26': 'decoder.62.activation_dropout_module',
                        'l_27': 'decoder.62.fc2',
                        'l_28': 'decoder.62.dropout_module',
                        'l_29': 'decoder.63.self_attn_layer_norm',
                        'l_30': 'decoder.63.self_attn',
                        'l_31': 'decoder.63.dropout_module',
                        'l_32': 'decoder.63.final_layer_norm',
                        'l_33': 'decoder.63.fc1',
                        'l_34': 'decoder.63.activation_dropout_module',
                        'l_35': 'decoder.63.fc2',
                        'l_36': 'decoder.63.dropout_module',
                        'l_37': 'decoder.64.self_attn_layer_norm'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/FusedLayerNorm[final_layer_norm] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/ColumnParallelLinear[fc1] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/FairseqDropout[activation_dropout_module] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/RowParallelLinear[fc2] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/FairseqDropout[dropout_module] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/ModelParallelMultiheadAttention[self_attn] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/FairseqDropout[dropout_module] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/FusedLayerNorm[final_layer_norm] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/ColumnParallelLinear[fc1] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/FairseqDropout[activation_dropout_module] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/RowParallelLinear[fc2] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[60]/FairseqDropout[dropout_module] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/ModelParallelMultiheadAttention[self_attn] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/FairseqDropout[dropout_module] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/FusedLayerNorm[final_layer_norm] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/ColumnParallelLinear[fc1] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/FairseqDropout[activation_dropout_module] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/RowParallelLinear[fc2] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[61]/FairseqDropout[dropout_module] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/ModelParallelMultiheadAttention[self_attn] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/FairseqDropout[dropout_module] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/FusedLayerNorm[final_layer_norm] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/ColumnParallelLinear[fc1] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/FairseqDropout[activation_dropout_module] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/RowParallelLinear[fc2] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[62]/FairseqDropout[dropout_module] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/ModelParallelMultiheadAttention[self_attn] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/FairseqDropout[dropout_module] <=> self.l_31
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/FusedLayerNorm[final_layer_norm] <=> self.l_32
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/ColumnParallelLinear[fc1] <=> self.l_33
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/FairseqDropout[activation_dropout_module] <=> self.l_34
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/RowParallelLinear[fc2] <=> self.l_35
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/FairseqDropout[dropout_module] <=> self.l_36
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_37
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1617 <=> x0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[59]/Tensor::__add___1632 <=> x1

        # moving inputs to current device no op if already on the correct device
        x0, x1 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x1)
        t_0 = self.l_1(t_0)
        t_1 = t_0.float()
        t_1 = torch.nn.functional.gelu(t_1)
        t_0 = t_1.type_as(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x1 + t_0
        t_1 = t_0.size(0)
        t_2 = x0.to(t_0)
        t_3 = slice(None, t_1, None)
        t_1 = slice(None, t_1, None)
        t_1 = (t_3, t_1)
        t_1 = t_2[t_1]
        t_3 = self.l_5(t_0)
        t_1 = self.l_6(query=t_3, key=t_3, value=t_3, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_1)
        t_1 = self.l_7(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_8(t_1)
        t_0 = self.l_9(t_0)
        t_4 = t_0.float()
        t_4 = torch.nn.functional.gelu(t_4)
        t_0 = t_4.type_as(t_0)
        t_0 = self.l_10(t_0)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = t_1 + t_0
        t_1 = t_0.size(0)
        t_2 = t_2.to(t_0)
        t_4 = slice(None, t_1, None)
        t_1 = slice(None, t_1, None)
        t_1 = (t_4, t_1)
        t_1 = t_2[t_1]
        t_4 = self.l_13(t_0)
        t_1 = self.l_14(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_1)
        t_1 = self.l_15(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_16(t_1)
        t_0 = self.l_17(t_0)
        t_5 = t_0.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_0 = t_5.type_as(t_0)
        t_0 = self.l_18(t_0)
        t_0 = self.l_19(t_0)
        t_0 = self.l_20(t_0)
        t_0 = t_1 + t_0
        t_1 = t_0.size(0)
        t_2 = t_2.to(t_0)
        t_5 = slice(None, t_1, None)
        t_1 = slice(None, t_1, None)
        t_1 = (t_5, t_1)
        t_1 = t_2[t_1]
        t_5 = self.l_21(t_0)
        t_1 = self.l_22(query=t_5, key=t_5, value=t_5, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_1)
        t_1 = self.l_23(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_24(t_1)
        t_0 = self.l_25(t_0)
        t_6 = t_0.float()
        t_6 = torch.nn.functional.gelu(t_6)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_26(t_0)
        t_0 = self.l_27(t_0)
        t_0 = self.l_28(t_0)
        t_0 = t_1 + t_0
        t_1 = t_0.size(0)
        t_2 = t_2.to(t_0)
        t_6 = slice(None, t_1, None)
        t_1 = slice(None, t_1, None)
        t_1 = (t_6, t_1)
        t_1 = t_2[t_1]
        t_6 = self.l_29(t_0)
        t_1 = self.l_30(query=t_6, key=t_6, value=t_6, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_1)
        t_1 = self.l_31(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_32(t_1)
        t_0 = self.l_33(t_0)
        t_7 = t_0.float()
        t_7 = torch.nn.functional.gelu(t_7)
        t_0 = t_7.type_as(t_0)
        t_0 = self.l_34(t_0)
        t_0 = self.l_35(t_0)
        t_0 = self.l_36(t_0)
        t_0 = t_1 + t_0
        t_1 = t_0.size(0)
        t_2 = t_2.to(t_0)
        t_7 = slice(None, t_1, None)
        t_1 = slice(None, t_1, None)
        t_8 = self.l_37(t_0)
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/Tensor::__add___1749
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1752
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1755
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1758
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/FusedLayerNorm[self_attn_layer_norm]
        return list(flatten((t_0, t_2, t_7, t_1, t_8)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition14(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/ModelParallelMultiheadAttention[self_attn]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:14'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1]
        self.lookup = { 'l_0': 'decoder.64.self_attn',
                        'l_1': 'decoder.64.dropout_module',
                        'l_2': 'decoder.64.final_layer_norm',
                        'l_3': 'decoder.64.fc1',
                        'l_4': 'decoder.64.activation_dropout_module',
                        'l_5': 'decoder.64.fc2',
                        'l_6': 'decoder.64.dropout_module',
                        'l_7': 'decoder.65.self_attn_layer_norm',
                        'l_8': 'decoder.65.self_attn',
                        'l_9': 'decoder.65.dropout_module',
                        'l_10': 'decoder.65.final_layer_norm',
                        'l_11': 'decoder.65.fc1',
                        'l_12': 'decoder.65.activation_dropout_module',
                        'l_13': 'decoder.65.fc2',
                        'l_14': 'decoder.65.dropout_module',
                        'l_15': 'decoder.66.self_attn_layer_norm',
                        'l_16': 'decoder.66.self_attn',
                        'l_17': 'decoder.66.dropout_module',
                        'l_18': 'decoder.66.final_layer_norm',
                        'l_19': 'decoder.66.fc1',
                        'l_20': 'decoder.66.activation_dropout_module',
                        'l_21': 'decoder.66.fc2',
                        'l_22': 'decoder.66.dropout_module',
                        'l_23': 'decoder.67.self_attn_layer_norm',
                        'l_24': 'decoder.67.self_attn',
                        'l_25': 'decoder.67.dropout_module',
                        'l_26': 'decoder.67.final_layer_norm',
                        'l_27': 'decoder.67.fc1',
                        'l_28': 'decoder.67.activation_dropout_module',
                        'l_29': 'decoder.67.fc2',
                        'l_30': 'decoder.67.dropout_module',
                        'l_31': 'decoder.68.self_attn_layer_norm',
                        'l_32': 'decoder.68.self_attn'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/ModelParallelMultiheadAttention[self_attn] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/FairseqDropout[dropout_module] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/FusedLayerNorm[final_layer_norm] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/ColumnParallelLinear[fc1] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/FairseqDropout[activation_dropout_module] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/RowParallelLinear[fc2] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/FairseqDropout[dropout_module] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/ModelParallelMultiheadAttention[self_attn] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/FairseqDropout[dropout_module] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/FusedLayerNorm[final_layer_norm] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/ColumnParallelLinear[fc1] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/FairseqDropout[activation_dropout_module] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/RowParallelLinear[fc2] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[65]/FairseqDropout[dropout_module] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/ModelParallelMultiheadAttention[self_attn] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/FairseqDropout[dropout_module] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/FusedLayerNorm[final_layer_norm] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/ColumnParallelLinear[fc1] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/FairseqDropout[activation_dropout_module] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/RowParallelLinear[fc2] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[66]/FairseqDropout[dropout_module] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/ModelParallelMultiheadAttention[self_attn] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/FairseqDropout[dropout_module] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/FusedLayerNorm[final_layer_norm] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/ColumnParallelLinear[fc1] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/FairseqDropout[activation_dropout_module] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/RowParallelLinear[fc2] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/FairseqDropout[dropout_module] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_31
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/ModelParallelMultiheadAttention[self_attn] <=> self.l_32
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[63]/Tensor::__add___1749 <=> x0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1752 <=> x1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1755 <=> x2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/prim::SliceConstruct_1758 <=> x3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[64]/FusedLayerNorm[self_attn_layer_norm] <=> x4

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = (x2, x3)
        t_0 = x1[t_0]
        t_0 = self.l_0(query=x4, key=x4, value=x4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_1(t_0)
        t_0 = x0 + t_0
        t_1 = self.l_2(t_0)
        t_1 = self.l_3(t_1)
        t_2 = t_1.float()
        t_2 = torch.nn.functional.gelu(t_2)
        t_1 = t_2.type_as(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = x1.to(t_1)
        t_3 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_3, t_0)
        t_0 = t_2[t_0]
        t_3 = self.l_7(t_1)
        t_0 = self.l_8(query=t_3, key=t_3, value=t_3, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_9(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_10(t_0)
        t_1 = self.l_11(t_1)
        t_4 = t_1.float()
        t_4 = torch.nn.functional.gelu(t_4)
        t_1 = t_4.type_as(t_1)
        t_1 = self.l_12(t_1)
        t_1 = self.l_13(t_1)
        t_1 = self.l_14(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_4 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_4, t_0)
        t_0 = t_2[t_0]
        t_4 = self.l_15(t_1)
        t_0 = self.l_16(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_17(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_18(t_0)
        t_1 = self.l_19(t_1)
        t_5 = t_1.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_1 = t_5.type_as(t_1)
        t_1 = self.l_20(t_1)
        t_1 = self.l_21(t_1)
        t_1 = self.l_22(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_5 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_5, t_0)
        t_0 = t_2[t_0]
        t_5 = self.l_23(t_1)
        t_0 = self.l_24(query=t_5, key=t_5, value=t_5, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_25(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_26(t_0)
        t_1 = self.l_27(t_1)
        t_6 = t_1.float()
        t_6 = torch.nn.functional.gelu(t_6)
        t_1 = t_6.type_as(t_1)
        t_1 = self.l_28(t_1)
        t_1 = self.l_29(t_1)
        t_1 = self.l_30(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_6 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_6, t_0)
        t_0 = t_2[t_0]
        t_6 = self.l_31(t_1)
        t_0 = self.l_32(query=t_6, key=t_6, value=t_6, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/Tensor::__add___1857
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1860
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/ModelParallelMultiheadAttention[self_attn]
        return list(flatten((t_1, t_2, t_0)))

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition15(nn.Module):
    LAYER_SCOPES=[
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/FusedLayerNorm[self_attn_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/ModelParallelMultiheadAttention[self_attn]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/FusedLayerNorm[final_layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/ColumnParallelLinear[fc1]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/FairseqDropout[activation_dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/RowParallelLinear[fc2]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/FairseqDropout[dropout_module]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/FusedLayerNorm[layer_norm]',
            'ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Linear[output_projection]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors, device='cuda:15'):
        super().__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1]
        self.lookup = { 'l_0': 'decoder.68.dropout_module',
                        'l_1': 'decoder.68.final_layer_norm',
                        'l_2': 'decoder.68.fc1',
                        'l_3': 'decoder.68.activation_dropout_module',
                        'l_4': 'decoder.68.fc2',
                        'l_5': 'decoder.68.dropout_module',
                        'l_6': 'decoder.69.self_attn_layer_norm',
                        'l_7': 'decoder.69.self_attn',
                        'l_8': 'decoder.69.dropout_module',
                        'l_9': 'decoder.69.final_layer_norm',
                        'l_10': 'decoder.69.fc1',
                        'l_11': 'decoder.69.activation_dropout_module',
                        'l_12': 'decoder.69.fc2',
                        'l_13': 'decoder.69.dropout_module',
                        'l_14': 'decoder.70.self_attn_layer_norm',
                        'l_15': 'decoder.70.self_attn',
                        'l_16': 'decoder.70.dropout_module',
                        'l_17': 'decoder.70.final_layer_norm',
                        'l_18': 'decoder.70.fc1',
                        'l_19': 'decoder.70.activation_dropout_module',
                        'l_20': 'decoder.70.fc2',
                        'l_21': 'decoder.70.dropout_module',
                        'l_22': 'decoder.71.self_attn_layer_norm',
                        'l_23': 'decoder.71.self_attn',
                        'l_24': 'decoder.71.dropout_module',
                        'l_25': 'decoder.71.final_layer_norm',
                        'l_26': 'decoder.71.fc1',
                        'l_27': 'decoder.71.activation_dropout_module',
                        'l_28': 'decoder.71.fc2',
                        'l_29': 'decoder.71.dropout_module',
                        'l_30': 'decoder.layer_norm',
                        'l_31': 'decoder.output_projection'}
        self.to(self.device)

    def forward(self, *args):
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/FairseqDropout[dropout_module] <=> self.l_0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/FusedLayerNorm[final_layer_norm] <=> self.l_1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/ColumnParallelLinear[fc1] <=> self.l_2
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/FairseqDropout[activation_dropout_module] <=> self.l_3
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/RowParallelLinear[fc2] <=> self.l_4
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/FairseqDropout[dropout_module] <=> self.l_5
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_6
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/ModelParallelMultiheadAttention[self_attn] <=> self.l_7
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/FairseqDropout[dropout_module] <=> self.l_8
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/FusedLayerNorm[final_layer_norm] <=> self.l_9
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/ColumnParallelLinear[fc1] <=> self.l_10
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/FairseqDropout[activation_dropout_module] <=> self.l_11
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/RowParallelLinear[fc2] <=> self.l_12
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[69]/FairseqDropout[dropout_module] <=> self.l_13
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_14
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/ModelParallelMultiheadAttention[self_attn] <=> self.l_15
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/FairseqDropout[dropout_module] <=> self.l_16
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/FusedLayerNorm[final_layer_norm] <=> self.l_17
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/ColumnParallelLinear[fc1] <=> self.l_18
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/FairseqDropout[activation_dropout_module] <=> self.l_19
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/RowParallelLinear[fc2] <=> self.l_20
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[70]/FairseqDropout[dropout_module] <=> self.l_21
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/FusedLayerNorm[self_attn_layer_norm] <=> self.l_22
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/ModelParallelMultiheadAttention[self_attn] <=> self.l_23
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/FairseqDropout[dropout_module] <=> self.l_24
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/FusedLayerNorm[final_layer_norm] <=> self.l_25
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/ColumnParallelLinear[fc1] <=> self.l_26
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/FairseqDropout[activation_dropout_module] <=> self.l_27
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/RowParallelLinear[fc2] <=> self.l_28
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[71]/FairseqDropout[dropout_module] <=> self.l_29
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/FusedLayerNorm[layer_norm] <=> self.l_30
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Linear[output_projection] <=> self.l_31
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[67]/Tensor::__add___1857 <=> x0
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Tensor::to_1860 <=> x1
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/ModelParallelTransformerDecoderLayer[68]/ModelParallelMultiheadAttention[self_attn] <=> x2

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2 = move_tensors(unflatten(args,self.input_structure), self.device)
        t_0 = self.l_0(x2)
        t_0 = x0 + t_0
        t_1 = self.l_1(t_0)
        t_1 = self.l_2(t_1)
        t_2 = t_1.float()
        t_2 = torch.nn.functional.gelu(t_2)
        t_1 = t_2.type_as(t_1)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = self.l_5(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = x1.to(t_1)
        t_3 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_3, t_0)
        t_0 = t_2[t_0]
        t_3 = self.l_6(t_1)
        t_0 = self.l_7(query=t_3, key=t_3, value=t_3, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_8(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_9(t_0)
        t_1 = self.l_10(t_1)
        t_4 = t_1.float()
        t_4 = torch.nn.functional.gelu(t_4)
        t_1 = t_4.type_as(t_1)
        t_1 = self.l_11(t_1)
        t_1 = self.l_12(t_1)
        t_1 = self.l_13(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_4 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_4, t_0)
        t_0 = t_2[t_0]
        t_4 = self.l_14(t_1)
        t_0 = self.l_15(query=t_4, key=t_4, value=t_4, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_16(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_17(t_0)
        t_1 = self.l_18(t_1)
        t_5 = t_1.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_1 = t_5.type_as(t_1)
        t_1 = self.l_19(t_1)
        t_1 = self.l_20(t_1)
        t_1 = self.l_21(t_1)
        t_1 = t_0 + t_1
        t_0 = t_1.size(0)
        t_2 = t_2.to(t_1)
        t_5 = slice(None, t_0, None)
        t_0 = slice(None, t_0, None)
        t_0 = (t_5, t_0)
        t_0 = t_2[t_0]
        t_2 = self.l_22(t_1)
        t_0 = self.l_23(query=t_2, key=t_2, value=t_2, key_padding_mask=None, incremental_state=None, need_weights=False, attn_mask=t_0)
        t_0 = self.l_24(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_25(t_0)
        t_1 = self.l_26(t_1)
        t_5 = t_1.float()
        t_5 = torch.nn.functional.gelu(t_5)
        t_1 = t_5.type_as(t_1)
        t_1 = self.l_27(t_1)
        t_1 = self.l_28(t_1)
        t_1 = self.l_29(t_1)
        t_1 = t_0 + t_1
        t_1 = self.l_30(t_1)
        t_1 = t_1.transpose(0, 1)
        t_1 = self.l_31(t_1)
        # returning:
        # ModelParallelTransformerLanguageModel/ModelParallelTransformerDecoder[decoder]/Linear[output_projection]
        return (t_1,)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


def traverse_model(module: nn.Module, depth: int, prefix: Optional[str] = None,
                   basic_blocks: Tuple[nn.Module] = (), full: bool = False) -> Iterator[Tuple[nn.Module, str, nn.Module]]:
    '''
    iterate over model layers yielding the layer,layer_scope,encasing_module
    Parameters:
    -----------
    model:
        the model to iterate over
    depth:
        how far down in the model tree to go
    basic_blocks:
        a list of modules that if encountered will not be broken down
    full:
        whether to yield only layers specified by the depth and basick_block options or to yield all layers
    '''
    if prefix is None:
        prefix = type(module).__name__

    for name, sub_module in module.named_children():
        scope = prefix + "/" + type(sub_module).__name__ + f"[{name}]"
        if len(list(sub_module.children())) == 0 or isinstance(sub_module, tuple(basic_blocks)) or depth == 0:
            if full:
                yield sub_module, scope, module, True
            else:
                yield sub_module, scope, module
        else:
            if full:
                yield sub_module, scope, module, False
            yield from traverse_model(sub_module, depth - 1, scope, basic_blocks, full)


def layerDict(model: nn.Module, depth=1000, basic_blocks=()) -> Dict[str, nn.Module]:
    return {s: l for l, s, _ in traverse_model(model, depth, basic_blocks=basic_blocks)}


def traverse_params_buffs(module: nn.Module, prefix: Optional[str] = None) -> Iterator[Tuple[torch.tensor, str]]:
    '''
    iterate over model's buffers and parameters yielding obj,obj_scope

    Parameters:
    -----------
    model:
        the model to iterate over
    '''
    if prefix is None:
        prefix = type(module).__name__

    # params
    for param_name, param in module.named_parameters(recurse=False):
        param_scope = f"{prefix}/{type(param).__name__}[{param_name}]"
        yield param, param_scope

    # buffs
    for buffer_name, buffer in module.named_buffers(recurse=False):
        buffer_scope = f"{prefix}/{type(buffer).__name__}[{buffer_name}]"
        yield buffer, buffer_scope

    # recurse
    for name, sub_module in module.named_children():
        yield from traverse_params_buffs(sub_module, prefix + "/" + type(sub_module).__name__ + f"[{name}]")


def tensorDict(model: nn.Module) -> OrderedDict[str, Tensor]:
    return collections.OrderedDict((s, t)for t, s in traverse_params_buffs(model))


def move_tensors(ts, device):
    def move(t):
        if isinstance(t, (nn.Module, Tensor)):
            return t.to(device)
        return t

    return nested_map(move, ts)


def nested_map(func, ts,full=False):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        return func(ts)
    elif isinstance(ts, (list, tuple, set)):
        return type(ts)(nested_map(func, t,full=full) for t in ts)
    elif isinstance(ts, dict):
        return {k: nested_map(func, v,full=full) for k, v in ts.items()}
    elif isinstance(ts, slice) and full:
        start = nested_map(func, ts.start,full=full)
        stop = nested_map(func, ts.stop,full=full)
        step = nested_map(func, ts.step,full=full)
        return slice(start, stop, step)
    return func(ts)


def flatten(ts):
    if isinstance(ts,torch.Size):
        # size is inheriting from tuple which is stupid
        yield ts
    elif isinstance(ts, (list, tuple, set)):
        yield from chain(*[flatten(t) for t in ts])
    elif isinstance(ts, dict):
        yield from chain(*[flatten(t) for k,t in sorted(ts.items(),key=lambda t:t[0])])
    else:
        yield ts


def unflatten(xs,structure):
    return _unflatten(xs,structure)[0]


def _unflatten(xs,structure):
    if isinstance(structure,torch.Size):
        #torch.Size is subclass of tuple which is stupid
        return xs[0],1

    if not isinstance(structure,(list,tuple,set,dict)):
        return xs[0],1
    
    if isinstance(structure,(list,tuple,set)):
        offset=0
        elements = []
        for s in structure:
            e,n = _unflatten(xs[offset:],s)
            elements.append(e)
            offset += n
        
        return type(structure)(elements),offset
    
    assert isinstance(structure,dict)
    offset = 0
    elements = dict()
    for k,v in sorted(structure.items(),key=lambda t: t[0]):
        e,n = _unflatten(xs[offset:],v)
        elements[k] = e
        offset += n
    
    return elements,offset


def state_dict(partition, device=None):
    # we return the state dict of this part as it should be in the original model
    state = nn.Module.state_dict(partition)
    lookup = partition.lookup
    result = dict()
    for k, v in state.items():
        if k in lookup:
            result[lookup[k]] = v if device is None else v.to(device)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            result[new_k] = v if device is None else v.to(device)
    return result


def load_state_dict(partition, state):
    reverse_lookup = {v: k for k, v in partition.lookup.items()}
    device = partition.device
    keys = list(partition.state_dict(None).keys())
    new_state = dict()
    for k in keys:
        if k in reverse_lookup:
            new_state[reverse_lookup[k]] = state[k].to(device)
            continue
        idx = k.rfind(".")
        to_replace = k[:idx]
        if to_replace in reverse_lookup:
            key = reverse_lookup[to_replace] + k[idx:]
            new_state[key] = state[k].to(device)
    nn.Module.load_state_dict(partition, new_state, strict=True)


def named_buffers(partition, recurse=True):
    # we return the named buffers of this part as it should be in the original model
    params = nn.Module.named_buffers(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def named_parameters(partition, recurse=True):
    # we return the named parameters of this part as it should be in the original model
    params = nn.Module.named_parameters(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def cpu(partition):
    partition.device = torch.device('cpu')
    return nn.Module.cpu(partition)


def cuda(partition, device=None):
    if device is None:
        device = torch.cuda.current_device()
    partition.device = torch.device(device)
    return nn.Module.cuda(partition, partition.device)


def to(partition, *args, **kwargs):
    device = None
    if 'device' in kwargs:
        device = kwargs['device']
    elif 'tensor' in kwargs:
        device = kwargs['tensor'].device
    if args:
        if isinstance(args[0], (torch.device, int, str)):
            device = args[0]
        if torch.is_tensor(args[0]):
            device = args[0].device
    if not (device is None):
        partition.device = torch.device(device)
    return nn.Module.to(partition, *args, **kwargs)

model_args = {'arch': 'transformer_lm_megatron'}