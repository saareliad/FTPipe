"""AutoGenerated with:
python partition_T5_models.py --objective stage_time --bwd_to_fwd_ratio -1 --n_iter 1 --hetrogenous_nodes --hetrogenous_bw --t5_task squad1 --lmhead --auto_infer_node_bwd_to_fwd_ratio --n_partitions 8 --analysis_batch_size 1 --partitioning_batch_size 1 --model t5-3b --save_memory_mode --model_too_big --max_seq_length 128
"""
import torch
import torch.functional
import math
import torch.nn.functional
from torch import Tensor
import torch.nn as nn
from itertools import chain
from typing import Optional, Tuple, Iterator, Iterable, OrderedDict, Dict
import collections
import os
from torch.nn.modules.loss import CrossEntropyLoss
from models.normal.NLP_models.modeling_t5 import T5LayerNorm
from models.normal.NLP_models.modeling_t5 import T5Attention
from torch.nn.modules.sparse import Embedding
from torch.nn.modules.dropout import Dropout
from torch.nn.modules.linear import Linear
# this is an auto generated file do not edit unless you know what you are doing


# partition adjacency
# model inputs {0, 1}
# partition 0 {'inputs': {'input3', 'input2', 'input0', 'input1'}, 'outputs': {1, 2, 3, 4, 5, 6, 7}}
# partition 1 {'inputs': {0, 'input2', 'input1'}, 'outputs': {2, 5}}
# partition 2 {'inputs': {0, 1}, 'outputs': {3, 5}}
# partition 3 {'inputs': {0, 2}, 'outputs': {4}}
# partition 4 {'inputs': {0, 3}, 'outputs': {5, 6, 7}}
# partition 5 {'inputs': {0, 1, 2, 4}, 'outputs': {6, 7}}
# partition 6 {'inputs': {0, 4, 5}, 'outputs': {7}}
# partition 7 {'inputs': {0, 4, 5, 6}, 'outputs': {'output'}}
# model outputs {7}


def create_pipeline_configuration(DEBUG=False):
    basic_blocks = (CrossEntropyLoss,T5LayerNorm,T5Attention,Embedding,Dropout,Linear)
    module_path = os.path.relpath(__file__).replace("/",".")[:-3]
    
    config = {
        'batch_dim': 0,
        'depth': 10000,
        'basic_blocks': ['torch.nn.modules.loss.CrossEntropyLoss', 'models.normal.NLP_models.modeling_t5.T5LayerNorm', 'models.normal.NLP_models.modeling_t5.T5Attention', 'torch.nn.modules.sparse.Embedding', 'torch.nn.modules.dropout.Dropout', 'torch.nn.modules.linear.Linear'],
        'model_inputs': {
            'input_ids': {
                'shape': torch.Size([1, 128]),
                'dtype': torch.int64,
                'is_batched': True},
            'attention_mask': {
                'shape': torch.Size([1, 128]),
                'dtype': torch.int64,
                'is_batched': True},
            'lm_labels': {
                'shape': torch.Size([1, 16]),
                'dtype': torch.int64,
                'is_batched': True},
            'decoder_attention_mask': {
                'shape': torch.Size([1, 16]),
                'dtype': torch.int64,
                'is_batched': True}},
        'model_outputs': {
            'T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]': {
                'shape': torch.Size([1]),
                'dtype': torch.float32,
                'is_batched': False}},
        'stages': {
            0: {
                'inputs': {
                    'input_ids': {
                        'shape': torch.Size([1, 128]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True},
                    'attention_mask': {
                        'shape': torch.Size([1, 128]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True},
                    'lm_labels': {
                        'shape': torch.Size([1, 16]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True},
                    'decoder_attention_mask': {
                        'shape': torch.Size([1, 16]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True}},
                'outputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 128]),
                        'dtype': torch.int64,
                        'is_batched': True},
                    'lm_labels': {
                        'shape': torch.Size([1, 16]),
                        'dtype': torch.int64,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Tensor::__mul___27': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___56': {
                        'shape': torch.Size([1, 32, 128, 128]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/Tensor::__add___87': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/Tensor::new_zeros_360': {
                        'shape': torch.Size([1, 16]),
                        'dtype': torch.int64,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___444': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/prim::TupleConstruct_476': {
                        'shape': (torch.Size([1, 16, 1024]), torch.Size([1, 32, 16, 16])),
                        'dtype': (torch.float32, torch.float32),
                        'is_batched': (True, True)},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___480': {
                        'shape': torch.Size([1, 32, 16, 16]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/Tensor::view_933': {
                        'shape': torch.Size([16]),
                        'dtype': torch.int64,
                        'is_batched': False}}},
            1: {
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([1, 128]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True},
                    'lm_labels': {
                        'shape': torch.Size([1, 16]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Tensor::__mul___27': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___56': {
                        'shape': torch.Size([1, 32, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/Tensor::__add___87': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/Tensor::new_zeros_360': {
                        'shape': torch.Size([1, 16]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/prim::TupleConstruct_476': {
                        'shape': (torch.Size([1, 16, 1024]), torch.Size([1, 32, 16, 16])),
                        'dtype': (torch.float32, torch.float32),
                        'req_grad': (True, True),
                        'is_batched': (True, True)}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/Tensor::__add___152': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]': {
                        'shape': torch.Size([1, 128, 16384]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/Tensor::__getitem___367': {
                        'shape': torch.Size([1, 15]),
                        'dtype': torch.int64,
                        'is_batched': True},
                    'T5ForConditionalGeneration/Tensor::__eq___370': {
                        'shape': torch.Size([1, 16]),
                        'dtype': torch.bool,
                        'is_batched': True},
                    'T5ForConditionalGeneration/Tensor::__ge___374': {
                        'shape': torch.Size([1, 16]),
                        'dtype': torch.bool,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__getitem___456': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.int64,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___478': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            2: {
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Tensor::__mul___27': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___56': {
                        'shape': torch.Size([1, 32, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/Tensor::__add___152': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]': {
                        'shape': torch.Size([1, 128, 16384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/Tensor::new_zeros_360': {
                        'shape': torch.Size([1, 16]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/Tensor::__getitem___367': {
                        'shape': torch.Size([1, 15]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/Tensor::__eq___370': {
                        'shape': torch.Size([1, 16]),
                        'dtype': torch.bool,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/Tensor::__ge___374': {
                        'shape': torch.Size([1, 16]),
                        'dtype': torch.bool,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__getitem___456': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___478': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/Tensor::__add___217': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::to_458': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            3: {
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Tensor::__mul___27': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___56': {
                        'shape': torch.Size([1, 32, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/Tensor::__add___217': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::to_458': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/Tensor::__add___277': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/float::__sub___460': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            4: {
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Tensor::__mul___27': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___56': {
                        'shape': torch.Size([1, 32, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/Tensor::__add___277': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/float::__sub___460': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/Tensor::__add___334': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]': {
                        'shape': torch.Size([1, 128, 16384]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___462': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            5: {
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Tensor::__mul___27': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___56': {
                        'shape': torch.Size([1, 32, 128, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/Tensor::__add___334': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]': {
                        'shape': torch.Size([1, 128, 16384]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___444': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___462': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___478': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___480': {
                        'shape': torch.Size([1, 32, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___506': {
                        'shape': torch.Size([1, 32, 16, 16]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___508': {
                        'shape': torch.Size([1, 32, 16, 128]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/Tensor::__add___608': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            6: {
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___444': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___462': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___506': {
                        'shape': torch.Size([1, 32, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___508': {
                        'shape': torch.Size([1, 32, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/Tensor::__add___608': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/Tensor::__add___770': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            7: {
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]': {
                        'shape': torch.Size([1, 128, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___444': {
                        'shape': torch.Size([1, 1, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___462': {
                        'shape': torch.Size([1, 1, 1, 128]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___506': {
                        'shape': torch.Size([1, 32, 16, 16]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___508': {
                        'shape': torch.Size([1, 32, 16, 128]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/Tensor::__add___770': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([1, 16, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'T5ForConditionalGeneration/Tensor::view_933': {
                        'shape': torch.Size([16]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': False}},
                'outputs': {
                    'T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]': {
                        'shape': torch.Size([1]),
                        'dtype': torch.float32,
                        'is_batched': False}}}}}
    
    config['stages'][0]['stage_cls'] = module_path+'.Partition0'
    config['stages'][1]['stage_cls'] = module_path+'.Partition1'
    config['stages'][2]['stage_cls'] = module_path+'.Partition2'
    config['stages'][3]['stage_cls'] = module_path+'.Partition3'
    config['stages'][4]['stage_cls'] = module_path+'.Partition4'
    config['stages'][5]['stage_cls'] = module_path+'.Partition5'
    config['stages'][6]['stage_cls'] = module_path+'.Partition6'
    config['stages'][7]['stage_cls'] = module_path+'.Partition7'
    
    config['stages'][0]['devices'] = ['cpu' if DEBUG else 'cuda:0']
    config['stages'][1]['devices'] = ['cpu' if DEBUG else 'cuda:1']
    config['stages'][2]['devices'] = ['cpu' if DEBUG else 'cuda:2']
    config['stages'][3]['devices'] = ['cpu' if DEBUG else 'cuda:3']
    config['stages'][4]['devices'] = ['cpu' if DEBUG else 'cuda:4']
    config['stages'][5]['devices'] = ['cpu' if DEBUG else 'cuda:5']
    config['stages'][6]['devices'] = ['cpu' if DEBUG else 'cuda:6']
    config['stages'][7]['devices'] = ['cpu' if DEBUG else 'cuda:7']
    
    return config

class Partition0(nn.Module):
    BASIC_BLOCKS=(
            Dropout,
            T5LayerNorm,
            Embedding,
            T5Attention,
            Linear,
        )
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/Embedding[embed_tokens]',
            'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/Embedding[embed_tokens]',
            'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition0, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:0')
        self.lookup = { 'l_0': 'encoder.embed_tokens',
                        'l_1': 'encoder.dropout',
                        'l_2': 'encoder.0.0.layer_norm',
                        'l_3': 'encoder.0.0.SelfAttention',
                        'l_4': 'encoder.0.0.dropout',
                        'l_5': 'encoder.0.1.layer_norm',
                        'l_6': 'encoder.0.1.DenseReluDense.wi',
                        'l_7': 'encoder.0.1.DenseReluDense.dropout',
                        'l_8': 'encoder.0.1.DenseReluDense.wo',
                        'l_9': 'encoder.0.1.dropout',
                        'l_10': 'encoder.1.0.layer_norm',
                        'l_11': 'encoder.1.0.SelfAttention',
                        'l_12': 'encoder.1.0.dropout',
                        'l_13': 'encoder.1.1.layer_norm',
                        'l_14': 'encoder.1.1.DenseReluDense.wi',
                        'l_15': 'encoder.1.1.DenseReluDense.dropout',
                        'l_16': 'encoder.1.1.DenseReluDense.wo',
                        'l_17': 'encoder.1.1.dropout',
                        'l_18': 'encoder.2.0.layer_norm',
                        'l_19': 'encoder.2.0.SelfAttention',
                        'l_20': 'encoder.2.0.dropout',
                        'l_21': 'encoder.2.1.layer_norm',
                        'l_22': 'encoder.2.1.DenseReluDense.wi',
                        'l_23': 'encoder.2.1.DenseReluDense.dropout',
                        'l_24': 'encoder.2.1.DenseReluDense.wo',
                        'l_25': 'encoder.2.1.dropout',
                        'l_26': 'encoder.3.0.layer_norm',
                        'l_27': 'encoder.3.0.SelfAttention',
                        'l_28': 'encoder.3.0.dropout',
                        'l_29': 'encoder.3.1.layer_norm',
                        'l_30': 'encoder.3.1.DenseReluDense.wi',
                        'l_31': 'encoder.3.1.DenseReluDense.dropout',
                        'l_32': 'encoder.3.1.DenseReluDense.wo',
                        'l_33': 'decoder.embed_tokens',
                        'l_34': 'decoder.dropout',
                        'l_35': 'decoder.0.0.layer_norm',
                        'l_36': 'decoder.0.0.SelfAttention',
                        'l_37': 'decoder.0.0.dropout'}

    def forward(self, input_ids, attention_mask, lm_labels, decoder_attention_mask):
        # T5ForConditionalGeneration/T5Stack[encoder]/Embedding[embed_tokens] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[0]/T5LayerFF[1]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[1]/T5LayerFF[1]/Dropout[dropout] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[2]/T5LayerFF[1]/Dropout[dropout] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/Embedding[embed_tokens] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_37
        # input0 <=> input_ids
        # input1 <=> attention_mask
        # input2 <=> lm_labels
        # input3 <=> decoder_attention_mask

        # moving inputs to current device no op if already on the correct device
        input_ids, attention_mask, lm_labels, decoder_attention_mask = move_tensors((input_ids, attention_mask, lm_labels, decoder_attention_mask), self.device)
        t_0 = input_ids.size()
        t_0 = t_0[-1]
        t_0 = input_ids.view(-1, t_0)
        t_0 = self.l_0(t_0)
        t_1 = slice(None, None, None)
        t_2 = slice(None, None, None)
        t_2 = (t_1, None, None, t_2)
        t_2 = attention_mask[t_2]
        t_2 = t_2.to(dtype=torch.float32)
        t_2 = 1.0 - t_2
        t_2 = t_2 * -10000.0
        t_0 = self.l_1(t_0)
        t_1 = self.l_2(t_0)
        t_1 = self.l_3(t_1, mask=t_2, position_bias=None, head_mask=None)
        t_3 = t_1[0]
        t_3 = self.l_4(t_3)
        t_3 = t_0 + t_3
        t_1 = t_1[1]
        t_1 = (t_3, t_1)
        t_3 = t_1[0]
        t_1 = t_1[1]
        t_0 = self.l_5(t_3)
        t_0 = self.l_6(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_7(t_0)
        t_0 = self.l_8(t_0)
        t_0 = self.l_9(t_0)
        t_0 = t_3 + t_0
        t_1 = (t_0, t_1)
        t_0 = t_1[0]
        t_1 = t_1[1]
        t_3 = self.l_10(t_0)
        t_3 = self.l_11(t_3, mask=t_2, position_bias=t_1, head_mask=None)
        t_3 = self.l_12(t_3)
        t_3 = t_0 + t_3
        t_0 = self.l_13(t_3)
        t_0 = self.l_14(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_15(t_0)
        t_0 = self.l_16(t_0)
        t_0 = self.l_17(t_0)
        t_0 = t_3 + t_0
        t_3 = self.l_18(t_0)
        t_3 = self.l_19(t_3, mask=t_2, position_bias=t_1, head_mask=None)
        t_3 = self.l_20(t_3)
        t_3 = t_0 + t_3
        t_0 = self.l_21(t_3)
        t_0 = self.l_22(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = self.l_25(t_0)
        t_0 = t_3 + t_0
        t_3 = self.l_26(t_0)
        t_3 = self.l_27(t_3, mask=t_2, position_bias=t_1, head_mask=None)
        t_3 = self.l_28(t_3)
        t_3 = t_0 + t_3
        t_0 = self.l_29(t_3)
        t_0 = self.l_30(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_31(t_0)
        t_0 = self.l_32(t_0)
        t_4 = lm_labels.shape
        t_4 = lm_labels.new_zeros(t_4)
        t_5 = t_4.size()
        t_6 = t_5[-1]
        t_6 = t_4.view(-1, t_6)
        t_6 = self.l_33(t_6)
        t_7 = decoder_attention_mask.device
        t_8 = t_5[0]
        t_5 = t_5[1]
        t_7 = torch.arange(t_5, device=t_7)
        t_9 = slice(None, None, None)
        t_9 = (None, None, t_9)
        t_9 = t_7[t_9]
        t_5 = t_9.repeat(t_8, t_5, 1)
        t_8 = slice(None, None, None)
        t_8 = (None, t_8, None)
        t_8 = t_7[t_8]
        t_8 = t_5 <= t_8
        t_5 = decoder_attention_mask.dtype
        t_5 = t_8.to(t_5)
        t_8 = slice(None, None, None)
        t_7 = slice(None, None, None)
        t_9 = slice(None, None, None)
        t_9 = (t_8, None, t_7, t_9)
        t_9 = t_5[t_9]
        t_5 = slice(None, None, None)
        t_7 = slice(None, None, None)
        t_7 = (t_5, None, None, t_7)
        t_7 = decoder_attention_mask[t_7]
        t_7 = t_9 * t_7
        t_7 = t_7.to(dtype=torch.float32)
        t_7 = 1.0 - t_7
        t_7 = t_7 * -10000.0
        t_6 = self.l_34(t_6)
        t_9 = self.l_35(t_6)
        t_9 = self.l_36(t_9, mask=t_7, position_bias=None, head_mask=None)
        t_5 = t_9[0]
        t_5 = self.l_37(t_5)
        t_5 = t_6 + t_5
        t_9 = t_9[1]
        t_9 = (t_5, t_9)
        t_5 = t_9[1]
        t_6 = lm_labels.view(-1)
        # returning:
        # input1
        # input2
        # T5ForConditionalGeneration/T5Stack[encoder]/Tensor::__mul___27
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___56
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/Tensor::__add___87
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]
        # T5ForConditionalGeneration/Tensor::new_zeros_360
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___444
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/prim::TupleConstruct_476
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___480
        # T5ForConditionalGeneration/Tensor::view_933
        return (attention_mask, lm_labels, t_2, t_1, t_3, t_0, t_4, t_7, t_9, t_5, t_6)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition1(nn.Module):
    BASIC_BLOCKS=(
            Dropout,
            Linear,
            T5LayerNorm,
            T5Attention,
        )
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition1, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:1')
        self.lookup = { 'l_0': 'encoder.3.1.dropout',
                        'l_1': 'encoder.4.0.layer_norm',
                        'l_2': 'encoder.4.0.SelfAttention',
                        'l_3': 'encoder.4.0.dropout',
                        'l_4': 'encoder.4.1.layer_norm',
                        'l_5': 'encoder.4.1.DenseReluDense.wi',
                        'l_6': 'encoder.4.1.DenseReluDense.dropout',
                        'l_7': 'encoder.4.1.DenseReluDense.wo',
                        'l_8': 'encoder.4.1.dropout',
                        'l_9': 'encoder.5.0.layer_norm',
                        'l_10': 'encoder.5.0.SelfAttention',
                        'l_11': 'encoder.5.0.dropout',
                        'l_12': 'encoder.5.1.layer_norm',
                        'l_13': 'encoder.5.1.DenseReluDense.wi',
                        'l_14': 'encoder.5.1.DenseReluDense.dropout',
                        'l_15': 'encoder.5.1.DenseReluDense.wo',
                        'l_16': 'encoder.5.1.dropout',
                        'l_17': 'encoder.6.0.layer_norm',
                        'l_18': 'encoder.6.0.SelfAttention',
                        'l_19': 'encoder.6.0.dropout',
                        'l_20': 'encoder.6.1.layer_norm',
                        'l_21': 'encoder.6.1.DenseReluDense.wi',
                        'l_22': 'encoder.6.1.DenseReluDense.dropout',
                        'l_23': 'encoder.6.1.DenseReluDense.wo',
                        'l_24': 'encoder.6.1.dropout',
                        'l_25': 'encoder.7.0.layer_norm',
                        'l_26': 'encoder.7.0.SelfAttention',
                        'l_27': 'encoder.7.0.dropout',
                        'l_28': 'encoder.7.1.layer_norm',
                        'l_29': 'encoder.7.1.DenseReluDense.wi',
                        'l_30': 'encoder.7.1.DenseReluDense.dropout',
                        'l_31': 'encoder.7.1.DenseReluDense.wo',
                        'l_32': 'encoder.7.1.dropout',
                        'l_33': 'encoder.8.0.layer_norm',
                        'l_34': 'encoder.8.0.SelfAttention',
                        'l_35': 'encoder.8.0.dropout',
                        'l_36': 'encoder.8.1.layer_norm',
                        'l_37': 'encoder.8.1.DenseReluDense.wi',
                        'l_38': 'encoder.8.1.DenseReluDense.dropout'}

    def forward(self, attention_mask, lm_labels, x0, x1, x2, x3, x4, x5):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/Dropout[dropout] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[4]/T5LayerFF[1]/Dropout[dropout] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[5]/T5LayerFF[1]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[6]/T5LayerFF[1]/Dropout[dropout] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[7]/T5LayerFF[1]/Dropout[dropout] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_38
        # input1 <=> attention_mask
        # input2 <=> lm_labels
        # T5ForConditionalGeneration/T5Stack[encoder]/Tensor::__mul___27 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___56 <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerSelfAttention[0]/Tensor::__add___87 <=> x2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[3]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> x3
        # T5ForConditionalGeneration/Tensor::new_zeros_360 <=> x4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerSelfAttention[0]/prim::TupleConstruct_476 <=> x5

        # moving inputs to current device no op if already on the correct device
        attention_mask, lm_labels, x0, x1, x2, x3, x4, x5 = move_tensors((attention_mask, lm_labels, x0, x1, x2, x3, x4, x5), self.device)
        t_0 = self.l_0(x3)
        t_0 = x2 + t_0
        t_1 = self.l_1(t_0)
        t_1 = self.l_2(t_1, mask=x0, position_bias=x1, head_mask=None)
        t_1 = self.l_3(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_4(t_1)
        t_0 = self.l_5(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_6(t_0)
        t_0 = self.l_7(t_0)
        t_0 = self.l_8(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_9(t_0)
        t_1 = self.l_10(t_1, mask=x0, position_bias=x1, head_mask=None)
        t_1 = self.l_11(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_12(t_1)
        t_0 = self.l_13(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_14(t_0)
        t_0 = self.l_15(t_0)
        t_0 = self.l_16(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_17(t_0)
        t_1 = self.l_18(t_1, mask=x0, position_bias=x1, head_mask=None)
        t_1 = self.l_19(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_20(t_1)
        t_0 = self.l_21(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_22(t_0)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_25(t_0)
        t_1 = self.l_26(t_1, mask=x0, position_bias=x1, head_mask=None)
        t_1 = self.l_27(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_28(t_1)
        t_0 = self.l_29(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_30(t_0)
        t_0 = self.l_31(t_0)
        t_0 = self.l_32(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_33(t_0)
        t_1 = self.l_34(t_1, mask=x0, position_bias=x1, head_mask=None)
        t_1 = self.l_35(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_36(t_1)
        t_0 = self.l_37(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_38(t_0)
        t_2 = slice(None, -1, None)
        t_2 = (Ellipsis, t_2)
        t_2 = lm_labels[t_2]
        t_3 = x4 == -100
        t_4 = x4 >= 0
        t_5 = slice(None, None, None)
        t_6 = slice(None, None, None)
        t_6 = (t_5, None, None, t_6)
        t_6 = attention_mask[t_6]
        t_5 = x5[0]
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/Tensor::__add___152
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]
        # T5ForConditionalGeneration/Tensor::__getitem___367
        # T5ForConditionalGeneration/Tensor::__eq___370
        # T5ForConditionalGeneration/Tensor::__ge___374
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__getitem___456
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___478
        return (t_1, t_0, t_2, t_3, t_4, t_6, t_5)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition2(nn.Module):
    BASIC_BLOCKS=(
            Dropout,
            Linear,
            T5LayerNorm,
            T5Attention,
        )
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition2, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:2')
        self.lookup = { 'l_0': 'encoder.8.1.DenseReluDense.wo',
                        'l_1': 'encoder.8.1.dropout',
                        'l_2': 'encoder.9.0.layer_norm',
                        'l_3': 'encoder.9.0.SelfAttention',
                        'l_4': 'encoder.9.0.dropout',
                        'l_5': 'encoder.9.1.layer_norm',
                        'l_6': 'encoder.9.1.DenseReluDense.wi',
                        'l_7': 'encoder.9.1.DenseReluDense.dropout',
                        'l_8': 'encoder.9.1.DenseReluDense.wo',
                        'l_9': 'encoder.9.1.dropout',
                        'l_10': 'encoder.10.0.layer_norm',
                        'l_11': 'encoder.10.0.SelfAttention',
                        'l_12': 'encoder.10.0.dropout',
                        'l_13': 'encoder.10.1.layer_norm',
                        'l_14': 'encoder.10.1.DenseReluDense.wi',
                        'l_15': 'encoder.10.1.DenseReluDense.dropout',
                        'l_16': 'encoder.10.1.DenseReluDense.wo',
                        'l_17': 'encoder.10.1.dropout',
                        'l_18': 'encoder.11.0.layer_norm',
                        'l_19': 'encoder.11.0.SelfAttention',
                        'l_20': 'encoder.11.0.dropout',
                        'l_21': 'encoder.11.1.layer_norm',
                        'l_22': 'encoder.11.1.DenseReluDense.wi',
                        'l_23': 'encoder.11.1.DenseReluDense.dropout',
                        'l_24': 'encoder.11.1.DenseReluDense.wo',
                        'l_25': 'encoder.11.1.dropout',
                        'l_26': 'encoder.12.0.layer_norm',
                        'l_27': 'encoder.12.0.SelfAttention',
                        'l_28': 'encoder.12.0.dropout',
                        'l_29': 'encoder.12.1.layer_norm',
                        'l_30': 'encoder.12.1.DenseReluDense.wi',
                        'l_31': 'encoder.12.1.DenseReluDense.dropout',
                        'l_32': 'encoder.12.1.DenseReluDense.wo',
                        'l_33': 'encoder.12.1.dropout',
                        'l_34': 'encoder.13.0.layer_norm',
                        'l_35': 'encoder.13.0.SelfAttention',
                        'l_36': 'encoder.13.0.dropout',
                        'l_37': 'decoder.0.1.layer_norm'}

    def forward(self, x0, x1, x2, x3, x4, x5, x6, x7, x8, x9):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[9]/T5LayerFF[1]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[10]/T5LayerFF[1]/Dropout[dropout] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[11]/T5LayerFF[1]/Dropout[dropout] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[12]/T5LayerFF[1]/Dropout[dropout] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[encoder]/Tensor::__mul___27 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___56 <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerSelfAttention[0]/Tensor::__add___152 <=> x2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[8]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> x3
        # T5ForConditionalGeneration/Tensor::new_zeros_360 <=> x4
        # T5ForConditionalGeneration/Tensor::__getitem___367 <=> x5
        # T5ForConditionalGeneration/Tensor::__eq___370 <=> x6
        # T5ForConditionalGeneration/Tensor::__ge___374 <=> x7
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__getitem___456 <=> x8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___478 <=> x9

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5, x6, x7, x8, x9 = move_tensors((x0, x1, x2, x3, x4, x5, x6, x7, x8, x9), self.device)
        t_0 = self.l_0(x3)
        t_0 = self.l_1(t_0)
        t_0 = x2 + t_0
        t_1 = self.l_2(t_0)
        t_1 = self.l_3(t_1, mask=x0, position_bias=x1, head_mask=None)
        t_1 = self.l_4(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_5(t_1)
        t_0 = self.l_6(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_7(t_0)
        t_0 = self.l_8(t_0)
        t_0 = self.l_9(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_10(t_0)
        t_1 = self.l_11(t_1, mask=x0, position_bias=x1, head_mask=None)
        t_1 = self.l_12(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_13(t_1)
        t_0 = self.l_14(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_15(t_0)
        t_0 = self.l_16(t_0)
        t_0 = self.l_17(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_18(t_0)
        t_1 = self.l_19(t_1, mask=x0, position_bias=x1, head_mask=None)
        t_1 = self.l_20(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_21(t_1)
        t_0 = self.l_22(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = self.l_25(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_26(t_0)
        t_1 = self.l_27(t_1, mask=x0, position_bias=x1, head_mask=None)
        t_1 = self.l_28(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_29(t_1)
        t_0 = self.l_30(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_31(t_0)
        t_0 = self.l_32(t_0)
        t_0 = self.l_33(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_34(t_0)
        t_1 = self.l_35(t_1, mask=x0, position_bias=x1, head_mask=None)
        t_1 = self.l_36(t_1)
        t_1 = t_0 + t_1
        t_0 = x5.clone()
        t_2 = x4.masked_fill_(x6, 0)
        t_3 = torch.all(x7)
        t_4 = x8.to(dtype=torch.float32)
        t_5 = self.l_37(x9)
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/Tensor::__add___217
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::to_458
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]
        return (t_1, t_4, t_5)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition3(nn.Module):
    BASIC_BLOCKS=(
            Dropout,
            Linear,
            T5LayerNorm,
            T5Attention,
        )
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition3, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:3')
        self.lookup = { 'l_0': 'encoder.13.1.layer_norm',
                        'l_1': 'encoder.13.1.DenseReluDense.wi',
                        'l_2': 'encoder.13.1.DenseReluDense.dropout',
                        'l_3': 'encoder.13.1.DenseReluDense.wo',
                        'l_4': 'encoder.13.1.dropout',
                        'l_5': 'encoder.14.0.layer_norm',
                        'l_6': 'encoder.14.0.SelfAttention',
                        'l_7': 'encoder.14.0.dropout',
                        'l_8': 'encoder.14.1.layer_norm',
                        'l_9': 'encoder.14.1.DenseReluDense.wi',
                        'l_10': 'encoder.14.1.DenseReluDense.dropout',
                        'l_11': 'encoder.14.1.DenseReluDense.wo',
                        'l_12': 'encoder.14.1.dropout',
                        'l_13': 'encoder.15.0.layer_norm',
                        'l_14': 'encoder.15.0.SelfAttention',
                        'l_15': 'encoder.15.0.dropout',
                        'l_16': 'encoder.15.1.layer_norm',
                        'l_17': 'encoder.15.1.DenseReluDense.wi',
                        'l_18': 'encoder.15.1.DenseReluDense.dropout',
                        'l_19': 'encoder.15.1.DenseReluDense.wo',
                        'l_20': 'encoder.15.1.dropout',
                        'l_21': 'encoder.16.0.layer_norm',
                        'l_22': 'encoder.16.0.SelfAttention',
                        'l_23': 'encoder.16.0.dropout',
                        'l_24': 'encoder.16.1.layer_norm',
                        'l_25': 'encoder.16.1.DenseReluDense.wi',
                        'l_26': 'encoder.16.1.DenseReluDense.dropout',
                        'l_27': 'encoder.16.1.DenseReluDense.wo',
                        'l_28': 'encoder.16.1.dropout',
                        'l_29': 'encoder.17.0.layer_norm',
                        'l_30': 'encoder.17.0.SelfAttention',
                        'l_31': 'encoder.17.0.dropout',
                        'l_32': 'encoder.17.1.layer_norm',
                        'l_33': 'encoder.17.1.DenseReluDense.wi',
                        'l_34': 'encoder.17.1.DenseReluDense.dropout',
                        'l_35': 'encoder.17.1.DenseReluDense.wo',
                        'l_36': 'encoder.17.1.dropout'}

    def forward(self, x0, x1, x2, x3):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerFF[1]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[14]/T5LayerFF[1]/Dropout[dropout] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[15]/T5LayerFF[1]/Dropout[dropout] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[16]/T5LayerFF[1]/Dropout[dropout] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/Dropout[dropout] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[encoder]/Tensor::__mul___27 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___56 <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[13]/T5LayerSelfAttention[0]/Tensor::__add___217 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::to_458 <=> x3

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3 = move_tensors((x0, x1, x2, x3), self.device)
        t_0 = self.l_0(x2)
        t_0 = self.l_1(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x2 + t_0
        t_1 = self.l_5(t_0)
        t_1 = self.l_6(t_1, mask=x0, position_bias=x1, head_mask=None)
        t_1 = self.l_7(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_8(t_1)
        t_0 = self.l_9(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_10(t_0)
        t_0 = self.l_11(t_0)
        t_0 = self.l_12(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_13(t_0)
        t_1 = self.l_14(t_1, mask=x0, position_bias=x1, head_mask=None)
        t_1 = self.l_15(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_16(t_1)
        t_0 = self.l_17(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_18(t_0)
        t_0 = self.l_19(t_0)
        t_0 = self.l_20(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_21(t_0)
        t_1 = self.l_22(t_1, mask=x0, position_bias=x1, head_mask=None)
        t_1 = self.l_23(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_24(t_1)
        t_0 = self.l_25(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_26(t_0)
        t_0 = self.l_27(t_0)
        t_0 = self.l_28(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_29(t_0)
        t_1 = self.l_30(t_1, mask=x0, position_bias=x1, head_mask=None)
        t_1 = self.l_31(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_32(t_1)
        t_0 = self.l_33(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_34(t_0)
        t_0 = self.l_35(t_0)
        t_0 = self.l_36(t_0)
        t_0 = t_1 + t_0
        t_1 = 1.0 - x3
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/Tensor::__add___277
        # T5ForConditionalGeneration/T5Stack[decoder]/float::__sub___460
        return (t_0, t_1)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition4(nn.Module):
    BASIC_BLOCKS=(
            Dropout,
            Linear,
            T5LayerNorm,
            T5Attention,
        )
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition4, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:4')
        self.lookup = { 'l_0': 'encoder.18.0.layer_norm',
                        'l_1': 'encoder.18.0.SelfAttention',
                        'l_2': 'encoder.18.0.dropout',
                        'l_3': 'encoder.18.1.layer_norm',
                        'l_4': 'encoder.18.1.DenseReluDense.wi',
                        'l_5': 'encoder.18.1.DenseReluDense.dropout',
                        'l_6': 'encoder.18.1.DenseReluDense.wo',
                        'l_7': 'encoder.18.1.dropout',
                        'l_8': 'encoder.19.0.layer_norm',
                        'l_9': 'encoder.19.0.SelfAttention',
                        'l_10': 'encoder.19.0.dropout',
                        'l_11': 'encoder.19.1.layer_norm',
                        'l_12': 'encoder.19.1.DenseReluDense.wi',
                        'l_13': 'encoder.19.1.DenseReluDense.dropout',
                        'l_14': 'encoder.19.1.DenseReluDense.wo',
                        'l_15': 'encoder.19.1.dropout',
                        'l_16': 'encoder.20.0.layer_norm',
                        'l_17': 'encoder.20.0.SelfAttention',
                        'l_18': 'encoder.20.0.dropout',
                        'l_19': 'encoder.20.1.layer_norm',
                        'l_20': 'encoder.20.1.DenseReluDense.wi',
                        'l_21': 'encoder.20.1.DenseReluDense.dropout',
                        'l_22': 'encoder.20.1.DenseReluDense.wo',
                        'l_23': 'encoder.20.1.dropout',
                        'l_24': 'encoder.21.0.layer_norm',
                        'l_25': 'encoder.21.0.SelfAttention',
                        'l_26': 'encoder.21.0.dropout',
                        'l_27': 'encoder.21.1.layer_norm',
                        'l_28': 'encoder.21.1.DenseReluDense.wi',
                        'l_29': 'encoder.21.1.DenseReluDense.dropout',
                        'l_30': 'encoder.21.1.DenseReluDense.wo',
                        'l_31': 'encoder.21.1.dropout',
                        'l_32': 'encoder.22.0.layer_norm',
                        'l_33': 'encoder.22.0.SelfAttention',
                        'l_34': 'encoder.22.0.dropout',
                        'l_35': 'encoder.22.1.layer_norm',
                        'l_36': 'encoder.22.1.DenseReluDense.wi',
                        'l_37': 'encoder.22.1.DenseReluDense.dropout'}

    def forward(self, x0, x1, x2, x3):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[18]/T5LayerFF[1]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[19]/T5LayerFF[1]/Dropout[dropout] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[20]/T5LayerFF[1]/Dropout[dropout] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[21]/T5LayerFF[1]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[encoder]/Tensor::__mul___27 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___56 <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[17]/T5LayerFF[1]/Tensor::__add___277 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/float::__sub___460 <=> x3

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3 = move_tensors((x0, x1, x2, x3), self.device)
        t_0 = self.l_0(x2)
        t_0 = self.l_1(t_0, mask=x0, position_bias=x1, head_mask=None)
        t_0 = self.l_2(t_0)
        t_0 = x2 + t_0
        t_1 = self.l_3(t_0)
        t_1 = self.l_4(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = self.l_7(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_8(t_1)
        t_0 = self.l_9(t_0, mask=x0, position_bias=x1, head_mask=None)
        t_0 = self.l_10(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_11(t_0)
        t_1 = self.l_12(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_13(t_1)
        t_1 = self.l_14(t_1)
        t_1 = self.l_15(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_16(t_1)
        t_0 = self.l_17(t_0, mask=x0, position_bias=x1, head_mask=None)
        t_0 = self.l_18(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_19(t_0)
        t_1 = self.l_20(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_21(t_1)
        t_1 = self.l_22(t_1)
        t_1 = self.l_23(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_24(t_1)
        t_0 = self.l_25(t_0, mask=x0, position_bias=x1, head_mask=None)
        t_0 = self.l_26(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_27(t_0)
        t_1 = self.l_28(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_29(t_1)
        t_1 = self.l_30(t_1)
        t_1 = self.l_31(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_32(t_1)
        t_0 = self.l_33(t_0, mask=x0, position_bias=x1, head_mask=None)
        t_0 = self.l_34(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_35(t_0)
        t_1 = self.l_36(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_37(t_1)
        t_2 = x3 * -1000000000.0
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/Tensor::__add___334
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___462
        return (t_0, t_1, t_2)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition5(nn.Module):
    BASIC_BLOCKS=(
            Dropout,
            Linear,
            T5LayerNorm,
            T5Attention,
        )
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5LayerNorm[final_layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/Dropout[dropout]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition5, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:5')
        self.lookup = { 'l_0': 'encoder.22.1.DenseReluDense.wo',
                        'l_1': 'encoder.22.1.dropout',
                        'l_2': 'encoder.23.0.layer_norm',
                        'l_3': 'encoder.23.0.SelfAttention',
                        'l_4': 'encoder.23.0.dropout',
                        'l_5': 'encoder.23.1.layer_norm',
                        'l_6': 'encoder.23.1.DenseReluDense.wi',
                        'l_7': 'encoder.23.1.DenseReluDense.dropout',
                        'l_8': 'encoder.23.1.DenseReluDense.wo',
                        'l_9': 'encoder.23.1.dropout',
                        'l_10': 'encoder.final_layer_norm',
                        'l_11': 'encoder.dropout',
                        'l_12': 'decoder.0.1.EncDecAttention',
                        'l_13': 'decoder.0.1.dropout',
                        'l_14': 'decoder.0.2.layer_norm',
                        'l_15': 'decoder.0.2.DenseReluDense.wi',
                        'l_16': 'decoder.0.2.DenseReluDense.dropout',
                        'l_17': 'decoder.0.2.DenseReluDense.wo',
                        'l_18': 'decoder.0.2.dropout',
                        'l_19': 'decoder.1.0.layer_norm',
                        'l_20': 'decoder.1.0.SelfAttention',
                        'l_21': 'decoder.1.0.dropout',
                        'l_22': 'decoder.1.1.layer_norm',
                        'l_23': 'decoder.1.1.EncDecAttention',
                        'l_24': 'decoder.1.1.dropout',
                        'l_25': 'decoder.1.2.layer_norm',
                        'l_26': 'decoder.1.2.DenseReluDense.wi',
                        'l_27': 'decoder.1.2.DenseReluDense.dropout',
                        'l_28': 'decoder.1.2.DenseReluDense.wo',
                        'l_29': 'decoder.1.2.dropout',
                        'l_30': 'decoder.2.0.layer_norm',
                        'l_31': 'decoder.2.0.SelfAttention',
                        'l_32': 'decoder.2.0.dropout',
                        'l_33': 'decoder.2.1.layer_norm',
                        'l_34': 'decoder.2.1.EncDecAttention',
                        'l_35': 'decoder.2.1.dropout',
                        'l_36': 'decoder.2.2.layer_norm',
                        'l_37': 'decoder.2.2.DenseReluDense.wi',
                        'l_38': 'decoder.2.2.DenseReluDense.dropout',
                        'l_39': 'decoder.2.2.DenseReluDense.wo',
                        'l_40': 'decoder.2.2.dropout',
                        'l_41': 'decoder.3.0.layer_norm',
                        'l_42': 'decoder.3.0.SelfAttention',
                        'l_43': 'decoder.3.0.dropout',
                        'l_44': 'decoder.3.1.layer_norm',
                        'l_45': 'decoder.3.1.EncDecAttention',
                        'l_46': 'decoder.3.1.dropout',
                        'l_47': 'decoder.3.2.layer_norm',
                        'l_48': 'decoder.3.2.DenseReluDense.wi',
                        'l_49': 'decoder.3.2.DenseReluDense.dropout',
                        'l_50': 'decoder.3.2.DenseReluDense.wo',
                        'l_51': 'decoder.3.2.dropout',
                        'l_52': 'decoder.4.0.layer_norm',
                        'l_53': 'decoder.4.0.SelfAttention',
                        'l_54': 'decoder.4.0.dropout',
                        'l_55': 'decoder.4.1.layer_norm',
                        'l_56': 'decoder.4.1.EncDecAttention',
                        'l_57': 'decoder.4.1.dropout',
                        'l_58': 'decoder.4.2.layer_norm',
                        'l_59': 'decoder.4.2.DenseReluDense.wi',
                        'l_60': 'decoder.4.2.DenseReluDense.dropout',
                        'l_61': 'decoder.4.2.DenseReluDense.wo',
                        'l_62': 'decoder.4.2.dropout',
                        'l_63': 'decoder.5.0.layer_norm',
                        'l_64': 'decoder.5.0.SelfAttention',
                        'l_65': 'decoder.5.0.dropout',
                        'l_66': 'decoder.5.1.layer_norm',
                        'l_67': 'decoder.5.1.EncDecAttention',
                        'l_68': 'decoder.5.1.dropout',
                        'l_69': 'decoder.5.2.layer_norm',
                        'l_70': 'decoder.5.2.DenseReluDense.wi',
                        'l_71': 'decoder.5.2.DenseReluDense.dropout',
                        'l_72': 'decoder.5.2.DenseReluDense.wo',
                        'l_73': 'decoder.5.2.dropout',
                        'l_74': 'decoder.6.0.layer_norm',
                        'l_75': 'decoder.6.0.SelfAttention',
                        'l_76': 'decoder.6.0.dropout',
                        'l_77': 'decoder.6.1.layer_norm',
                        'l_78': 'decoder.6.1.EncDecAttention',
                        'l_79': 'decoder.6.1.dropout'}

    def forward(self, x0, x1, x2, x3, x4, x5, x6, x7, x8):
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[23]/T5LayerFF[1]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/T5LayerNorm[final_layer_norm] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[1]/T5LayerFF[2]/Dropout[dropout] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[2]/T5LayerFF[2]/Dropout[dropout] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_50
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[3]/T5LayerFF[2]/Dropout[dropout] <=> self.l_51
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_52
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_53
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_54
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_55
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_56
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_57
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_58
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_59
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_60
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_61
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[4]/T5LayerFF[2]/Dropout[dropout] <=> self.l_62
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_63
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_64
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_65
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_66
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_67
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_68
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_69
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_70
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_71
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_72
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[5]/T5LayerFF[2]/Dropout[dropout] <=> self.l_73
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_74
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_75
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_76
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_77
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_78
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_79
        # T5ForConditionalGeneration/T5Stack[encoder]/Tensor::__mul___27 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___56 <=> x1
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerSelfAttention[0]/Tensor::__add___334 <=> x2
        # T5ForConditionalGeneration/T5Stack[encoder]/T5Block[22]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___444 <=> x4
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___462 <=> x5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___478 <=> x6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___480 <=> x7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> x8

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5, x6, x7, x8 = move_tensors((x0, x1, x2, x3, x4, x5, x6, x7, x8), self.device)
        t_0 = self.l_0(x3)
        t_0 = self.l_1(t_0)
        t_0 = x2 + t_0
        t_1 = self.l_2(t_0)
        t_1 = self.l_3(t_1, mask=x0, position_bias=x1, head_mask=None)
        t_1 = self.l_4(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_5(t_1)
        t_0 = self.l_6(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_7(t_0)
        t_0 = self.l_8(t_0)
        t_0 = self.l_9(t_0)
        t_0 = t_1 + t_0
        t_0 = self.l_10(t_0)
        t_0 = self.l_11(t_0)
        t_1 = self.l_12(x8, mask=x5, kv=t_0, position_bias=None, head_mask=None)
        t_2 = t_1[0]
        t_2 = self.l_13(t_2)
        t_2 = x6 + t_2
        t_1 = t_1[1]
        t_1 = (t_2, t_1)
        t_2 = t_1[0]
        t_1 = t_1[1]
        t_3 = self.l_14(t_2)
        t_3 = self.l_15(t_3)
        t_3 = torch.nn.functional.relu(t_3, inplace=False)
        t_3 = self.l_16(t_3)
        t_3 = self.l_17(t_3)
        t_3 = self.l_18(t_3)
        t_3 = t_2 + t_3
        t_1 = (t_3, x7, t_1)
        t_3 = t_1[0]
        t_2 = t_1[1]
        t_1 = t_1[2]
        t_4 = self.l_19(t_3)
        t_4 = self.l_20(t_4, mask=x4, position_bias=t_2, head_mask=None)
        t_4 = self.l_21(t_4)
        t_4 = t_3 + t_4
        t_3 = self.l_22(t_4)
        t_3 = self.l_23(t_3, mask=x5, kv=t_0, position_bias=t_1, head_mask=None)
        t_3 = self.l_24(t_3)
        t_3 = t_4 + t_3
        t_4 = self.l_25(t_3)
        t_4 = self.l_26(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_27(t_4)
        t_4 = self.l_28(t_4)
        t_4 = self.l_29(t_4)
        t_4 = t_3 + t_4
        t_3 = self.l_30(t_4)
        t_3 = self.l_31(t_3, mask=x4, position_bias=t_2, head_mask=None)
        t_3 = self.l_32(t_3)
        t_3 = t_4 + t_3
        t_4 = self.l_33(t_3)
        t_4 = self.l_34(t_4, mask=x5, kv=t_0, position_bias=t_1, head_mask=None)
        t_4 = self.l_35(t_4)
        t_4 = t_3 + t_4
        t_3 = self.l_36(t_4)
        t_3 = self.l_37(t_3)
        t_3 = torch.nn.functional.relu(t_3, inplace=False)
        t_3 = self.l_38(t_3)
        t_3 = self.l_39(t_3)
        t_3 = self.l_40(t_3)
        t_3 = t_4 + t_3
        t_4 = self.l_41(t_3)
        t_4 = self.l_42(t_4, mask=x4, position_bias=t_2, head_mask=None)
        t_4 = self.l_43(t_4)
        t_4 = t_3 + t_4
        t_3 = self.l_44(t_4)
        t_3 = self.l_45(t_3, mask=x5, kv=t_0, position_bias=t_1, head_mask=None)
        t_3 = self.l_46(t_3)
        t_3 = t_4 + t_3
        t_4 = self.l_47(t_3)
        t_4 = self.l_48(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_49(t_4)
        t_4 = self.l_50(t_4)
        t_4 = self.l_51(t_4)
        t_4 = t_3 + t_4
        t_3 = self.l_52(t_4)
        t_3 = self.l_53(t_3, mask=x4, position_bias=t_2, head_mask=None)
        t_3 = self.l_54(t_3)
        t_3 = t_4 + t_3
        t_4 = self.l_55(t_3)
        t_4 = self.l_56(t_4, mask=x5, kv=t_0, position_bias=t_1, head_mask=None)
        t_4 = self.l_57(t_4)
        t_4 = t_3 + t_4
        t_3 = self.l_58(t_4)
        t_3 = self.l_59(t_3)
        t_3 = torch.nn.functional.relu(t_3, inplace=False)
        t_3 = self.l_60(t_3)
        t_3 = self.l_61(t_3)
        t_3 = self.l_62(t_3)
        t_3 = t_4 + t_3
        t_4 = self.l_63(t_3)
        t_4 = self.l_64(t_4, mask=x4, position_bias=t_2, head_mask=None)
        t_4 = self.l_65(t_4)
        t_4 = t_3 + t_4
        t_3 = self.l_66(t_4)
        t_3 = self.l_67(t_3, mask=x5, kv=t_0, position_bias=t_1, head_mask=None)
        t_3 = self.l_68(t_3)
        t_3 = t_4 + t_3
        t_4 = self.l_69(t_3)
        t_4 = self.l_70(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_71(t_4)
        t_4 = self.l_72(t_4)
        t_4 = self.l_73(t_4)
        t_4 = t_3 + t_4
        t_3 = self.l_74(t_4)
        t_3 = self.l_75(t_3, mask=x4, position_bias=t_2, head_mask=None)
        t_3 = self.l_76(t_3)
        t_3 = t_4 + t_3
        t_4 = self.l_77(t_3)
        t_4 = self.l_78(t_4, mask=x5, kv=t_0, position_bias=t_1, head_mask=None)
        t_4 = self.l_79(t_4)
        t_4 = t_3 + t_4
        # returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___506
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___508
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/Tensor::__add___608
        return (t_0, t_2, t_1, t_4)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition6(nn.Module):
    BASIC_BLOCKS=(
            Dropout,
            Linear,
            T5LayerNorm,
            T5Attention,
        )
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition6, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:6')
        self.lookup = { 'l_0': 'decoder.6.2.layer_norm',
                        'l_1': 'decoder.6.2.DenseReluDense.wi',
                        'l_2': 'decoder.6.2.DenseReluDense.dropout',
                        'l_3': 'decoder.6.2.DenseReluDense.wo',
                        'l_4': 'decoder.6.2.dropout',
                        'l_5': 'decoder.7.0.layer_norm',
                        'l_6': 'decoder.7.0.SelfAttention',
                        'l_7': 'decoder.7.0.dropout',
                        'l_8': 'decoder.7.1.layer_norm',
                        'l_9': 'decoder.7.1.EncDecAttention',
                        'l_10': 'decoder.7.1.dropout',
                        'l_11': 'decoder.7.2.layer_norm',
                        'l_12': 'decoder.7.2.DenseReluDense.wi',
                        'l_13': 'decoder.7.2.DenseReluDense.dropout',
                        'l_14': 'decoder.7.2.DenseReluDense.wo',
                        'l_15': 'decoder.7.2.dropout',
                        'l_16': 'decoder.8.0.layer_norm',
                        'l_17': 'decoder.8.0.SelfAttention',
                        'l_18': 'decoder.8.0.dropout',
                        'l_19': 'decoder.8.1.layer_norm',
                        'l_20': 'decoder.8.1.EncDecAttention',
                        'l_21': 'decoder.8.1.dropout',
                        'l_22': 'decoder.8.2.layer_norm',
                        'l_23': 'decoder.8.2.DenseReluDense.wi',
                        'l_24': 'decoder.8.2.DenseReluDense.dropout',
                        'l_25': 'decoder.8.2.DenseReluDense.wo',
                        'l_26': 'decoder.8.2.dropout',
                        'l_27': 'decoder.9.0.layer_norm',
                        'l_28': 'decoder.9.0.SelfAttention',
                        'l_29': 'decoder.9.0.dropout',
                        'l_30': 'decoder.9.1.layer_norm',
                        'l_31': 'decoder.9.1.EncDecAttention',
                        'l_32': 'decoder.9.1.dropout',
                        'l_33': 'decoder.9.2.layer_norm',
                        'l_34': 'decoder.9.2.DenseReluDense.wi',
                        'l_35': 'decoder.9.2.DenseReluDense.dropout',
                        'l_36': 'decoder.9.2.DenseReluDense.wo',
                        'l_37': 'decoder.9.2.dropout',
                        'l_38': 'decoder.10.0.layer_norm',
                        'l_39': 'decoder.10.0.SelfAttention',
                        'l_40': 'decoder.10.0.dropout',
                        'l_41': 'decoder.10.1.layer_norm',
                        'l_42': 'decoder.10.1.EncDecAttention',
                        'l_43': 'decoder.10.1.dropout',
                        'l_44': 'decoder.10.2.layer_norm',
                        'l_45': 'decoder.10.2.DenseReluDense.wi',
                        'l_46': 'decoder.10.2.DenseReluDense.dropout',
                        'l_47': 'decoder.10.2.DenseReluDense.wo',
                        'l_48': 'decoder.10.2.dropout',
                        'l_49': 'decoder.11.0.layer_norm',
                        'l_50': 'decoder.11.0.SelfAttention',
                        'l_51': 'decoder.11.0.dropout',
                        'l_52': 'decoder.11.1.layer_norm',
                        'l_53': 'decoder.11.1.EncDecAttention',
                        'l_54': 'decoder.11.1.dropout',
                        'l_55': 'decoder.11.2.layer_norm',
                        'l_56': 'decoder.11.2.DenseReluDense.wi',
                        'l_57': 'decoder.11.2.DenseReluDense.dropout',
                        'l_58': 'decoder.11.2.DenseReluDense.wo',
                        'l_59': 'decoder.11.2.dropout',
                        'l_60': 'decoder.12.0.layer_norm',
                        'l_61': 'decoder.12.0.SelfAttention',
                        'l_62': 'decoder.12.0.dropout',
                        'l_63': 'decoder.12.1.layer_norm',
                        'l_64': 'decoder.12.1.EncDecAttention',
                        'l_65': 'decoder.12.1.dropout',
                        'l_66': 'decoder.12.2.layer_norm',
                        'l_67': 'decoder.12.2.DenseReluDense.wi',
                        'l_68': 'decoder.12.2.DenseReluDense.dropout',
                        'l_69': 'decoder.12.2.DenseReluDense.wo',
                        'l_70': 'decoder.12.2.dropout',
                        'l_71': 'decoder.13.0.layer_norm',
                        'l_72': 'decoder.13.0.SelfAttention',
                        'l_73': 'decoder.13.0.dropout',
                        'l_74': 'decoder.13.1.layer_norm',
                        'l_75': 'decoder.13.1.EncDecAttention',
                        'l_76': 'decoder.13.1.dropout',
                        'l_77': 'decoder.13.2.layer_norm',
                        'l_78': 'decoder.13.2.DenseReluDense.wi',
                        'l_79': 'decoder.13.2.DenseReluDense.dropout',
                        'l_80': 'decoder.13.2.DenseReluDense.wo',
                        'l_81': 'decoder.13.2.dropout',
                        'l_82': 'decoder.14.0.layer_norm',
                        'l_83': 'decoder.14.0.SelfAttention',
                        'l_84': 'decoder.14.0.dropout',
                        'l_85': 'decoder.14.1.layer_norm',
                        'l_86': 'decoder.14.1.EncDecAttention',
                        'l_87': 'decoder.14.1.dropout',
                        'l_88': 'decoder.14.2.layer_norm',
                        'l_89': 'decoder.14.2.DenseReluDense.wi',
                        'l_90': 'decoder.14.2.DenseReluDense.dropout',
                        'l_91': 'decoder.14.2.DenseReluDense.wo',
                        'l_92': 'decoder.14.2.dropout',
                        'l_93': 'decoder.15.0.layer_norm',
                        'l_94': 'decoder.15.0.SelfAttention',
                        'l_95': 'decoder.15.0.dropout',
                        'l_96': 'decoder.15.1.layer_norm',
                        'l_97': 'decoder.15.1.EncDecAttention',
                        'l_98': 'decoder.15.1.dropout',
                        'l_99': 'decoder.15.2.layer_norm'}

    def forward(self, x0, x1, x2, x3, x4, x5):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerFF[2]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[7]/T5LayerFF[2]/Dropout[dropout] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[8]/T5LayerFF[2]/Dropout[dropout] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[9]/T5LayerFF[2]/Dropout[dropout] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[10]/T5LayerFF[2]/Dropout[dropout] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_50
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_51
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_52
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_53
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_54
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_55
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_56
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_57
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_58
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[11]/T5LayerFF[2]/Dropout[dropout] <=> self.l_59
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_60
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_61
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_62
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_63
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_64
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_65
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_66
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_67
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_68
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_69
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[12]/T5LayerFF[2]/Dropout[dropout] <=> self.l_70
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_71
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_72
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_73
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_74
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_75
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_76
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_77
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_78
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_79
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_80
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[13]/T5LayerFF[2]/Dropout[dropout] <=> self.l_81
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_82
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_83
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_84
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_85
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_86
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_87
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_88
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_89
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_90
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_91
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[14]/T5LayerFF[2]/Dropout[dropout] <=> self.l_92
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_93
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_94
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_95
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_96
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_97
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_98
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_99
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___444 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___462 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___506 <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___508 <=> x4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[6]/T5LayerCrossAttention[1]/Tensor::__add___608 <=> x5

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5 = move_tensors((x0, x1, x2, x3, x4, x5), self.device)
        t_0 = self.l_0(x5)
        t_0 = self.l_1(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x5 + t_0
        t_1 = self.l_5(t_0)
        t_1 = self.l_6(t_1, mask=x1, position_bias=x3, head_mask=None)
        t_1 = self.l_7(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_8(t_1)
        t_0 = self.l_9(t_0, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_0 = self.l_10(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_11(t_0)
        t_1 = self.l_12(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_13(t_1)
        t_1 = self.l_14(t_1)
        t_1 = self.l_15(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_16(t_1)
        t_0 = self.l_17(t_0, mask=x1, position_bias=x3, head_mask=None)
        t_0 = self.l_18(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_19(t_0)
        t_1 = self.l_20(t_1, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_1 = self.l_21(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_22(t_1)
        t_0 = self.l_23(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_24(t_0)
        t_0 = self.l_25(t_0)
        t_0 = self.l_26(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_27(t_0)
        t_1 = self.l_28(t_1, mask=x1, position_bias=x3, head_mask=None)
        t_1 = self.l_29(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_30(t_1)
        t_0 = self.l_31(t_0, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_0 = self.l_32(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_33(t_0)
        t_1 = self.l_34(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_35(t_1)
        t_1 = self.l_36(t_1)
        t_1 = self.l_37(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_38(t_1)
        t_0 = self.l_39(t_0, mask=x1, position_bias=x3, head_mask=None)
        t_0 = self.l_40(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_41(t_0)
        t_1 = self.l_42(t_1, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_1 = self.l_43(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_44(t_1)
        t_0 = self.l_45(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_46(t_0)
        t_0 = self.l_47(t_0)
        t_0 = self.l_48(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_49(t_0)
        t_1 = self.l_50(t_1, mask=x1, position_bias=x3, head_mask=None)
        t_1 = self.l_51(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_52(t_1)
        t_0 = self.l_53(t_0, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_0 = self.l_54(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_55(t_0)
        t_1 = self.l_56(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_57(t_1)
        t_1 = self.l_58(t_1)
        t_1 = self.l_59(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_60(t_1)
        t_0 = self.l_61(t_0, mask=x1, position_bias=x3, head_mask=None)
        t_0 = self.l_62(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_63(t_0)
        t_1 = self.l_64(t_1, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_1 = self.l_65(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_66(t_1)
        t_0 = self.l_67(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_68(t_0)
        t_0 = self.l_69(t_0)
        t_0 = self.l_70(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_71(t_0)
        t_1 = self.l_72(t_1, mask=x1, position_bias=x3, head_mask=None)
        t_1 = self.l_73(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_74(t_1)
        t_0 = self.l_75(t_0, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_0 = self.l_76(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_77(t_0)
        t_1 = self.l_78(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_79(t_1)
        t_1 = self.l_80(t_1)
        t_1 = self.l_81(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_82(t_1)
        t_0 = self.l_83(t_0, mask=x1, position_bias=x3, head_mask=None)
        t_0 = self.l_84(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_85(t_0)
        t_1 = self.l_86(t_1, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_1 = self.l_87(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_88(t_1)
        t_0 = self.l_89(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_90(t_0)
        t_0 = self.l_91(t_0)
        t_0 = self.l_92(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_93(t_0)
        t_1 = self.l_94(t_1, mask=x1, position_bias=x3, head_mask=None)
        t_1 = self.l_95(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_96(t_1)
        t_0 = self.l_97(t_0, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_0 = self.l_98(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_99(t_0)
        # returning:
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/Tensor::__add___770
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5LayerNorm[layer_norm]
        return (t_0, t_1)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition7(nn.Module):
    BASIC_BLOCKS=(
            Dropout,
            T5LayerNorm,
            CrossEntropyLoss,
            T5Attention,
            Linear,
        )
    LAYER_SCOPES=[
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5LayerNorm[final_layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/Linear[lm_head]',
            'T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition7, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:7')
        self.lookup = { 'l_0': 'decoder.15.2.DenseReluDense.wi',
                        'l_1': 'decoder.15.2.DenseReluDense.dropout',
                        'l_2': 'decoder.15.2.DenseReluDense.wo',
                        'l_3': 'decoder.15.2.dropout',
                        'l_4': 'decoder.16.0.layer_norm',
                        'l_5': 'decoder.16.0.SelfAttention',
                        'l_6': 'decoder.16.0.dropout',
                        'l_7': 'decoder.16.1.layer_norm',
                        'l_8': 'decoder.16.1.EncDecAttention',
                        'l_9': 'decoder.16.1.dropout',
                        'l_10': 'decoder.16.2.layer_norm',
                        'l_11': 'decoder.16.2.DenseReluDense.wi',
                        'l_12': 'decoder.16.2.DenseReluDense.dropout',
                        'l_13': 'decoder.16.2.DenseReluDense.wo',
                        'l_14': 'decoder.16.2.dropout',
                        'l_15': 'decoder.17.0.layer_norm',
                        'l_16': 'decoder.17.0.SelfAttention',
                        'l_17': 'decoder.17.0.dropout',
                        'l_18': 'decoder.17.1.layer_norm',
                        'l_19': 'decoder.17.1.EncDecAttention',
                        'l_20': 'decoder.17.1.dropout',
                        'l_21': 'decoder.17.2.layer_norm',
                        'l_22': 'decoder.17.2.DenseReluDense.wi',
                        'l_23': 'decoder.17.2.DenseReluDense.dropout',
                        'l_24': 'decoder.17.2.DenseReluDense.wo',
                        'l_25': 'decoder.17.2.dropout',
                        'l_26': 'decoder.18.0.layer_norm',
                        'l_27': 'decoder.18.0.SelfAttention',
                        'l_28': 'decoder.18.0.dropout',
                        'l_29': 'decoder.18.1.layer_norm',
                        'l_30': 'decoder.18.1.EncDecAttention',
                        'l_31': 'decoder.18.1.dropout',
                        'l_32': 'decoder.18.2.layer_norm',
                        'l_33': 'decoder.18.2.DenseReluDense.wi',
                        'l_34': 'decoder.18.2.DenseReluDense.dropout',
                        'l_35': 'decoder.18.2.DenseReluDense.wo',
                        'l_36': 'decoder.18.2.dropout',
                        'l_37': 'decoder.19.0.layer_norm',
                        'l_38': 'decoder.19.0.SelfAttention',
                        'l_39': 'decoder.19.0.dropout',
                        'l_40': 'decoder.19.1.layer_norm',
                        'l_41': 'decoder.19.1.EncDecAttention',
                        'l_42': 'decoder.19.1.dropout',
                        'l_43': 'decoder.19.2.layer_norm',
                        'l_44': 'decoder.19.2.DenseReluDense.wi',
                        'l_45': 'decoder.19.2.DenseReluDense.dropout',
                        'l_46': 'decoder.19.2.DenseReluDense.wo',
                        'l_47': 'decoder.19.2.dropout',
                        'l_48': 'decoder.20.0.layer_norm',
                        'l_49': 'decoder.20.0.SelfAttention',
                        'l_50': 'decoder.20.0.dropout',
                        'l_51': 'decoder.20.1.layer_norm',
                        'l_52': 'decoder.20.1.EncDecAttention',
                        'l_53': 'decoder.20.1.dropout',
                        'l_54': 'decoder.20.2.layer_norm',
                        'l_55': 'decoder.20.2.DenseReluDense.wi',
                        'l_56': 'decoder.20.2.DenseReluDense.dropout',
                        'l_57': 'decoder.20.2.DenseReluDense.wo',
                        'l_58': 'decoder.20.2.dropout',
                        'l_59': 'decoder.21.0.layer_norm',
                        'l_60': 'decoder.21.0.SelfAttention',
                        'l_61': 'decoder.21.0.dropout',
                        'l_62': 'decoder.21.1.layer_norm',
                        'l_63': 'decoder.21.1.EncDecAttention',
                        'l_64': 'decoder.21.1.dropout',
                        'l_65': 'decoder.21.2.layer_norm',
                        'l_66': 'decoder.21.2.DenseReluDense.wi',
                        'l_67': 'decoder.21.2.DenseReluDense.dropout',
                        'l_68': 'decoder.21.2.DenseReluDense.wo',
                        'l_69': 'decoder.21.2.dropout',
                        'l_70': 'decoder.22.0.layer_norm',
                        'l_71': 'decoder.22.0.SelfAttention',
                        'l_72': 'decoder.22.0.dropout',
                        'l_73': 'decoder.22.1.layer_norm',
                        'l_74': 'decoder.22.1.EncDecAttention',
                        'l_75': 'decoder.22.1.dropout',
                        'l_76': 'decoder.22.2.layer_norm',
                        'l_77': 'decoder.22.2.DenseReluDense.wi',
                        'l_78': 'decoder.22.2.DenseReluDense.dropout',
                        'l_79': 'decoder.22.2.DenseReluDense.wo',
                        'l_80': 'decoder.22.2.dropout',
                        'l_81': 'decoder.23.0.layer_norm',
                        'l_82': 'decoder.23.0.SelfAttention',
                        'l_83': 'decoder.23.0.dropout',
                        'l_84': 'decoder.23.1.layer_norm',
                        'l_85': 'decoder.23.1.EncDecAttention',
                        'l_86': 'decoder.23.1.dropout',
                        'l_87': 'decoder.23.2.layer_norm',
                        'l_88': 'decoder.23.2.DenseReluDense.wi',
                        'l_89': 'decoder.23.2.DenseReluDense.dropout',
                        'l_90': 'decoder.23.2.DenseReluDense.wo',
                        'l_91': 'decoder.23.2.dropout',
                        'l_92': 'decoder.final_layer_norm',
                        'l_93': 'decoder.dropout',
                        'l_94': 'lm_head',
                        'l_95': 'lm_loss'}

    def forward(self, x0, x1, x2, x3, x4, x5, x6, x7):
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/Dropout[dropout] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[16]/T5LayerFF[2]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_21
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_22
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_23
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_24
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[17]/T5LayerFF[2]/Dropout[dropout] <=> self.l_25
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_26
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_27
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_28
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_29
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_30
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_31
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_32
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_33
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_34
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_35
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[18]/T5LayerFF[2]/Dropout[dropout] <=> self.l_36
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_37
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_38
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_39
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_40
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_41
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_42
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_43
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_44
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_45
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_46
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[19]/T5LayerFF[2]/Dropout[dropout] <=> self.l_47
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_48
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_49
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_50
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_51
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_52
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_53
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_54
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_55
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_56
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_57
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[20]/T5LayerFF[2]/Dropout[dropout] <=> self.l_58
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_59
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_60
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_61
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_62
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_63
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_64
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_65
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_66
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_67
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_68
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[21]/T5LayerFF[2]/Dropout[dropout] <=> self.l_69
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_70
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_71
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_72
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_73
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_74
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_75
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_76
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_77
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_78
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_79
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[22]/T5LayerFF[2]/Dropout[dropout] <=> self.l_80
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_81
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/T5Attention[SelfAttention] <=> self.l_82
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_83
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_84
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention] <=> self.l_85
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_86
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_87
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_88
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_89
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_90
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[23]/T5LayerFF[2]/Dropout[dropout] <=> self.l_91
        # T5ForConditionalGeneration/T5Stack[decoder]/T5LayerNorm[final_layer_norm] <=> self.l_92
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout] <=> self.l_93
        # T5ForConditionalGeneration/Linear[lm_head] <=> self.l_94
        # T5ForConditionalGeneration/CrossEntropyLoss[lm_loss] <=> self.l_95
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___444 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___462 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___506 <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___508 <=> x4
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerCrossAttention[1]/Tensor::__add___770 <=> x5
        # T5ForConditionalGeneration/T5Stack[decoder]/T5Block[15]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> x6
        # T5ForConditionalGeneration/Tensor::view_933 <=> x7

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2, x3, x4, x5, x6, x7 = move_tensors((x0, x1, x2, x3, x4, x5, x6, x7), self.device)
        t_0 = self.l_0(x6)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = x5 + t_0
        t_1 = self.l_4(t_0)
        t_1 = self.l_5(t_1, mask=x1, position_bias=x3, head_mask=None)
        t_1 = self.l_6(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_7(t_1)
        t_0 = self.l_8(t_0, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_0 = self.l_9(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_10(t_0)
        t_1 = self.l_11(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_12(t_1)
        t_1 = self.l_13(t_1)
        t_1 = self.l_14(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_15(t_1)
        t_0 = self.l_16(t_0, mask=x1, position_bias=x3, head_mask=None)
        t_0 = self.l_17(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_18(t_0)
        t_1 = self.l_19(t_1, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_1 = self.l_20(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_21(t_1)
        t_0 = self.l_22(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = self.l_25(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_26(t_0)
        t_1 = self.l_27(t_1, mask=x1, position_bias=x3, head_mask=None)
        t_1 = self.l_28(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_29(t_1)
        t_0 = self.l_30(t_0, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_0 = self.l_31(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_32(t_0)
        t_1 = self.l_33(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_34(t_1)
        t_1 = self.l_35(t_1)
        t_1 = self.l_36(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_37(t_1)
        t_0 = self.l_38(t_0, mask=x1, position_bias=x3, head_mask=None)
        t_0 = self.l_39(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_40(t_0)
        t_1 = self.l_41(t_1, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_1 = self.l_42(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_43(t_1)
        t_0 = self.l_44(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_45(t_0)
        t_0 = self.l_46(t_0)
        t_0 = self.l_47(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_48(t_0)
        t_1 = self.l_49(t_1, mask=x1, position_bias=x3, head_mask=None)
        t_1 = self.l_50(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_51(t_1)
        t_0 = self.l_52(t_0, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_0 = self.l_53(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_54(t_0)
        t_1 = self.l_55(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_56(t_1)
        t_1 = self.l_57(t_1)
        t_1 = self.l_58(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_59(t_1)
        t_0 = self.l_60(t_0, mask=x1, position_bias=x3, head_mask=None)
        t_0 = self.l_61(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_62(t_0)
        t_1 = self.l_63(t_1, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_1 = self.l_64(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_65(t_1)
        t_0 = self.l_66(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_67(t_0)
        t_0 = self.l_68(t_0)
        t_0 = self.l_69(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_70(t_0)
        t_1 = self.l_71(t_1, mask=x1, position_bias=x3, head_mask=None)
        t_1 = self.l_72(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_73(t_1)
        t_0 = self.l_74(t_0, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_0 = self.l_75(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_76(t_0)
        t_1 = self.l_77(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_78(t_1)
        t_1 = self.l_79(t_1)
        t_1 = self.l_80(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_81(t_1)
        t_0 = self.l_82(t_0, mask=x1, position_bias=x3, head_mask=None)
        t_0 = self.l_83(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_84(t_0)
        t_1 = self.l_85(t_1, mask=x2, kv=x0, position_bias=x4, head_mask=None)
        t_1 = self.l_86(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_87(t_1)
        t_0 = self.l_88(t_0)
        t_0 = torch.nn.functional.relu(t_0, inplace=False)
        t_0 = self.l_89(t_0)
        t_0 = self.l_90(t_0)
        t_0 = self.l_91(t_0)
        t_0 = t_1 + t_0
        t_0 = self.l_92(t_0)
        t_0 = self.l_93(t_0)
        t_0 = t_0 * 0.03125
        t_0 = self.l_94(t_0)
        t_1 = t_0.size(-1)
        t_1 = t_0.view(-1, t_1)
        t_1 = self.l_95(t_1, x7)
        # returning:
        # T5ForConditionalGeneration/CrossEntropyLoss[lm_loss]
        return (t_1,)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


def traverse_model(module: nn.Module, depth: int, prefix: Optional[str] = None,
                   basic_blocks: Tuple[nn.Module] = (), full: bool = False) -> Iterator[Tuple[nn.Module, str, nn.Module]]:
    '''
    iterate over model layers yielding the layer,layer_scope,encasing_module
    Parameters:
    -----------
    model:
        the model to iterate over
    depth:
        how far down in the model tree to go
    basic_blocks:
        a list of modules that if encountered will not be broken down
    full:
        whether to yield only layers specified by the depth and basick_block options or to yield all layers
    '''
    if prefix is None:
        prefix = type(module).__name__

    for name, sub_module in module.named_children():
        scope = prefix + "/" + type(sub_module).__name__ + f"[{name}]"
        if len(list(sub_module.children())) == 0 or isinstance(sub_module, tuple(basic_blocks)) or depth == 0:
            if full:
                yield sub_module, scope, module, True
            else:
                yield sub_module, scope, module
        else:
            if full:
                yield sub_module, scope, module, False
            yield from traverse_model(sub_module, depth - 1, scope, basic_blocks, full)


def layerDict(model: nn.Module, depth=1000, basic_blocks=()) -> Dict[str, nn.Module]:
    return {s: l for l, s, _ in traverse_model(model, depth, basic_blocks=basic_blocks)}


def traverse_params_buffs(module: nn.Module, prefix: Optional[str] = None) -> Iterator[Tuple[torch.tensor, str]]:
    '''
    iterate over model's buffers and parameters yielding obj,obj_scope

    Parameters:
    -----------
    model:
        the model to iterate over
    '''
    if prefix is None:
        prefix = type(module).__name__

    # params
    for param_name, param in module.named_parameters(recurse=False):
        param_scope = f"{prefix}/{type(param).__name__}[{param_name}]"
        yield param, param_scope

    # buffs
    for buffer_name, buffer in module.named_buffers(recurse=False):
        buffer_scope = f"{prefix}/{type(buffer).__name__}[{buffer_name}]"
        yield buffer, buffer_scope

    # recurse
    for name, sub_module in module.named_children():
        yield from traverse_params_buffs(sub_module, prefix + "/" + type(sub_module).__name__ + f"[{name}]")


def tensorDict(model: nn.Module) -> OrderedDict[str, Tensor]:
    return collections.OrderedDict((s, t)for t, s in traverse_params_buffs(model))


def move_tensors(ts, device):
    def move(t):
        if isinstance(t, (nn.Module, Tensor)):
            return t.to(device)
        return t

    return nested_map(move, ts)


def nested_map(func, ts,full=False):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        return func(ts)
    elif isinstance(ts, (list, tuple, set)):
        return type(ts)(nested_map(func, t,full=full) for t in ts)
    elif isinstance(ts, dict):
        return {k: nested_map(func, v,full=full) for k, v in ts.items()}
    elif isinstance(ts, slice) and full:
        start = nested_map(func, ts.start,full=full)
        stop = nested_map(func, ts.stop,full=full)
        step = nested_map(func, ts.step,full=full)
        return slice(start, stop, step)
    return func(ts)


def state_dict(partition, device=None):
    # we return the state dict of this part as it should be in the original model
    state = nn.Module.state_dict(partition)
    lookup = partition.lookup
    result = dict()
    for k, v in state.items():
        if k in lookup:
            result[lookup[k]] = v if device is None else v.to(device)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            result[new_k] = v if device is None else v.to(device)
    return result


def load_state_dict(partition, state):
    reverse_lookup = {v: k for k, v in partition.lookup.items()}
    device = partition.device
    keys = list(partition.state_dict(None).keys())
    new_state = dict()
    for k in keys:
        if k in reverse_lookup:
            new_state[reverse_lookup[k]] = state[k].to(device)
            continue
        idx = k.rfind(".")
        to_replace = k[:idx]
        if to_replace in reverse_lookup:
            key = reverse_lookup[to_replace] + k[idx:]
            new_state[key] = state[k].to(device)
    nn.Module.load_state_dict(partition, new_state, strict=True)


def named_buffers(partition, recurse=True):
    # we return the named buffers of this part as it should be in the original model
    params = nn.Module.named_buffers(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def named_parameters(partition, recurse=True):
    # we return the named parameters of this part as it should be in the original model
    params = nn.Module.named_parameters(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def cpu(partition):
    partition.device = torch.device('cpu')
    return nn.Module.cpu(partition)


def cuda(partition, device=None):
    if device is None:
        device = torch.cuda.current_device()
    partition.device = torch.device(device)
    return nn.Module.cuda(partition, partition.device)


def to(partition, *args, **kwargs):
    device = None
    if 'device' in kwargs:
        device = kwargs['device']
    elif 'tensor' in kwargs:
        device = kwargs['tensor'].device
    if args:
        if isinstance(args[0], (torch.device, int, str)):
            device = args[0]
        if torch.is_tensor(args[0]):
            device = args[0].device
    if not (device is None):
        partition.device = torch.device(device)
    return nn.Module.to(partition, *args, **kwargs)

"""analysis summary
-I- Printing Report
warnings:
tensor attention_mask sent to more than 1 target. Inaccurate (backward) communication time analysis
tensor lm_labels sent to more than 1 target. Inaccurate (backward) communication time analysis
tensor T5ForConditionalGeneration/T5Stack[encoder]/Tensor::__mul___27 sent to more than 1 target. Inaccurate (backward) communication time analysis
tensor T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___56 sent to more than 1 target. Inaccurate (backward) communication time analysis
Partition0 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___56 is not contiguous!
tensor T5ForConditionalGeneration/Tensor::new_zeros_360 sent to more than 1 target. Inaccurate (backward) communication time analysis
tensor T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___444 sent to more than 1 target. Inaccurate (backward) communication time analysis
Partition0 output:T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___480 is not contiguous!
tensor T5ForConditionalGeneration/T5Stack[decoder]/T5Block[0]/tuple::__getitem___478 sent to more than 1 target. Inaccurate (backward) communication time analysis
tensor T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___462 sent to more than 1 target. Inaccurate (backward) communication time analysis
tensor T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] sent to more than 1 target. Inaccurate (backward) communication time analysis
tensor T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___506 sent to more than 1 target. Inaccurate (backward) communication time analysis
Partition5 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___506 is not contiguous!
tensor T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___508 sent to more than 1 target. Inaccurate (backward) communication time analysis
Partition5 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___508 is not contiguous!
Number of stages: 8
cutting edges are edges between partitions
number of cutting edges: 169

backward times include recomputation

real times are based on real measurements of execution time of generated partitions ms
forward {0: 7.82, 1: 8.47, 2: 7.63, 3: 7.25, 4: 8.41, 5: 8.58, 6: 9.34, 7: 10.11}
backward {0: 24.25, 1: 22.33, 2: 20.58, 3: 18.54, 4: 21.19, 5: 38.32, 6: 44.82, 7: 41.97}

balance is ratio of computation time between fastest and slowest parts. (between 0 and 1 higher is better)

real balance:
forward 0.718
backward 0.414

Assuming bandwidth of 12 GBps between GPUs

communication volumes size of activations of each partition
0: input size:'0.00 MB', recieve_time:'0.00 ms', out:'3.28 MB', send time:'0.27 ms'
1: input size:'3.25 MB', recieve_time:'0.27 ms', out:'8.98 MB', send time:'0.75 ms'
2: input size:'11.08 MB', recieve_time:'0.92 ms', out:'0.59 MB', send time:'0.05 ms'
3: input size:'2.62 MB', recieve_time:'0.22 ms', out:'0.52 MB', send time:'0.04 ms'
4: input size:'2.62 MB', recieve_time:'0.22 ms', out:'8.91 MB', send time:'0.74 ms'
5: input size:'11.18 MB', recieve_time:'0.93 ms', out:'0.88 MB', send time:'0.07 ms'
6: input size:'0.89 MB', recieve_time:'0.07 ms', out:'0.13 MB', send time:'0.01 ms'
7: input size:'0.95 MB', recieve_time:'0.08 ms', out:'0.00 MB', send time:'0.00 ms'

Compuatation Communication ratio (comp/(comp+comm)):
forward {0: 0.97, 1: 0.91, 2: 0.99, 3: 0.99, 4: 0.91, 5: 0.99, 6: 1.0, 7: 1.0} 
backward {0: 1.0, 1: 0.99, 2: 0.96, 3: 0.99, 4: 0.99, 5: 0.98, 6: 1.0, 7: 1.0}

Pipeline Slowdown: (compared to sequential executation with no communication, and same recompute policy)
forward 1.232
backward 1.564

Expected utilization by partition
forward {0: 0.74, 1: 0.77, 2: 0.75, 3: 0.72, 4: 0.76, 5: 0.84, 6: 0.92, 7: 1.0}
backward {0: 0.54, 1: 0.49, 2: 0.44, 3: 0.41, 4: 0.47, 5: 0.84, 6: 1.0, 7: 0.94}

worstcase: bwd: 44.819 fwd: 10.110
expected_speedup_compared_to_seq_no_recomp_no_comm: 4.476
Expected speedup for 8 partitions is: 5.324
"""