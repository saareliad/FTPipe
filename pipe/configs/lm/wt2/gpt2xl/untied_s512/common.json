{
    "logdir": "logs/debug/",
    "data_dir": "/home_local/saareliad/data",
    "out_dir": "results/new_gpt2xl/lm/gpt2xl_b512/untied/wd/wa",
    "statistics": "lm_loss_per_batch",
    "auto_file_name": true,
    "out_filename": "wd",
    "distributed_backend": "mpi",
    "model": "new_gpt2_xl_tied_lm_p8_seq_512",
    "model_name_or_path": "gpt2-xl",
    "dataset": "wt2",
    "trainer": {
        "type": "lm",
        "args": {
        }
    },
    "bs_train": 2,
    "bs_test": 2,
    "train_seq_len": 512,
    "valid_seq_len": 512,
    "test_seq_len": 512,
    "num_data_workers": 10,
    "optimizer": {
        "type": "adamw",
        "args": {
            "lr": 5e-5,
            "weight_decay": 0.01,
            "eps": 1e-8
        }
    },
    "lr_scheduler": {
        "type": "get_linear_schedule_with_warmup",
        "preproc_args": {
            "num_training_steps": "epochs_to_steps",
            "num_warmup_steps": "ratio_from_num_training_steps"
        },
        "args": {
            "num_warmup_steps": 0.06,
            "num_training_steps": -1,
            "last_epoch": -1
        }
    },
    "epochs": 1,
    "steps": -1,
    "seed_from_cmd": true,
    "num_chunks": 1,
    "verbose_comm": false,
    "flush_rate": -1,
    "work_scheduler": "1F1B",
    "cudnn_benchmark": true,
    "max_buffers": 1,
    "step_every": 8,
    "train_batches_limit": -1,
    "dont_drop_last": true,
    "keep_buffers_alive": false,
    "log_frequency": 80,
    "overwrite_cache": true
}
