"""AutoGenerated with:
python partition_gpt2_models.py --analysis_batch_size 2 --async_pipeline --block_size 512 --bwd_to_fwd_ratio 1 --lmhead --model_name_or_path gpt2-xl --model_type gpt2 --n_iter 4 --n_partitions 8 --output_file results/gpt2_tied_p8/new_b512_ratio1 --overwrite_cache --partitioning_batch_size 2 --seed 42 --train_data_file wikitext-2-raw/wiki.train.raw --use_METIS --basic_blocks Attention --metis_contig --metis_niter 100
"""
import torch.functional
import torch.nn.functional
import torch
import math
from torch import Tensor
import torch.nn as nn
from itertools import chain
from typing import Optional, Tuple, Iterator, Iterable, OrderedDict, Dict
import collections
import os
from torch.nn.modules.linear import Linear
from models.normal.NLP_models.modeling_gpt2 import Gelu
from models.normal.NLP_models.modeling_gpt2 import Attention
from torch.nn.modules.dropout import Dropout
from torch.nn.modules.sparse import Embedding
from torch.nn.modules.normalization import LayerNorm
from transformers.modeling_utils import Conv1D
from models.normal.NLP_models.modeling_gpt2 import LMOutput
# this is an auto generated file do not edit unless you know what you are doing


# partition adjacency
# model inputs {0, 7}
# partition 0 {'inputs': {'input0'}, 'outputs': {1}}
# partition 1 {'inputs': {0}, 'outputs': {2}}
# partition 2 {'inputs': {1}, 'outputs': {3}}
# partition 3 {'inputs': {2}, 'outputs': {4}}
# partition 4 {'inputs': {3}, 'outputs': {5}}
# partition 5 {'inputs': {4}, 'outputs': {6}}
# partition 6 {'inputs': {5}, 'outputs': {7}}
# partition 7 {'inputs': {'input1', 6}, 'outputs': {'output'}}
# model outputs {7}


def create_pipeline_configuration(DEBUG=False):
    basic_blocks = (Linear,Gelu,Attention,Dropout,Embedding,LayerNorm,Conv1D,LMOutput)
    module_path = os.path.relpath(__file__).replace("/",".")[:-3]
    
    config = {
        'batch_dim': 0,
        'depth': 10000,
        'basic_blocks': ['torch.nn.modules.linear.Linear', 'models.normal.NLP_models.modeling_gpt2.Gelu', 'models.normal.NLP_models.modeling_gpt2.Attention', 'torch.nn.modules.dropout.Dropout', 'torch.nn.modules.sparse.Embedding', 'torch.nn.modules.normalization.LayerNorm', 'transformers.modeling_utils.Conv1D', 'models.normal.NLP_models.modeling_gpt2.LMOutput'],
        'model_inputs': {
            'input0': {
                'shape': torch.Size([2, 512]),
                'dtype': torch.int64,
                'is_batched': True},
            'input1': {
                'shape': torch.Size([2, 512]),
                'dtype': torch.int64,
                'is_batched': True}},
        'model_outputs': {
            'GPT2LMHeadModel/LMOutput[compute_output]': {
                'shape': torch.Size([1]),
                'dtype': torch.float32,
                'is_batched': False}},
        'stages': {
            0: {
                'inputs': {
                    'input0': {
                        'shape': torch.Size([2, 512]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True}},
                'outputs': {
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Tensor::__add___62': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            1: {
                'inputs': {
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Tensor::__add___62': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True}},
                'outputs': {
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Tensor::__add___119': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            2: {
                'inputs': {
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Tensor::__add___119': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True}},
                'outputs': {
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Tensor::__add___173': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2]': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            3: {
                'inputs': {
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Tensor::__add___173': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2]': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True}},
                'outputs': {
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Tensor::__add___224': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            4: {
                'inputs': {
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Tensor::__add___224': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True}},
                'outputs': {
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Tensor::__add___281': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2]': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            5: {
                'inputs': {
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Tensor::__add___281': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2]': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True}},
                'outputs': {
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Tensor::__add___332': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            6: {
                'inputs': {
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Tensor::__add___332': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True}},
                'outputs': {
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Tensor::__add___389': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'is_batched': True},
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2]': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'is_batched': True}}},
            7: {
                'inputs': {
                    'input1': {
                        'shape': torch.Size([2, 512]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True},
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Tensor::__add___389': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True},
                    'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2]': {
                        'shape': torch.Size([2, 512, 1600]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True}},
                'outputs': {
                    'GPT2LMHeadModel/LMOutput[compute_output]': {
                        'shape': torch.Size([1]),
                        'dtype': torch.float32,
                        'is_batched': False}}}}}
    
    config['stages'][0]['stage_cls'] = module_path+'.Partition0'
    config['stages'][1]['stage_cls'] = module_path+'.Partition1'
    config['stages'][2]['stage_cls'] = module_path+'.Partition2'
    config['stages'][3]['stage_cls'] = module_path+'.Partition3'
    config['stages'][4]['stage_cls'] = module_path+'.Partition4'
    config['stages'][5]['stage_cls'] = module_path+'.Partition5'
    config['stages'][6]['stage_cls'] = module_path+'.Partition6'
    config['stages'][7]['stage_cls'] = module_path+'.Partition7'
    
    config['stages'][0]['devices'] = ['cpu' if DEBUG else 'cuda:0']
    config['stages'][1]['devices'] = ['cpu' if DEBUG else 'cuda:1']
    config['stages'][2]['devices'] = ['cpu' if DEBUG else 'cuda:2']
    config['stages'][3]['devices'] = ['cpu' if DEBUG else 'cuda:3']
    config['stages'][4]['devices'] = ['cpu' if DEBUG else 'cuda:4']
    config['stages'][5]['devices'] = ['cpu' if DEBUG else 'cuda:5']
    config['stages'][6]['devices'] = ['cpu' if DEBUG else 'cuda:6']
    config['stages'][7]['devices'] = ['cpu' if DEBUG else 'cuda:7']
    
    return config

class Partition0(nn.Module):
    BASIC_BLOCKS=(
            Dropout,
            Embedding,
            Attention,
            Gelu,
            LayerNorm,
            Conv1D,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wte]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition0, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:0')
        self.lookup = { 'l_0': 'transformer.wte',
                        'l_1': 'transformer.wpe',
                        'l_2': 'transformer.blocks.0.ln_1',
                        'l_3': 'transformer.blocks.0.attn',
                        'l_4': 'transformer.blocks.0.ln_2',
                        'l_5': 'transformer.blocks.0.mlp.c_fc',
                        'l_6': 'transformer.blocks.0.mlp.act',
                        'l_7': 'transformer.blocks.0.mlp.c_proj',
                        'l_8': 'transformer.blocks.0.mlp.dropout',
                        'l_9': 'transformer.blocks.1.ln_1',
                        'l_10': 'transformer.blocks.1.attn',
                        'l_11': 'transformer.blocks.1.ln_2',
                        'l_12': 'transformer.blocks.1.mlp.c_fc',
                        'l_13': 'transformer.blocks.1.mlp.act',
                        'l_14': 'transformer.blocks.1.mlp.c_proj',
                        'l_15': 'transformer.blocks.1.mlp.dropout',
                        'l_16': 'transformer.blocks.2.ln_1',
                        'l_17': 'transformer.blocks.2.attn',
                        'l_18': 'transformer.blocks.2.ln_2',
                        'l_19': 'transformer.blocks.2.mlp.c_fc',
                        'l_20': 'transformer.blocks.2.mlp.act',
                        'l_21': 'transformer.blocks.2.mlp.c_proj',
                        'l_22': 'transformer.blocks.2.mlp.dropout',
                        'l_23': 'transformer.blocks.3.ln_1',
                        'l_24': 'transformer.blocks.3.attn',
                        'l_25': 'transformer.blocks.3.ln_2',
                        'l_26': 'transformer.blocks.3.mlp.c_fc',
                        'l_27': 'transformer.blocks.3.mlp.act',
                        'l_28': 'transformer.blocks.3.mlp.c_proj',
                        'l_29': 'transformer.blocks.3.mlp.dropout',
                        'l_30': 'transformer.blocks.4.ln_1',
                        'l_31': 'transformer.blocks.4.attn',
                        'l_32': 'transformer.blocks.4.ln_2',
                        'l_33': 'transformer.blocks.4.mlp.c_fc',
                        'l_34': 'transformer.blocks.4.mlp.act',
                        'l_35': 'transformer.blocks.4.mlp.c_proj',
                        'l_36': 'transformer.blocks.4.mlp.dropout',
                        'l_37': 'transformer.blocks.5.ln_1',
                        'l_38': 'transformer.blocks.5.attn'}

    def forward(self, x0):
        # GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wte] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Embedding[wpe] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_1] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/Attention[attn] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/LayerNorm[ln_2] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_fc] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Gelu[act] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Conv1D[c_proj] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[0]/MLP[mlp]/Dropout[dropout] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_1] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/Attention[attn] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/LayerNorm[ln_2] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_fc] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Gelu[act] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Conv1D[c_proj] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[1]/MLP[mlp]/Dropout[dropout] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_1] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/Attention[attn] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/LayerNorm[ln_2] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_fc] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Gelu[act] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Conv1D[c_proj] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[2]/MLP[mlp]/Dropout[dropout] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_1] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/Attention[attn] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/LayerNorm[ln_2] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_fc] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Gelu[act] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Conv1D[c_proj] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[3]/MLP[mlp]/Dropout[dropout] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_1] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Attention[attn] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/LayerNorm[ln_2] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_fc] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Gelu[act] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Conv1D[c_proj] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/MLP[mlp]/Dropout[dropout] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_1] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn] <=> self.l_38
        # input0 <=> x0

        # moving inputs to current device no op if already on the correct device
        x0 = move_tensors((x0), self.device)
        t_0 = x0.size()
        t_0 = t_0[-1]
        t_0 = x0.view(-1, t_0)
        t_1 = t_0.size(-1)
        t_1 = torch.arange(t_1, dtype=torch.int64, device=self.device)
        t_1 = t_1.unsqueeze(0)
        t_1 = t_1.expand_as(t_0)
        t_0 = self.l_0(t_0)
        t_1 = self.l_1(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_2(t_1)
        t_0 = self.l_3(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_4(t_0)
        t_1 = self.l_5(t_1)
        t_1 = self.l_6(t_1)
        t_1 = self.l_7(t_1)
        t_1 = self.l_8(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_9(t_1)
        t_0 = self.l_10(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_11(t_0)
        t_1 = self.l_12(t_1)
        t_1 = self.l_13(t_1)
        t_1 = self.l_14(t_1)
        t_1 = self.l_15(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_16(t_1)
        t_0 = self.l_17(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_18(t_0)
        t_1 = self.l_19(t_1)
        t_1 = self.l_20(t_1)
        t_1 = self.l_21(t_1)
        t_1 = self.l_22(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_23(t_1)
        t_0 = self.l_24(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_25(t_0)
        t_1 = self.l_26(t_1)
        t_1 = self.l_27(t_1)
        t_1 = self.l_28(t_1)
        t_1 = self.l_29(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_30(t_1)
        t_0 = self.l_31(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_32(t_0)
        t_1 = self.l_33(t_1)
        t_1 = self.l_34(t_1)
        t_1 = self.l_35(t_1)
        t_1 = self.l_36(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_37(t_1)
        t_0 = self.l_38(t_0)
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Tensor::__add___62
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn]
        return (t_1, t_0)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition1(nn.Module):
    BASIC_BLOCKS=(
            Dropout,
            Conv1D,
            Gelu,
            LayerNorm,
            Attention,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition1, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:1')
        self.lookup = { 'l_0': 'transformer.blocks.5.ln_2',
                        'l_1': 'transformer.blocks.5.mlp.c_fc',
                        'l_2': 'transformer.blocks.5.mlp.act',
                        'l_3': 'transformer.blocks.5.mlp.c_proj',
                        'l_4': 'transformer.blocks.5.mlp.dropout',
                        'l_5': 'transformer.blocks.6.ln_1',
                        'l_6': 'transformer.blocks.6.attn',
                        'l_7': 'transformer.blocks.6.ln_2',
                        'l_8': 'transformer.blocks.6.mlp.c_fc',
                        'l_9': 'transformer.blocks.6.mlp.act',
                        'l_10': 'transformer.blocks.6.mlp.c_proj',
                        'l_11': 'transformer.blocks.6.mlp.dropout',
                        'l_12': 'transformer.blocks.7.ln_1',
                        'l_13': 'transformer.blocks.7.attn',
                        'l_14': 'transformer.blocks.7.ln_2',
                        'l_15': 'transformer.blocks.7.mlp.c_fc',
                        'l_16': 'transformer.blocks.7.mlp.act',
                        'l_17': 'transformer.blocks.7.mlp.c_proj',
                        'l_18': 'transformer.blocks.7.mlp.dropout',
                        'l_19': 'transformer.blocks.8.ln_1',
                        'l_20': 'transformer.blocks.8.attn',
                        'l_21': 'transformer.blocks.8.ln_2',
                        'l_22': 'transformer.blocks.8.mlp.c_fc',
                        'l_23': 'transformer.blocks.8.mlp.act',
                        'l_24': 'transformer.blocks.8.mlp.c_proj',
                        'l_25': 'transformer.blocks.8.mlp.dropout',
                        'l_26': 'transformer.blocks.9.ln_1',
                        'l_27': 'transformer.blocks.9.attn',
                        'l_28': 'transformer.blocks.9.ln_2',
                        'l_29': 'transformer.blocks.9.mlp.c_fc',
                        'l_30': 'transformer.blocks.9.mlp.act',
                        'l_31': 'transformer.blocks.9.mlp.c_proj',
                        'l_32': 'transformer.blocks.9.mlp.dropout',
                        'l_33': 'transformer.blocks.10.ln_1',
                        'l_34': 'transformer.blocks.10.attn',
                        'l_35': 'transformer.blocks.10.ln_2',
                        'l_36': 'transformer.blocks.10.mlp.c_fc',
                        'l_37': 'transformer.blocks.10.mlp.act',
                        'l_38': 'transformer.blocks.10.mlp.c_proj',
                        'l_39': 'transformer.blocks.10.mlp.dropout',
                        'l_40': 'transformer.blocks.11.ln_1',
                        'l_41': 'transformer.blocks.11.attn'}

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/LayerNorm[ln_2] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_fc] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Gelu[act] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Conv1D[c_proj] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/MLP[mlp]/Dropout[dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_1] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/Attention[attn] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/LayerNorm[ln_2] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_fc] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Gelu[act] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Conv1D[c_proj] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[6]/MLP[mlp]/Dropout[dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_1] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/Attention[attn] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/LayerNorm[ln_2] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_fc] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Gelu[act] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Conv1D[c_proj] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[7]/MLP[mlp]/Dropout[dropout] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_1] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/Attention[attn] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/LayerNorm[ln_2] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_fc] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Gelu[act] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Conv1D[c_proj] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[8]/MLP[mlp]/Dropout[dropout] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_1] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/Attention[attn] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/LayerNorm[ln_2] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_fc] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Gelu[act] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Conv1D[c_proj] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[9]/MLP[mlp]/Dropout[dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_1] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/Attention[attn] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/LayerNorm[ln_2] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_fc] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Gelu[act] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Conv1D[c_proj] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[10]/MLP[mlp]/Dropout[dropout] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_1] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Attention[attn] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[4]/Tensor::__add___62 <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[5]/Attention[attn] <=> x1

        # moving inputs to current device no op if already on the correct device
        x0, x1 = move_tensors((x0, x1), self.device)
        t_0 = x0 + x1
        t_1 = self.l_0(t_0)
        t_1 = self.l_1(t_1)
        t_1 = self.l_2(t_1)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_5(t_1)
        t_0 = self.l_6(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_7(t_0)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_12(t_1)
        t_0 = self.l_13(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_14(t_0)
        t_1 = self.l_15(t_1)
        t_1 = self.l_16(t_1)
        t_1 = self.l_17(t_1)
        t_1 = self.l_18(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_19(t_1)
        t_0 = self.l_20(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_21(t_0)
        t_1 = self.l_22(t_1)
        t_1 = self.l_23(t_1)
        t_1 = self.l_24(t_1)
        t_1 = self.l_25(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_26(t_1)
        t_0 = self.l_27(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_28(t_0)
        t_1 = self.l_29(t_1)
        t_1 = self.l_30(t_1)
        t_1 = self.l_31(t_1)
        t_1 = self.l_32(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_33(t_1)
        t_0 = self.l_34(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_35(t_0)
        t_1 = self.l_36(t_1)
        t_1 = self.l_37(t_1)
        t_1 = self.l_38(t_1)
        t_1 = self.l_39(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_40(t_1)
        t_0 = self.l_41(t_0)
        t_0 = t_1 + t_0
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Tensor::__add___119
        return (t_0,)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition2(nn.Module):
    BASIC_BLOCKS=(
            Dropout,
            Conv1D,
            Gelu,
            LayerNorm,
            Attention,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition2, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:2')
        self.lookup = { 'l_0': 'transformer.blocks.11.ln_2',
                        'l_1': 'transformer.blocks.11.mlp.c_fc',
                        'l_2': 'transformer.blocks.11.mlp.act',
                        'l_3': 'transformer.blocks.11.mlp.c_proj',
                        'l_4': 'transformer.blocks.11.mlp.dropout',
                        'l_5': 'transformer.blocks.12.ln_1',
                        'l_6': 'transformer.blocks.12.attn',
                        'l_7': 'transformer.blocks.12.ln_2',
                        'l_8': 'transformer.blocks.12.mlp.c_fc',
                        'l_9': 'transformer.blocks.12.mlp.act',
                        'l_10': 'transformer.blocks.12.mlp.c_proj',
                        'l_11': 'transformer.blocks.12.mlp.dropout',
                        'l_12': 'transformer.blocks.13.ln_1',
                        'l_13': 'transformer.blocks.13.attn',
                        'l_14': 'transformer.blocks.13.ln_2',
                        'l_15': 'transformer.blocks.13.mlp.c_fc',
                        'l_16': 'transformer.blocks.13.mlp.act',
                        'l_17': 'transformer.blocks.13.mlp.c_proj',
                        'l_18': 'transformer.blocks.13.mlp.dropout',
                        'l_19': 'transformer.blocks.14.ln_1',
                        'l_20': 'transformer.blocks.14.attn',
                        'l_21': 'transformer.blocks.14.ln_2',
                        'l_22': 'transformer.blocks.14.mlp.c_fc',
                        'l_23': 'transformer.blocks.14.mlp.act',
                        'l_24': 'transformer.blocks.14.mlp.c_proj',
                        'l_25': 'transformer.blocks.14.mlp.dropout',
                        'l_26': 'transformer.blocks.15.ln_1',
                        'l_27': 'transformer.blocks.15.attn',
                        'l_28': 'transformer.blocks.15.ln_2',
                        'l_29': 'transformer.blocks.15.mlp.c_fc',
                        'l_30': 'transformer.blocks.15.mlp.act',
                        'l_31': 'transformer.blocks.15.mlp.c_proj',
                        'l_32': 'transformer.blocks.15.mlp.dropout',
                        'l_33': 'transformer.blocks.16.ln_1',
                        'l_34': 'transformer.blocks.16.attn',
                        'l_35': 'transformer.blocks.16.ln_2',
                        'l_36': 'transformer.blocks.16.mlp.c_fc',
                        'l_37': 'transformer.blocks.16.mlp.act',
                        'l_38': 'transformer.blocks.16.mlp.c_proj',
                        'l_39': 'transformer.blocks.16.mlp.dropout',
                        'l_40': 'transformer.blocks.17.ln_1',
                        'l_41': 'transformer.blocks.17.attn',
                        'l_42': 'transformer.blocks.17.ln_2'}

    def forward(self, x0):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/LayerNorm[ln_2] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_fc] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Gelu[act] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Conv1D[c_proj] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/MLP[mlp]/Dropout[dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_1] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/Attention[attn] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/LayerNorm[ln_2] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_fc] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Gelu[act] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Conv1D[c_proj] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[12]/MLP[mlp]/Dropout[dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_1] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/Attention[attn] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/LayerNorm[ln_2] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_fc] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Gelu[act] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Conv1D[c_proj] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[13]/MLP[mlp]/Dropout[dropout] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_1] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/Attention[attn] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/LayerNorm[ln_2] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_fc] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Gelu[act] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Conv1D[c_proj] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[14]/MLP[mlp]/Dropout[dropout] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_1] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/Attention[attn] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/LayerNorm[ln_2] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_fc] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Gelu[act] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Conv1D[c_proj] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[15]/MLP[mlp]/Dropout[dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_1] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/Attention[attn] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/LayerNorm[ln_2] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_fc] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Gelu[act] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Conv1D[c_proj] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[16]/MLP[mlp]/Dropout[dropout] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_1] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Attention[attn] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[11]/Tensor::__add___119 <=> x0

        # moving inputs to current device no op if already on the correct device
        x0 = move_tensors((x0), self.device)
        t_0 = self.l_0(x0)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = self.l_4(t_0)
        t_0 = x0 + t_0
        t_1 = self.l_5(t_0)
        t_1 = self.l_6(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_7(t_1)
        t_0 = self.l_8(t_0)
        t_0 = self.l_9(t_0)
        t_0 = self.l_10(t_0)
        t_0 = self.l_11(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_12(t_0)
        t_1 = self.l_13(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_14(t_1)
        t_0 = self.l_15(t_0)
        t_0 = self.l_16(t_0)
        t_0 = self.l_17(t_0)
        t_0 = self.l_18(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_19(t_0)
        t_1 = self.l_20(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_21(t_1)
        t_0 = self.l_22(t_0)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = self.l_25(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_26(t_0)
        t_1 = self.l_27(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_28(t_1)
        t_0 = self.l_29(t_0)
        t_0 = self.l_30(t_0)
        t_0 = self.l_31(t_0)
        t_0 = self.l_32(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_33(t_0)
        t_1 = self.l_34(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_35(t_1)
        t_0 = self.l_36(t_0)
        t_0 = self.l_37(t_0)
        t_0 = self.l_38(t_0)
        t_0 = self.l_39(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_40(t_0)
        t_1 = self.l_41(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_42(t_1)
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Tensor::__add___173
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2]
        return (t_1, t_0)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition3(nn.Module):
    BASIC_BLOCKS=(
            Conv1D,
            Attention,
            Gelu,
            LayerNorm,
            Dropout,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition3, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:3')
        self.lookup = { 'l_0': 'transformer.blocks.17.mlp.c_fc',
                        'l_1': 'transformer.blocks.17.mlp.act',
                        'l_2': 'transformer.blocks.17.mlp.c_proj',
                        'l_3': 'transformer.blocks.17.mlp.dropout',
                        'l_4': 'transformer.blocks.18.ln_1',
                        'l_5': 'transformer.blocks.18.attn',
                        'l_6': 'transformer.blocks.18.ln_2',
                        'l_7': 'transformer.blocks.18.mlp.c_fc',
                        'l_8': 'transformer.blocks.18.mlp.act',
                        'l_9': 'transformer.blocks.18.mlp.c_proj',
                        'l_10': 'transformer.blocks.18.mlp.dropout',
                        'l_11': 'transformer.blocks.19.ln_1',
                        'l_12': 'transformer.blocks.19.attn',
                        'l_13': 'transformer.blocks.19.ln_2',
                        'l_14': 'transformer.blocks.19.mlp.c_fc',
                        'l_15': 'transformer.blocks.19.mlp.act',
                        'l_16': 'transformer.blocks.19.mlp.c_proj',
                        'l_17': 'transformer.blocks.19.mlp.dropout',
                        'l_18': 'transformer.blocks.20.ln_1',
                        'l_19': 'transformer.blocks.20.attn',
                        'l_20': 'transformer.blocks.20.ln_2',
                        'l_21': 'transformer.blocks.20.mlp.c_fc',
                        'l_22': 'transformer.blocks.20.mlp.act',
                        'l_23': 'transformer.blocks.20.mlp.c_proj',
                        'l_24': 'transformer.blocks.20.mlp.dropout',
                        'l_25': 'transformer.blocks.21.ln_1',
                        'l_26': 'transformer.blocks.21.attn',
                        'l_27': 'transformer.blocks.21.ln_2',
                        'l_28': 'transformer.blocks.21.mlp.c_fc',
                        'l_29': 'transformer.blocks.21.mlp.act',
                        'l_30': 'transformer.blocks.21.mlp.c_proj',
                        'l_31': 'transformer.blocks.21.mlp.dropout',
                        'l_32': 'transformer.blocks.22.ln_1',
                        'l_33': 'transformer.blocks.22.attn',
                        'l_34': 'transformer.blocks.22.ln_2',
                        'l_35': 'transformer.blocks.22.mlp.c_fc',
                        'l_36': 'transformer.blocks.22.mlp.act',
                        'l_37': 'transformer.blocks.22.mlp.c_proj',
                        'l_38': 'transformer.blocks.22.mlp.dropout',
                        'l_39': 'transformer.blocks.23.ln_1',
                        'l_40': 'transformer.blocks.23.attn'}

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_fc] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Gelu[act] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Conv1D[c_proj] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/MLP[mlp]/Dropout[dropout] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_1] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/Attention[attn] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/LayerNorm[ln_2] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_fc] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Gelu[act] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Conv1D[c_proj] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[18]/MLP[mlp]/Dropout[dropout] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_1] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/Attention[attn] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/LayerNorm[ln_2] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_fc] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Gelu[act] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Conv1D[c_proj] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[19]/MLP[mlp]/Dropout[dropout] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_1] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/Attention[attn] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/LayerNorm[ln_2] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_fc] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Gelu[act] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Conv1D[c_proj] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[20]/MLP[mlp]/Dropout[dropout] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_1] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/Attention[attn] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/LayerNorm[ln_2] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_fc] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Gelu[act] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Conv1D[c_proj] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[21]/MLP[mlp]/Dropout[dropout] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_1] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Attention[attn] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/LayerNorm[ln_2] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_fc] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Gelu[act] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Conv1D[c_proj] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/MLP[mlp]/Dropout[dropout] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_1] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/Tensor::__add___173 <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[17]/LayerNorm[ln_2] <=> x1

        # moving inputs to current device no op if already on the correct device
        x0, x1 = move_tensors((x0, x1), self.device)
        t_0 = self.l_0(x1)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = x0 + t_0
        t_1 = self.l_4(t_0)
        t_1 = self.l_5(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_6(t_1)
        t_0 = self.l_7(t_0)
        t_0 = self.l_8(t_0)
        t_0 = self.l_9(t_0)
        t_0 = self.l_10(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_11(t_0)
        t_1 = self.l_12(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_13(t_1)
        t_0 = self.l_14(t_0)
        t_0 = self.l_15(t_0)
        t_0 = self.l_16(t_0)
        t_0 = self.l_17(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_18(t_0)
        t_1 = self.l_19(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_20(t_1)
        t_0 = self.l_21(t_0)
        t_0 = self.l_22(t_0)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_25(t_0)
        t_1 = self.l_26(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_27(t_1)
        t_0 = self.l_28(t_0)
        t_0 = self.l_29(t_0)
        t_0 = self.l_30(t_0)
        t_0 = self.l_31(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_32(t_0)
        t_1 = self.l_33(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_34(t_1)
        t_0 = self.l_35(t_0)
        t_0 = self.l_36(t_0)
        t_0 = self.l_37(t_0)
        t_0 = self.l_38(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_39(t_0)
        t_1 = self.l_40(t_1)
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Tensor::__add___224
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn]
        return (t_0, t_1)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition4(nn.Module):
    BASIC_BLOCKS=(
            Dropout,
            Conv1D,
            Gelu,
            LayerNorm,
            Attention,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition4, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:4')
        self.lookup = { 'l_0': 'transformer.blocks.23.ln_2',
                        'l_1': 'transformer.blocks.23.mlp.c_fc',
                        'l_2': 'transformer.blocks.23.mlp.act',
                        'l_3': 'transformer.blocks.23.mlp.c_proj',
                        'l_4': 'transformer.blocks.23.mlp.dropout',
                        'l_5': 'transformer.blocks.24.ln_1',
                        'l_6': 'transformer.blocks.24.attn',
                        'l_7': 'transformer.blocks.24.ln_2',
                        'l_8': 'transformer.blocks.24.mlp.c_fc',
                        'l_9': 'transformer.blocks.24.mlp.act',
                        'l_10': 'transformer.blocks.24.mlp.c_proj',
                        'l_11': 'transformer.blocks.24.mlp.dropout',
                        'l_12': 'transformer.blocks.25.ln_1',
                        'l_13': 'transformer.blocks.25.attn',
                        'l_14': 'transformer.blocks.25.ln_2',
                        'l_15': 'transformer.blocks.25.mlp.c_fc',
                        'l_16': 'transformer.blocks.25.mlp.act',
                        'l_17': 'transformer.blocks.25.mlp.c_proj',
                        'l_18': 'transformer.blocks.25.mlp.dropout',
                        'l_19': 'transformer.blocks.26.ln_1',
                        'l_20': 'transformer.blocks.26.attn',
                        'l_21': 'transformer.blocks.26.ln_2',
                        'l_22': 'transformer.blocks.26.mlp.c_fc',
                        'l_23': 'transformer.blocks.26.mlp.act',
                        'l_24': 'transformer.blocks.26.mlp.c_proj',
                        'l_25': 'transformer.blocks.26.mlp.dropout',
                        'l_26': 'transformer.blocks.27.ln_1',
                        'l_27': 'transformer.blocks.27.attn',
                        'l_28': 'transformer.blocks.27.ln_2',
                        'l_29': 'transformer.blocks.27.mlp.c_fc',
                        'l_30': 'transformer.blocks.27.mlp.act',
                        'l_31': 'transformer.blocks.27.mlp.c_proj',
                        'l_32': 'transformer.blocks.27.mlp.dropout',
                        'l_33': 'transformer.blocks.28.ln_1',
                        'l_34': 'transformer.blocks.28.attn',
                        'l_35': 'transformer.blocks.28.ln_2',
                        'l_36': 'transformer.blocks.28.mlp.c_fc',
                        'l_37': 'transformer.blocks.28.mlp.act',
                        'l_38': 'transformer.blocks.28.mlp.c_proj',
                        'l_39': 'transformer.blocks.28.mlp.dropout',
                        'l_40': 'transformer.blocks.29.ln_1',
                        'l_41': 'transformer.blocks.29.attn',
                        'l_42': 'transformer.blocks.29.ln_2'}

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/LayerNorm[ln_2] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_fc] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Gelu[act] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Conv1D[c_proj] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/MLP[mlp]/Dropout[dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_1] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/Attention[attn] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/LayerNorm[ln_2] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_fc] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Gelu[act] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Conv1D[c_proj] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[24]/MLP[mlp]/Dropout[dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_1] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/Attention[attn] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/LayerNorm[ln_2] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_fc] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Gelu[act] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Conv1D[c_proj] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[25]/MLP[mlp]/Dropout[dropout] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_1] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/Attention[attn] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/LayerNorm[ln_2] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_fc] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Gelu[act] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Conv1D[c_proj] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[26]/MLP[mlp]/Dropout[dropout] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_1] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/Attention[attn] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/LayerNorm[ln_2] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_fc] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Gelu[act] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Conv1D[c_proj] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[27]/MLP[mlp]/Dropout[dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_1] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/Attention[attn] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/LayerNorm[ln_2] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_fc] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Gelu[act] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Conv1D[c_proj] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[28]/MLP[mlp]/Dropout[dropout] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_1] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Attention[attn] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[22]/Tensor::__add___224 <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[23]/Attention[attn] <=> x1

        # moving inputs to current device no op if already on the correct device
        x0, x1 = move_tensors((x0, x1), self.device)
        t_0 = x0 + x1
        t_1 = self.l_0(t_0)
        t_1 = self.l_1(t_1)
        t_1 = self.l_2(t_1)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_5(t_1)
        t_0 = self.l_6(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_7(t_0)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_12(t_1)
        t_0 = self.l_13(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_14(t_0)
        t_1 = self.l_15(t_1)
        t_1 = self.l_16(t_1)
        t_1 = self.l_17(t_1)
        t_1 = self.l_18(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_19(t_1)
        t_0 = self.l_20(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_21(t_0)
        t_1 = self.l_22(t_1)
        t_1 = self.l_23(t_1)
        t_1 = self.l_24(t_1)
        t_1 = self.l_25(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_26(t_1)
        t_0 = self.l_27(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_28(t_0)
        t_1 = self.l_29(t_1)
        t_1 = self.l_30(t_1)
        t_1 = self.l_31(t_1)
        t_1 = self.l_32(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_33(t_1)
        t_0 = self.l_34(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_35(t_0)
        t_1 = self.l_36(t_1)
        t_1 = self.l_37(t_1)
        t_1 = self.l_38(t_1)
        t_1 = self.l_39(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_40(t_1)
        t_0 = self.l_41(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_42(t_0)
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Tensor::__add___281
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2]
        return (t_0, t_1)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition5(nn.Module):
    BASIC_BLOCKS=(
            Conv1D,
            Attention,
            Gelu,
            LayerNorm,
            Dropout,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition5, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:5')
        self.lookup = { 'l_0': 'transformer.blocks.29.mlp.c_fc',
                        'l_1': 'transformer.blocks.29.mlp.act',
                        'l_2': 'transformer.blocks.29.mlp.c_proj',
                        'l_3': 'transformer.blocks.29.mlp.dropout',
                        'l_4': 'transformer.blocks.30.ln_1',
                        'l_5': 'transformer.blocks.30.attn',
                        'l_6': 'transformer.blocks.30.ln_2',
                        'l_7': 'transformer.blocks.30.mlp.c_fc',
                        'l_8': 'transformer.blocks.30.mlp.act',
                        'l_9': 'transformer.blocks.30.mlp.c_proj',
                        'l_10': 'transformer.blocks.30.mlp.dropout',
                        'l_11': 'transformer.blocks.31.ln_1',
                        'l_12': 'transformer.blocks.31.attn',
                        'l_13': 'transformer.blocks.31.ln_2',
                        'l_14': 'transformer.blocks.31.mlp.c_fc',
                        'l_15': 'transformer.blocks.31.mlp.act',
                        'l_16': 'transformer.blocks.31.mlp.c_proj',
                        'l_17': 'transformer.blocks.31.mlp.dropout',
                        'l_18': 'transformer.blocks.32.ln_1',
                        'l_19': 'transformer.blocks.32.attn',
                        'l_20': 'transformer.blocks.32.ln_2',
                        'l_21': 'transformer.blocks.32.mlp.c_fc',
                        'l_22': 'transformer.blocks.32.mlp.act',
                        'l_23': 'transformer.blocks.32.mlp.c_proj',
                        'l_24': 'transformer.blocks.32.mlp.dropout',
                        'l_25': 'transformer.blocks.33.ln_1',
                        'l_26': 'transformer.blocks.33.attn',
                        'l_27': 'transformer.blocks.33.ln_2',
                        'l_28': 'transformer.blocks.33.mlp.c_fc',
                        'l_29': 'transformer.blocks.33.mlp.act',
                        'l_30': 'transformer.blocks.33.mlp.c_proj',
                        'l_31': 'transformer.blocks.33.mlp.dropout',
                        'l_32': 'transformer.blocks.34.ln_1',
                        'l_33': 'transformer.blocks.34.attn',
                        'l_34': 'transformer.blocks.34.ln_2',
                        'l_35': 'transformer.blocks.34.mlp.c_fc',
                        'l_36': 'transformer.blocks.34.mlp.act',
                        'l_37': 'transformer.blocks.34.mlp.c_proj',
                        'l_38': 'transformer.blocks.34.mlp.dropout',
                        'l_39': 'transformer.blocks.35.ln_1',
                        'l_40': 'transformer.blocks.35.attn'}

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_fc] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Gelu[act] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Conv1D[c_proj] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/MLP[mlp]/Dropout[dropout] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_1] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/Attention[attn] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/LayerNorm[ln_2] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_fc] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Gelu[act] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Conv1D[c_proj] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[30]/MLP[mlp]/Dropout[dropout] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_1] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/Attention[attn] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/LayerNorm[ln_2] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_fc] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Gelu[act] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Conv1D[c_proj] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[31]/MLP[mlp]/Dropout[dropout] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_1] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/Attention[attn] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/LayerNorm[ln_2] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_fc] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Gelu[act] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Conv1D[c_proj] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[32]/MLP[mlp]/Dropout[dropout] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_1] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/Attention[attn] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/LayerNorm[ln_2] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_fc] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Gelu[act] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Conv1D[c_proj] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[33]/MLP[mlp]/Dropout[dropout] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_1] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Attention[attn] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/LayerNorm[ln_2] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_fc] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Gelu[act] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Conv1D[c_proj] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/MLP[mlp]/Dropout[dropout] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_1] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/Tensor::__add___281 <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[29]/LayerNorm[ln_2] <=> x1

        # moving inputs to current device no op if already on the correct device
        x0, x1 = move_tensors((x0, x1), self.device)
        t_0 = self.l_0(x1)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = x0 + t_0
        t_1 = self.l_4(t_0)
        t_1 = self.l_5(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_6(t_1)
        t_0 = self.l_7(t_0)
        t_0 = self.l_8(t_0)
        t_0 = self.l_9(t_0)
        t_0 = self.l_10(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_11(t_0)
        t_1 = self.l_12(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_13(t_1)
        t_0 = self.l_14(t_0)
        t_0 = self.l_15(t_0)
        t_0 = self.l_16(t_0)
        t_0 = self.l_17(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_18(t_0)
        t_1 = self.l_19(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_20(t_1)
        t_0 = self.l_21(t_0)
        t_0 = self.l_22(t_0)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_25(t_0)
        t_1 = self.l_26(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_27(t_1)
        t_0 = self.l_28(t_0)
        t_0 = self.l_29(t_0)
        t_0 = self.l_30(t_0)
        t_0 = self.l_31(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_32(t_0)
        t_1 = self.l_33(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_34(t_1)
        t_0 = self.l_35(t_0)
        t_0 = self.l_36(t_0)
        t_0 = self.l_37(t_0)
        t_0 = self.l_38(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_39(t_0)
        t_1 = self.l_40(t_1)
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Tensor::__add___332
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn]
        return (t_0, t_1)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition6(nn.Module):
    BASIC_BLOCKS=(
            Dropout,
            Conv1D,
            Gelu,
            LayerNorm,
            Attention,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition6, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:6')
        self.lookup = { 'l_0': 'transformer.blocks.35.ln_2',
                        'l_1': 'transformer.blocks.35.mlp.c_fc',
                        'l_2': 'transformer.blocks.35.mlp.act',
                        'l_3': 'transformer.blocks.35.mlp.c_proj',
                        'l_4': 'transformer.blocks.35.mlp.dropout',
                        'l_5': 'transformer.blocks.36.ln_1',
                        'l_6': 'transformer.blocks.36.attn',
                        'l_7': 'transformer.blocks.36.ln_2',
                        'l_8': 'transformer.blocks.36.mlp.c_fc',
                        'l_9': 'transformer.blocks.36.mlp.act',
                        'l_10': 'transformer.blocks.36.mlp.c_proj',
                        'l_11': 'transformer.blocks.36.mlp.dropout',
                        'l_12': 'transformer.blocks.37.ln_1',
                        'l_13': 'transformer.blocks.37.attn',
                        'l_14': 'transformer.blocks.37.ln_2',
                        'l_15': 'transformer.blocks.37.mlp.c_fc',
                        'l_16': 'transformer.blocks.37.mlp.act',
                        'l_17': 'transformer.blocks.37.mlp.c_proj',
                        'l_18': 'transformer.blocks.37.mlp.dropout',
                        'l_19': 'transformer.blocks.38.ln_1',
                        'l_20': 'transformer.blocks.38.attn',
                        'l_21': 'transformer.blocks.38.ln_2',
                        'l_22': 'transformer.blocks.38.mlp.c_fc',
                        'l_23': 'transformer.blocks.38.mlp.act',
                        'l_24': 'transformer.blocks.38.mlp.c_proj',
                        'l_25': 'transformer.blocks.38.mlp.dropout',
                        'l_26': 'transformer.blocks.39.ln_1',
                        'l_27': 'transformer.blocks.39.attn',
                        'l_28': 'transformer.blocks.39.ln_2',
                        'l_29': 'transformer.blocks.39.mlp.c_fc',
                        'l_30': 'transformer.blocks.39.mlp.act',
                        'l_31': 'transformer.blocks.39.mlp.c_proj',
                        'l_32': 'transformer.blocks.39.mlp.dropout',
                        'l_33': 'transformer.blocks.40.ln_1',
                        'l_34': 'transformer.blocks.40.attn',
                        'l_35': 'transformer.blocks.40.ln_2',
                        'l_36': 'transformer.blocks.40.mlp.c_fc',
                        'l_37': 'transformer.blocks.40.mlp.act',
                        'l_38': 'transformer.blocks.40.mlp.c_proj',
                        'l_39': 'transformer.blocks.40.mlp.dropout',
                        'l_40': 'transformer.blocks.41.ln_1',
                        'l_41': 'transformer.blocks.41.attn',
                        'l_42': 'transformer.blocks.41.ln_2'}

    def forward(self, x0, x1):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/LayerNorm[ln_2] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_fc] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Gelu[act] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Conv1D[c_proj] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/MLP[mlp]/Dropout[dropout] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_1] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/Attention[attn] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/LayerNorm[ln_2] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_fc] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Gelu[act] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Conv1D[c_proj] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[36]/MLP[mlp]/Dropout[dropout] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_1] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/Attention[attn] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/LayerNorm[ln_2] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_fc] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Gelu[act] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Conv1D[c_proj] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[37]/MLP[mlp]/Dropout[dropout] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_1] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/Attention[attn] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/LayerNorm[ln_2] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_fc] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Gelu[act] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Conv1D[c_proj] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[38]/MLP[mlp]/Dropout[dropout] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_1] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/Attention[attn] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/LayerNorm[ln_2] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_fc] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Gelu[act] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Conv1D[c_proj] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[39]/MLP[mlp]/Dropout[dropout] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_1] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/Attention[attn] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/LayerNorm[ln_2] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_fc] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Gelu[act] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Conv1D[c_proj] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[40]/MLP[mlp]/Dropout[dropout] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_1] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Attention[attn] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[34]/Tensor::__add___332 <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[35]/Attention[attn] <=> x1

        # moving inputs to current device no op if already on the correct device
        x0, x1 = move_tensors((x0, x1), self.device)
        t_0 = x0 + x1
        t_1 = self.l_0(t_0)
        t_1 = self.l_1(t_1)
        t_1 = self.l_2(t_1)
        t_1 = self.l_3(t_1)
        t_1 = self.l_4(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_5(t_1)
        t_0 = self.l_6(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_7(t_0)
        t_1 = self.l_8(t_1)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_12(t_1)
        t_0 = self.l_13(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_14(t_0)
        t_1 = self.l_15(t_1)
        t_1 = self.l_16(t_1)
        t_1 = self.l_17(t_1)
        t_1 = self.l_18(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_19(t_1)
        t_0 = self.l_20(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_21(t_0)
        t_1 = self.l_22(t_1)
        t_1 = self.l_23(t_1)
        t_1 = self.l_24(t_1)
        t_1 = self.l_25(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_26(t_1)
        t_0 = self.l_27(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_28(t_0)
        t_1 = self.l_29(t_1)
        t_1 = self.l_30(t_1)
        t_1 = self.l_31(t_1)
        t_1 = self.l_32(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_33(t_1)
        t_0 = self.l_34(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_35(t_0)
        t_1 = self.l_36(t_1)
        t_1 = self.l_37(t_1)
        t_1 = self.l_38(t_1)
        t_1 = self.l_39(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_40(t_1)
        t_0 = self.l_41(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_42(t_0)
        # returning:
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Tensor::__add___389
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2]
        return (t_0, t_1)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


class Partition7(nn.Module):
    BASIC_BLOCKS=(
            Linear,
            Conv1D,
            Attention,
            Gelu,
            LayerNorm,
            LMOutput,
            Dropout,
        )
    LAYER_SCOPES=[
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Gelu[act]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj]',
            'GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout]',
            'GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]',
            'GPT2LMHeadModel/Linear[lm_head]',
            'GPT2LMHeadModel/LMOutput[compute_output]',
        ]
    TENSORS=[
        ]
    def __init__(self, layers, tensors):
        super(Partition7, self).__init__()

        #initialize partition layers
        for idx,layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}',layers[layer_scope])

        #initialize partition tensors
        b=p=0
        for tensor_scope in self.TENSORS:
            tensor=tensors[tensor_scope]
            if isinstance(tensor,nn.Parameter):
                self.register_parameter(f'p_{p}',tensor)
                p+=1
            else:
                self.register_buffer(f'b_{b}',tensor)
                b+=1

        self.device = torch.device('cuda:7')
        self.lookup = { 'l_0': 'transformer.blocks.41.mlp.c_fc',
                        'l_1': 'transformer.blocks.41.mlp.act',
                        'l_2': 'transformer.blocks.41.mlp.c_proj',
                        'l_3': 'transformer.blocks.41.mlp.dropout',
                        'l_4': 'transformer.blocks.42.ln_1',
                        'l_5': 'transformer.blocks.42.attn',
                        'l_6': 'transformer.blocks.42.ln_2',
                        'l_7': 'transformer.blocks.42.mlp.c_fc',
                        'l_8': 'transformer.blocks.42.mlp.act',
                        'l_9': 'transformer.blocks.42.mlp.c_proj',
                        'l_10': 'transformer.blocks.42.mlp.dropout',
                        'l_11': 'transformer.blocks.43.ln_1',
                        'l_12': 'transformer.blocks.43.attn',
                        'l_13': 'transformer.blocks.43.ln_2',
                        'l_14': 'transformer.blocks.43.mlp.c_fc',
                        'l_15': 'transformer.blocks.43.mlp.act',
                        'l_16': 'transformer.blocks.43.mlp.c_proj',
                        'l_17': 'transformer.blocks.43.mlp.dropout',
                        'l_18': 'transformer.blocks.44.ln_1',
                        'l_19': 'transformer.blocks.44.attn',
                        'l_20': 'transformer.blocks.44.ln_2',
                        'l_21': 'transformer.blocks.44.mlp.c_fc',
                        'l_22': 'transformer.blocks.44.mlp.act',
                        'l_23': 'transformer.blocks.44.mlp.c_proj',
                        'l_24': 'transformer.blocks.44.mlp.dropout',
                        'l_25': 'transformer.blocks.45.ln_1',
                        'l_26': 'transformer.blocks.45.attn',
                        'l_27': 'transformer.blocks.45.ln_2',
                        'l_28': 'transformer.blocks.45.mlp.c_fc',
                        'l_29': 'transformer.blocks.45.mlp.act',
                        'l_30': 'transformer.blocks.45.mlp.c_proj',
                        'l_31': 'transformer.blocks.45.mlp.dropout',
                        'l_32': 'transformer.blocks.46.ln_1',
                        'l_33': 'transformer.blocks.46.attn',
                        'l_34': 'transformer.blocks.46.ln_2',
                        'l_35': 'transformer.blocks.46.mlp.c_fc',
                        'l_36': 'transformer.blocks.46.mlp.act',
                        'l_37': 'transformer.blocks.46.mlp.c_proj',
                        'l_38': 'transformer.blocks.46.mlp.dropout',
                        'l_39': 'transformer.blocks.47.ln_1',
                        'l_40': 'transformer.blocks.47.attn',
                        'l_41': 'transformer.blocks.47.ln_2',
                        'l_42': 'transformer.blocks.47.mlp.c_fc',
                        'l_43': 'transformer.blocks.47.mlp.act',
                        'l_44': 'transformer.blocks.47.mlp.c_proj',
                        'l_45': 'transformer.blocks.47.mlp.dropout',
                        'l_46': 'transformer.ln_f',
                        'l_47': 'lm_head',
                        'l_48': 'compute_output'}

    def forward(self, x0, x1, x2):
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_fc] <=> self.l_0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Gelu[act] <=> self.l_1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Conv1D[c_proj] <=> self.l_2
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/MLP[mlp]/Dropout[dropout] <=> self.l_3
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_1] <=> self.l_4
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/Attention[attn] <=> self.l_5
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/LayerNorm[ln_2] <=> self.l_6
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_fc] <=> self.l_7
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Gelu[act] <=> self.l_8
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Conv1D[c_proj] <=> self.l_9
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[42]/MLP[mlp]/Dropout[dropout] <=> self.l_10
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_1] <=> self.l_11
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/Attention[attn] <=> self.l_12
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/LayerNorm[ln_2] <=> self.l_13
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_fc] <=> self.l_14
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Gelu[act] <=> self.l_15
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Conv1D[c_proj] <=> self.l_16
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[43]/MLP[mlp]/Dropout[dropout] <=> self.l_17
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_1] <=> self.l_18
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/Attention[attn] <=> self.l_19
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/LayerNorm[ln_2] <=> self.l_20
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_fc] <=> self.l_21
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Gelu[act] <=> self.l_22
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Conv1D[c_proj] <=> self.l_23
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[44]/MLP[mlp]/Dropout[dropout] <=> self.l_24
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_1] <=> self.l_25
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/Attention[attn] <=> self.l_26
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/LayerNorm[ln_2] <=> self.l_27
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_fc] <=> self.l_28
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Gelu[act] <=> self.l_29
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Conv1D[c_proj] <=> self.l_30
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[45]/MLP[mlp]/Dropout[dropout] <=> self.l_31
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_1] <=> self.l_32
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/Attention[attn] <=> self.l_33
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/LayerNorm[ln_2] <=> self.l_34
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_fc] <=> self.l_35
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Gelu[act] <=> self.l_36
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Conv1D[c_proj] <=> self.l_37
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[46]/MLP[mlp]/Dropout[dropout] <=> self.l_38
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_1] <=> self.l_39
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/Attention[attn] <=> self.l_40
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/LayerNorm[ln_2] <=> self.l_41
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_fc] <=> self.l_42
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Gelu[act] <=> self.l_43
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Conv1D[c_proj] <=> self.l_44
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[47]/MLP[mlp]/Dropout[dropout] <=> self.l_45
        # GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f] <=> self.l_46
        # GPT2LMHeadModel/Linear[lm_head] <=> self.l_47
        # GPT2LMHeadModel/LMOutput[compute_output] <=> self.l_48
        # input1 <=> x0
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/Tensor::__add___389 <=> x1
        # GPT2LMHeadModel/GPT2Model[transformer]/Sequential[blocks]/Block[41]/LayerNorm[ln_2] <=> x2

        # moving inputs to current device no op if already on the correct device
        x0, x1, x2 = move_tensors((x0, x1, x2), self.device)
        t_0 = self.l_0(x2)
        t_0 = self.l_1(t_0)
        t_0 = self.l_2(t_0)
        t_0 = self.l_3(t_0)
        t_0 = x1 + t_0
        t_1 = self.l_4(t_0)
        t_1 = self.l_5(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_6(t_1)
        t_0 = self.l_7(t_0)
        t_0 = self.l_8(t_0)
        t_0 = self.l_9(t_0)
        t_0 = self.l_10(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_11(t_0)
        t_1 = self.l_12(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_13(t_1)
        t_0 = self.l_14(t_0)
        t_0 = self.l_15(t_0)
        t_0 = self.l_16(t_0)
        t_0 = self.l_17(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_18(t_0)
        t_1 = self.l_19(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_20(t_1)
        t_0 = self.l_21(t_0)
        t_0 = self.l_22(t_0)
        t_0 = self.l_23(t_0)
        t_0 = self.l_24(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_25(t_0)
        t_1 = self.l_26(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_27(t_1)
        t_0 = self.l_28(t_0)
        t_0 = self.l_29(t_0)
        t_0 = self.l_30(t_0)
        t_0 = self.l_31(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_32(t_0)
        t_1 = self.l_33(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_34(t_1)
        t_0 = self.l_35(t_0)
        t_0 = self.l_36(t_0)
        t_0 = self.l_37(t_0)
        t_0 = self.l_38(t_0)
        t_0 = t_1 + t_0
        t_1 = self.l_39(t_0)
        t_1 = self.l_40(t_1)
        t_1 = t_0 + t_1
        t_0 = self.l_41(t_1)
        t_0 = self.l_42(t_0)
        t_0 = self.l_43(t_0)
        t_0 = self.l_44(t_0)
        t_0 = self.l_45(t_0)
        t_0 = t_1 + t_0
        t_0 = self.l_46(t_0)
        t_0 = self.l_47(t_0)
        t_0 = self.l_48(t_0, labels=x0)
        # returning:
        # GPT2LMHeadModel/LMOutput[compute_output]
        return (t_0,)

    def state_dict(self,device=None):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self,device=device)

    def load_state_dict(self, state):
        return load_state_dict(self,state)

    def named_parameters(self,recurse=True):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self,recurse=recurse)

    def named_buffers(self,recurse=True):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self,recurse=recurse)

    def cpu(self):
        return cpu(self)

    def cuda(self,device=None):
        return cuda(self,device=device)

    def to(self, *args, **kwargs):
        return to(self,*args,**kwargs)


def traverse_model(module: nn.Module, depth: int, prefix: Optional[str] = None,
                   basic_blocks: Tuple[nn.Module] = (), full: bool = False) -> Iterator[Tuple[nn.Module, str, nn.Module]]:
    '''
    iterate over model layers yielding the layer,layer_scope,encasing_module
    Parameters:
    -----------
    model:
        the model to iterate over
    depth:
        how far down in the model tree to go
    basic_blocks:
        a list of modules that if encountered will not be broken down
    full:
        whether to yield only layers specified by the depth and basick_block options or to yield all layers
    '''
    if prefix is None:
        prefix = type(module).__name__

    for name, sub_module in module.named_children():
        scope = prefix + "/" + type(sub_module).__name__ + f"[{name}]"
        if len(list(sub_module.children())) == 0 or isinstance(sub_module, tuple(basic_blocks)) or depth == 0:
            if full:
                yield sub_module, scope, module, True
            else:
                yield sub_module, scope, module
        else:
            if full:
                yield sub_module, scope, module, False
            yield from traverse_model(sub_module, depth - 1, scope, basic_blocks, full)


def layerDict(model: nn.Module, depth=1000, basic_blocks=()) -> Dict[str, nn.Module]:
    return {s: l for l, s, _ in traverse_model(model, depth, basic_blocks=basic_blocks)}


def traverse_params_buffs(module: nn.Module, prefix: Optional[str] = None) -> Iterator[Tuple[torch.tensor, str]]:
    '''
    iterate over model's buffers and parameters yielding obj,obj_scope

    Parameters:
    -----------
    model:
        the model to iterate over
    '''
    if prefix is None:
        prefix = type(module).__name__

    # params
    for param_name, param in module.named_parameters(recurse=False):
        param_scope = f"{prefix}/{type(param).__name__}[{param_name}]"
        yield param, param_scope

    # buffs
    for buffer_name, buffer in module.named_buffers(recurse=False):
        buffer_scope = f"{prefix}/{type(buffer).__name__}[{buffer_name}]"
        yield buffer, buffer_scope

    # recurse
    for name, sub_module in module.named_children():
        yield from traverse_params_buffs(sub_module, prefix + "/" + type(sub_module).__name__ + f"[{name}]")


def tensorDict(model: nn.Module) -> OrderedDict[str, Tensor]:
    return collections.OrderedDict((s, t)for t, s in traverse_params_buffs(model))


def move_tensors(ts, device):
    def move(t):
        if isinstance(t, (nn.Module, Tensor)):
            return t.to(device)
        return t

    return nested_map(move, ts)


def nested_map(func, ts,full=False):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        return func(ts)
    elif isinstance(ts, (list, tuple, set)):
        return type(ts)(nested_map(func, t,full=full) for t in ts)
    elif isinstance(ts, dict):
        return {k: nested_map(func, v,full=full) for k, v in ts.items()}
    elif isinstance(ts, slice) and full:
        start = nested_map(func, ts.start,full=full)
        stop = nested_map(func, ts.stop,full=full)
        step = nested_map(func, ts.step,full=full)
        return slice(start, stop, step)
    return func(ts)


def state_dict(partition, device=None):
    # we return the state dict of this part as it should be in the original model
    state = nn.Module.state_dict(partition)
    lookup = partition.lookup
    result = dict()
    for k, v in state.items():
        if k in lookup:
            result[lookup[k]] = v if device is None else v.to(device)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            result[new_k] = v if device is None else v.to(device)
    return result


def load_state_dict(partition, state):
    reverse_lookup = {v: k for k, v in partition.lookup.items()}
    device = partition.device
    keys = list(partition.state_dict(None).keys())
    new_state = dict()
    for k in keys:
        if k in reverse_lookup:
            new_state[reverse_lookup[k]] = state[k].to(device)
            continue
        idx = k.rfind(".")
        to_replace = k[:idx]
        if to_replace in reverse_lookup:
            key = reverse_lookup[to_replace] + k[idx:]
            new_state[key] = state[k].to(device)
    nn.Module.load_state_dict(partition, new_state, strict=True)


def named_buffers(partition, recurse=True):
    # we return the named buffers of this part as it should be in the original model
    params = nn.Module.named_buffers(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def named_parameters(partition, recurse=True):
    # we return the named parameters of this part as it should be in the original model
    params = nn.Module.named_parameters(partition, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield (lookup[k], v)
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield (new_k, v)


def cpu(partition):
    partition.device = torch.device('cpu')
    return nn.Module.cpu(partition)


def cuda(partition, device=None):
    if device is None:
        device = torch.cuda.current_device()
    partition.device = torch.device(device)
    return nn.Module.cuda(partition, partition.device)


def to(partition, *args, **kwargs):
    device = None
    if 'device' in kwargs:
        device = kwargs['device']
    elif 'tensor' in kwargs:
        device = kwargs['tensor'].device
    if args:
        if isinstance(args[0], (torch.device, int, str)):
            device = args[0]
        if torch.is_tensor(args[0]):
            device = args[0].device
    if not (device is None):
        partition.device = torch.device(device)
    return nn.Module.to(partition, *args, **kwargs)

"""analysis summary
-I- Printing Report
Number of stages: 8
cutting edges are edges between partitions
number of cutting edges: 14

backward times include recomputation
Analysis for async_pipeline=True: last partition will not do recomputation.

real times are based on real measurements of execution time of generated partitions ms
forward {0: 44.48, 1: 47.96, 2: 48.76, 3: 48.44, 4: 48.63, 5: 48.41, 6: 48.62, 7: 67.28}
backward {0: 125.11, 1: 137.75, 2: 137.85, 3: 137.52, 4: 138.6, 5: 137.93, 6: 138.28, 7: 130.62}

balance is ratio of computation time between fastest and slowest parts. (between 0 and 1 higher is better)

real balance:
forward 0.661
backward 0.903

Assuming bandwidth of 12 GBps between GPUs

communication volumes size of activations of each partition
0: input size:'0.01 MB', recieve_time:'0.00 ms', out:'13.11 MB', send time:'1.09 ms'
1: input size:'13.11 MB', recieve_time:'1.09 ms', out:'6.55 MB', send time:'0.55 ms'
2: input size:'6.55 MB', recieve_time:'0.55 ms', out:'13.11 MB', send time:'1.09 ms'
3: input size:'13.11 MB', recieve_time:'1.09 ms', out:'13.11 MB', send time:'1.09 ms'
4: input size:'13.11 MB', recieve_time:'1.09 ms', out:'13.11 MB', send time:'1.09 ms'
5: input size:'13.11 MB', recieve_time:'1.09 ms', out:'13.11 MB', send time:'1.09 ms'
6: input size:'13.11 MB', recieve_time:'1.09 ms', out:'13.11 MB', send time:'1.09 ms'
7: input size:'13.12 MB', recieve_time:'1.09 ms', out:'0.00 MB', send time:'0.00 ms'

Compuatation Communication ratio (comp/(comp+comm)):
forward {0: 0.98, 1: 0.99, 2: 0.98, 3: 0.98, 4: 0.98, 5: 0.98, 6: 0.98, 7: 1.0} 
backward {0: 1.0, 1: 0.99, 2: 1.0, 3: 0.99, 4: 0.99, 5: 0.99, 6: 0.99, 7: 0.99}

Pipeline Slowdown: (compared to sequential executation with no communication, and same recompute policy)
forward 1.361
backward 1.030

Expected utilization by partition
forward {0: 0.64, 1: 0.7, 2: 0.7, 3: 0.7, 4: 0.7, 5: 0.7, 6: 0.7, 7: 1.0}
backward {0: 0.9, 1: 0.98, 2: 0.99, 3: 0.98, 4: 0.99, 5: 0.99, 6: 0.99, 7: 0.93}

worstcase: bwd: 138.596 fwd: 67.280
expected_speedup_compared_to_seq_no_recomp_no_comm: 5.556
Expected speedup for 8 partitions is: 7.029
"""