# from pipe.configs.python_configs.configs import empty_config
from ml_collections import ConfigDict

from pipe.main import start

# Data
# from pipe.data.datasets import register_dataset_func
# def get_train_ds():
#     from torchvision.datasets import CIFAR10
#     return CIFAR10(root="", train=True, download=False)
# register_dataset_func("my_data", get_train_ds=get_train_ds)

# Q: why register dataset?
# A: dataset sharding:
# # after the model is partitioned, the data is automatically loaded exactly where needed: (labels: at the end of the pipeline).
# # this requires introducing the dataset to the pipeline, each stage holding its own dataloader,
# which is synchronized with the rest.
# This all happens automatically.
# see pipe/data/from_args_and_kw.py

# Model
# Important: use same get_model function for partitioning and runtime.
# This has several steps:
# (A) partitioning, creating autogenerated
# (B) registering autogenerated
# (C) running autogenerated.

# Start by defining a "get_model" function
# def get_model():
#     import torch
#     from torch.nn import Linear
#     model = torch.nn.Sequential(Linear(10,10),
#                                Linear(10, 1))
#     return model

# Then, create a partitioning task: simply get_model and get_inputs.
# see: autopipe/tasks

# Now, register the model.
# from pipe.models.registery.model_handler import register_model_func
# register_model_func(generated_file_name_or_path="TODO: autogen name",
#                     _get_normal_model_instance=get_model)


# Optimizer
# This is little tricky, since optimizers should slightly change to be used in (async) pipeline (since pytorch 1.5):
# current practice is copy and operate on tensor.data or alike to avoid autograd version checks..
# see AVAILABLE_OPTIMIZERS

# Trainer
# This class represent user's training code. combines:
# (1) loss calculation.
# (2) statistics.
# (3) call to optimizer.step()
# (4) other features, like gradient accumulation and gap aware penalty, and any additional features.
# -- statistics are currently handled in their own class for our PoC, to allow per-stage statistics.
# -- the trainer & statistics class would change in the future.

# Config: the config we use in the pipeline, combining all.

def get_config():
    # Get Config
    config = ConfigDict()
    config.logdir = 'logs/t5/mpipe/'
    config.data_dir = '/home_local/saareliad/data'
    config.out_dir = 'results/t5/super_glue/boolq'
    config.auto_file_name = True
    config.out_filename = 'test_vs'
    config.distributed_backend = 'mpi'
    config.model = 't5_3b_tied_lmheads_512_4_8p_bw12_async_squad1_mpipe'
    config.stage_to_device_map = [0, 1, 2, 3, 4, 5, 6, 7, 6, 1, 4, 5, 3, 2, 0]
    config.nprocs = 15
    config.dataset = 't5_tfds'
    config.mixture_or_task_name = 'super_glue_boolq_v102'
    config.preproc_batch_size = 128
    config.trainer = ConfigDict()
    config.trainer.type = 't5'
    config.trainer.args = ConfigDict()
    config.trainer.args.loss_multiplier = 2.59
    config.statistics = 'squad_loss_per_batch'
    config.step_every = 5
    config.bs_train = 4
    config.bs_test = 4
    config.max_seq_length = 512
    config.answer_max_seq_length = 4
    config.num_data_workers = 5
    config.optimizer = ConfigDict()
    config.optimizer.type = 'adafactor'
    config.optimizer.args = ConfigDict()
    config.optimizer.args.lr = 0.001
    config.optimizer.args.weight_decay = 0
    config.optimizer.args.scale_parameter = True
    config.optimizer.args.relative_step = False
    config.lr_scheduler = ConfigDict()
    config.lr_scheduler.type = 'get_constant_schedule_with_warmup'
    config.lr_scheduler.preproc_args = ConfigDict()
    config.lr_scheduler.args = ConfigDict()
    config.lr_scheduler.args.num_warmup_steps = 200
    config.lr_scheduler.args.last_epoch = -1
    config.epochs = -1
    config.steps = 3200
    config.seed_from_cmd = False
    config.seed = 42
    config.bs_train_from_cmd = False
    config.bs_test_from_cmd = False
    config.num_chunks = 1
    config.verbose_comm = False
    config.flush_rate = -1
    # config.work_scheduler = 'virtual_stages_1f1b'
    # config.supremum_staleness = 100
    config.cudnn_benchmark = True
    config.max_buffers = 1
    config.keep_buffers_alive = False
    config.train_batches_limit = -1
    config.test_batches_limit = 0
    config.log_frequency = 200
    config.model_name_or_path = 't5-3b'
    config.do_lower_case = True
    config.overwrite_cache = False
    config.dont_drop_last = True
    config.model_type = 't5'
    config.precomputed_masks = True
    config.save_checkpoints = True
    config.load_model_one_by_one = False

    # stale, async pipeline
    # config.base_config_path = 'common.json'
    # config.base_config_path_is_relative = True
    config.weight_stashing = False
    config.work_scheduler = '1f1b'
    config.checkpoints_save_name_prefix = 'stale_adafactor'
    config.checkpoints_save_dir = '/nfs_Disk2/mpipe/checkpoints/t5/3b/boolq/stale/'

    config = config.to_dict()
    return config


if __name__ == '__main__':
    config = get_config()
    start(python_args_dict=config)
