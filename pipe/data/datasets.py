import abc
from typing import Type

import torch
from torch.utils.data import DistributedSampler

AVAILABLE_DATASETS = {
    #    e.g 'cifar10', 'cifar100', 'imagenet', 'wt2', 'squad1', 'squad2', 'glue', "t5_squad"
}
HARDCODED_JUST_XY = set()  # HACK: used to hardcode this.


class CommonDatasetHandler(abc.ABC):
    def __init__(self):
        pass

    def get_train_ds(self, *args, **kw):
        raise NotImplementedError()

    def get_test_ds(self, *args, **kw):
        NotImplementedError()

    def get_validation_ds(self, *args, **kw):
        NotImplementedError()

    def get_modify_trainer_fn(self):
        pass

    def modify_dataloader_keywords(self, dataloader_keywords):
        return dataloader_keywords


def register_dataset(name, common_handler: Type[CommonDatasetHandler]):
    AVAILABLE_DATASETS[name] = common_handler


def register_hardcoded_just_xy_dataset(name):
    HARDCODED_JUST_XY.add(name)


def register_dataset_func(name,
                     get_train_ds,
                     get_test_ds=None,
                     get_validation_ds=None,
                     get_modify_trainer_fn=None,
                     modify_dataloader_keywords=None):
    if get_test_ds is None and get_train_ds is None and get_validation_ds is None:
        ValueError("must provide at least 1 none none dataset function")

    d = dict(get_train_ds=get_train_ds,
             get_test_ds=get_test_ds,
             get_validation_ds=get_validation_ds,
             get_modify_trainer_fn=get_modify_trainer_fn,
             modify_dataloader_keywords=modify_dataloader_keywords
             )
    for k, v in list(d.items()):
        if v is None:
            d.pop(k)

    common_handler = type("AutoGeneratedDatasetHandler",
                          (CommonDatasetHandler,),
                          d
                          )

    return register_dataset(name=name, common_handler=common_handler)


##################################
# A modified DistributedSampler
##################################


class MyNewDistributedSampler(DistributedSampler):
    # Better use this class, as it was tested by pytorch.
    # only problem with it is *deterministic shuffling*, which will be the same for all experiments.
    # so we add experiment seed to make it fun.

    MAX_INT = 2 ** 32  # Used to prevent overflow

    def __init__(self, experiment_manual_seed, *args, **kw):
        super().__init__(*args, **kw)
        self.experiment_manual_seed = experiment_manual_seed

    def __iter__(self):
        # deterministically shuffle based on epoch
        g = torch.Generator()
        # My only change
        g.manual_seed(
            ((1 + self.epoch) * self.experiment_manual_seed) % self.MAX_INT)
        if self.shuffle:
            indices = torch.randperm(len(self.dataset), generator=g).tolist()
        else:
            if self.num_replicas == 1 and self.rank == 0:
                return iter(range(len(self.dataset)))

            indices = list(range(len(self.dataset)))

        # add extra samples to make it evenly divisible
        indices += indices[:(self.total_size - len(indices))]
        assert len(indices) == self.total_size

        # subsample
        indices = indices[self.rank:self.total_size:self.num_replicas]
        assert len(indices) == self.num_samples

        return iter(indices)
